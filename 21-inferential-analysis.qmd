```{r inferential-setup, cache = FALSE, include = FALSE}
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
library(poissonreg)
library(infer)
tidymodels_prefer()
theme_set(theme_bw())

data("bioChemists", package = "pscl")
```

# Análisis Inferencial {#sec-inferential}

::: rmdnote
En @sec-model-types, describimos una taxonomía de modelos y dijimos que la mayoría de los modelos pueden clasificarse como descriptivos, inferenciales y/o predictivos.
:::

La mayoría de los capítulos de este libro se han centrado en los modelos desde la perspectiva de la precisión de los valores predichos, una cualidad importante de los modelos para todos los propósitos, pero más relevante para los modelos predictivos. Los modelos inferenciales generalmente se crean no solo para sus predicciones, sino también para hacer inferencias o juicios sobre algún componente del modelo, como un valor de coeficiente u otro parámetro. Estos resultados se utilizan a menudo para responder algunas (con suerte) preguntas o hipótesis predefinidas. En los modelos predictivos, las predicciones sobre datos reservados se utilizan para validar o caracterizar la calidad del modelo. Los métodos inferenciales se centran en validar los supuestos probabilísticos o estructurales que se hacen antes de ajustar el modelo.

Por ejemplo, en la regresión lineal ordinaria, la suposición común es que los valores residuales son independientes y siguen una distribución gaussiana con una varianza constante. Si bien es posible que tenga conocimientos científicos o de dominio para dar credibilidad a este supuesto para el análisis de su modelo, los residuos del modelo ajustado generalmente se examinan para determinar si el supuesto fue una buena idea. Como resultado, los métodos para determinar si se han cumplido los supuestos del modelo no son tan simples como observar las predicciones de reserva, aunque eso también puede ser muy útil.

Usaremos valores p en este capítulo. Sin embargo, el marco de tidymodels tiende a promover intervalos de confianza sobre los valores p como método para cuantificar la evidencia de una hipótesis alternativa. Como se mostró anteriormente en @sec-tidyposterior, los métodos bayesianos suelen ser superiores tanto a los valores p como a los intervalos de confianza en términos de facilidad de interpretación (pero pueden ser más costosos desde el punto de vista computacional).

::: rmdwarning
En los últimos años ha habido un impulso para alejarse de los valores p en favor de otros métodos [@pvalue]. Consulte el Volumen 73 de [*The American Statistician*](https://www.tandfonline.com/toc/utas20/73/) para obtener más información y discusión.
:::

En este capítulo, describimos cómo usar `r pkg(tidymodels)` para ajustar y evaluar modelos inferenciales. En algunos casos, el marco tidymodels puede ayudar a los usuarios a trabajar con los objetos producidos por sus modelos. En otros, puede ayudar a evaluar la calidad de un modelo determinado.

## Inferencia Para Datos De Recuento

Para comprender cómo se pueden utilizar los paquetes tidymodels para el modelado inferencial, centrémonos en un ejemplo con datos de recuento. Usaremos datos de publicaciones de bioquímica del paquete `r pkg(pscl)`. Estos datos consisten en información sobre 915 Ph.D. se gradúan en bioquímica e intenta explicar los factores que impactan su productividad académica (medida a través del número o recuento de artículos publicados en tres años). Los predictores incluyen el género del graduado, su estado civil, el número de hijos del graduado que tengan al menos cinco años, el prestigio de su departamento y el número de artículos producidos por su mentor en el mismo período de tiempo. Los datos reflejan doctorados en bioquímica que terminaron su educación entre 1956 y 1963. Los datos son una muestra algo sesgada de todos los doctorados en bioquímica otorgados durante este período (basado en la integridad de la información).

::: rmdnote
Recuerde que en el [Capítulo @sec-trust] hicimos la pregunta "¿Es nuestro modelo aplicable para predecir un punto de datos específico?" Es muy importante definir a qué poblaciones se aplica un análisis inferencial. Para estos datos, los resultados probablemente se aplicarían a los doctorados en bioquímica dados aproximadamente en el período en que se recopilaron los datos. ¿Se aplica también a otros tipos de doctorado en química (por ejemplo, química medicinal, etc.)? Éstas son preguntas importantes que se deben abordar (y documentar) al realizar análisis inferenciales.
:::

Un gráfico de los datos mostrados en @fig-counts indica que muchos graduados no publicaron ningún artículo durante este tiempo y que el resultado sigue una distribución sesgada a la derecha:

```{r inferential-count-dist, eval=FALSE}
library(tidymodels)
tidymodels_prefer()

data("bioChemists", package = "pscl")

ggplot(bioChemists, aes(x = art)) + 
  geom_histogram(binwidth = 1, color = "white") + 
  labs(x = "Número de artículos dentro de los 3 años posteriores a la graduación")
```

```{r}
#| label: fig-counts
#| ref.label: "inferential-count-dist"
#| echo: FALSE
#| out.width: "80%"
#| fig.cap: "Distribución del número de artículos escritos dentro de los 3 años posteriores a la graduación."
#| fig.alt: "La distribución del número de artículos escritos dentro de los 3 años posteriores a la graduación. La distribución está sesgada a la derecha y la mayoría de los datos tienen recuentos de cero o uno."
```

Dado que los datos de los resultados son recuentos, la suposición de distribución más común es que el resultado tiene una distribución de Poisson. En este capítulo se utilizarán estos datos para varios tipos de análisis.

## Comparaciones Con Pruebas De Dos Muestras

Podemos comenzar con la prueba de hipótesis. El objetivo del autor original con este conjunto de datos sobre publicaciones de bioquímica era determinar si existe una diferencia en las publicaciones entre hombres y mujeres [@Long1992]. Los datos del estudio muestran:

```{r inferential-counts}
bioChemists %>% 
  group_by(fem) %>% 
  summarize(counts = sum(art), n = length(art))
```

Había muchas más publicaciones de hombres, aunque también había más hombres en los datos. El enfoque más simple para analizar estos datos sería hacer una comparación de dos muestras usando la función `poisson.test()` en el paquete `r pkg(stats)`. Requiere los conteos para uno o dos grupos.

Para nuestra aplicación, las hipótesis para comparar los dos sexos son:

```{=tex}
\begin{align}
H_0&: \lambda_m = \lambda_f \notag \\
H_a&: \lambda_m \ne \lambda_f \notag
\end{align}
```
donde los valores $\lambda$ son las tasas de publicaciones (durante el mismo período de tiempo).

Una aplicación básica de la prueba es:Una aplicación básica de la prueba es:[^21-inferential-analysis-1]

[^21-inferential-analysis-1]: El argumento `T` nos permite dar cuenta del tiempo en que se contaron los eventos (publicaciones), que fue de tres años tanto para hombres como para mujeres. Hay más hombres que mujeres en estos datos, pero `poisson.test()` tiene una funcionalidad limitada, por lo que se pueden utilizar análisis más sofisticados para explicar esta diferencia.

```{r inferential-test-basic}
poisson.test(c(930, 619), T = 3)
```

La función informa un valor p así como un intervalo de confianza para la relación de las tasas de publicación. Los resultados indican que la diferencia observada es mayor que el ruido experiencial y favorece a $H_a$.

Un problema con el uso de esta función es que los resultados regresan como un objeto "htest". Si bien este tipo de objeto tiene una estructura bien definida, puede resultar difícil consumirlo para operaciones posteriores, como informes o visualizaciones. La herramienta más impactante que ofrece tidymodels para modelos inferenciales son las funciones `tidy()` en el paquete `r pkg(broom)`. Como se vio anteriormente, esta función crea un tibble bien formado y con un nombre predecible a partir del objeto. Podemos `tidy()` los resultados de nuestra prueba de comparación de dos muestras:

```{r inferential-test-tidy}
poisson.test(c(930, 619)) %>% 
  tidy()
```

::: rmdnote
Entre [`r pkg(broom)`](https://broom.tidymodels.org/) y [`r pkg(broom.mixed)`](https://CRAN.R-project.org/package=%20broom.mixed), existen métodos `tidy()` para más de 150 modelos.
:::

Si bien la distribución de Poisson es razonable, es posible que también deseemos evaluarla utilizando menos supuestos distributivos. Dos métodos que podrían resultar útiles son las pruebas de arranque y de permutación [@davison1997bootstrap].

El paquete `r pkg(infer)`, parte del marco tidymodels, es una herramienta poderosa e intuitiva para probar hipótesis [@ModernDive]. Su sintaxis es concisa y está diseñada para no estadísticos.

Primero, `specify()` que usaremos la diferencia en el número medio de artículos entre los sexos y luego `calculate()` la estadística a partir de los datos. Recuerde que el estimador de máxima verosimilitud para la media de Poisson es la media muestral. Las hipótesis probadas aquí son las mismas que las de la prueba anterior (pero se llevan a cabo mediante un procedimiento de prueba diferente).

Con `r pkg(infer)`, especificamos el resultado y la covariable, luego indicamos la estadística de interés:

```{r inferential-mean-diff-obs}
library(infer)

observed <- 
  bioChemists %>%
  specify(art ~ fem) %>%
  calculate(stat = "diff in means", order = c("Men", "Women"))
observed
```

A partir de aquí, calculamos un intervalo de confianza para esta media creando la distribución de arranque mediante `generate()`; se calcula la misma estadística para cada versión remuestreada de los datos:

```{r inferential-mean-diff-boot-gen}
set.seed(2101)
bootstrapped <- 
  bioChemists %>%
  specify(art ~ fem)  %>%
  generate(reps = 2000, type = "bootstrap") %>%
  calculate(stat = "diff in means", order = c("Men", "Women"))
bootstrapped
```

Un intervalo percentil se calcula usando:

```{r inferential-mean-diff-boot-ci}
percentile_ci <- get_ci(bootstrapped)
percentile_ci
```

El paquete `r pkg(infer)` tiene una API de alto nivel para mostrar los resultados del análisis, como se muestra en @fig-bootstrapped-mean.

```{r inferential-mean-diff-boot, eval = FALSE}
visualize(bootstrapped) +
    shade_confidence_interval(endpoints = percentile_ci)
```

```{r}
#| label: fig-bootstrapped-mean
#| ref.label: "inferential-mean-diff-boot"
#| echo: FALSE
#| out.width: "80%"
#| fig.cap: "La distribución bootstrap de la diferencia de medias. La región resaltada es el intervalo de confianza."
#| fig.alt: "La distribución bootstrap de la diferencia de medias. La región resaltada es el intervalo de confianza, que no incluye un valor de cero."
```

Dado que el intervalo visualizado en @fig-bootstrapped-mean no incluye cero, estos resultados indican que los hombres han publicado más artículos que las mujeres.

Si requerimos un valor p, el paquete `r pkg(infer)` puede calcular el valor mediante una prueba de permutación, que se muestra en el siguiente código. La sintaxis es muy similar al código de arranque que usamos anteriormente. Agregamos un verbo `hypothesize()` para indicar el tipo de suposición a probar y la llamada `generate()` contiene una opción para mezclar los datos.

```{r inferential-mean-diff-perm-gen}
set.seed(2102)
permuted <- 
  bioChemists %>%
  specify(art ~ fem)  %>%
  hypothesize(null = "independence") %>%
  generate(reps = 2000, type = "permute") %>%
  calculate(stat = "diff in means", order = c("Men", "Women"))
permuted
```

El siguiente código de visualización también es muy similar al enfoque de arranque. Este código genera @fig-permutation-dist donde la línea vertical indica el valor observado:

```{r inferential-mean-diff-perm, eval = FALSE}
visualize(permuted) +
    shade_p_value(obs_stat = observed, direction = "two-sided")
```

```{r}
#| label: fig-permutation-dist
#| ref.label: "inferential-mean-diff-perm"
#| echo: FALSE
#| out.width: "80%"
#| fig.cap: "Distribución empírica del estadístico de prueba bajo la hipótesis nula. La línea vertical indica la estadística de prueba observada."
#| fig.alt: "La distribución empírica del estadístico de prueba bajo la hipótesis nula. La línea vertical indica la estadística de prueba observada y está muy alejada de la corriente principal de la distribución."
```

The actual p-value is:

```{r inferential-mean-diff-perm-pvalue}
permuted %>%
  get_p_value(obs_stat = observed, direction = "two-sided")
```

La línea vertical que representa la hipótesis nula en @fig-permutation-dist está muy lejos de la distribución de permutación. Esto significa que, si de hecho la hipótesis nula fuera cierta, la probabilidad de observar datos al menos tan extremos como los que tenemos a mano es extremadamente pequeña.

Las pruebas de dos muestras que se muestran en esta sección probablemente no sean óptimas porque no tienen en cuenta otros factores que podrían explicar la relación observada entre la tasa de publicación y el sexo. Pasemos a un modelo más complejo que pueda considerar covariables adicionales.

## Modelos log-lineales

El resto de este capítulo se centrará en un modelo lineal generalizado [@Dobson99] donde asumimos que los recuentos siguen una distribución de Poisson. Para este modelo, las covariables/predictores ingresan al modelo de forma log-lineal:

$$
\log(\lambda) = \beta_0 + \beta_1x_1 + \ldots + \beta_px_p
$$

donde $\lambda$ es el valor esperado de los recuentos.

Ajustemos un modelo simple que contenga todas las columnas predictoras. El paquete `r pkg(poissonreg)`, un paquete de extensión `r pkg(parsnip)` en tidymodels, se ajustará a esta especificación de modelo:

```{r inferential-glm}
library(poissonreg)

# El motor predeterminado es 'glm'
log_lin_spec <- poisson_reg()

log_lin_fit <- 
  log_lin_spec %>% 
  fit(art ~ ., data = bioChemists)
log_lin_fit
```

El método `tidy()` resume sucintamente los coeficientes del modelo (junto con intervalos de confianza del 90%):

```{r inferential-glm-tidy}
tidy(log_lin_fit, conf.int = TRUE, conf.level = 0.90)
```

En este resultado, los valores p corresponden a pruebas de hipótesis separadas para cada parámetro:

```{=tex}
\begin{align}
H_0&: \beta_j = 0 \notag \\
H_a&: \beta_j \ne 0 \notag
\end{align}
```
para cada uno de los parámetros del modelo. Al observar estos resultados, es posible que el `phd` (el prestigio de su departamento) no tenga ninguna relación con el resultado.

Si bien la distribución de Poisson es el supuesto habitual para datos como estos, puede resultar beneficioso realizar una verificación aproximada de los supuestos del modelo ajustando los modelos sin utilizar la probabilidad de Poisson para calcular los intervalos de confianza. El paquete `r pkg(rsample)` tiene una función conveniente para calcular intervalos de confianza de arranque para los modelos `lm()` y `glm()`. Podemos usar esta función, mientras declaramos explícitamente `family = poisson`, para calcular una gran cantidad de ajustes del modelo. De forma predeterminada, calculamos un intervalo bootstrap-t de confianza del 90% (los intervalos percentiles también están disponibles):

```{r inferential-glm-ci}
set.seed(2103)
glm_boot <- 
  reg_intervals(art ~ ., data = bioChemists, model_fn = "glm", family = poisson)
glm_boot
```

::: rmdwarning
Cuando comparamos estos resultados (en @fig-glm-intervals) con los resultados puramente paramétricos de `glm()`, los intervalos de arranque son algo más amplios. Si los datos fueran verdaderamente de Poisson, estos intervalos tendrían anchos más similares.
:::

```{r}
#| label: fig-glm-intervals
#| echo: FALSE
#| fig.cap: "Dos tipos de intervalos de confianza para el modelo de regresión de Poisson"
#| fig.alt: "Dos tipos de intervalos de confianza para el modelo de regresión de Poisson. el intervalo para el modelo PhD es el único intervalo que se superpone a cero. Los intervalos paramétricos tienden a ser más amplios que los intervalos de arranque."

glm_boot |> 
  select(term, method = .method, .estimate, .lower, .upper) |> 
  bind_rows(
    tidy(log_lin_fit, conf.int = TRUE, conf.level = 0.90) |> 
      filter(term != "(Intercept)") |> 
      mutate(method = "parametric") |> 
      select(term, method, .estimate = estimate, .lower = conf.low, .upper = conf.high)
  ) |>
  ggplot(aes(x = .estimate, y = term, color = method, pch = method)) +
  geom_vline(xintercept = 0, lty = 3) +
  geom_point(size = 2.5, position = position_dodge(width = 1 / 2)) +
  geom_errorbar(aes(xmin = .lower, xmax = .upper),
                width = 1 / 4,
                position = position_dodge(width = 1 / 2))  +
  labs(x = "coeficientes GLM", y = NULL, color = NULL, pch = NULL) + 
  scale_color_brewer(palette = "Paired")
```

Determinar qué predictores incluir en el modelo es un problema difícil. Un enfoque consiste en realizar pruebas de índice de verosimilitud (LRT) [@McCullaghNelder89] entre modelos anidados. Según los intervalos de confianza, tenemos evidencia de que un modelo más simple sin `phd` puede ser suficiente. Ajustemos un modelo más pequeño y luego realicemos una prueba estadística:

```{=tex}
\begin{align}
H_0&: \beta_{phd} = 0 \notag \\
H_a&: \beta_{phd} \ne 0 \notag
\end{align}
```
Esta hipótesis se probó previamente cuando mostramos los resultados ordenados de `log_lin_fit`. Ese enfoque particular utilizó resultados de un ajuste de modelo único mediante una estadística de Wald (es decir, el parámetro dividido por su error estándar). Para ese enfoque, el valor p era `r tidy(log_lin_fit) |> filter(term == "phd") |> pluck("p.value") |> format.pval()`. Podemos ordenar los resultados del LRT para obtener el valor p:

```{r inferential-reduced, warning = FALSE}
log_lin_reduced <- 
  log_lin_spec %>% 
  fit(art ~ ment + kid5 + fem + mar, data = bioChemists)

anova(
  extract_fit_engine(log_lin_reduced),
  extract_fit_engine(log_lin_fit),
  test = "LRT"
) %>%
  tidy()
```

Los resultados son los mismos y, en base a estos y al intervalo de confianza para este parámetro, excluiremos `phd` de análisis adicionales ya que no parece estar asociado con el resultado.

## Un Modelo Más Complejo

Podemos pasar a modelos aún más complejos dentro de nuestro enfoque de tidymodels. Para los datos de recuento, hay ocasiones en las que el número de recuentos de ceros es mayor de lo que prescribiría una distribución de Poisson simple. Un modelo más complejo apropiado para esta situación es el modelo de Poisson (ZIP) con inflación cero; consulte @Mullahy, @Lambert1992 y @JSSv027i08. Aquí, hay dos conjuntos de covariables: uno para los datos de recuento y otros que afectan la probabilidad (indicada como $\pi$) de ceros. La ecuación para la media $\lambda$ es:

$$\lambda = 0 \pi + (1 - \pi) \lambda_{nz}$$

donde

```{=tex}
\begin{align}
\log(\lambda_{nz}) &= \beta_0 + \beta_1x_1 + \ldots + \beta_px_p \notag \\
\log\left(\frac{\pi}{1-\pi}\right) &= \gamma_0 + \gamma_1z_1 + \ldots + \gamma_qz_q \notag 
\end{align}
```
y las covariables $x$ afectan los valores de recuento, mientras que las covariables $z$ influyen en la probabilidad de un cero. No es necesario que los dos conjuntos de predictores sean mutuamente excluyentes.

Ajustaremos un modelo con un conjunto completo de covariables $z$:

```{r inference-zip-model}
zero_inflated_spec <- poisson_reg() %>% set_engine("zeroinfl")

zero_inflated_fit <- 
  zero_inflated_spec %>% 
  fit(art ~ fem + mar + kid5 + ment | fem + mar + kid5 + phd + ment,
      data = bioChemists)

zero_inflated_fit
```

Dado que los coeficientes de este modelo también se estiman utilizando la máxima verosimilitud, intentemos utilizar otra prueba de razón de verosimilitud para comprender si los términos del nuevo modelo son útiles. *Simultáneamente* probaremos que:

```{=tex}
\begin{align}
H_0&: \gamma_1 = 0, \gamma_2 = 0, \cdots, \gamma_5 = 0 \notag \\
H_a&: \text{at least one } \gamma \ne 0  \notag
\end{align}
```
Probemos ANOVA nuevamente:

```{r inference-zip-anova, error = TRUE}
anova(
  extract_fit_engine(zero_inflated_fit),
  extract_fit_engine(log_lin_reduced),
  test = "LRT"
) %>%
  tidy()
```

¡No se implementa un método `anova()` para objetos `zeroinfl`!

Una alternativa es utilizar una *estadística de criterio de información*, como el criterio de información de Akaike (AIC) [@claeskens2016statistical]. Esto calcula la probabilidad logarítmica (del conjunto de entrenamiento) y penaliza ese valor según el tamaño del conjunto de entrenamiento y la cantidad de parámetros del modelo. En la parametrización de R, los valores AIC más pequeños son mejores. En este caso, no estamos realizando una prueba estadística formal sino *estimando* la capacidad de los datos para ajustarse a los datos.

Los resultados indican que el modelo ZIP es preferible:

```{r inference-zip-aic}
zero_inflated_fit %>% extract_fit_engine() %>% AIC()
log_lin_reduced   %>% extract_fit_engine() %>% AIC()
```

Sin embargo, es difícil contextualizar este par de valores individuales y evaluar *cuán* diferentes son en realidad. Para resolver este problema, volveremos a muestrear una gran cantidad de cada uno de estos dos modelos. A partir de estos, podemos calcular los valores AIC para cada uno y determinar con qué frecuencia los resultados favorecen el modelo ZIP. Básicamente, caracterizaremos la incertidumbre de las estadísticas del AIC para medir su diferencia en relación con el ruido en los datos.

También calcularemos más intervalos de confianza de arranque para los parámetros en un momento, por lo que especificamos la opción `apparent = TRUE` al crear las muestras de arranque. Esto es necesario para algunos tipos de intervalos.

Primero, creamos los 4000 ajustes del modelo:

```{r inference-zip-comp, cache=FALSE}
zip_form <- art ~ fem + mar + kid5 + ment | fem + mar + kid5 + phd + ment
glm_form <- art ~ fem + mar + kid5 + ment

set.seed(2104)
bootstrap_models <-
  bootstraps(bioChemists, times = 2000, apparent = TRUE) %>%
  mutate(
    glm = map(splits, ~ fit(log_lin_spec,       glm_form, data = analysis(.x))),
    zip = map(splits, ~ fit(zero_inflated_spec, zip_form, data = analysis(.x)))
  )
bootstrap_models
```

Ahora podemos extraer los ajustes del modelo y sus correspondientes valores AIC:

```{r inference-zip-aic-resampled, cache=FALSE}
bootstrap_models <-
  bootstrap_models %>%
  mutate(
    glm_aic = map_dbl(glm, ~ extract_fit_engine(.x) %>% AIC()),
    zip_aic = map_dbl(zip, ~ extract_fit_engine(.x) %>% AIC())
  )
mean(bootstrap_models$zip_aic < bootstrap_models$glm_aic)
```

A partir de estos resultados, parece definitivo que tener en cuenta el número excesivo de conteos de cero es una buena idea.

::: rmdnote
Podríamos haber usado `fit_resamples()` o un conjunto de flujo de trabajo para realizar estos cálculos. En esta sección, usamos `mutate()` y `map()` para calcular los modelos y demostrar cómo se pueden usar las herramientas tidymodels para modelos que no son compatibles con uno de los paquetes `r pkg(parsnip)`.
:::

Dado que hemos calculado los ajustes del modelo remuestreado, creemos intervalos de arranque para los coeficientes del modelo de probabilidad cero (es decir, $\gamma_j$). Podemos extraerlos con el método `tidy()` y usar la opción `type = "zero"` para obtener estas estimaciones:

```{r inference-zip-coefs, cache=FALSE}
bootstrap_models <-
  bootstrap_models %>%
  mutate(zero_coefs  = map(zip, ~ tidy(.x, type = "zero")))

# One example:
bootstrap_models$zero_coefs[[1]]
```

Es una buena idea visualizar las distribuciones de arranque de los coeficientes, como en @fig-zip-bootstrap.

```{r inference-zip-bootstrap, eval=FALSE}
bootstrap_models %>% 
  unnest(zero_coefs) %>% 
  ggplot(aes(x = estimate)) +
  geom_histogram(bins = 25, color = "white") + 
  facet_wrap(~ term, scales = "free_x") + 
  geom_vline(xintercept = 0, lty = 2, color = "gray70")
```

```{r}
#| label: fig-zip-bootstrap
#| ref.label: "inference-zip-bootstrap"
#| echo: FALSE
#| fig.cap: "Distribuciones bootstrap de los coeficientes del modelo ZIP. Las líneas verticales indican las estimaciones observadas."
#| fig.alt: "Distribuciones bootstrap de los coeficientes del modelo ZIP. Las líneas verticales indican las estimaciones observadas. El predictor de mención que parece ser importante para el modelo."
```

Una de las covariables (`ment`) que parece ser importante tiene una distribución muy sesgada. El espacio extra en algunas de las facetas indica que hay algunos valores atípicos en las estimaciones. Esto *podría* ocurrir cuando los modelos no convergieran; esos resultados probablemente deberían excluirse de las nuevas muestras. Para los resultados visualizados en @fig-zip-bootstrap, los valores atípicos se deben únicamente a estimaciones extremas de parámetros; todos los modelos convergieron.

El paquete `r pkg(rsample)` contiene un conjunto de funciones denominadas `int_*()` que calculan diferentes tipos de intervalos de arranque. Dado que el método `tidy()` contiene estimaciones de error estándar, se pueden calcular los intervalos bootstrap-t. También calcularemos los intervalos percentiles estándar. De forma predeterminada, se calculan intervalos de confianza del 90%.

```{r inference-zip-intervals}
bootstrap_models %>% int_pctl(zero_coefs)
bootstrap_models %>% int_t(zero_coefs)
```

A partir de estos resultados, podemos tener una buena idea de qué predictores incluir en el modelo de probabilidad de conteo cero. Puede ser sensato reajustar un modelo más pequeño para evaluar si la distribución de arranque para "ment" todavía está sesgada.

## Más Análisis Inferencial {#sec-inference-options}

Este capítulo demostró solo un pequeño subconjunto de lo que está disponible para el análisis inferencial en modelos tidy y se ha centrado en los métodos frecuentistas y de remuestreo. Podría decirse que el análisis bayesiano es un enfoque de inferencia muy eficaz y, a menudo, superior. Hay una variedad de modelos bayesianos disponibles a través de `r pkg(parsnip)`. Además, el paquete `r pkg(multilevelmod)` permite a los usuarios ajustarse a modelos jerárquicos bayesianos y no bayesianos (por ejemplo, modelos mixtos). Los paquetes `r pkg(broom.mixed)` y `r pkg(tidybayes)` son excelentes herramientas para extraer datos para gráficos y resúmenes. Finalmente, para conjuntos de datos con una única jerarquía, como datos de medidas longitudinales o repetidas simples, la función `group_vfold_cv()` de `r pkg(rsample)` facilita caracterizaciones directas fuera de la muestra del rendimiento del modelo.

## Resumen Del Capítulo {#sec-inference-summary}

El marco tidymodels sirve para algo más que el modelado predictivo. Los paquetes y funciones de tidymodels se pueden utilizar para probar hipótesis, así como para ajustar y evaluar modelos inferenciales. El marco tidymodels brinda soporte para trabajar con modelos R que no son tidymodels y puede ayudar a evaluar las cualidades estadísticas de sus modelos.
