```{r pre-proc-table-setup, include = FALSE}
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
library(cli)
library(kableExtra)

tk <- symbol$tick
x  <- symbol$times
cl <- symbol$circle_dotted
```

# Preprocesamiento Recomendado {#sec-pre-proc-table}

El tipo de preprocesamiento necesario depende del tipo de modelo que se ajuste. Por ejemplo, los modelos que utilizan funciones de distancia o productos escalares deben tener todos sus predictores en la misma escala para que la distancia se mida adecuadamente.

Para obtener más información sobre cada uno de estos modelos y otros que podrían estar disponibles, consulte <https://www.tidymodels.org/find/parsnip/>.

Este Apéndice proporciona recomendaciones para los niveles básicos de preprocesamiento que se necesitan para diversas funciones del modelo. En @tbl-preprocessing, los métodos de preprocesamiento se clasifican como:

-   **dummy**: ¿Los predictores cualitativos requieren una codificación numérica (por ejemplo, mediante variables ficticias u otros métodos)?

-   **zv**: ¿Deberían eliminarse las columnas con un único valor único?

-   **impute**: Si faltan algunos predictores, ¿deberían estimarse mediante imputación?

-   **decorrelate**: Si existen predictores correlacionados, ¿debería mitigarse esta correlación? Esto podría significar filtrar los predictores, utilizar el análisis de componentes principales o una técnica basada en modelos (por ejemplo, regularización).

-   **normalize**: ¿Deben centrarse y escalarse los predictores?

-   **transform**: ¿Es útil transformar los predictores para que sean más simétricos?

La información en @tbl-preprocessing no es exhaustiva y depende en cierta medida de la implementación. Por ejemplo, como se indica debajo de la tabla, es posible que algunos modelos no requieran una operación de preprocesamiento particular, pero la implementación puede requerirla. En la tabla, `r tk` indica que el método es necesario para el modelo y `r x` indica que no. El símbolo `r cl` significa que la técnica *puede* ayudar al modelo, pero no es necesario.

```{r}
#| label: tbl-preprocessing
#| echo: FALSE
#| results: "asis"
#| tbl-cap: "Métodos de preprocesamiento para diferentes modelos."

tkp <- paste0(symbol$tick, symbol$sup_2)
cl1 <- paste0(symbol$circle_dotted, symbol$sup_1)
xp  <- paste0(symbol$times, symbol$sup_2)

tab <- 
  tribble(
    ~ model,            ~ dummy,   ~ zv, ~ impute, ~ decorrelate, ~ normalize, ~ transform, 
    "discrim_flexible()",    tk,      x,       tk,            tk,           x,          cl,
    "discrim_linear()",      tk,     tk,       tk,            tk,           x,          cl,
    "discrim_regularized()", tk,     tk,       tk,            tk,           x,          cl,
    "naive_Bayes()",          x,     tk,       tk,           cl1,           x,           x,
    "C5_rules()",             x,      x,        x,             x,           x,           x,
    "cubist_rules()",         x,      x,        x,             x,           x,           x,
    "rule_fit()",            tk,      x,       tk,           cl1,          tk,           x,
    "bag_mars()",            tk,      x,       tk,            cl,           x,          cl,
    "bag_tree()",             x,      x,        x,           cl1,           x,           x,
    "pls()",                 tk,     tk,       tk,             x,          tk,          tk,
    "poisson_reg()",         tk,     tk,       tk,            tk,           x,          cl,
    "linear_reg()",          tk,     tk,       tk,            tk,           x,          cl,
    "mars()",                tk,      x,       tk,            cl,           x,          cl,
    "logistic_reg()",        tk,     tk,       tk,            tk,           x,          cl,
    "multinom_reg()",        tk,     tk,       tk,            tk,          xp,          cl,
    "decision_tree()",        x,      x,        x,           cl1,           x,           x,
    "rand_forest()",          x,     cl,      tkp,           cl1,           x,           x,
    "boost_tree()",          xp,     cl,      tkp,           cl1,           x,           x,
    "mlp()",                 tk,     tk,       tk,            tk,          tk,          tk,
    "svm_*()",               tk,     tk,       tk,            tk,          tk,          tk,
    "nearest_neighbor()",    tk,     tk,       tk,            cl,          tk,          tk,
    "gen_additive_mod()",    tk,     tk,       tk,            tk,           x,          cl,
    "bart()",                 x,      x,        x,           cl1,           x,           x
  ) 

tab |> 
  arrange(model) |> 
  mutate(model = paste0("<tt>", model, "</tt>")) |> 
  kable(
    'html',
    escape = FALSE,
    align = c("l", rep("c", ncol(tab) - 1))
  ) |> 
  kable_styling(full_width = FALSE)
```

Notas a pie de página:

1.  Es posible que la descorrelación de predictores no ayude a mejorar el rendimiento. Sin embargo, menos predictores correlacionados pueden mejorar la estimación de las puntuaciones de importancia de la varianza (ver [Fig. 11.4](https://bookdown.org/max/FES/recursive-feature-elimination.html#fig:greedy-rf-imp) de @fes). Básicamente, la selección de predictores altamente correlacionados es casi aleatoria.
2.  El preprocesamiento necesario para estos modelos depende de la implementación. Específicamente:

-   *Teóricamente*, cualquier modelo basado en árboles no requiere imputación. Sin embargo, muchas implementaciones de conjuntos de árboles requieren imputación.
-   Si bien los métodos de impulso basados en árboles generalmente no requieren la creación de variables ficticias, los modelos que utilizan el motor "xgboost" sí la requieren.
