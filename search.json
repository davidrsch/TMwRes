[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modelado Ordenado con R",
    "section": "",
    "text": "Hola Mundo\n¡Bienvenido a Tidy Modeling con R! Este libro es una guía para utilizar una colección de software en el lenguaje de programación R para la construcción de modelos llamada tidymodels y tiene dos objetivos principales:\nEn el Capítulo 1  Software Para Modelar, describimos una taxonomía para modelos y destacamos cómo es un buen software para modelado. Las ideas y la sintaxis del tidyverse, que presentamos (o revisamos) en el Capítulo 2  Una Introducción Tidyverse, son la base del enfoque de tidymodels para estos desafíos de metodología y práctica. El Capítulo 3  Una Revisión De Los Fundamentos Del Modelado Con R proporciona un recorrido rápido por las funciones de modelado convencionales de R base y resume las necesidades no satisfechas en esa área.\nDespués de eso, este libro se divide en partes, comenzando con los conceptos básicos del modelado con principios de datos ordenados. Los capítulos 4  Los Datos De Vivienda De Ames a 9  Juzgar La Eficacia Del Modelo presentan un conjunto de datos de ejemplo sobre los precios de la vivienda y demuestran cómo utilizar los paquetes fundamentales de tidymodels: recipes, parsnip, workflows, yardstick y otros.\nLa siguiente parte del libro avanza con más detalles sobre el proceso de creación de un modelo eficaz. Los capítulos 10  Remuestrear Para Evaluar El Rendimiento a 15  Probando Muchos Modelos se centran en crear buenas estimaciones de rendimiento, así como en ajustar los hiperparámetros del modelo.\nFinalmente, la última sección de este libro, Capítulos 16  Reducción De Dimensionalidad a 21  Análisis Inferencial, cubre otros temas importantes para la construcción de modelos. Analizamos enfoques de ingeniería de características más avanzados, como la reducción de dimensionalidad y la codificación de predictores de alta cardinalidad, así como también cómo responder preguntas sobre por qué un modelo hace ciertas predicciones y cuándo confiar en las predicciones de su modelo.\nNo asumimos que los lectores tengan una amplia experiencia en la construcción de modelos y estadísticas. Se requieren algunos conocimientos estadísticos, como muestreo aleatorio, varianza, correlación, regresión lineal básica y otros temas que generalmente se encuentran en un curso básico de estadística o análisis de datos de pregrado. Suponemos que el lector está al menos ligeramente familiarizado con dplyr, ggplot2 y el operador “pipe” %&gt;% en R, y que está interesado en aplicar estas herramientas al modelado. Para los usuarios que aún no tienen estos conocimientos básicos de R, recomendamos libros como R for Data Science de Wickham y Grolemund (2016). La investigación y el análisis de datos son una parte importante de cualquier proceso de modelo.\nEste libro no pretende ser una referencia exhaustiva sobre técnicas de modelado; sugerimos otros recursos para aprender más sobre los propios métodos estadísticos. Para obtener información general sobre el tipo de modelo más común, el modelo lineal, sugerimos Fox (2008). Para modelos predictivos, Kuhn y Johnson (2013) y Kuhn y Johnson (2020) son buenos recursos. Para los métodos de aprendizaje automático, Goodfellow, Bengio, y Courville (2016) es una fuente de información excelente (pero formal). En algunos casos, describimos los modelos que utilizamos con cierto detalle, pero de una manera menos matemática y, con suerte, más intuitiva.",
    "crumbs": [
      "Hola Mundo"
    ]
  },
  {
    "objectID": "index.html#reconocimientos",
    "href": "index.html#reconocimientos",
    "title": "Modelado Ordenado con R",
    "section": "Reconocimientos",
    "text": "Reconocimientos\nEstamos muy agradecidos por las contribuciones, la ayuda y las perspectivas de las personas que nos han apoyado en este proyecto. Hay varios a los que nos gustaría agradecer en particular.\nNos gustaría agradecer a nuestros colegas de RStudio en el equipo tidymodels (Davis Vaughan, Hannah Frick, Emil Hvitfeldt y Simon Couch), así como al resto de nuestros compañeros de trabajo en el equipo de código abierto de RStudio. Gracias a Desirée De Leon por el diseño del sitio del trabajo en línea. También nos gustaría agradecer a nuestros revisores técnicos, Chelsea Parlett-Pelleriti y Dan Simpson, por sus comentarios detallados y perspicaces que mejoraron sustancialmente este libro, así como a nuestros editores, Nicole Tache y Rita Fernando, por su perspectiva y orientación durante el proceso. de escritura y publicación.\n\nEste libro se escribió abiertamente y varias personas contribuyeron a través de solicitudes de extracción o problemas. Un agradecimiento especial para las thirty-eight personas que contribuyeron a través de pull requests de GitHub (en orden alfabético por nombre de usuario): @arisp99, Brad Hill (@bradisbrad), Bryce Roney (@bryceroney), Cedric Batailler (@cedricbatailler), Ildikó Czeller (@czeildi), David Kane (@davidkane9), @DavZim, @DCharIAA, Emil Hvitfeldt (@EmilHvitfeldt), Emilio (@emilopezcano), Fgazzelloni (@Fgazzelloni), Hannah Frick (@hfrick), Hlynur (@hlynurhallgrims), Howard Baek (@howardbaek), Jae Yeon Kim (@jaeyk), Jonathan D. Trattner (@jdtrat), Jeffrey Girard (@jmgirard), John W Pickering (@JohnPickering), Jon Harmon (@jonthegeek), Joseph B. Rickert (@joseph-rickert), Maximilian Rohde (@maxdrohde), Michael Grund (@michaelgrund), @MikeJohnPage, Mine Cetinkaya-Rundel (@mine-cetinkaya-rundel), Mohammed Hamdy (@mmhamdy), @nattalides, Y. Yu (@PursuitOfDataScience), Riaz Hedayati (@riazhedayati), Rob Wiederstein (@RobWiederstein), Scott (@scottyd22), Simon Schölzel (@simonschoe), Simon Sayz (@tagasimon), @thrkng, Tanner Stauss (@tmstauss), Tony ElHabr (@tonyelhabr), Dmitry Zotikov (@x1o), Xiaochi (@xiaochi-liu), Zach Bogart (@zachbogart).",
    "crumbs": [
      "Hola Mundo"
    ]
  },
  {
    "objectID": "index.html#usando-ejemplos-de-código",
    "href": "index.html#usando-ejemplos-de-código",
    "title": "Modelado Ordenado con R",
    "section": "Usando Ejemplos De Código",
    "text": "Usando Ejemplos De Código\nEste libro fue escrito con RStudio usando bookdown. El sito web está alojado a través de Netlify, y construido automáticamente después de cada push por GitHub Actions. La fuente completa está disponible en GitHub. Generamos todos los gráficos de este libro usando ggplot2 y su tema blanco y negro (theme_bw()).\nEsta versión del libro fue construida con R version 4.3.3 (2024-02-29), pandoc versión 3.1.11, y los siguientes paquetes: applicable (0.1.0, RSPM), av (0.9.0, RSPM), baguette (1.0.2, RSPM), beans (0.1.0, RSPM), bestNormalize (1.9.1, RSPM), bookdown (0.39, RSPM), broom (1.0.5, RSPM), censored (0.3.1, RSPM), corrplot (0.92, RSPM), corrr (0.4.4, RSPM), Cubist (0.4.2.1, RSPM), DALEXtra (2.3.0, RSPM), dials (1.2.1, RSPM), dimRed (0.2.6, RSPM), discrim (1.0.1, RSPM), doMC (1.3.5, RSPM), dplyr (1.1.4, RSPM), earth (5.3.3, RSPM), embed (1.1.4, RSPM), fastICA (1.2-4, RSPM), finetune (1.2.0, RSPM), forcats (1.0.0, RSPM), ggforce (0.4.2, RSPM), ggplot2 (3.5.0, RSPM), glmnet (4.1-8, RSPM), gridExtra (2.3, RSPM), infer (1.0.7, RSPM), kableExtra (1.4.0, RSPM), kernlab (0.9-32, RSPM), kknn (1.3.1, RSPM), klaR (1.7-3, RSPM), knitr (1.46, RSPM), learntidymodels (0.0.0.9001, Github), lime (0.5.3, RSPM), lme4 (1.1-35.3, RSPM), lubridate (1.9.3, RSPM), mda (0.5-4, RSPM), mixOmics (6.26.0, Bioconductor), modeldata (1.3.0, RSPM), multilevelmod (1.0.0, RSPM), nlme (3.1-164, CRAN), nnet (7.3-19, CRAN), parsnip (1.2.1, RSPM), patchwork (1.2.0, RSPM), pillar (1.9.0, RSPM), poissonreg (1.0.1, RSPM), prettyunits (1.2.0, RSPM), probably (1.0.3, RSPM), pscl (1.5.9, RSPM), purrr (1.0.2, RSPM), ranger (0.16.0, RSPM), recipes (1.0.10, RSPM), rlang (1.1.3, RSPM), rmarkdown (2.26, RSPM), rpart (4.1.23, CRAN), rsample (1.2.1, RSPM), rstanarm (2.32.1, RSPM), rules (1.0.2, RSPM), sessioninfo (1.2.2, RSPM), stacks (1.0.4, RSPM), stringr (1.5.1, RSPM), svglite (2.1.3, RSPM), text2vec (0.6.4, RSPM), textrecipes (1.0.6, RSPM), themis (1.0.2, RSPM), tibble (3.2.1, RSPM), tidymodels (1.2.0, RSPM), tidyposterior (1.0.1, RSPM), tidyverse (2.0.0, RSPM), tune (1.2.1, RSPM), uwot (0.2.2, RSPM), workflows (1.1.4, RSPM), workflowsets (1.1.0, RSPM), xgboost (1.7.7.1, RSPM), and yardstick (1.3.1, RSPM).",
    "crumbs": [
      "Hola Mundo"
    ]
  },
  {
    "objectID": "index.html#sobre-la-traducción",
    "href": "index.html#sobre-la-traducción",
    "title": "Modelado Ordenado con R",
    "section": "Sobre La Traducción",
    "text": "Sobre La Traducción\n Esta traducción de “Modelado Ordenado con R” es un proyecto personal de David Díaz Rodríguez con el objetivo de facilitar el estudio de construcción de modelos mediante el uso de R, tanto al propio traductor como a todas aquellas personas de habla hispana que deseen aprender sobre este tema.\nSeñalar que esta es una traducción textual del libro por lo que cuando los autores se refieren así mismo en primera persona, serán Max Kuhn & Julia Slige no el traductor.\nLa traducción fue realizada usando Google Translate y fueron corregidos algunos errores gramaticales y de coherencia. Si detecta algún error relacionado con el contenido de la traducción, siéntase libre de abrir un issue o un pull request en este repositorio.\n\n\n\n\nFox, J. 2008. Applied Regression Analysis and Generalized Linear Models. Second. Thousand Oaks, CA: Sage.\n\n\nGoodfellow, I, Y Bengio, y A Courville. 2016. Deep Learning. MIT Press.\n\n\nKuhn, M, y K Johnson. 2013. Applied Predictive Modeling. Springer.\n\n\n———. 2020. Feature engineering and selection: A practical approach for predictive models. CRC Press.",
    "crumbs": [
      "Hola Mundo"
    ]
  },
  {
    "objectID": "01-software-modeling.html",
    "href": "01-software-modeling.html",
    "title": "1  Software Para Modelar",
    "section": "",
    "text": "1.1 Fundamentos Del Software De Modelado\nEs importante que el software de modelado que utilice sea fácil de utilizar correctamente. La interfaz de usuario no debe estar tan mal diseñada que el usuario no sepa que la utilizó de manera inapropiada. Por ejemplo, Baggerly y Coombes (2009) informa innumerables problemas en los análisis de datos de una publicación de biología computacional de alto perfil. Uno de los problemas estaba relacionado con cómo se requería que los usuarios agregaran los nombres de las entradas del modelo. La interfaz de usuario del software facilitó la compensación de los nombres de las columnas de datos de las columnas de datos reales. Esto dio lugar a que se identificaran genes equivocados como importantes para el tratamiento de pacientes con cáncer y, finalmente, contribuyó a la interrupción de varios ensayos clínicos. (Carlson 2012).\nSi necesitamos modelos de alta calidad, el software debe facilitar su uso adecuado. Abrams (2003) describe un principio interesante para guiarnos:\nEl software de análisis y modelado de datos debería abrazar esta idea.\nEn segundo lugar, el software de modelado debería promover una buena metodología científica. Cuando se trabaja con modelos predictivos complejos, puede resultar fácil cometer errores, sin saberlo, relacionados con falacias lógicas o suposiciones inapropiadas. Muchos modelos de aprendizaje automático son tan hábiles en descubrir patrones que pueden encontrar sin esfuerzo patrones empíricos en los datos que no logran reproducir más adelante. Algunos de los errores metodológicos son insidiosos en el sentido de que el problema puede pasar desapercibido hasta un momento posterior, cuando se obtienen nuevos datos que contienen el resultado verdadero.\nEste mismo principio también se aplica a la programación. Siempre que sea posible, el software debería poder proteger a los usuarios de cometer errores. El software debería facilitar a los usuarios hacer lo correcto.\nEstos dos aspectos del desarrollo de modelos (facilidad de uso adecuado y buenas prácticas metodológicas) son cruciales. Dado que las herramientas para crear modelos son fácilmente accesibles y los modelos pueden tener un impacto tan profundo, muchas más personas los están creando. En términos de experiencia técnica y capacitación, los antecedentes de los creadores variarán. Es importante que sus herramientas sean sólidas para la experiencia del usuario. Las herramientas deben ser lo suficientemente potentes como para crear modelos de alto rendimiento, pero, por otro lado, deben ser fáciles de utilizar de forma adecuada. Este libro describe un conjunto de software para modelado que ha sido diseñado teniendo en cuenta estas características.\nEl software está basado en el lenguaje de programación R (R Core Team 2014). R ha sido diseñado especialmente para el análisis y modelado de datos. Es una implementación del lenguaje S (con reglas de alcance léxico adaptadas de Scheme y Lisp) que se creó en la década de 1970 para\nR es de código abierto y gratuito. Es un poderoso lenguaje de programación que se puede utilizar para muchos propósitos diferentes, pero se especializa en análisis de datos, modelado, visualización y aprendizaje automático. R es fácilmente extensible; Tiene un vasto ecosistema de paquetes, en su mayoría módulos aportados por los usuarios que se centran en un tema específico, como modelado, visualización, etc.\nUna colección de paquetes se llama tidyverse (Wickham et al. 2019). Tidyverse es una colección obstinada de paquetes R diseñados para la ciencia de datos. Todos los paquetes comparten una filosofía de diseño, gramática y estructuras de datos subyacentes. Varias de estas filosofías de diseño están directamente informadas por los aspectos del software para modelado descritos en este capítulo. Si nunca ha usado los paquetes tidyverse, el Capítulo 2 contiene una revisión de los conceptos básicos. Dentro de tidyverse, el subconjunto de paquetes centrados específicamente en el modelado se conoce como paquetes tidymodels. Este libro es una guía práctica para realizar modelados utilizando los paquetes tidyverse y tidymodels. Muestra cómo utilizar un conjunto de paquetes, cada uno con su propósito específico, juntos para crear modelos de alta calidad.",
    "crumbs": [
      "INTRODUCCIÓN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Para Modelar</span>"
    ]
  },
  {
    "objectID": "01-software-modeling.html#fundamentos-del-software-de-modelado",
    "href": "01-software-modeling.html#fundamentos-del-software-de-modelado",
    "title": "1  Software Para Modelar",
    "section": "",
    "text": "El pozo del éxito: en marcado contraste con una cumbre, un pico o un viaje a través de un desierto para encontrar la victoria a través de muchas pruebas y sorpresas, queremos que nuestros clientes simplemente adopten prácticas ganadoras utilizando nuestra plataforma y marcos.\n\n\n\n\nA medida que nuestros modelos se han vuelto más poderosos y complejos, también se ha vuelto más fácil cometer errores latentes.\n\n\n\n\n\n“convertir ideas en software, de forma rápida y fiel” (Chambers 1998)",
    "crumbs": [
      "INTRODUCCIÓN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Para Modelar</span>"
    ]
  },
  {
    "objectID": "01-software-modeling.html#sec-model-types",
    "href": "01-software-modeling.html#sec-model-types",
    "title": "1  Software Para Modelar",
    "section": "1.2 Tipos De Modelos",
    "text": "1.2 Tipos De Modelos\nAntes de continuar, describamos una taxonomía de tipos de modelos, agrupados por propósito. Esta taxonomía informa tanto cómo se utiliza un modelo como muchos aspectos de cómo se puede crear o evaluar el modelo. Si bien esta lista no es exhaustiva, la mayoría de los modelos caen en al menos una de estas categorías:\n\nModelos descriptivos\nEl propósito de un modelo descriptivo es describir o ilustrar las características de algunos datos. Es posible que el análisis no tenga otro propósito que enfatizar visualmente alguna tendencia o artefacto en los datos.\nPor ejemplo, desde hace algún tiempo es posible realizar mediciones de ARN a gran escala utilizando microarrays. Los primeros métodos de laboratorio colocaban una muestra biológica en un pequeño microchip. Ubicaciones muy pequeñas en el chip pueden medir una señal basada en la abundancia de una secuencia de ARN específica. El chip contendría miles (o más) de resultados, cada uno de los cuales sería una cuantificación del ARN relacionado con un proceso biológico. Sin embargo, podría haber problemas de calidad en el chip que podrían provocar malos resultados. Por ejemplo, una huella digital dejada accidentalmente en una parte del chip podría provocar mediciones inexactas al escanearla.\nUno de los primeros métodos para evaluar estas cuestiones fueron los modelos a nivel de sonda, o PLM (Bolstad 2004). Se crearía un modelo estadístico que tuviera en cuenta las diferencias conocidas en los datos, como el chip, la secuencia de ARN, el tipo de secuencia, etc. Si hubiera otros factores desconocidos en los datos, estos efectos se capturarían en los residuos del modelo. Cuando los residuos se trazaron según su ubicación en el chip, un chip de buena calidad no mostraría patrones. Cuando ocurría un problema, se podía discernir algún tipo de patrón espacial. A menudo, el tipo de patrón sugeriría el problema subyacente (por ejemplo, una huella digital) y una posible solución (limpiar el chip y volver a escanear, repetir la muestra, etc.). Figura 1.1 (a) muestra una aplicación de este método para dos microarrays tomados de Gentleman et al. (2005). Las imágenes muestran dos valores de color diferentes; Las áreas más oscuras son aquellas donde la intensidad de la señal fue mayor de lo esperado por el modelo, mientras que el color más claro muestra valores inferiores a los esperados. El panel de la izquierda muestra un patrón bastante aleatorio, mientras que el panel de la derecha muestra un artefacto indeseable en el medio del chip.\n\n\n\n\n\n\n\n\nFigura 1.1: Dos ejemplos de cómo se pueden utilizar modelos descriptivos para ilustrar patrones específicos\n\n\n\n\n\nOtro ejemplo de modelo descriptivo es el modelo de suavizado de diagrama de dispersión estimado localmente, más comúnmente conocido como LOESS (Cleveland 1979). En este caso, se ajusta un modelo de regresión suave y flexible a un conjunto de datos, generalmente con una única variable independiente, y se utiliza la línea de regresión ajustada para dilucidar alguna tendencia en los datos. Estos tipos de suavizadores se utilizan para descubrir formas potenciales de representar una variable en un modelo. Esto se demuestra en Figura 1.1 (b), donde el suavizador flexible ilumina una tendencia no lineal. De este gráfico se desprende claramente que existe una relación altamente no lineal entre el precio de venta de una casa y su latitud.\n\n\nModelos inferenciales\nEl objetivo de un modelo inferencial es producir una decisión para una pregunta de investigación o explorar una hipótesis específica, de forma similar a cómo se utilizan las pruebas estadísticas.1 Un modelo inferencial comienza con una conjetura o idea predefinida sobre una población y produce una conclusión estadística como una estimación de intervalo o el rechazo de una hipótesis.\nPor ejemplo, el objetivo de un ensayo clínico podría ser confirmar que una nueva terapia logra prolongar la vida mejor que una alternativa, como una terapia existente o ningún tratamiento. Si el criterio de valoración clínico se relaciona con la supervivencia de un paciente, la hipótesis nula podría ser que el nuevo tratamiento tiene una mediana de supervivencia igual o menor, siendo la hipótesis alternativa que la nueva terapia tiene una mediana de supervivencia más alta. Si este ensayo se evaluara utilizando pruebas de significancia de hipótesis nulas tradicionales mediante modelado, la prueba de significancia produciría un valor p usando alguna metodología predefinida basada en un conjunto de suposiciones para los datos. Valores pequeños para el valor p en los resultados del modelo indicarían que hay evidencia de que la nueva terapia ayuda a los pacientes a vivir más tiempo. Valores grandes para el valor p en los resultados del modelo concluirían que no se puede mostrar tal diferencia; Esta falta de evidencia podría deberse a varias razones, incluida la terapia que no funciona.\n¿Cuáles son los aspectos importantes de este tipo de análisis? Las técnicas de modelado inferencial suelen producir algún tipo de resultado probabilístico, como un valor p, un intervalo de confianza o una probabilidad posterior. Generalmente, para calcular tal cantidad, se deben hacer supuestos probabilísticos formales sobre los datos y los procesos subyacentes que generaron los datos. La calidad de los resultados del modelado estadístico depende en gran medida de estos supuestos predefinidos, así como de hasta qué punto los datos observados parecen coincidir con ellos. Los factores más críticos aquí son teóricos: “Si mis datos fueran independientes y los residuos siguieran la distribución X, entonces el estadístico de prueba Y se puede utilizar para producir un valor p. De lo contrario, el valor p resultante podría ser inexacto.”\n\nUn aspecto de los análisis inferenciales es que tiende a haber un ciclo de retroalimentación retardada en la comprensión de qué tan bien coinciden los datos con los supuestos del modelo. En nuestro ejemplo de ensayo clínico, si la significación estadística (y clínica) indica que la nueva terapia debería estar disponible para que la utilicen los pacientes, aún pueden pasar años antes de que se utilice en el campo y se generen suficientes datos para una evaluación independiente de si el análisis estadístico original condujo a la decisión adecuada.\n\n\n\nModelos predictivos\nA veces, los datos se modelan para producir la predicción más precisa posible para datos nuevos. Aquí, el objetivo principal es que los valores predichos tengan la mayor fidelidad posible al valor real de los nuevos datos.\nUn ejemplo sencillo sería que un comprador de libros predijera cuántas copias de un libro en particular debería enviar a su tienda durante el próximo mes. Una predicción excesiva desperdicia espacio y dinero debido al exceso de libros. Si la predicción es menor de lo que debería ser, hay pérdida de oportunidades y menos ganancias.\nPara este tipo de modelo, el tipo de problema es de estimación más que de inferencia. Por ejemplo, al comprador normalmente no le preocupa una pregunta como “¿Venderé más de 100 copias del libro X el próximo mes?” sino “¿Cuántas copias del libro X comprarán los clientes el próximo mes?” Además, dependiendo del contexto, puede que no haya ningún interés en saber por qué el valor previsto es X. En otras palabras, hay más interés en el valor en sí que en evaluar una hipótesis formal relacionada con los datos. La predicción también puede incluir medidas de incertidumbre. En el caso del comprador de libros, proporcionar un error de pronóstico puede resultar útil para decidir cuántos libros comprar. También puede servir como métrica para evaluar qué tan bien funcionó el método de predicción.\n¿Cuáles son los factores más importantes que afectan los modelos predictivos? Hay muchas formas diferentes de crear un modelo predictivo, por lo que los factores importantes dependen de cómo se desarrolló el modelo.2\nSe podría derivar un modelo mecanicista utilizando primeros principios para producir una ecuación modelo que dependa de suposiciones. Por ejemplo, al predecir la cantidad de una droga que hay en el cuerpo de una persona en un momento determinado, se hacen algunas suposiciones formales sobre cómo se administra, absorbe, metaboliza y elimina la droga. En base a esto, se puede utilizar un conjunto de ecuaciones diferenciales para derivar una ecuación modelo específica. Los datos se utilizan para estimar los parámetros desconocidos de esta ecuación para poder generar predicciones. Al igual que los modelos inferenciales, los modelos predictivos mecanicistas dependen en gran medida de los supuestos que definen las ecuaciones de sus modelos. Sin embargo, a diferencia de los modelos inferenciales, es fácil hacer afirmaciones basadas en datos sobre qué tan bien se desempeña el modelo en función de qué tan bien predice los datos existentes. Aquí el ciclo de retroalimentación para el practicante de modelos es mucho más rápido de lo que sería para una prueba de hipótesis.\nModelos impulsados empíricamente se crean con suposiciones más vagas. Estos modelos tienden a caer en la categoría de aprendizaje automático. Un buen ejemplo es el modelo K-vecino más cercano (KNN). Dado un conjunto de datos de referencia, se predice una nueva muestra utilizando los valores de los K datos más similares en el conjunto de referencia. Por ejemplo, si un comprador de libros necesita una predicción para un libro nuevo, es posible que haya disponibles datos históricos de libros existentes. Un modelo de cinco vecinos más cercanos estimaría el número de libros nuevos a comprar basándose en las cifras de ventas de los cinco libros que son más similares al nuevo (para alguna definición de “similar”). Este modelo se define únicamente por la estructura de la predicción (el promedio de cinco libros similares). No se hacen suposiciones teóricas o probabilísticas sobre las cifras de ventas o las variables que se utilizan para definir la similitud. De hecho, el método principal para evaluar la idoneidad del modelo es evaluar su precisión utilizando datos existentes. Si la estructura de este tipo de modelo fuera una buena elección, las predicciones se aproximarían a los valores reales.",
    "crumbs": [
      "INTRODUCCIÓN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Para Modelar</span>"
    ]
  },
  {
    "objectID": "01-software-modeling.html#conexiones-entre-tipos-de-modelos",
    "href": "01-software-modeling.html#conexiones-entre-tipos-de-modelos",
    "title": "1  Software Para Modelar",
    "section": "1.3 Conexiones Entre Tipos De Modelos",
    "text": "1.3 Conexiones Entre Tipos De Modelos\n\nTenga en cuenta que hemos definido el tipo de modelo por cómo se utiliza, más que por sus cualidades matemáticas.\n\nUn modelo de regresión lineal ordinario podría pertenecer a cualquiera de estas tres clases de modelos, dependiendo de cómo se utilice:\n\nSe puede utilizar un suavizador descriptivo, similar a LOESS, llamado splines de suavizado restringido (Durrleman y Simon 1989) para describir tendencias en datos usando regresión lineal ordinaria con términos especializados.\nUn modelo de análisis de varianza (ANOVA) es un método popular para producir los valores p utilizados para la inferencia. Los modelos ANOVA son un caso especial de regresión lineal.\nSi un modelo de regresión lineal simple produce predicciones precisas, puede utilizarse como modelo predictivo.\n\nHay muchos ejemplos de modelos predictivos que no pueden (o al menos no deberían) usarse para inferencias. Incluso si se hicieran suposiciones probabilísticas para los datos, la naturaleza del modelo K de vecinos más cercanos, por ejemplo, hace que las matemáticas necesarias para la inferencia sean intratables.\nExiste una conexión adicional entre los tipos de modelos. Si bien el propósito principal de los modelos descriptivos e inferenciales puede no estar relacionado con la predicción, no se debe ignorar la capacidad predictiva del modelo. Por ejemplo, la regresión logística es un modelo popular para datos en los que el resultado es cualitativo con dos valores posibles. Puede modelar cómo se relacionan las variables con la probabilidad de los resultados. Cuando se utiliza de manera inferencial, se presta mucha atención a las cualidades estadísticas del modelo. Por ejemplo, los analistas tienden a centrarse fuertemente en la selección de variables independientes contenidas en el modelo. Se pueden utilizar muchas iteraciones de la construcción de modelos para determinar un subconjunto mínimo de variables independientes que tengan una relación “estadísticamente significativa” con la variable de resultado. Esto generalmente se logra cuando todos los valores p de las variables independientes están por debajo de cierto valor (por ejemplo, 0,05). A partir de aquí, el analista puede centrarse en hacer afirmaciones cualitativas sobre la influencia relativa que tienen las variables en el resultado (por ejemplo, “Existe una relación estadísticamente significativa entre la edad y las probabilidades de enfermedad cardíaca”).\nSin embargo, este enfoque puede resultar peligroso cuando se utiliza la significación estadística como única medida de la calidad del modelo. Es posible que este modelo estadísticamente optimizado tenga una precisión deficiente o que tenga un desempeño deficiente en alguna otra medida de capacidad predictiva. Si bien es posible que el modelo no se utilice para la predicción, ¿cuánto se debe confiar en las inferencias de un modelo que tiene valores p significativos pero una precisión lamentable? El rendimiento predictivo tiende a estar relacionado con qué tan cerca están los valores ajustados del modelo a los datos observados.\n\nSi un modelo tiene una fidelidad limitada a los datos, las inferencias generadas por el modelo deberían ser muy sospechosas. En otras palabras, la significancia estadística puede no ser prueba suficiente de que un modelo es apropiado.\n\nEsto puede parecer intuitivamente obvio, pero a menudo se ignora en el análisis de datos del mundo real.",
    "crumbs": [
      "INTRODUCCIÓN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Para Modelar</span>"
    ]
  },
  {
    "objectID": "01-software-modeling.html#sec-model-terminology",
    "href": "01-software-modeling.html#sec-model-terminology",
    "title": "1  Software Para Modelar",
    "section": "1.4 Terminología",
    "text": "1.4 Terminología\nAntes de continuar, describiremos terminología adicional relacionada con el modelado y los datos. Estas descripciones pretenden ser útiles a medida que lee este libro, pero no son exhaustivas.\nEn primer lugar, muchos modelos pueden clasificarse como supervisados o no supervisados. Los modelos no supervisados ​​son aquellos que aprenden patrones, grupos u otras características de los datos pero carecen de un resultado, es decir, una variable dependiente. El análisis de componentes principales (PCA), la agrupación en clústeres y los codificadores automáticos son ejemplos de modelos no supervisados; se utilizan para comprender las relaciones entre variables o conjuntos de variables sin una relación explícita entre los predictores y un resultado. Los modelos supervisados ​​son aquellos que tienen una variable de resultado. La regresión lineal, las redes neuronales y muchas otras metodologías entran en esta categoría.\nDentro de los modelos supervisados, hay dos subcategorías principales:\n\nRegresión predice un resultado numérico.\nClasificación predice un resultado que es un conjunto ordenado o desordenado de valores cualitativos.\n\nEstas son definiciones imperfectas y no tienen en cuenta todos los tipos de modelos posibles. En el Capítulo 6, nos referimos a esta característica de las técnicas supervisadas como modo modelo.\nDiferentes variables pueden tener diferentes roles, especialmente en un análisis de modelado supervisado. Los resultados (también conocidos como etiquetas, criterios de valoración o variables dependientes) son el valor que se predice en los modelos supervisados. Las variables independientes, que son el sustrato para hacer predicciones del resultado, también se denominan predictores, características o covariables (según el contexto). Los términos resultados y predictores se utilizan con mayor frecuencia en este libro.\nEn términos de los datos o variables en sí, ya sea que se utilicen para modelos supervisados ​​o no supervisados, como predictores o resultados, las dos categorías principales son cuantitativas y cualitativas. Ejemplos del primero son números reales como “3.14159” y números enteros como “42”. Los valores cualitativos, también conocidos como datos nominales, son aquellos que representan algún tipo de estado discreto que no se puede ubicar naturalmente en una escala numérica, como “rojo”, “verde” y “azul”.",
    "crumbs": [
      "INTRODUCCIÓN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Para Modelar</span>"
    ]
  },
  {
    "objectID": "01-software-modeling.html#sec-model-phases",
    "href": "01-software-modeling.html#sec-model-phases",
    "title": "1  Software Para Modelar",
    "section": "1.5 ¿Cómo Encaja El Modelado En El Proceso De Análisis de Datos?",
    "text": "1.5 ¿Cómo Encaja El Modelado En El Proceso De Análisis de Datos?\n¿En qué circunstancias se crean los modelos? ¿Hay pasos que preceden a tal empresa? ¿Es la creación de modelos el primer paso en el análisis de datos?\n\nHay algunas fases críticas del análisis de datos que siempre vienen antes del modelado.\n\nEn primer lugar, está el proceso crónicamente subestimado de limpiar los datos. Independientemente de las circunstancias, debe investigar los datos para asegurarse de que sean aplicables a los objetivos de su proyecto, precisos y apropiados. Estos pasos fácilmente pueden llevar más tiempo que el resto del proceso de análisis de datos (según las circunstancias).\nLa limpieza de datos también puede superponerse con la segunda fase de comprensión de los datos, a menudo denominada análisis de datos exploratorios (EDA). EDA saca a la luz cómo se relacionan las diferentes variables entre sí, sus distribuciones, rangos típicos y otros atributos. Una buena pregunta para hacer en esta fase es: “¿Cómo obtuve estos datos?” Esta pregunta puede ayudarle a comprender cómo se muestrearon o filtraron los datos disponibles y si estas operaciones fueron apropiadas. Por ejemplo, al fusionar tablas de bases de datos, una unión puede salir mal y eliminar accidentalmente una o más subpoblaciones. Otra buena idea es preguntar si los datos son relevantes. Por ejemplo, para predecir si los pacientes tienen la enfermedad de Alzheimer, no sería prudente tener un conjunto de datos que contenga sujetos con la enfermedad y una muestra aleatoria de adultos sanos de la población general. Dada la naturaleza progresiva de la enfermedad, el modelo puede simplemente predecir quiénes son los pacientes de mayor edad.\nFinalmente, antes de iniciar un proceso de análisis de datos, deben existir expectativas claras sobre el objetivo del modelo y cómo se juzgará el desempeño (y el éxito). Se debe identificar al menos una métrica de desempeño con objetivos realistas de lo que se puede lograr. Las métricas estadísticas comunes, que se analizan con más detalle en el Capítulo 9, son la precisión de la clasificación, las tasas de verdaderos y falsos positivos, el error cuadrático medio, etc. Deben sopesarse los beneficios y desventajas relativos de estas métricas. También es importante que la métrica sea pertinente; la alineación con los objetivos más amplios del análisis de datos es fundamental.\nEl proceso de investigación de los datos puede no ser sencillo. Wickham y Grolemund (2016) contiene una excelente ilustración del proceso general de análisis de datos, reproducida en Figura 1.2. La ingesta de datos y la limpieza/ordenamiento se muestran como pasos iniciales. Cuando comienzan los pasos analíticos para la comprensión, son un proceso heurístico; No podemos predeterminar cuánto tiempo pueden tardar. El ciclo de transformación, modelado y visualización a menudo requiere múltiples iteraciones.\n\n\n\n\n\n\n\n\nFigura 1.2: El proceso de ciencia de datos (de R para Ciencia de Datos, usado con permiso)\n\n\n\n\n\nEste proceso iterativo es especialmente cierto para el modelado. Figura 1.3 emula la ruta típica para determinar un modelo apropiado. Las fases generales son:\n\nAnálisis de datos exploratorios (EDA): Inicialmente hay un vaivén entre el análisis numérico y la visualización de datos (representado en Figura 1.2) donde diferentes descubrimientos conducen a más preguntas y misiones secundarias de análisis de datos para comprender mejor los datos.\nIngeniería de características: La comprensión obtenida con EDA da como resultado la creación de términos de modelo específicos que facilitan el modelado preciso de los datos observados. Esto puede incluir metodologías complejas (por ejemplo, PCA) o características más simples (usando la proporción de dos predictores). Capítulo 8 se centra completamente en este importante paso.\nAjuste y selección de modelos (círculos grandes con segmentos alternos): Se genera una variedad de modelos y se compara su rendimiento. Algunos modelos requieren ajuste de parámetros en el que se deben especificar u optimizar algunos parámetros estructurales. Los segmentos alternos dentro de los círculos significan la división repetida de datos utilizada durante el remuestreo (consulte el Capítulo 10).\nEvaluación del modelo: Durante esta fase del desarrollo del modelo, evaluamos las métricas de rendimiento del modelo, examinamos gráficos residuales y realizamos otros análisis similares a EDA para comprender qué tan bien funcionan los modelos. En algunos casos, las comparaciones formales entre modelos (Capítulo 11) le ayudan a comprender si hay diferencias en los modelos dentro del ruido experimental.\n\n\n\n\n\n\n\n\n\nFigura 1.3: Un esquema para el proceso de modelado típico.\n\n\n\n\n\nDespués de una secuencia inicial de estas tareas, se obtiene una mayor comprensión sobre qué modelos son superiores y qué subpoblaciones de datos no se están estimando de manera efectiva. Esto conduce a EDA adicional y a ingeniería de características, otra ronda de modelado, etc. Una vez que se logran los objetivos del análisis de datos, normalmente los últimos pasos son finalizar, documentar y comunicar el modelo. Para los modelos predictivos, es común al final validar el modelo con un conjunto adicional de datos reservados para este propósito específico.\nComo ejemplo, Kuhn y Johnson (2020) utiliza datos para modelar el número de pasajeros diario del sistema de trenes públicos de Chicago utilizando predictores como la fecha, los resultados anteriores del número de pasajeros, el clima y otros factores. @ tbl-inner-monologue muestra una aproximación al hipotético monólogo interno de estos autores al analizar estos datos y eventualmente seleccionar un modelo con suficiente rendimiento.\n\n\n\nTabla 1.1: Monólogo interior hipotético de un desarrollador de modelos.\n\n\n\n\n\n\nPensamientos\nActividad\n\n\n\n\nLos valores de pasajeros diarios entre estaciones están extremadamente correlacionados.\nEDA\n\n\nEl número de pasajeros entre semana y los fines de semana se ve muy diferente.\nEDA\n\n\nUn día del verano de 2010 tiene un número anormalmente grande de ciclistas.\nEDA\n\n\n¿Qué estaciones tuvieron los valores más bajos de pasajeros diarios?\nEDA\n\n\nLas fechas deben codificarse al menos como día de la semana y año.\nIngeniería de características\n\n\nQuizás se podría utilizar PCA en los predictores correlacionados para facilitar su uso por parte de los modelos.\nIngeniería de características\n\n\nLos registros meteorológicos horarios probablemente deberían resumirse en mediciones diarias.\nIngeniería de características\n\n\nComencemos con una regresión lineal simple, K vecinos más cercanos y un árbol de decisión mejorado.\nEntrenamiento del modelo\n\n\n¿Cuántos vecinos se deben utilizar?\nAjuste del modelo\n\n\n¿Deberíamos ejecutar muchas iteraciones de impulso o solo unas pocas?\nAjuste del modelo\n\n\n¿Cuántos vecinos parecían óptimos para estos datos?\nAjuste del modelo\n\n\n¿Qué modelos tienen los errores cuadráticos medios más bajos?\nEvaluación del Modelo\n\n\n¿Qué días fueron mal predichos?\nEDA\n\n\nLas puntuaciones de importancia variable indican que la información meteorológica no es predictiva. Los eliminaremos del siguiente conjunto de modelos.\nEvaluación del Modelo\n\n\nParece que deberíamos centrarnos en muchas iteraciones de impulso para ese modelo.\nEvaluación del Modelo\n\n\nNecesitamos codificar funciones de vacaciones para mejorar las predicciones en (y alrededor de) esas fechas.\nIngeniería de características\n\n\nEliminemos a KNN de la lista de modelos.\nEvaluación del Modelo",
    "crumbs": [
      "INTRODUCCIÓN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Para Modelar</span>"
    ]
  },
  {
    "objectID": "01-software-modeling.html#sec-software-summary",
    "href": "01-software-modeling.html#sec-software-summary",
    "title": "1  Software Para Modelar",
    "section": "1.6 Resumen Del Capítulo",
    "text": "1.6 Resumen Del Capítulo\nEste capítulo se centró en cómo los modelos describen relaciones en los datos y en diferentes tipos de modelos, como modelos descriptivos, modelos inferenciales y modelos predictivos. La capacidad predictiva de un modelo puede utilizarse para evaluarlo, incluso cuando su objetivo principal no sea la predicción. El modelado en sí forma parte del proceso más amplio de análisis de datos, y el análisis de datos exploratorio es una parte clave de la construcción de modelos de alta calidad.\n\n\n\n\nAbrams, B. 2003. «The Pit of Success». https://blogs.msdn.microsoft.com/brada/2003/10/02/the-pit-of-success/.\n\n\nBaggerly, K, y K Coombes. 2009. «Deriving chemosensitivity from cell lines: Forensic bioinformatics and reproducible research in high-throughput biology». The Annals of Applied Statistics 3 (4): 1309-34.\n\n\nBolstad, B. 2004. Low-level analysis of high-density oligonucleotide array data: Background, normalization and summarization. University of California, Berkeley.\n\n\nBreiman, L. 2001. «Statistical modeling: The two cultures». Statistical Science 16 (3): 199-231.\n\n\nCarlson, B. 2012. «Putting oncology patients at risk». Biotechnology Healthcare 9 (3): 17-21.\n\n\nChambers, J. 1998. Programming with Data: A Guide to the S Language. Berlin, Heidelberg: Springer-Verlag.\n\n\nCleveland, W. 1979. «Robust locally weighted regression and smoothing scatterplots». Journal of the American Statistical Association 74 (368): 829-36.\n\n\nDurrleman, S, y R Simon. 1989. «Flexible regression models with cubic splines». Statistics in Medicine 8 (5): 551-61.\n\n\nGentleman, R, V Carey, W Huber, R Irizarry, y S Dudoit. 2005. Bioinformatics and Computational Biology Solutions Using R and Bioconductor. Berlin, Heidelberg: Springer-Verlag.\n\n\nKuhn, M, y K Johnson. 2020. Feature engineering and selection: A practical approach for predictive models. CRC Press.\n\n\nR Core Team. 2014. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. http://www.R-project.org/.\n\n\nShmueli, G. 2010. «To explain or to predict?» Statistical science 25 (3): 289-310.\n\n\nWickham, H, M Averick, J Bryan, W Chang, L McGowan, R François, G Grolemund, et al. 2019. «Welcome to the Tidyverse». Journal of Open Source Software 4 (43).\n\n\nWickham, H, y G Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media, Inc.",
    "crumbs": [
      "INTRODUCCIÓN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Para Modelar</span>"
    ]
  },
  {
    "objectID": "01-software-modeling.html#footnotes",
    "href": "01-software-modeling.html#footnotes",
    "title": "1  Software Para Modelar",
    "section": "",
    "text": "De hecho, muchas pruebas estadísticas específicas son equivalentes a modelos. Por ejemplo, las pruebas t y los métodos de análisis de varianza (ANOVA) son casos particulares del modelo lineal generalizado.↩︎\nSe pueden encontrar discusiones más amplias sobre estas distinciones en Breiman (2001) y Shmueli (2010).↩︎",
    "crumbs": [
      "INTRODUCCIÓN",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Software Para Modelar</span>"
    ]
  },
  {
    "objectID": "02-tidyverse.html",
    "href": "02-tidyverse.html",
    "title": "2  Una Introducción Tidyverse",
    "section": "",
    "text": "2.1 Principios De Tidyverse\nEl conjunto completo de estrategias y tácticas para escribir código R en el estilo tidyverse se puede encontrar en el sitio web https://design.tidyverse.org. Aquí podemos describir brevemente varios de los principios generales de diseño de tidyverse, su motivación y cómo pensamos sobre el modelado como una aplicación de estos principios.",
    "crumbs": [
      "INTRODUCCIÓN",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Una Introducción Tidyverse</span>"
    ]
  },
  {
    "objectID": "02-tidyverse.html#principios-de-tidyverse",
    "href": "02-tidyverse.html#principios-de-tidyverse",
    "title": "2  Una Introducción Tidyverse",
    "section": "",
    "text": "2.1.1 Diseño para humanos\nTidyverse se centra en diseñar paquetes y funciones de R que puedan ser fácilmente comprendidos y utilizados por una amplia gama de personas. Tanto históricamente como hoy, un porcentaje sustancial de usuarios de R no son personas que crean software o herramientas, sino personas que crean análisis o modelos. Como tal, los usuarios de R normalmente no tienen (ni necesitan) experiencia en informática, y muchos no están interesados ​​en escribir sus propios paquetes R.\nPor esta razón, es fundamental que sea fácil trabajar con el código R para lograr sus objetivos. La documentación, la formación, la accesibilidad y otros factores juegan un papel importante para lograrlo. Sin embargo, si la sintaxis en sí es difícil de comprender para las personas, la documentación es una mala solución. El software en sí debe ser intuitivo.\nPara contrastar el enfoque tidyverse con la semántica R más tradicional, considere ordenar un marco de datos. Los marcos de datos pueden representar diferentes tipos de datos en cada columna y varios valores en cada fila. Usando solo el lenguaje central, podemos ordenar un marco de datos usando una o más columnas reordenando las filas mediante las reglas de subíndice de R junto con order(); no puedes utilizar con éxito una función que podrías sentirte tentado a probar en tal situación debido a su nombre, sort(). Para ordenar los datos de mtcars por dos de sus columnas, la llamada podría verse así:\n\nmtcars[order(mtcars$gear, mtcars$mpg), ]\n\nSi bien es muy eficiente desde el punto de vista computacional, sería difícil argumentar que se trata de una interfaz de usuario intuitiva. En dplyr, por el contrario, la función tidyverse arrange() toma un conjunto de nombres de variables como argumentos de entrada directamente:\n\nlibrary(dplyr)\narrange(.data = mtcars, gear, mpg)\n\n\nLos nombres de variables utilizados aquí están “sin comillas”; muchas funciones tradicionales de R requieren una cadena de caracteres para especificar variables, pero las funciones tidyverse toman nombres sin comillas o funciones de selección. Los selectores permiten una o más reglas legibles que se aplican a los nombres de las columnas. Por ejemplo, ends_with(\"t\") seleccionaría las columnas drat y wt del marco de datos mtcars.\n\nAdemás, el nombramiento es crucial. Si era nuevo en R y estaba escribiendo análisis de datos o código de modelado que involucra álgebra lineal, es posible que se sienta bloqueado al buscar una función que calcule la matriz inversa. El uso de apropos(\"inv\") no produce candidatos. Resulta que la función R base para esta tarea es solve(), para resolver sistemas de ecuaciones lineales. Para una matriz X, usarías solve(X) para invertir X (sin vector para el lado derecho de la ecuación). Esto sólo está documentado en la descripción de uno de los argumentos en el archivo de ayuda. En esencia, necesita saber el nombre de la solución para poder encontrarla.\nEl enfoque de tidyverse consiste en utilizar nombres de funciones que sean descriptivos y explícitos en lugar de aquellos que sean breves e implícitos. Hay un enfoque en los verbos (por ejemplo, “adaptar”, “arreglar”, etc.) para los métodos generales. Los pares verbo-sustantivo son particularmente eficaces; considere invert_matrix() como un nombre de función hipotético. En el contexto del modelado, también es importante evitar jergas muy técnicas, como letras griegas o términos oscuros. Los nombres deben ser lo más autodocumentados posible.\nCuando hay funciones similares en un paquete, los nombres de las funciones están diseñados para optimizarse para completarse con tabulaciones. Por ejemplo, el paquete glue tiene una colección de funciones que comienzan con un prefijo común (glue_) que permite a los usuarios encontrar rápidamente la función que buscan.\n\n\n2.1.2 Reutilizar estructuras de datos existentes\nSiempre que sea posible, las funciones deben evitar devolver una estructura de datos novedosa. Si los resultados son propicios para una estructura de datos existente, se debe utilizar. Esto reduce la carga cognitiva al utilizar software; no se requieren sintaxis ni métodos adicionales.\nEl marco de datos es la estructura de datos preferida en los paquetes tidyverse y tidymodels, porque su estructura se adapta bien a una gama tan amplia de tareas de ciencia de datos. Específicamente, los modelos tidyverse y tidy favorecen el tibble, una reinvención moderna del marco de datos de R que describimos en la siguiente sección sobre el ejemplo de sintaxis de tidyverse.\nComo ejemplo, el paquete rsample se puede utilizar para crear resamples de un conjunto de datos, como la validación cruzada o el bootstrap (descrito en el Capítulo 10). Las funciones de remuestreo devuelven un tibble con una columna llamada “divisiones” de objetos que definen los conjuntos de datos remuestreados. Tres muestras de arranque de un conjunto de datos podrían verse así:\n\nboot_samp &lt;- rsample::bootstraps(mtcars, times = 3)\nboot_samp\n## # Bootstrap sampling \n## # A tibble: 3 × 2\n##   splits          id        \n##   &lt;list&gt;          &lt;chr&gt;     \n## 1 &lt;split [32/10]&gt; Bootstrap1\n## 2 &lt;split [32/10]&gt; Bootstrap2\n## 3 &lt;split [32/9]&gt;  Bootstrap3\nclass(boot_samp)\n## [1] \"bootstraps\" \"rset\"       \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nCon este enfoque, se pueden usar funciones basadas en vectores con estas columnas, como vapply() o purrr::map().1 Este objeto boot_samp tiene múltiples clases pero hereda métodos para marcos de datos (\"data.frame\") y tibbles (\"tbl_df\"). Además, se pueden agregar nuevas columnas a los resultados sin afectar la clase de los datos. Es mucho más fácil y versátil para los usuarios trabajar con esto que un tipo de objeto completamente nuevo que no hace que su estructura de datos sea obvia.\nUna desventaja de depender de estructuras de datos comunes es la posible pérdida de rendimiento computacional. En algunas situaciones, los datos se pueden codificar en formatos especializados que son representaciones más eficientes de los datos. Por ejemplo:\n\nEn química computacional, el formato de archivo de datos estructurales (SDF) es una herramienta para tomar estructuras químicas y codificarlas en un formato con el que sea computacionalmente eficiente trabajar.\nLos datos que tienen una gran cantidad de valores iguales (como ceros para datos binarios) se pueden almacenar en un formato de matriz dispersa. Este formato puede reducir el tamaño de los datos y permitir técnicas computacionales más eficientes.\n\nEstos formatos son ventajosos cuando el problema tiene un alcance adecuado y los posibles métodos de procesamiento de datos están bien definidos y son adecuados para dicho formato.2 Sin embargo, una vez que se violan dichas restricciones, los formatos de datos especializados son menos útiles. Por ejemplo, si realizamos una transformación de los datos que los convierte en números fraccionarios, la salida ya no es escasa; La representación matricial dispersa es útil para un paso algorítmico específico en el modelado, pero a menudo esto no es cierto antes o después de ese paso específico.\n\nUna estructura de datos especializada no es lo suficientemente flexible para un flujo de trabajo de modelado completo como lo es una estructura de datos común.\n\nUna característica importante del tibble producido por rsample es que la columna splits es una lista. En este caso, cada elemento de la lista tiene el mismo tipo de objeto: un objeto rsplit que contiene información sobre qué filas de mtcars pertenecen a la muestra de arranque. Las columnas de lista pueden ser muy útiles en el análisis de datos y, como se verá a lo largo de este libro, son importantes para los modelos ordenados.\n\n\n2.1.3 Diseño para la tubería y programación funcional.\nEl operador de canalización magrittr (%&gt;%) es una herramienta para encadenar una secuencia de funciones R.3 Para demostrarlo, considere los siguientes comandos que ordenan un marco de datos y luego conserve las primeras 10 filas:\n\nsmall_mtcars &lt;- arrange(mtcars, gear)\nsmall_mtcars &lt;- slice(small_mtcars, 1:10)\n\n# or more compactly: \nsmall_mtcars &lt;- slice(arrange(mtcars, gear), 1:10)\n\nEl operador de tubería sustituye el valor del lado izquierdo del operador como primer argumento del lado derecho, por lo que podemos implementar el mismo resultado que antes con:\n\nsmall_mtcars &lt;- \n  mtcars %&gt;% \n  arrange(gear) %&gt;% \n  slice(1:10)\n\nLa versión canalizada de esta secuencia es más legible; esta legibilidad aumenta a medida que se agregan más operaciones a una secuencia. Este enfoque de programación funciona en este ejemplo porque todas las funciones que utilizamos devuelven la misma estructura de datos (un marco de datos) que luego es el primer argumento de la siguiente función. Esto es por diseño. Cuando sea posible, cree funciones que puedan incorporarse a un conjunto de operaciones.\nSi ha utilizado ggplot2, esto no es diferente a la superposición de componentes de la trama en un objeto ggplot con el operador +. Para hacer un diagrama de dispersión con una línea de regresión, la llamada inicial a ggplot() se aumenta con dos operaciones adicionales:\n\nlibrary(ggplot2)\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() + \n  geom_smooth(method = lm)\n\nSi bien es similar a la canalización dplyr, tenga en cuenta que el primer argumento de esta canalización es un conjunto de datos (mtcars) y que cada llamada de función devuelve un objeto ggplot. No todas las canalizaciones necesitan mantener los valores devueltos (objetos de trazado) iguales que el valor inicial (un marco de datos). El uso del operador de canalización con operaciones dplyr ha hecho que muchos usuarios de R esperen devolver un marco de datos cuando se utilizan canalizaciones; como se muestra con ggplot2, no tiene por qué ser así. Las canalizaciones son increíblemente útiles para modelar flujos de trabajo, pero el modelado de canalizaciones puede devolver, en lugar de un marco de datos, objetos como componentes del modelo.\nR tiene excelentes herramientas para crear, cambiar y operar funciones, lo que lo convierte en un excelente lenguaje para la programación funcional. Este enfoque puede reemplazar los bucles iterativos en muchas situaciones, como cuando una función devuelve un valor sin otros efectos secundarios.4\nVeamos un ejemplo. Suponga que está interesado en el logaritmo de la relación entre la eficiencia del combustible y el peso del automóvil. Para aquellos nuevos en R y/o que vienen de otros lenguajes de programación, un bucle puede parecer una buena opción:\n\nn &lt;- nrow(mtcars)\nratios &lt;- rep(NA_real_, n)\nfor (car in 1:n) {\n  ratios[car] &lt;- log(mtcars$mpg[car]/mtcars$wt[car])\n}\nhead(ratios)\n## [1] 2.081 1.988 2.285 1.896 1.693 1.655\n\nAquellos con más experiencia en R sabrán que existe una versión vectorizada mucho más simple y rápida que se puede calcular mediante:\n\nratios &lt;- log(mtcars$mpg/mtcars$wt)\n\nSin embargo, en muchos casos del mundo real, la operación de interés por elementos es demasiado compleja para una solución vectorizada. En tal caso, un buen enfoque es escribir una función para realizar los cálculos. Cuando diseñamos para programación funcional, es importante que la salida dependa sólo de las entradas y que la función no tenga efectos secundarios. Las violaciones de estas ideas en la siguiente función se muestran con comentarios:\n\ncompute_log_ratio &lt;- function(mpg, wt) {\n  log_base &lt;- getOption(\"log_base\", default = exp(1)) # obtiene datos externos\n  results &lt;- log(mpg/wt, base = log_base)\n  print(mean(results))                                # imprime en la consola\n  done &lt;&lt;- TRUE                                       # establece datos externos\n  results\n}\n\nUna mejor versión sería:\n\ncompute_log_ratio &lt;- function(mpg, wt, log_base = exp(1)) {\n  log(mpg/wt, base = log_base)\n}\n\nThe purrr package contains tools for functional programming. Let’s focus on the map() family of functions, which operates on vectors and always returns the same type of output. The most basic function, map(), always returns a list and uses the basic syntax of map(vector, function). For example, to take the square root of our data, we could:\n\nmap(head(mtcars$mpg, 3), sqrt)\n## [[1]]\n## [1] 4.583\n## \n## [[2]]\n## [1] 4.583\n## \n## [[3]]\n## [1] 4.775\n\nExisten variantes especializadas de map() que devuelven valores cuando sabemos o esperamos que la función genere uno de los tipos de vectores básicos. Por ejemplo, dado que la raíz cuadrada devuelve un número de doble precisión:\n\nmap_dbl(head(mtcars$mpg, 3), sqrt)\n## [1] 4.583 4.583 4.775\n\nTambién hay funciones de mapeo que operan en múltiples vectores:\n\nlog_ratios &lt;- map2_dbl(mtcars$mpg, mtcars$wt, compute_log_ratio)\nhead(log_ratios)\n## [1] 2.081 1.988 2.285 1.896 1.693 1.655\n\nLas funciones map() también permiten funciones temporales y anónimas definidas usando el carácter de tilde. Los valores de los argumentos son .x y .y para map2():\n\nmap2_dbl(mtcars$mpg, mtcars$wt, ~ log(.x/.y)) %&gt;% \n  head()\n## [1] 2.081 1.988 2.285 1.896 1.693 1.655\n\nEstos ejemplos han sido triviales pero, en secciones posteriores, se aplicarán a problemas más complejos.\n\nPara la programación funcional en el modelado ordenado, las funciones deben definirse de modo que funciones como map() puedan usarse para cálculos iterativos.",
    "crumbs": [
      "INTRODUCCIÓN",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Una Introducción Tidyverse</span>"
    ]
  },
  {
    "objectID": "02-tidyverse.html#sec-examples-of-tidyverse-syntax",
    "href": "02-tidyverse.html#sec-examples-of-tidyverse-syntax",
    "title": "2  Una Introducción Tidyverse",
    "section": "2.2 Ejemplos De Sintaxis De Tidyverse",
    "text": "2.2 Ejemplos De Sintaxis De Tidyverse\nComencemos nuestra discusión sobre la sintaxis de tidyverse explorando más profundamente qué es un tibble y cómo funcionan. Los tibbles tienen reglas ligeramente diferentes a los marcos de datos básicos en R. Por ejemplo, los tibbles funcionan naturalmente con nombres de columnas que no son nombres de variables sintácticamente válidos:\n\n# Quiere nombres válidos:\ndata.frame(`variable 1` = 1:2, two = 3:4)\n##   variable.1 two\n## 1          1   3\n## 2          2   4\n# Pero se puede obligar a utilizarlos con una opción adicional:\ndf &lt;- data.frame(`variable 1` = 1:2, two = 3:4, check.names = FALSE)\ndf\n##   variable 1 two\n## 1          1   3\n## 2          2   4\n\n# Pero los tibbles simplemente funcionan:\ntbbl &lt;- tibble(`variable 1` = 1:2, two = 3:4)\ntbbl\n## # A tibble: 2 × 2\n##   `variable 1`   two\n##          &lt;int&gt; &lt;int&gt;\n## 1            1     3\n## 2            2     4\n\nLos marcos de datos estándar permiten coincidencia parcial de argumentos para que el código que utiliza solo una parte de los nombres de las columnas siga funcionando. Tibbles evita que esto suceda, ya que puede provocar errores accidentales.\n\ndf$tw\n## [1] 3 4\n\ntbbl$tw\n## Warning: Unknown or uninitialised column: `tw`.\n## NULL\n\nTibbles también previene uno de los errores de R más comunes: eliminar dimensiones. Si un marco de datos estándar subconjunto de columnas en una sola columna, el objeto se convierte en un vector. Tibbles nunca hace esto:\n\ndf[, \"two\"]\n## [1] 3 4\n\ntbbl[, \"two\"]\n## # A tibble: 2 × 1\n##     two\n##   &lt;int&gt;\n## 1     3\n## 2     4\n\nHay otras ventajas al usar tibbles en lugar de marcos de datos, como una mejor impresión y más.5\nPara demostrar algo de sintaxis, usemos funciones tidyverse para leer datos que podrían usarse en el modelado. El conjunto de datos proviene del portal de datos de la ciudad de Chicago y contiene datos diarios sobre el número de pasajeros de las estaciones de trenes elevados de la ciudad. El conjunto de datos tiene columnas para:\n\nel identificador de la estación (númerico)\nel nombre de la estación (texto)\nla fecha (texto en formato mm/dd/yyyy)\nel día de la semana (texto)\nel número de pasajeros (númerico)\n\nNuestro canalización tidyverse llevará a cabo las siguientes tareas, en orden:\n\nUtilice el paquete tidyverse readr para leer los datos del sitio web de origen y convertirlos en un tibble. Para hacer esto, la función read_csv() puede determinar el tipo de datos leyendo un número inicial de filas. Alternativamente, si los nombres y tipos de las columnas ya se conocen, se puede crear una especificación de columna en R y pasarla a read_csv().\nFiltre los datos para eliminar algunas columnas que no son necesarias (como el ID de la estación) y cambie la columna “nombre de la estación” a “estación”. Para esto se utiliza la función select(). Al filtrar, utilice los nombres de las columnas o una función selectora dplyr. Al seleccionar nombres, se puede declarar un nuevo nombre de variable utilizando el formato de argumento nuevo_nombre = antiguo_nombre.\nConvierta el campo de fecha al formato de fecha R usando la función mdy() del paquete lubridate. También convertimos los números de pasajeros a miles. Ambos cálculos se ejecutan utilizando la función dplyr::mutate().\nUtilice el número máximo de viajes para cada estación y combinación de días. Esto mitiga el problema de una pequeña cantidad de días que tienen más de un registro de número de pasajeros en determinadas estaciones. Agrupamos los datos de número de pasajeros por estación y día, y luego resumimos dentro de cada una de las combinaciones únicas 1999 con la estadística máxima.\n\nEl código tidyverse para estos pasos es:\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\nurl &lt;- \"https://data.cityofchicago.org/api/views/5neh-572f/rows.csv?accessType=DOWNLOAD&bom=true&format=true\"\n\nall_stations &lt;- \n  # Paso 1: leer los datos.\n  read_csv(url) %&gt;% \n  # Paso 2: filtrar columnas y cambiar el nombre de la estación\n  dplyr::select(station = stationname, date, rides) %&gt;% \n  # Paso 3: convierta el campo de fecha de caracteres a una codificación de fecha.\n  # Además, coloque los datos en unidades de 1K viajes.\n  mutate(date = mdy(date), rides = rides / 1000) %&gt;% \n  # Paso 4: resuma los múltiples registros utilizando el máximo.\n  group_by(date, station) %&gt;% \n  summarize(rides = max(rides), .groups = \"drop\")\n\nEsta canalización de operaciones ilustra por qué tidyverse es popular. Se utiliza una serie de manipulaciones de datos que tienen funciones simples y fáciles de entender para cada transformación; La serie se presenta de forma simplificada y legible. La atención se centra en cómo el usuario interactúa con el software. Este enfoque permite que más personas aprendan R y alcancen sus objetivos de análisis, y adoptar estos mismos principios para modelar en R tiene los mismos beneficios.",
    "crumbs": [
      "INTRODUCCIÓN",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Una Introducción Tidyverse</span>"
    ]
  },
  {
    "objectID": "02-tidyverse.html#resumen-del-capítulo",
    "href": "02-tidyverse.html#resumen-del-capítulo",
    "title": "2  Una Introducción Tidyverse",
    "section": "2.3 Resumen Del Capítulo",
    "text": "2.3 Resumen Del Capítulo\nEste capítulo presentó tidyverse, centrándose en las aplicaciones para modelado y cómo los principios de diseño de tidyverse informan el marco de trabajo de tidymodels. Piense en el marco de tidymodels como una aplicación de los principios de tidyverse al dominio de la construcción de modelos. Describimos las diferencias en las convenciones entre tidyverse y base R, e introdujimos dos componentes importantes del sistema tidyverse, tibbles y el operador de canalización %&gt;%. La limpieza y el procesamiento de datos pueden parecer mundanos a veces, pero estas tareas son importantes para el modelado en el mundo real; Ilustramos cómo usar las funciones tibbles, pipe y tidyverse en un ejercicio de ejemplo de importación y procesamiento de datos.\n\n\n\n\nWickham, H, M Averick, J Bryan, W Chang, L McGowan, R François, G Grolemund, et al. 2019. «Welcome to the Tidyverse». Journal of Open Source Software 4 (43).\n\n\nWickham, H, y G Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media, Inc.",
    "crumbs": [
      "INTRODUCCIÓN",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Una Introducción Tidyverse</span>"
    ]
  },
  {
    "objectID": "02-tidyverse.html#footnotes",
    "href": "02-tidyverse.html#footnotes",
    "title": "2  Una Introducción Tidyverse",
    "section": "",
    "text": "Si nunca antes ha visto :: en el código R, es un método explícito para llamar a una función. El valor del lado izquierdo es el espacio de nombres donde reside la función (normalmente un nombre de paquete). El lado derecho es el nombre de la función. En los casos en que dos paquetes usan el mismo nombre de función, esta sintaxis garantiza que se llame a la función correcta.↩︎\nNo todos los algoritmos pueden aprovechar representaciones dispersas de datos. En tales casos, una matriz dispersa se debe convertir a un formato más convencional antes de continuar.↩︎\nEn R 4.1.0, también se introdujo un operador de canalización nativo |&gt;. En este libro, utilizamos la canalización magrittr ya que los usuarios de versiones anteriores de R no tendrán la nueva canalización nativa.↩︎\nEjemplos de efectos secundarios de funciones podrían incluir cambiar datos globales o imprimir un valor.↩︎\nEl capítulo 10 de Wickham y Grolemund (2016) tiene más detalles sobre tibbles.↩︎",
    "crumbs": [
      "INTRODUCCIÓN",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Una Introducción Tidyverse</span>"
    ]
  },
  {
    "objectID": "03-base-r.html",
    "href": "03-base-r.html",
    "title": "3  Una Revisión De Los Fundamentos Del Modelado Con R",
    "section": "",
    "text": "3.1 Un Ejemplo\nPara demostrar algunos fundamentos del modelado en base R, usemos datos experimentales de McDonald (2009), a través de Mangiafico (2015), sobre la relación entre la temperatura ambiente y la tasa de chirridos de los grillos por minuto. Se recolectaron datos para dos especies: O. exclamationis y O. niveus. Los datos están contenidos en un marco de datos llamado “crickets” (grillos en español) con un total de puntos de datos “r nrow(crickets)”. Estos datos se muestran en Figura 3.1 usando el siguiente código ggplot2.\nlibrary(tidyverse)\n\ndata(crickets, package = \"modeldata\")\nnames(crickets)\n\n# Grápica de la temperatura en el eje x, la tasa de chirrido en el eje y. Los elementos\n# de la gráfica se colorean de forma diferente para cada especie:\nggplot(crickets, \n       aes(x = temp, y = rate, color = species, pch = species, lty = species)) + \n  # Traza puntos para cada punto de datos y color por especie.\n  geom_point(size = 2) + \n  # Muestra un ajuste de modelo lineal simple creado por separado para cada especie:\n  geom_smooth(method = lm, se = FALSE, alpha = 0.5) + \n  scale_color_brewer(palette = \"Paired\") +\n  labs(x = \"Temperatura (C)\", y = \"Tasa de chirrido (por minuto)\")\n## [1] \"species\" \"temp\"    \"rate\"\n\n\n\n\n\n\n\nFigura 3.1: Relación entre la tasa de chirrido y la temperatura de dos especies diferentes de grillos\nLos datos muestran tendencias bastante lineales para cada especie. Para una temperatura dada, O. exclamationis parece chirriar más por minuto que las otras especies. Para un modelo inferencial, los investigadores podrían haber especificado las siguientes hipótesis nulas antes de ver los datos:\nPuede haber algún valor científico o práctico en predecir la tasa de chirrido, pero en este ejemplo nos centraremos en la inferencia.\nPara ajustar un modelo lineal ordinario en R, se usa comúnmente la función lm(). Los argumentos importantes de esta función son una fórmula modelo y un marco de datos que contiene los datos. La fórmula es simbólica. Por ejemplo, la fórmula simple:\nespecifica que la tasa de chirrido es el resultado (ya que está en el lado izquierdo de la tilde ~) y que el valor de la temperatura es el predictor.1 Supongamos que los datos contienen la hora del día en el que se obtuvieron las medidas en una columna llamada hora. La formula:\nno sumaría los valores de tiempo y temperatura. Esta fórmula representaría simbólicamente que la temperatura y el tiempo deben agregarse como efectos principales separados al modelo. Un efecto principal es un término del modelo que contiene una única variable predictiva.\nNo hay mediciones de tiempo en estos datos pero las especies se pueden agregar al modelo de la misma manera:\nLa especie no es una variable cuantitativa; en el marco de datos, se representa como una columna de factor con niveles \"O. exclamationis\" y \"O. niveus\". La gran mayoría de las funciones del modelo no pueden operar con datos no numéricos. Para las especies, el modelo necesita codificar los datos de las especies en un formato numérico. El enfoque más común es utilizar variables indicadoras (también conocidas como variables ficticias) en lugar de los valores cualitativos originales. En este caso, dado que la especie tiene dos valores posibles, la fórmula del modelo codificará automáticamente esta columna como numérica agregando una nueva columna que tiene un valor de cero cuando la especie es \"O. exclamationis\" y un valor de uno cuando la especie es \"O. niveus\". La maquinaria de fórmulas subyacente convierte automáticamente estos valores para el conjunto de datos utilizado para crear el modelo, así como para cualquier punto de datos nuevo (por ejemplo, cuando el modelo se utiliza para predicción).\nLa fórmula del modelo rate ~ temp + species crea un modelo con diferentes intersecciones en el eje y para cada especie; las pendientes de las líneas de regresión también podrían ser diferentes para cada especie. Para adaptarse a esta estructura, se puede agregar un término de interacción al modelo. Esto se puede especificar de varias maneras diferentes, y la más básica utiliza los dos puntos:\nAdemás de la conveniencia de crear variables indicadoras automáticamente, la fórmula ofrece algunas otras sutilezas:\nVolviendo a nuestros grillos cantando, usemos un modelo de interacción bidireccional. En este libro, utilizamos el sufijo _fit para objetos R que son modelos ajustados.\ninteraction_fit &lt;-  lm(rate ~ (temp + species)^2, data = crickets) \n\n# Para imprimir un breve resumen del modelo:\ninteraction_fit\n## \n## Call:\n## lm(formula = rate ~ (temp + species)^2, data = crickets)\n## \n## Coefficients:\n##           (Intercept)                   temp       speciesO. niveus  \n##               -11.041                  3.751                 -4.348  \n## temp:speciesO. niveus  \n##                -0.234\nEste resultado es un poco difícil de leer. Para las variables indicadoras de especies, R combina el nombre de la variable (species) con el nivel del factor (O. niveus) sin delimitador.\nAntes de entrar en resultados inferenciales para este modelo, se debe evaluar el ajuste mediante gráficos de diagnóstico. Podemos usar el método plot() para objetos lm. Este método produce un conjunto de cuatro gráficos para el objeto, cada uno de los cuales muestra diferentes aspectos del ajuste, como se muestra en Figura 3.2.\n# Coloca dos gráficas una juunto a la otra:\npar(mfrow = c(1, 2))\n\n# Muestra residuos frente a valores previstos:\nplot(interaction_fit, which = 1)\n\n# Una gráfica de cuantiles normales sobre los residuos:\nplot(interaction_fit, which = 2)\nFigura 3.2: Gráficos de diagnóstico residuales para el modelo lineal con interacciones, que parecen lo suficientemente razonables para realizar análisis inferenciales.\nNuestra siguiente tarea con los grillos es evaluar si es necesaria la inclusión del término de interacción. El enfoque más apropiado para este modelo es recalcular el modelo sin el término de interacción y utilizar el método anova().\n# Montar un modelo reducido:\nmain_effect_fit &lt;-  lm(rate ~ temp + species, data = crickets) \n\n# Comparar los modelos\nanova(main_effect_fit, interaction_fit)\n## Analysis of Variance Table\n## \n## Model 1: rate ~ temp + species\n## Model 2: rate ~ (temp + species)^2\n##   Res.Df  RSS Df Sum of Sq    F Pr(&gt;F)\n## 1     28 89.3                         \n## 2     27 85.1  1      4.28 1.36   0.25\nEsta prueba estadística genera un valor p de 0.25. Esto implica que falta evidencia contra la hipótesis nula de que el modelo no necesita el término de interacción. Por esta razón, realizaremos más análisis del modelo sin la interacción.\nLos gráficos residuales deben reevaluarse para asegurarnos de que nuestros supuestos teóricos sean lo suficientemente válidos como para confiar en los valores p producidos por el modelo (los gráficos no se muestran aquí, pero alerta de spoiler: lo son).\nPodemos usar el método summary() para inspeccionar los coeficientes, errores estándar y valores p de cada término del modelo:\nsummary(main_effect_fit)\n## \n## Call:\n## lm(formula = rate ~ temp + species, data = crickets)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -3.013 -1.130 -0.391  0.965  3.780 \n## \n## Coefficients:\n##                  Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)       -7.2109     2.5509   -2.83   0.0086 ** \n## temp               3.6028     0.0973   37.03  &lt; 2e-16 ***\n## speciesO. niveus -10.0653     0.7353  -13.69  6.3e-14 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.79 on 28 degrees of freedom\n## Multiple R-squared:  0.99,   Adjusted R-squared:  0.989 \n## F-statistic: 1.33e+03 on 2 and 28 DF,  p-value: &lt;2e-16\nLa tasa de chirrido para cada especie aumenta en 3.6 chirridos a medida que la temperatura aumenta en un solo grado. Este término muestra una fuerte significación estadística como lo demuestra el valor p. El término de especie tiene un valor de -10.07. Esto indica que, en todos los valores de temperatura, O. niveus tiene una frecuencia de chirrido que es aproximadamente 10 menos chirridos por minuto que O. exclamationis. De manera similar al término de temperatura, el efecto de especie se asocia con un valor p muy pequeño.\nEl único problema en este análisis es el valor de la intersección. Indica que a 0° C, hay chirridos negativos por minuto para ambas especies. Si bien esto no tiene sentido, los datos solo llegan a 17.2° C e interpretar el modelo a 0° C sería una extrapolación. Esta sería una mala idea. Dicho esto, el ajuste del modelo es bueno dentro del rango aplicable de los valores de temperatura; las conclusiones deben limitarse al rango de temperatura observado.\nSi necesitáramos estimar la tasa de chirrido a una temperatura que no se observó en el experimento, podríamos usar el método predecit(). Toma el objeto modelo y un marco de datos de nuevos valores para la predicción. Por ejemplo, el modelo estima la tasa de chirrido para O. exclamationis para temperaturas entre 15° C y 20° C se puede calcular mediante:\nnew_values &lt;- data.frame(species = \"O. exclamationis\", temp = 15:20)\npredict(main_effect_fit, new_values)\n##     1     2     3     4     5     6 \n## 46.83 50.43 54.04 57.64 61.24 64.84\nSi bien este análisis obviamente no ha sido una demostración exhaustiva de las capacidades de modelado de R, sí resalta algunas características importantes para el resto de este libro:\nFinalmente, como se mencionó anteriormente, este marco se publicó por primera vez en 1992. La mayoría de estas ideas y métodos se desarrollaron en ese período, pero siguen siendo notablemente relevantes hasta el día de hoy. Destaca que el lenguaje S y, por extensión, R, ha sido diseñado para el análisis de datos desde sus inicios.",
    "crumbs": [
      "INTRODUCCIÓN",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Una Revisión De Los Fundamentos Del Modelado Con R</span>"
    ]
  },
  {
    "objectID": "03-base-r.html#un-ejemplo",
    "href": "03-base-r.html#un-ejemplo",
    "title": "3  Una Revisión De Los Fundamentos Del Modelado Con R",
    "section": "",
    "text": "La temperatura no tiene ningún efecto sobre la tasa de chirrido.\nNo hay diferencias entre la tasa de chirrido de las especies.\n\n\n\nrate ~ temp\n\nrate ~ temp + time\n\n\nrate ~ temp + species\n\n\nSupongamos que hubiera cinco especies en lugar de dos. La fórmula del modelo, en este caso, crearía cuatro columnas binarias que son indicadores binarios para cuatro de las especies. El nivel de referencia del factor (es decir, el primer nivel) siempre se deja fuera del conjunto de predictores. La idea es que, si se conocen los valores de las cuatro variables indicadoras, se pueda determinar el valor de la especie. Analizamos las variables de los indicadores binarios con más detalle en la Sección 8.4.1.\n\n\nrate ~ temp + species + temp:species\n\n# Se puede utilizar un atajo para expandir todas las interacciones que contienen\n# dos variables:\nrate ~ (temp + species) ^ 2\n\n# Otro atajo para ampliar los factores para incluir todos las posibles\n# interacciones (equivalentes para este ejemplo):\nrate ~ temp * species\n\n\nLas funciones en línea se pueden utilizar en la fórmula. Por ejemplo, para utilizar el registro natural de la temperatura, podemos crear la fórmula rate ~ log(temp). Dado que la fórmula es simbólica por defecto, la matemática literal también se puede aplicar a los predictores usando la función de identidad I(). Para usar unidades Fahrenheit, la fórmula podría ser rate ~ I( (temp * 9/5) + 32 ) para convertir de Celsius.\nR tiene muchas funciones que son útiles dentro de las fórmulas. Por ejemplo, “poly(x, 3)” agrega términos lineales, cuadráticos y cúbicos para “x” al modelo como efectos principales. El paquete splines también tiene varias funciones para crear términos spline no lineales en la fórmula.\nPara conjuntos de datos donde hay muchos predictores, el acceso directo al período está disponible. El punto representa los efectos principales para todas las columnas que no están en el lado izquierdo de la tilde. El uso de ~ (.)^3 agregaría efectos principales, así como todas las interacciones de dos y tres variables, al modelo.\n\n\n\n\n\n\n\n\nCuando se trata de los detalles técnicos de la evaluación de expresiones, R es perezoso (en lugar de entusiasta). Esto significa que las funciones de ajuste del modelo normalmente calculan las cantidades mínimas posibles en el último momento posible. Por ejemplo, si está interesado en la tabla de coeficientes para cada término del modelo, esto no se calcula automáticamente con el modelo, sino que se calcula mediante el método summary().\n\n\n\n\n\n\n\n\n\n\n\n\nTenga en cuenta que el valor no numérico de species se pasa al método de predicción, a diferencia de la variable indicadora binaria numérica.\n\n\n\nEl lenguaje tiene una sintaxis expresiva para especificar términos de modelo tanto para modelos simples como para modelos bastante complejos.\nEl método de la fórmula R tiene muchas ventajas para el modelado que también se aplican a datos nuevos cuando se generan predicciones.\nExisten numerosas funciones auxiliares (por ejemplo, anova(), summary() y predict()) que puede utilizar para realizar cálculos específicos después de crear el modelo ajustado.",
    "crumbs": [
      "INTRODUCCIÓN",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Una Revisión De Los Fundamentos Del Modelado Con R</span>"
    ]
  },
  {
    "objectID": "03-base-r.html#sec-formula",
    "href": "03-base-r.html#sec-formula",
    "title": "3  Una Revisión De Los Fundamentos Del Modelado Con R",
    "section": "3.2 ¿Qué Hace La Fórmula R?",
    "text": "3.2 ¿Qué Hace La Fórmula R?\nMuchos paquetes de modelado utilizan la fórmula del modelo R. Suele tener múltiples propósitos:\n\nLa fórmula define las columnas que utiliza el modelo.\nLa maquinaria R estándar utiliza la fórmula para codificar las columnas en un formato apropiado.\nLos roles de las columnas están definidos por la fórmula.\n\nEn su mayor parte, la comprensión de los profesionales sobre lo que hace la fórmula está dominada por el último propósito. Nuestro enfoque al escribir una fórmula suele ser declarar cómo se deben usar las columnas. Por ejemplo, la especificación anterior que analizamos configura predictores que se utilizarán de una manera específica:\n(temp + species)^2\nNuestro enfoque, al ver esto, es que hay dos predictores y el modelo debe contener sus efectos principales y las interacciones bidireccionales. Sin embargo, esta fórmula también implica que, dado que species es un factor, también se deben crear columnas de variables indicadoras para este predictor (ver Sección 8.4.1) y multiplicar esas columnas por la columna temp para crear el interacciones. Esta transformación representa nuestro segundo punto sobre codificación; la fórmula también define cómo se codifica cada columna y puede crear columnas adicionales que no están en los datos originales.\n\nEste es un punto importante que surgirá varias veces en este texto, especialmente cuando analicemos la ingeniería de funciones más compleja en el Capítulo 8 y más allá. La fórmula en R tiene algunas limitaciones y nuestros enfoques para superarlas se enfrentan a los tres aspectos.",
    "crumbs": [
      "INTRODUCCIÓN",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Una Revisión De Los Fundamentos Del Modelado Con R</span>"
    ]
  },
  {
    "objectID": "03-base-r.html#sec-tidiness-modeling",
    "href": "03-base-r.html#sec-tidiness-modeling",
    "title": "3  Una Revisión De Los Fundamentos Del Modelado Con R",
    "section": "3.3 ¿Por Qué El Orden Es Importante Para Modelar?",
    "text": "3.3 ¿Por Qué El Orden Es Importante Para Modelar?\nUno de los puntos fuertes de R es que anima a los desarrolladores a crear una interfaz de usuario que se ajuste a sus necesidades. A modo de ejemplo, aquí se muestran tres métodos comunes para crear un diagrama de dispersión de dos variables numéricas en un marco de datos llamado plot_data:\n\nplot(plot_data$x, plot_data$y)\n\nlibrary(lattice)\nxyplot(y ~ x, data = plot_data)\n\nlibrary(ggplot2)\nggplot(plot_data, aes(x = x, y = y)) + geom_point()\n\nEn estos tres casos, grupos separados de desarrolladores idearon tres interfaces distintas para la misma tarea. Cada uno tiene ventajas y desventajas.\nEn comparación, la Guía del desarrollador de Python defiende la noción de que, al abordar un problema:\n\n“Debería haber una, y preferiblemente sólo una, manera obvia de hacerlo.”\n\nR es bastante diferente de Python a este respecto. Una ventaja de la diversidad de interfaces de R es que puede evolucionar con el tiempo y adaptarse a diferentes necesidades de diferentes usuarios.\nDesafortunadamente, parte de la diversidad sintáctica se debe a un enfoque en las necesidades de la persona que desarrolla el código en lugar de las necesidades de la persona que usa el código. Las inconsistencias entre paquetes pueden ser un obstáculo para los usuarios de R.\nSuponga que su proyecto de modelado tiene un resultado con dos clases. Existe una variedad de modelos estadísticos y de aprendizaje automático entre los que puede elegir. Para producir una estimación de probabilidad de clase para cada muestra, es común que una función de modelo tenga un método predict() correspondiente. Sin embargo, existe una heterogeneidad significativa en los valores de los argumentos utilizados por esos métodos para hacer predicciones de probabilidad de clase; esta heterogeneidad puede resultar difícil de navegar incluso para los usuarios experimentados. En Tabla 3.1 se muestra una muestra de estos valores de argumentos para diferentes modelos.\n\n\n\nTabla 3.1: Nombres de argumentos heterogéneos para diferentes funciones de modelado.\n\n\n\n\n\n\nFunción\nPaquete\nCódigo\n\n\n\n\nlda()\nMASS\npredict(object)\n\n\nglm()\nstats\npredict(object, type = \"response\")\n\n\ngbm()\ngbm\npredict(object, type = \"response\", n.trees)\n\n\nmda()\nmda\npredict(object, type = \"posterior\")\n\n\nrpart()\nrpart\npredict(object, type = \"prob\")\n\n\nvarious\nRWeka\npredict(object, type = \"probability\")\n\n\nlogitboost()\nLogitBoost\npredict(object, type = \"raw\", nIter)\n\n\npamr.train()\npamr\npamr.predict(object, type = \"posterior\")\n\n\n\n\n\n\n\n\n\nTenga en cuenta que el último ejemplo tiene una función personalizada para hacer predicciones en lugar de utilizar la interfaz predict() más común (el método genérico predict()). Esta falta de coherencia es una barrera para el uso diario de R para modelado.\nComo otro ejemplo de imprevisibilidad, el lenguaje R tiene convenciones para datos faltantes que se manejan de manera inconsistente. La regla general es que los datos faltantes propagan más datos faltantes; el promedio de un conjunto de valores al que le falta un punto de datos también falta, y así sucesivamente. Cuando los modelos hacen predicciones, la gran mayoría requiere que todos los predictores tengan valores completos. Hay varias opciones integradas en R en este punto con la función genérica na.action(). Esto establece la política sobre cómo debe comportarse una función si faltan valores. Las dos políticas más comunes son na.fail() y na.omit(). El primero produce un error si hay datos faltantes, mientras que el segundo elimina los datos faltantes antes de los cálculos mediante eliminación entre mayúsculas y minúsculas. De nuestro ejemplo anterior:\n\n# Añade un valor faltante al conjunto de predicción\nnew_values$temp[1] &lt;- NA\n\n# El método de predicción para `lm` por defecto es `na.pass`:\npredict(main_effect_fit, new_values)\n##     1     2     3     4     5     6 \n##    NA 50.43 54.04 57.64 61.24 64.84\n\n# Alternativamente \npredict(main_effect_fit, new_values, na.action = na.fail)\n## Error in na.fail.default(structure(list(temp = c(NA, 16L, 17L, 18L, 19L, : missing values in object\n\npredict(main_effect_fit, new_values, na.action = na.omit)\n##     2     3     4     5     6 \n## 50.43 54.04 57.64 61.24 64.84\n\nDesde el punto de vista del usuario, na.omit() puede ser problemático. En nuestro ejemplo, new_values tiene filas 6 pero solo 5 se devolvería con na.omit(). Para ajustar esto, el usuario tendría que determinar qué fila tenía el valor faltante e intercalar un valor faltante en el lugar apropiado si las predicciones se fusionaran en new_values.2 Si bien es poco común que una función de predicción utiliza na.omit() como política de datos faltantes, esto ocurre. Los usuarios que han determinado esto como la causa de un error en su código lo encuentran bastante memorable.\nPara resolver los problemas de uso descritos aquí, los paquetes tidymodels tienen un conjunto de objetivos de diseño. La mayoría de los objetivos de diseño de tidymodels se incluyen en la rúbrica existente de “Diseño para humanos” de tidyverse (Wickham et al. 2019), pero con aplicaciones específicas para código de modelado. Hay algunos objetivos de diseño de tidymodels adicionales que complementan los del tidyverse. Algunos ejemplos:\n\nR tiene excelentes capacidades para la programación orientada a objetos, y las usamos en lugar de crear nuevos nombres de funciones (como una nueva función hipotética predict_samples()).\nLos valores predeterminados sensatos son muy importantes. Además, las funciones no deberían tener argumentos predeterminados cuando es más apropiado obligar al usuario a tomar una decisión (por ejemplo, el argumento del nombre del archivo para read_csv()).\nDe manera similar, los valores de los argumentos cuyo valor predeterminado pueda derivarse de los datos deberían serlo. Por ejemplo, para glm(), el argumento family podría verificar el tipo de datos en el resultado y, si no se proporcionó ninguna family, se podría determinar internamente un valor predeterminado.\nLas funciones deben tomar las estructuras de datos que tienen los usuarios en lugar de la estructura de datos que desean los desarrolladores. Por ejemplo, la única interfaz de una función modelo no debe limitarse a matrices. Con frecuencia, los usuarios tendrán predictores no numéricos, como factores.\n\nMuchas de estas ideas se describen en las pautas de tidymodels para la implementación de modelos.3 En los capítulos siguientes, ilustraremos ejemplos de problemas existentes, junto con sus soluciones.\n\nAlgunos paquetes R existentes proporcionan una interfaz unificada para armonizar estas API de modelado heterogéneas, como caret y mlr. El marco tidymodels es similar a estos en cuanto a que adopta una unificación de la interfaz de funciones, así como también exige coherencia en los nombres de las funciones y los valores de retorno. Se diferencia en sus obstinados objetivos de diseño y en su implementación de modelado, que se analizan en detalle a lo largo de este libro.\n\nLa función broom::tidy(), que utilizamos a lo largo de este libro, es otra herramienta para estandarizar la estructura de los objetos R. Puede devolver muchos tipos de objetos R en un formato más utilizable. Por ejemplo, supongamos que los predictores se seleccionan en función de su correlación con la columna de resultados. Usando purrr::map(), los resultados de cor.test() se pueden devolver en una lista para cada predictor:\n\ncorr_res &lt;- map(mtcars %&gt;% select(-mpg), cor.test, y = mtcars$mpg)\n\n# El primero de diez resultados en el vector:\ncorr_res[[1]]\n## \n##  Pearson's product-moment correlation\n## \n## data:  .x[[i]] and mtcars$mpg\n## t = -8.9, df = 30, p-value = 6e-10\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  -0.9258 -0.7163\n## sample estimates:\n##     cor \n## -0.8522\n\nSi queremos utilizar estos resultados en una gráfica, el formato estándar de los resultados de las pruebas de hipótesis no es muy útil. El método tidy() puede devolver esto como un tibble con nombres estandarizados:\n\nlibrary(broom)\n\ntidy(corr_res[[1]])\n## # A tibble: 1 × 8\n##   estimate statistic  p.value parameter conf.low conf.high method        alternative\n##      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;      \n## 1   -0.852     -8.92 6.11e-10        30   -0.926    -0.716 Pearson's pr… two.sided\n\nEstos resultados se pueden “apilar” y agregar a un ggplot(), como se muestra en Figura 3.3.\n\ncorr_res %&gt;% \n  # Convierte cada uno a un formato ordenado; `map_dfr()` apila los marcos de datos \n  map_dfr(tidy, .id = \"predictor\") %&gt;% \n  ggplot(aes(x = fct_reorder(predictor, estimate))) + \n  geom_point(aes(y = estimate)) + \n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .1) +\n  labs(x = NULL, y = \"Correlación con mpg\")\n\n\n\n\n\n\n\n\n\nFigura 3.3: Correlaciones (e intervalos de confianza del 95%) entre los predictores y el resultado en el conjunto de datos mtcars\n\n\n\n\n\nEs posible crear un gráfico de este tipo utilizando las funciones básicas del lenguaje R, pero reformatear automáticamente los resultados genera un código más conciso con menos posibilidades de errores.",
    "crumbs": [
      "INTRODUCCIÓN",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Una Revisión De Los Fundamentos Del Modelado Con R</span>"
    ]
  },
  {
    "objectID": "03-base-r.html#combinando-modelos-base-r-y-tidyverse",
    "href": "03-base-r.html#combinando-modelos-base-r-y-tidyverse",
    "title": "3  Una Revisión De Los Fundamentos Del Modelado Con R",
    "section": "3.4 Combinando Modelos Base R Y Tidyverse",
    "text": "3.4 Combinando Modelos Base R Y Tidyverse\nLas funciones de modelado de R del lenguaje central u otros paquetes de R se pueden usar junto con tidyverse, especialmente con los paquetes dplyr, purrr y tidyr. Por ejemplo, si quisiéramos ajustar modelos separados para cada especie de grillo, primero podemos desglosar los datos del grillo en esta columna usando dplyr::group_nest():\n\nsplit_by_species &lt;- \n  crickets %&gt;% \n  group_nest(species) \nsplit_by_species\n## # A tibble: 2 × 2\n##   species                        data\n##   &lt;fct&gt;            &lt;list&lt;tibble[,2]&gt;&gt;\n## 1 O. exclamationis           [14 × 2]\n## 2 O. niveus                  [17 × 2]\n\nLa columna data contiene las columnas rate y temp de crickets en una columna de lista. A partir de esto, la función purrr::map() puede crear modelos individuales para cada especie:\n\nmodel_by_species &lt;- \n  split_by_species %&gt;% \n  mutate(model = map(data, ~ lm(rate ~ temp, data = .x)))\nmodel_by_species\n## # A tibble: 2 × 3\n##   species                        data model \n##   &lt;fct&gt;            &lt;list&lt;tibble[,2]&gt;&gt; &lt;list&gt;\n## 1 O. exclamationis           [14 × 2] &lt;lm&gt;  \n## 2 O. niveus                  [17 × 2] &lt;lm&gt;\n\nPara recopilar los coeficientes de cada uno de estos modelos, use broom::tidy() para convertirlos a un formato de marco de datos consistente para que se puedan desanidar:\n\nmodel_by_species %&gt;% \n  mutate(coef = map(model, tidy)) %&gt;% \n  select(species, coef) %&gt;% \n  unnest(cols = c(coef))\n## # A tibble: 4 × 6\n##   species          term        estimate std.error statistic  p.value\n##   &lt;fct&gt;            &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 O. exclamationis (Intercept)   -11.0      4.77      -2.32 3.90e- 2\n## 2 O. exclamationis temp            3.75     0.184     20.4  1.10e-10\n## 3 O. niveus        (Intercept)   -15.4      2.35      -6.56 9.07e- 6\n## 4 O. niveus        temp            3.52     0.105     33.6  1.57e-15\n\n\nLas columnas de lista pueden ser muy poderosas en proyectos de modelado. Las columnas de lista proporcionan contenedores para cualquier tipo de objetos R, desde un modelo ajustado hasta la importante estructura del marco de datos.",
    "crumbs": [
      "INTRODUCCIÓN",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Una Revisión De Los Fundamentos Del Modelado Con R</span>"
    ]
  },
  {
    "objectID": "03-base-r.html#el-metapaquete-tidymodels",
    "href": "03-base-r.html#el-metapaquete-tidymodels",
    "title": "3  Una Revisión De Los Fundamentos Del Modelado Con R",
    "section": "3.5 El Metapaquete Tidymodels",
    "text": "3.5 El Metapaquete Tidymodels\nEl tidyverse (Capítulo 2) está diseñado como un conjunto de paquetes R modulares, cada uno con un alcance bastante limitado. El marco tidymodels sigue un diseño similar. Por ejemplo, el paquete rsample se centra en la división y el remuestreo de datos. Aunque los métodos de remuestreo son críticos para otras actividades de modelado (por ejemplo, medir el desempeño), residen en un solo paquete y las métricas de desempeño están contenidas en un paquete diferente e independiente, yardstick. Hay muchos beneficios al adoptar esta filosofía de paquetes modulares, desde una implementación de modelos menos inflada hasta un mantenimiento de paquetes más fluido.\nLa desventaja de esta filosofía es que hay muchos paquetes en el marco de tidymodels. Para compensar esto, el paquete tidymodels (que puedes considerar como un metapaquete como el paquete tidyverse) carga un conjunto central de paquetes tidymodels y tidyverse. Al cargar el paquete se muestran qué paquetes están adjuntos:\n\nlibrary(tidymodels)\n## ── Attaching packages ─────────────────────────────────────────── tidymodels 1.2.0 ──\n## ✔ dials        1.2.1      ✔ rsample      1.2.1 \n## ✔ infer        1.0.7      ✔ tune         1.2.1 \n## ✔ modeldata    1.3.0      ✔ workflows    1.1.4 \n## ✔ parsnip      1.2.1      ✔ workflowsets 1.1.0 \n## ✔ recipes      1.0.10     ✔ yardstick    1.3.1\n## ── Conflicts ────────────────────────────────────────────── tidymodels_conflicts() ──\n## ✖ scales::discard()        masks purrr::discard()\n## ✖ dplyr::filter()          masks stats::filter()\n## ✖ recipes::fixed()         masks stringr::fixed()\n## ✖ kableExtra::group_rows() masks dplyr::group_rows()\n## ✖ dplyr::lag()             masks stats::lag()\n## ✖ yardstick::spec()        masks readr::spec()\n## ✖ recipes::step()          masks stats::step()\n## • Learn how to get started at https://www.tidymodels.org/start/\n\nSi ha utilizado tidyverse, notará algunos nombres familiares ya que algunos paquetes de tidyverse, como dplyr y ggplot2, se cargan junto con los paquetes tidymodels. Ya hemos dicho que el marco tidymodels aplica los principios de tidyverse al modelado, pero el marco tidymodels también se basa literalmente en algunos de los paquetes tidyverse más fundamentales, como estos.\nLa carga del metapaquete también muestra si hay conflictos de nombres de funciones con paquetes cargados previamente. Como ejemplo de un conflicto de nombres, antes de cargar tidymodels, al invocar la función filter() se ejecutará la función en el paquete stats. Después de cargar tidymodels, ejecutará la función dplyr del mismo nombre.\nHay algunas formas de manejar los conflictos de nombres. La función se puede llamar con su espacio de nombres (por ejemplo, stats::filter()). Esta no es una mala práctica, pero hace que el código sea menos legible.\nOtra opción es utilizar el paquete conflicted. Podemos establecer una regla que permanezca vigente hasta el final de la sesión de R para garantizar que siempre se ejecute una función específica si no se proporciona ningún espacio de nombres en el código. Como ejemplo, si preferimos la versión dplyr de la función anterior:\n\nlibrary(conflicted)\nconflict_prefer(\"filter\", winner = \"dplyr\")\n\nPor conveniencia, tidymodels contiene una función que captura la mayoría de los conflictos de nombres comunes que podemos encontrar:\n\ntidymodels_prefer(quiet = FALSE)\n## [conflicted] Will prefer agua::refit over any other package.\n## [conflicted] Will prefer dials::Laplace over any other package.\n## [conflicted] Will prefer dials::max_rules over any other package.\n## [conflicted] Will prefer dials::neighbors over any other package.\n## [conflicted] Will prefer dials::prune over any other package.\n## [conflicted] Will prefer dials::smoothness over any other package.\n## [conflicted] Will prefer dplyr::collapse over any other package.\n## [conflicted] Will prefer dplyr::combine over any other package.\n## [conflicted] Will prefer dplyr::filter over any other package.\n## [conflicted] Will prefer dplyr::rename over any other package.\n## [conflicted] Will prefer dplyr::select over any other package.\n## [conflicted] Will prefer dplyr::slice over any other package.\n## [conflicted] Will prefer ggplot2::`%+%` over any other package.\n## [conflicted] Will prefer ggplot2::margin over any other package.\n## [conflicted] Will prefer parsnip::bart over any other package.\n## [conflicted] Will prefer parsnip::fit over any other package.\n## [conflicted] Will prefer parsnip::mars over any other package.\n## [conflicted] Will prefer parsnip::pls over any other package.\n## [conflicted] Will prefer purrr::cross over any other package.\n## [conflicted] Will prefer purrr::invoke over any other package.\n## [conflicted] Will prefer purrr::map over any other package.\n## [conflicted] Will prefer recipes::discretize over any other package.\n## [conflicted] Will prefer recipes::step over any other package.\n## [conflicted] Will prefer rsample::populate over any other package.\n## [conflicted] Will prefer scales::rescale over any other package.\n## [conflicted] Will prefer themis::step_downsample over any other package.\n## [conflicted] Will prefer themis::step_upsample over any other package.\n## [conflicted] Will prefer tidyr::expand over any other package.\n## [conflicted] Will prefer tidyr::extract over any other package.\n## [conflicted] Will prefer tidyr::pack over any other package.\n## [conflicted] Will prefer tidyr::unpack over any other package.\n## [conflicted] Will prefer tune::parameters over any other package.\n## [conflicted] Will prefer tune::tune over any other package.\n## [conflicted] Will prefer yardstick::get_weights over any other package.\n## [conflicted] Will prefer yardstick::precision over any other package.\n## [conflicted] Will prefer yardstick::recall over any other package.\n## [conflicted] Will prefer yardstick::spec over any other package.\n## [conflicted] Will prefer recipes::update over Matrix::update.\n## ── Conflicts ───────────────────────────────────────────────── tidymodels_prefer() ──\n\n\nTenga en cuenta que el uso de esta función le permite optar por utilizar conflicted::conflict_prefer() para todos los conflictos de espacios de nombres, lo que convierte cada conflicto en un error y le obliga a elegir qué función utilizar. La función tidymodels::tidymodels_prefer() maneja los conflictos más comunes de las funciones de tidymodels, pero necesitarás manejar otros conflictos en tu sesión de R tú mismo.",
    "crumbs": [
      "INTRODUCCIÓN",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Una Revisión De Los Fundamentos Del Modelado Con R</span>"
    ]
  },
  {
    "objectID": "03-base-r.html#resumen-del-capítulo",
    "href": "03-base-r.html#resumen-del-capítulo",
    "title": "3  Una Revisión De Los Fundamentos Del Modelado Con R",
    "section": "3.6 Resumen Del Capítulo",
    "text": "3.6 Resumen Del Capítulo\nEste capítulo revisó las convenciones básicas del lenguaje R para crear y usar modelos que son una base importante para el resto de este libro. El operador de fórmula es un aspecto expresivo e importante del ajuste de modelos en R y, a menudo, sirve para múltiples propósitos en funciones que no son de tidymodels. Los enfoques tradicionales de R para modelar tienen algunas limitaciones, especialmente cuando se trata de manejar y visualizar con fluidez los resultados del modelo. El metapaquete tidymodels aplica la filosofía de diseño tidyverse a los paquetes de modelado.\n\n\n\n\nChambers, J, y T Hastie, eds. 1992. Statistical Models in S. Boca Raton, FL: CRC Press, Inc.\n\n\nMangiafico, S. 2015. «An R companion for the handbook of biological statistics». https://rcompanion.org/handbook/.\n\n\nMcDonald, J. 2009. Handbook of Biological Statistics. Sparky House Publishing.\n\n\nWickham, H, M Averick, J Bryan, W Chang, L McGowan, R François, G Grolemund, et al. 2019. «Welcome to the Tidyverse». Journal of Open Source Software 4 (43).",
    "crumbs": [
      "INTRODUCCIÓN",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Una Revisión De Los Fundamentos Del Modelado Con R</span>"
    ]
  },
  {
    "objectID": "03-base-r.html#footnotes",
    "href": "03-base-r.html#footnotes",
    "title": "3  Una Revisión De Los Fundamentos Del Modelado Con R",
    "section": "",
    "text": "La mayoría de las funciones del modelo agregan implícitamente una columna de intersección.↩︎\nUna política base R llamada na.exclude() hace exactamente esto.↩︎\nhttps://tidymodels.github.io/model-implementation-principles↩︎",
    "crumbs": [
      "INTRODUCCIÓN",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Una Revisión De Los Fundamentos Del Modelado Con R</span>"
    ]
  },
  {
    "objectID": "04-ames.html",
    "href": "04-ames.html",
    "title": "4  Los Datos De Vivienda De Ames",
    "section": "",
    "text": "4.1 Explorando Las Características De Las Casas En Ames\nComencemos nuestro análisis exploratorio de datos centrándonos en el resultado que queremos predecir: el último precio de venta de la casa (en USD). Podemos crear un histograma para ver la distribución de los precios de venta en Figura 4.1.\nlibrary(tidymodels)\ntidymodels_prefer()\n\nggplot(ames, aes(x = Sale_Price)) + \n  geom_histogram(bins = 50, col= \"white\")\nFigura 4.1: Precios de venta de casas en Ames (Iowa)\nEste gráfico nos muestra que los datos están sesgados hacia la derecha; Hay más casas baratas que caras. El precio de venta medio fue $160,000, y la casa más cara era $755,000. Al modelar este resultado, se puede argumentar con fuerza que el precio debe transformarse logarítmicamente. Las ventajas de este tipo de transformación son que no se pronosticarían casas con precios de venta negativos y que los errores al predecir casas caras no tendrán una influencia indebida en el modelo. Además, desde una perspectiva estadística, una transformada logarítmica también puede estabilizar la varianza de una manera que haga que la inferencia sea más legítima. Podemos usar pasos similares para visualizar ahora los datos transformados, que se muestran en Figura 4.2.\nggplot(ames, aes(x = Sale_Price)) + \n  geom_histogram(bins = 50, col= \"white\") +\n  scale_x_log10()\nFigura 4.2: Precios de venta de casas en Ames, Iowa después de una transformación logarítmica (base 10)\nSi bien no es perfecto, esto probablemente dará como resultado mejores modelos que el uso de datos no transformados, por las razones que acabamos de describir.\nLas unidades de los coeficientes del modelo podrían ser más difíciles de interpretar, al igual que las medidas de desempeño. Por ejemplo, el error cuadrático medio (RMSE) es una métrica de rendimiento común que se utiliza en los modelos de regresión. Utiliza la diferencia entre los valores observados y predichos en sus cálculos. Si el precio de venta está en la escala logarítmica, estas diferencias (es decir, los residuos) también están en la escala logarítmica. Puede resultar difícil comprender la calidad de un modelo cuyo RMSE es 0,15 en una escala logarítmica de este tipo.\nA pesar de estos inconvenientes, los modelos utilizados en este libro utilizan la transformación logarítmica para este resultado. A partir de este momento, la columna de resultados está preregistrada en el marco de datos ames:\names &lt;- ames %&gt;% mutate(Sale_Price = log10(Sale_Price))\nOtro aspecto importante de estos datos para nuestro modelado es su ubicación geográfica. Esta información espacial está contenida en los datos de dos maneras: una etiqueta cualitativa de Vecindario y datos cuantitativos de longitud y latitud. Para visualizar la información espacial, usemos ambos juntos para trazar los datos en un mapa en Figura 4.3.\nFigura 4.3: Barrios en Ames, IA\nPodemos ver algunos patrones notables. Primero, hay un vacío de puntos de datos en el centro de Ames. Esto corresponde al campus de la Universidad Estatal de Iowa donde no hay casas residenciales. En segundo lugar, si bien hay varios barrios adyacentes, otros están geográficamente aislados. Por ejemplo, como muestra Figura 4.4, Timberland está ubicado aparte de casi todos los demás vecindarios.\nFigura 4.4: Ubicaciones de casas en Timberland\nFigura 4.5 visualiza cómo el barrio de Meadow Village en el suroeste de Ames es como una isla de propiedades dentro del mar de propiedades que conforman el barrio de Mitchell.\nFigura 4.5: Ubicación de las casas en Meadow Village y Mitchell\nUna inspección detallada del mapa también muestra que las etiquetas de los barrios no son totalmente fiables. Por ejemplo, Figura 4.6 muestra que algunas propiedades etiquetadas como ubicadas en Northridge están rodeadas de casas en el vecindario adyacente de Somerset.\nFigura 4.6: Ubicación de las casas en Somerset y Northridge\nAdemás, hay diez casas aisladas etiquetadas como en Crawford que, como puedes ver en Figura 4.7, no están cerca de la mayoría de las otras casas en ese vecindario.\nFigura 4.7: Ubicación de las casas en Crawford\nTambién es notable el vecindario del “Departamento de Transporte de Iowa (DOT) y Ferrocarriles” adyacente a la carretera principal en el lado este de Ames, que se muestra en Figura 4.8. Hay varios grupos de viviendas dentro de este vecindario, así como algunos valores atípicos longitudinales; las dos casas más al este están aisladas de las otras ubicaciones.\nFigura 4.8: Casas etiquetadas como Departamento de Transporte de Iowa (DOT) y Ferrocarril\nComo se describe en el Capítulo 1, es fundamental realizar un análisis de datos exploratorio antes de comenzar cualquier modelado. Estos datos sobre vivienda tienen características que presentan desafíos interesantes sobre cómo se deben procesar y modelar los datos. Describimos muchos de estos en capítulos posteriores. Algunas preguntas básicas que podrían examinarse durante esta etapa exploratoria incluyen:\nMuchas de estas preguntas serán revisadas a medida que estos datos se utilicen a lo largo de este libro.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Los Datos De Vivienda De Ames</span>"
    ]
  },
  {
    "objectID": "04-ames.html#sec-exploring-features-of-homes-in-ames",
    "href": "04-ames.html#sec-exploring-features-of-homes-in-ames",
    "title": "4  Los Datos De Vivienda De Ames",
    "section": "",
    "text": "Las desventajas de transformar el resultado se relacionan principalmente con la interpretación de los resultados del modelo.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Hay algo extraño o notable en las distribuciones de los predictores individuales? ¿Hay mucha asimetría o distribuciones patológicas?\n¿Existen altas correlaciones entre los predictores? Por ejemplo, existen múltiples predictores relacionados con el tamaño de la casa. ¿Algunas son redundantes?\n¿Existen asociaciones entre los predictores y los resultados?",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Los Datos De Vivienda De Ames</span>"
    ]
  },
  {
    "objectID": "04-ames.html#sec-ames-summary",
    "href": "04-ames.html#sec-ames-summary",
    "title": "4  Los Datos De Vivienda De Ames",
    "section": "4.2 Resumen Del Capítulo",
    "text": "4.2 Resumen Del Capítulo\nEste capítulo presentó el conjunto de datos de vivienda de Ames e investigó algunas de sus características. Este conjunto de datos se utilizará en capítulos posteriores para demostrar la sintaxis de tidymodels. El análisis de datos exploratorios como este es un componente esencial de cualquier proyecto de modelado; EDA descubre información que contribuye a una mejor práctica de modelado.\nEl código importante para preparar el conjunto de datos de Ames que llevaremos a los capítulos siguientes es:\n\nlibrary(tidymodels)\ndata(ames)\names &lt;- ames %&gt;% mutate(Sale_Price = log10(Sale_Price))\n\n\n\n\n\nDe Cock, D. 2011. «Ames, Iowa: Alternative to the Boston housing data as an end of semester regression project». Journal of Statistics Education 19 (3).",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Los Datos De Vivienda De Ames</span>"
    ]
  },
  {
    "objectID": "04-ames.html#footnotes",
    "href": "04-ames.html#footnotes",
    "title": "4  Los Datos De Vivienda De Ames",
    "section": "",
    "text": "Para obtener una descripción completa de las diferencias, consulte https://github.com/topepo/AmesHousing/blob/master/R/make_ames.R.↩︎",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Los Datos De Vivienda De Ames</span>"
    ]
  },
  {
    "objectID": "05-data-spending.html",
    "href": "05-data-spending.html",
    "title": "5  Gastar Nuestros Datos",
    "section": "",
    "text": "5.1 Métodos Comunes Para Dividir Datos\nEl enfoque principal para la validación del modelo empírico es dividir el conjunto de datos existente en dos conjuntos distintos, el conjunto de entrenamiento y el conjunto de prueba. Una parte de los datos se utiliza para desarrollar y optimizar el modelo. Este conjunto de entrenamiento suele ser la mayoría de los datos. Estos datos son una zona de pruebas para la construcción de modelos donde se pueden ajustar diferentes modelos, se investigan estrategias de ingeniería de características, etc. Como profesionales del modelado, pasamos la gran mayor parte del proceso de modelado utilizando el conjunto de entrenamiento como sustrato para desarrollar el modelo.\nLa otra parte de los datos se coloca en el conjunto de prueba. Esto se mantiene en reserva hasta que se elijan uno o dos modelos como los métodos con mayor probabilidad de éxito. Luego, el conjunto de prueba se utiliza como árbitro final para determinar la eficacia del modelo. Es fundamental mirar el conjunto de prueba sólo una vez; de lo contrario, pasa a formar parte del proceso de modelado.\nSupongamos que asignamos el 80% de los datos al conjunto de entrenamiento y el 20% restante a las pruebas. El método más común es utilizar muestreo aleatorio simple. El paquete rsample tiene herramientas para realizar divisiones de datos como esta; la función initial_split() fue creada para este propósito. Toma el marco de datos como argumento, así como la proporción que se colocará en el entrenamiento. Usando el marco de datos producido por el fragmento de código del resumen en Sección 4.2 que preparó el conjunto de datos de Ames:\nlibrary(tidymodels)\ntidymodels_prefer()\n\n# Configure el flujo de números aleatorios usando `set.seed()` para que los resultados puedan ser\n# reproducido más tarde. \nset.seed(501)\n\n# Guarde la información dividida para una división 80/20 de los datos\names_split &lt;- initial_split(ames, prop = 0.80)\names_split\n## &lt;Training/Testing/Total&gt;\n## &lt;2344/586/2930&gt;\nLa información impresa denota la cantidad de datos en el conjunto de entrenamiento (\\(n = 2,344\\)), la cantidad en el conjunto de prueba (\\(n = 586\\)), y el tamaño del conjunto original de muestras. (\\(n = 2,930\\)).\nEl objeto ames_split es un objeto rsplit y contiene sólo la información de partición; Para obtener los conjuntos de datos resultantes, aplicamos dos funciones más:\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\ndim(ames_train)\n## [1] 2344   74\nEstos objetos son marcos de datos con las mismas columnas que los datos originales pero solo las filas apropiadas para cada conjunto.\nEl muestreo aleatorio simple es apropiado en muchos casos, pero existen excepciones. Cuando hay un desequilibrio de clases dramático en los problemas de clasificación, una clase ocurre con mucha menos frecuencia que otra. El uso de una muestra aleatoria simple puede asignar al azar estas muestras poco frecuentes de manera desproporcionada al conjunto de entrenamiento o prueba. Para evitar esto, se puede utilizar muestreo estratificado. La división de entrenamiento/prueba se realiza por separado dentro de cada clase y luego estas submuestras se combinan en el conjunto general de entrenamiento y prueba. Para problemas de regresión, los datos de resultados se pueden agrupar artificialmente en cuartiles y luego se puede realizar un muestreo estratificado en cuatro ocasiones distintas. Este es un método eficaz para mantener similares las distribuciones del resultado entre el conjunto de entrenamiento y de prueba. La distribución del resultado del precio de venta para los datos de vivienda de Ames se muestra en Figura 5.1.\nFigura 5.1: La distribución del precio de venta (en unidades logarítmicas) de los datos de vivienda de Ames. Las líneas verticales indican los cuartiles de los datos. Algunos datos de vivienda de Ames se muestran en Figura 5.1.\nComo se analiza en el Capítulo 4, la distribución del precio de venta está sesgada hacia la derecha, con proporcionalmente más casas económicas que caras a ambos lados del centro de la distribución. La preocupación aquí con una simple división es que las casas más caras no estarían bien representadas en el conjunto de capacitación; esto aumentaría el riesgo de que nuestro modelo fuera ineficaz para predecir el precio de dichas propiedades. Las líneas verticales de puntos en Figura 5.1 indican los cuatro cuartiles de estos datos. Una muestra aleatoria estratificada realizaría la división 80/20 dentro de cada uno de estos subconjuntos de datos y luego agruparía los resultados. En rsample, esto se logra usando el argumento strata:\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\ndim(ames_train)\n## [1] 2342   74\nSólo se puede utilizar una columna para la estratificación.\n¿Hay situaciones en las que el muestreo aleatorio no es la mejor opción? Un caso es cuando los datos tienen un componente temporal significativo, como los datos de series temporales. Aquí, es más común utilizar los datos más recientes como conjunto de prueba. El paquete rsample contiene una función llamada initial_time_split() que es muy similar a initial_split(). En lugar de utilizar muestreo aleatorio, el argumento “prop” denota qué proporción de la primera parte de los datos debe usarse como conjunto de entrenamiento; la función supone que los datos han sido preclasificados en un orden apropiado.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Gastar Nuestros Datos</span>"
    ]
  },
  {
    "objectID": "05-data-spending.html#sec-splitting-methods",
    "href": "05-data-spending.html#sec-splitting-methods",
    "title": "5  Gastar Nuestros Datos",
    "section": "",
    "text": "¿Cómo deberíamos llevar a cabo esta división de los datos? La respuesta depende del contexto.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEl uso del muestreo estratificado tiene muy pocas desventajas.\n\n\n\nLa proporción de datos que deben asignarse para la división depende en gran medida del contexto del problema en cuestión. Muy pocos datos en el conjunto de entrenamiento obstaculizan la capacidad del modelo para encontrar estimaciones de parámetros apropiadas. Por el contrario, muy pocos datos en el conjunto de pruebas reducen la calidad de las estimaciones de rendimiento. Partes de la comunidad estadística evitan los conjuntos de pruebas en general porque creen que todos los datos deben usarse para la estimación de parámetros. Si bien este argumento tiene sus méritos, es una buena práctica de modelado tener un conjunto imparcial de observaciones como árbitro final de la calidad del modelo. Se debe evitar un conjunto de pruebas sólo cuando los datos sean patológicamente pequeños.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Gastar Nuestros Datos</span>"
    ]
  },
  {
    "objectID": "05-data-spending.html#sec-what-about-a-validation-set",
    "href": "05-data-spending.html#sec-what-about-a-validation-set",
    "title": "5  Gastar Nuestros Datos",
    "section": "5.2 ¿Qué Pasa Con Un Conjunto De Validación?",
    "text": "5.2 ¿Qué Pasa Con Un Conjunto De Validación?\nAl describir los objetivos de la división de datos, seleccionamos el conjunto de pruebas como los datos que deben usarse para evaluar adecuadamente el desempeño del modelo en los modelos finales. Esto plantea la pregunta: “¿Cómo podemos saber qué es mejor si no medimos el rendimiento hasta el conjunto de pruebas?”\nEs común escuchar acerca de conjuntos de validación como respuesta a esta pregunta, especialmente en la literatura sobre redes neuronales y aprendizaje profundo. Durante los primeros días de las redes neuronales, los investigadores se dieron cuenta de que medir el rendimiento reprediciendo las muestras del conjunto de entrenamiento conducía a resultados demasiado optimistas (de manera significativa y poco realista). Esto llevó a modelos que se sobreajustaron, lo que significa que tuvieron un desempeño muy bueno en el conjunto de entrenamiento pero pobre en el conjunto de prueba.[^05-data-spend-1] Para combatir este problema, se retuvo y utilizó un pequeño conjunto de datos de validación. para medir el desempeño a medida que se entrenaba la red. Una vez que la tasa de error del conjunto de validación comenzara a aumentar, el entrenamiento se detendría. En otras palabras, el conjunto de validación era un medio para tener una idea aproximada de qué tan bien se desempeñaba el modelo antes del conjunto de prueba.\n\nSi los conjuntos de validación son un subconjunto del conjunto de entrenamiento o una tercera asignación en la división inicial de los datos se reduce en gran medida a la semántica.\n\nLos conjuntos de validación se analizan más en Sección 10.2.2 como un caso especial de métodos de remuestreo que se utilizan en el conjunto de entrenamiento. Si va a utilizar un conjunto de validación, puede comenzar con una función de división diferente 1:\n\nset.seed(52)\n# Para dedicar el 60 % a la capacitación, el 20 % a la validación y el 20 % a las pruebas:\names_val_split &lt;- initial_validation_split(ames, prop = c(0.6, 0.2))\names_val_split\n## &lt;Training/Validation/Testing/Total&gt;\n## &lt;1758/586/586/2930&gt;\n\nAl imprimir la división ahora se muestra el tamaño del conjunto de entrenamiento. (1,758), del conjunto de validación (586), y del conjunto de prueba ((586).\nPara obtener los datos de entrenamiento, validación y prueba, se utiliza la misma sintaxis:\n\names_train &lt;- training(ames_val_split)\names_test &lt;- testing(ames_val_split)\names_val &lt;- validation(ames_val_split)\n\nSección 10.2.2 demostrará cómo utilizar el objeto ames_val_split para remuestreo y optimización del modelo.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Gastar Nuestros Datos</span>"
    ]
  },
  {
    "objectID": "05-data-spending.html#datos-multinivel",
    "href": "05-data-spending.html#datos-multinivel",
    "title": "5  Gastar Nuestros Datos",
    "section": "5.3 Datos Multinivel",
    "text": "5.3 Datos Multinivel\nCon los datos de vivienda de Ames, una propiedad se considera la unidad experimental independiente. Es seguro asumir que, estadísticamente, los datos de una propiedad son independientes de otras propiedades. Para otras aplicaciones, ese no es siempre el caso:\n\nPara datos longitudinales, por ejemplo, la misma unidad experimental independiente se puede medir en múltiples puntos de tiempo. Un ejemplo sería un sujeto humano en un ensayo médico.\nUn lote de producto manufacturado también podría considerarse la unidad experimental independiente. En los diseños de medidas repetidas, los puntos de datos replicados de un lote se recopilan varias veces.\nJohnson et al. (2018) Informe un experimento en el que se tomaron muestras de diferentes árboles en las partes superior e inferior de un tallo. Aquí, el árbol es la unidad experimental y la jerarquía de datos es la muestra dentro de la posición del tallo dentro del árbol.\n\nEl capítulo 9 de Kuhn y Johnson (2020) contiene otros ejemplos.\nEn estas situaciones, el conjunto de datos tendrá varias filas por unidad experimental. Un simple remuestreo entre filas llevaría a que algunos datos dentro de una unidad experimental estuvieran en el conjunto de entrenamiento y otros en el conjunto de prueba. La división de datos debe ocurrir en el nivel de unidad experimental independiente de los datos. Por ejemplo, para producir una división 80/20 del conjunto de datos de vivienda de Ames, el 80% de las propiedades deben asignarse al conjunto de entrenamiento.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Gastar Nuestros Datos</span>"
    ]
  },
  {
    "objectID": "05-data-spending.html#otras-consideraciones-para-un-presupuesto-de-datos",
    "href": "05-data-spending.html#otras-consideraciones-para-un-presupuesto-de-datos",
    "title": "5  Gastar Nuestros Datos",
    "section": "5.4 Otras Consideraciones Para Un Presupuesto De Datos",
    "text": "5.4 Otras Consideraciones Para Un Presupuesto De Datos\nAl decidir cómo gastar los datos disponibles, tenga en cuenta algunas cosas más. En primer lugar, es fundamental poner en cuarentena el conjunto de pruebas de cualquier actividad de construcción de modelos. Mientras lee este libro, observe qué datos están expuestos al modelo en un momento dado.\n\nEl problema de la fuga de información ocurre cuando en el proceso de modelado se utilizan datos fuera del conjunto de entrenamiento.\n\nPor ejemplo, en una competencia de aprendizaje automático, los datos del conjunto de pruebas podrían proporcionarse sin los valores reales de los resultados para que el modelo pueda calificarse y clasificarse. Un método potencial para mejorar la puntuación podría ser ajustar el modelo utilizando los puntos de ajuste de entrenamiento que sean más similares a los valores del conjunto de prueba. Si bien el conjunto de prueba no se utiliza directamente para ajustar el modelo, aún tiene una gran influencia. En general, esta técnica es muy problemática ya que reduce el error de generalización del modelo para optimizar el rendimiento en un conjunto de datos específico. Hay formas más sutiles de utilizar los datos del conjunto de prueba durante el entrenamiento. Mantener los datos de entrenamiento en un marco de datos separado del conjunto de prueba es una pequeña verificación para garantizar que la fuga de información no se produzca por accidente.\nEn segundo lugar, las técnicas para submuestrear el conjunto de capacitación pueden mitigar problemas específicos (por ejemplo, desequilibrios de clases). Esta es una técnica válida y común que deliberadamente da como resultado que los datos del conjunto de entrenamiento diverjan de la población de la que se extrajeron los datos. Es fundamental que el conjunto de prueba siga reflejando lo que el modelo encontraría en la naturaleza. En otras palabras, el conjunto de prueba siempre debe parecerse a los datos nuevos que se proporcionarán al modelo.\nA continuación, al comienzo de este capítulo, advertimos sobre el uso de los mismos datos para diferentes tareas. El Capítulo 10 discutirá metodologías sólidas basadas en datos para el uso de datos que reducirán los riesgos relacionados con el sesgo, el sobreajuste y otros problemas. Muchos de estos métodos aplican las herramientas de división de datos presentadas en este capítulo.\nFinalmente, las consideraciones de este capítulo se aplican al desarrollo y elección de un modelo confiable, el tema principal de este libro. Al entrenar un modelo elegido final para producción, después de determinar el rendimiento esperado con nuevos datos, los profesionales suelen utilizar todos los datos disponibles para una mejor estimación de los parámetros.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Gastar Nuestros Datos</span>"
    ]
  },
  {
    "objectID": "05-data-spending.html#sec-splitting-summary",
    "href": "05-data-spending.html#sec-splitting-summary",
    "title": "5  Gastar Nuestros Datos",
    "section": "5.5 Resumen Del Capítulo",
    "text": "5.5 Resumen Del Capítulo\nLa división de datos es la estrategia fundamental para la validación empírica de modelos. Incluso en la era de la recopilación de datos desenfrenada, un proyecto de modelado típico tiene una cantidad limitada de datos apropiados, y es necesario gastar sabiamente los datos de un proyecto. En este capítulo, analizamos varias estrategias para dividir los datos en distintos grupos para modelado y evaluación.\nEn este punto de control, los fragmentos de código importantes para preparar y dividir son:\n\nlibrary(tidymodels)\ndata(ames)\names &lt;- ames %&gt;% mutate(Sale_Price = log10(Sale_Price))\n\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\n\n\n\n\nJohnson, D, P Eckart, N Alsamadisi, H Noble, C Martin, y R Spicer. 2018. «Polar auxin transport is implicated in vessel differentiation and spatial patterning during secondary growth in Populus». American Journal of Botany 105 (2): 186-96.\n\n\nKuhn, M, y K Johnson. 2020. Feature engineering and selection: A practical approach for predictive models. CRC Press.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Gastar Nuestros Datos</span>"
    ]
  },
  {
    "objectID": "05-data-spending.html#footnotes",
    "href": "05-data-spending.html#footnotes",
    "title": "5  Gastar Nuestros Datos",
    "section": "",
    "text": "Esta interfaz está disponible a partir de la versión 1.2.0 de rsample (alrededor de septiembre de 2023).↩︎",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Gastar Nuestros Datos</span>"
    ]
  },
  {
    "objectID": "06-fitting-models.html",
    "href": "06-fitting-models.html",
    "title": "6  Creando Modelos Con parsnip",
    "section": "",
    "text": "6.1 Crear Un Modelo\nUna vez que los datos se han codificado en un formato listo para un algoritmo de modelado, como una matriz numérica, se pueden utilizar en el proceso de construcción del modelo.\nSupongamos que nuestra elección inicial fue un modelo de regresión lineal. Esto equivale a especificar que los datos del resultado son numéricos y que los predictores están relacionados con el resultado en términos de pendientes e intersecciones simples:\n\\[y_i = \\beta_0 + \\beta_1 x_{1i} + \\ldots + \\beta_p x_{pi}\\]\nSe pueden utilizar diversos métodos para estimar los parámetros del modelo:\nEn R, el paquete stats se puede utilizar para el primer caso. La sintaxis para la regresión lineal usando la función lm() es:\ndonde ... simboliza otras opciones para pasar a lm(). La función not tiene una interfaz x/y, donde podríamos pasar nuestro resultado como y y nuestros predictores como x.\nPara estimar con regularización, el segundo caso, se puede ajustar un modelo bayesiano usando el paquete rstanarm:\nEn este caso, las otras opciones pasadas a través de ... incluirían argumentos para las distribuciones anteriores de los parámetros, así como detalles sobre los aspectos numéricos del modelo. Al igual que con lm(), solo está disponible la interfaz de fórmula.\nUn enfoque popular no bayesiano para la regresión regularizada es el modelo glmnet (Friedman, Hastie, y Tibshirani 2010). Su sintaxis es:\nEn este caso, los datos del predictor ya deben estar formateados en una matriz numérica; solo hay un método x/y y ningún método de fórmula.\nTenga en cuenta que estas interfaces son heterogéneas en la forma en que se pasan los datos a la función del modelo o en términos de sus argumentos. El primer problema es que, para ajustar los modelos a diferentes paquetes, los datos deben formatearse de diferentes maneras. lm() y stan_glm() solo tienen interfaces de fórmula mientras que glmnet() no. Para otro tipo de modelos, las interfaces pueden ser aún más dispares. Para una persona que intenta realizar un análisis de datos, estas diferencias requieren la memorización de la sintaxis de cada paquete y pueden resultar muy frustrantes.\nPara tidymodels, el enfoque para especificar un modelo pretende ser más unificado:\nEstas especificaciones se construyen sin hacer referencia a los datos. Por ejemplo, para los tres casos que describimos:\nlibrary(tidymodels)\ntidymodels_prefer()\n\nlinear_reg() %&gt;% set_engine(\"lm\")\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm\n\nlinear_reg() %&gt;% set_engine(\"glmnet\") \n## Linear Regression Model Specification (regression)\n## \n## Computational engine: glmnet\n\nlinear_reg() %&gt;% set_engine(\"stan\")\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: stan\nUna vez que se han especificado los detalles del modelo, la estimación del modelo se puede realizar con la función fit() (para usar una fórmula) o con la función fit_xy() (cuando sus datos ya están preprocesados). El paquete parsnip permite al usuario ser indiferente a la interfaz del modelo subyacente; siempre puedes usar una fórmula incluso si la función del paquete de modelado solo tiene la interfaz x/y.\nLa función translate() puede proporcionar detalles sobre cómo parsnip convierte el código del usuario a la sintaxis del paquete:\nlinear_reg() %&gt;% set_engine(\"lm\") %&gt;% translate()\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm \n## \n## Model fit template:\n## stats::lm(formula = missing_arg(), data = missing_arg(), weights = missing_arg())\n\nlinear_reg(penalty = 1) %&gt;% set_engine(\"glmnet\") %&gt;% translate()\n## Linear Regression Model Specification (regression)\n## \n## Main Arguments:\n##   penalty = 1\n## \n## Computational engine: glmnet \n## \n## Model fit template:\n## glmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n##     family = \"gaussian\")\n\nlinear_reg() %&gt;% set_engine(\"stan\") %&gt;% translate()\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: stan \n## \n## Model fit template:\n## rstanarm::stan_glm(formula = missing_arg(), data = missing_arg(), \n##     weights = missing_arg(), family = stats::gaussian, refresh = 0)\nTenga en cuenta que missing_arg() es solo un marcador de posición para los datos que aún no se han proporcionado.\nVeamos cómo predecir el precio de venta de las casas en los datos de Ames en función únicamente de la longitud y la latitud:2\nlm_model &lt;- \n  linear_reg() %&gt;% \n  set_engine(\"lm\")\n\nlm_form_fit &lt;- \n  lm_model %&gt;% \n  # Recuerde que a Sale_Price se le ha aplicado una transformación logarítmica previamente\n  fit(Sale_Price ~ Longitude + Latitude, data = ames_train)\n\nlm_xy_fit &lt;- \n  lm_model %&gt;% \n  fit_xy(\n    x = ames_train %&gt;% select(Longitude, Latitude),\n    y = ames_train %&gt;% pull(Sale_Price)\n  )\n\nlm_form_fit\n## parsnip model object\n## \n## \n## Call:\n## stats::lm(formula = Sale_Price ~ Longitude + Latitude, data = data)\n## \n## Coefficients:\n## (Intercept)    Longitude     Latitude  \n##     -302.97        -2.07         2.71\nlm_xy_fit\n## parsnip model object\n## \n## \n## Call:\n## stats::lm(formula = ..y ~ ., data = data)\n## \n## Coefficients:\n## (Intercept)    Longitude     Latitude  \n##     -302.97        -2.07         2.71\nparsnip no solo permite una interfaz de modelo coherente para diferentes paquetes, sino que también proporciona coherencia en los argumentos del modelo. Es común que diferentes funciones que se ajustan al mismo modelo tengan diferentes nombres de argumentos. Las funciones del modelo de random forest son un buen ejemplo. Tres argumentos comúnmente utilizados son la cantidad de árboles en el conjunto, la cantidad de predictores para muestrear aleatoriamente con cada división dentro de un árbol y la cantidad de puntos de datos necesarios para realizar una división. Para tres paquetes R diferentes que implementan este algoritmo, esos argumentos se muestran en Tabla 6.1.\nTabla 6.1: Ejemplos de nombres de argumentos para diferentes funciones de random forest.\n\n\n\n\n\n\nTipo de argumento\nranger\nrandomForest\nsparklyr\n\n\n\n\n# predictores muestreo\nmtry\nmtry\nfeature_subset_strategy\n\n\n# árboles\nnum.trees\nntree\nnum_trees\n\n\n# puntos de datos por división\nmin.node.size\nnodesize\nmin_instances_per_node\nEn un esfuerzo por hacer que la especificación de argumentos sea menos complicada, parsnip utiliza nombres de argumentos comunes dentro y entre paquetes. Tabla 6.2 muestra, para radom forest, qué utilizan los modelos parsnip.\nTabla 6.2: Nombres de argumentos de random forest utilizados por parsnip.\n\n\n\n\n\n\nTipo de argumento\nparsnip\n\n\n\n\n# predictores muestreo\nmtry\n\n\n# árboles\ntrees\n\n\n# puntos de datos por división\nmin_n\nEs cierto que este es un conjunto más de argumentos para memorizar. Sin embargo, cuando otros tipos de modelos tienen los mismos tipos de argumentos, estos nombres aún se aplican. Por ejemplo, los conjuntos de árboles potenciados también crean una gran cantidad de modelos basados ​​en árboles, por lo que allí también se utilizan trees, al igual que min_n, etc.\nAlgunos de los nombres de los argumentos originales pueden ser bastante jerga. Por ejemplo, para especificar la cantidad de regularización que se utilizará en un modelo glmnet, se utiliza la letra griega lambda. Si bien esta notación matemática se usa comúnmente en la literatura estadística, para muchas personas no es obvio qué representa “lambda” (especialmente aquellos que consumen los resultados del modelo). Dado que esta es la penalización utilizada en la regularización, parsnip estandariza el nombre del argumento penalty. De manera similar, el número de vecinos en un modelo KNN se denomina neighbors en lugar de k. Nuestra regla general al estandarizar los nombres de los argumentos es:\nPara comprender cómo los nombres de los argumentos parsnip se asignan a los nombres originales, use el archivo de ayuda para el modelo (disponible a través de ?rand_forest), así como la función translate():\nrand_forest(trees = 1000, min_n = 5) %&gt;% \n  set_engine(\"ranger\") %&gt;% \n  set_mode(\"regression\") %&gt;% \n  translate()\n## Random Forest Model Specification (regression)\n## \n## Main Arguments:\n##   trees = 1000\n##   min_n = 5\n## \n## Computational engine: ranger \n## \n## Model fit template:\n## ranger::ranger(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n##     num.trees = 1000, min.node.size = min_rows(~5, x), num.threads = 1, \n##     verbose = FALSE, seed = sample.int(10^5, 1))\nLas funciones de modelado en parsnip separan los argumentos del modelo en dos categorías:\nPor ejemplo, en la traducción del código de random forest anterior, los argumentos num.threads, verbose y seed se agregaron de forma predeterminada. Estos argumentos son específicos de la implementación ranger de modelos random forest y no tendrían sentido como argumentos principales. Los argumentos específicos del motor se pueden especificar en set_engine(). Por ejemplo, para que la función ranger::ranger() imprima más información sobre el ajuste:\nrand_forest(trees = 1000, min_n = 5) %&gt;% \n  set_engine(\"ranger\", verbose = TRUE) %&gt;% \n  set_mode(\"regression\") \n## Random Forest Model Specification (regression)\n## \n## Main Arguments:\n##   trees = 1000\n##   min_n = 5\n## \n## Engine-Specific Arguments:\n##   verbose = TRUE\n## \n## Computational engine: ranger",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Creando Modelos Con parsnip</span>"
    ]
  },
  {
    "objectID": "06-fitting-models.html#sec-create-a-model",
    "href": "06-fitting-models.html#sec-create-a-model",
    "title": "6  Creando Modelos Con parsnip",
    "section": "",
    "text": "La regresión lineal ordinaria utiliza el método tradicional de mínimos cuadrados para resolver los parámetros del modelo.\nRegresión lineal regularizada añade una penalización al método de mínimos cuadrados para fomentar la simplicidad eliminando predictores y/o reduciendo sus coeficientes hacia cero. Esto se puede ejecutar utilizando técnicas bayesianas o no bayesianas.\n\n\nmodel &lt;- lm(formula, data, ...)\n\n\nmodel &lt;- stan_glm(formula, data, family = \"gaussian\", ...)\n\n\nmodel &lt;- glmnet(x = matrix, y = vector, family = \"gaussian\", ...)\n\n\n\n\nEspecifique el tipo de modelo según su estructura matemática (por ejemplo, regresión lineal, random forest, KNN, etc.).\nEspecificar motor para montar el modelo. La mayoría de las veces esto refleja el paquete de software que se debe utilizar, como Stan o glmnet. Estos son modelos por derecho propio, y parsnip proporciona interfaces consistentes al usarlos como motores para el modelado.\nCuando sea necesario, declarar el moda del modelo. El modo refleja el tipo de resultado de la predicción. Para resultados numéricos, el modo es la regresión; para resultados cualitativos, es clasificación.1 Si un algoritmo modelo solo puede abordar un tipo de resultado de predicción, como la regresión lineal, el modo ya está establecido.\n\n\n\n\n\n\n\n\nProporcionamos un argumento de penalty (penalización) requerido para el motor glmnet. Además, para los motores Stan y glmnet, el argumento famiy(familia) se agregó automáticamente como valor predeterminado. Como se mostrará más adelante en esta sección, esta opción se puede cambiar.\n\n\n\n\n\n\n\n\n\n\nSi un profesional incluyera estos nombres en un gráfico o tabla, ¿entenderían el nombre las personas que vieran esos resultados?\n\n\n\n\n\nLos argumentos principales se utilizan con más frecuencia y tienden a estar disponibles en todos los motores.\nLos argumentos del motor son específicos de un motor en particular o se usan con menos frecuencia.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Creando Modelos Con parsnip</span>"
    ]
  },
  {
    "objectID": "06-fitting-models.html#utilizar-los-resultados-del-modelo",
    "href": "06-fitting-models.html#utilizar-los-resultados-del-modelo",
    "title": "6  Creando Modelos Con parsnip",
    "section": "6.2 Utilizar Los Resultados Del Modelo",
    "text": "6.2 Utilizar Los Resultados Del Modelo\nUna vez creado y ajustado el modelo, podemos utilizar los resultados de diversas formas; es posible que queramos trazar, imprimir o examinar de otro modo el resultado del modelo. Varias cantidades se almacenan en un objeto modelo parsnip, incluido el modelo ajustado. Esto se puede encontrar en un elemento llamado fit, que se puede devolver usando la función extract_fit_engine():\n\nlm_form_fit %&gt;% extract_fit_engine()\n## \n## Call:\n## stats::lm(formula = Sale_Price ~ Longitude + Latitude, data = data)\n## \n## Coefficients:\n## (Intercept)    Longitude     Latitude  \n##     -302.97        -2.07         2.71\n\nSe pueden aplicar métodos normales a este objeto, como imprimir y trazar:\n\nlm_form_fit %&gt;% extract_fit_engine() %&gt;% vcov()\n##             (Intercept) Longitude Latitude\n## (Intercept)     207.311   1.57466 -1.42397\n## Longitude         1.575   0.01655 -0.00060\n## Latitude         -1.424  -0.00060  0.03254\n\n\nNunca pase el elemento fit de un modelo parsnip a una función de predicción del modelo, es decir, use predict(lm_form_fit) pero no use predict(lm_form_fit$fit). Si los datos fueron preprocesados ​​de alguna manera, se generarán predicciones incorrectas (a veces, sin errores). La función de predicción del modelo subyacente no tiene idea de si se ha realizado alguna transformación en los datos antes de ejecutar el modelo. Consulte Sección 6.3 para obtener más información sobre cómo hacer predicciones.\n\nUn problema con algunos métodos existentes en base R es que los resultados se almacenan de una manera que puede no ser la más útil. Por ejemplo, el método summary() para objetos lm se puede utilizar para imprimir los resultados del ajuste del modelo, incluida una tabla con los valores de los parámetros, sus estimaciones de incertidumbre y los valores p. Estos resultados particulares también se pueden guardar:\n\nmodel_res &lt;- \n  lm_form_fit %&gt;% \n  extract_fit_engine() %&gt;% \n  summary()\n\n# Se puede acceder a la tabla de coeficientes del modelo mediante el método \"coef\".\nparam_est &lt;- coef(model_res)\nclass(param_est)\n## [1] \"matrix\" \"array\"\nparam_est\n##             Estimate Std. Error t value  Pr(&gt;|t|)\n## (Intercept) -302.974    14.3983  -21.04 3.640e-90\n## Longitude     -2.075     0.1286  -16.13 1.395e-55\n## Latitude       2.710     0.1804   15.02 9.289e-49\n\nHay algunas cosas a tener en cuenta sobre este resultado. Primero, el objeto es una matriz numérica. Lo más probable es que se haya elegido esta estructura de datos, ya que todos los resultados calculados son numéricos y un objeto de matriz se almacena de manera más eficiente que un marco de datos. Esta elección probablemente se hizo a finales de la década de 1970, cuando la eficiencia computacional era extremadamente crítica. En segundo lugar, los datos no numéricos (las etiquetas de los coeficientes) están contenidos en los nombres de las filas. Mantener las etiquetas de los parámetros como nombres de filas es muy coherente con las convenciones del lenguaje S original.\nUn siguiente paso razonable podría ser crear una visualización de los valores de los parámetros. Para ello, sería sensato convertir la matriz de parámetros en un marco de datos. Podríamos agregar los nombres de las filas como una columna para que puedan usarse en un gráfico. Sin embargo, observe que varios de los nombres de columnas de matriz existentes no serían nombres de columnas R válidos para marcos de datos ordinarios (por ejemplo, \"Pr(&gt;|t|)\"). Otra complicación es la coherencia de los nombres de las columnas. Para objetos lm, la columna para el valor p es \"Pr(&gt;|t|)\", pero para otros modelos, se podría usar una prueba diferente y, como resultado, el nombre de la columna sería diferente ( por ejemplo, \"Pr(&gt;|z|)\") y el tipo de prueba se codificaría en el nombre de la columna.\nSi bien estos pasos adicionales de formato de datos no son imposibles de superar, son un obstáculo, especialmente porque pueden ser diferentes para distintos tipos de modelos. La matriz no es una estructura de datos altamente reutilizable, principalmente porque limita los datos a ser de un solo tipo (por ejemplo, numéricos). Además, mantener algunos datos en los nombres de las dimensiones también es problemático, ya que esos datos deben extraerse para que sean de uso general.\nComo solución, el paquete broom puede convertir muchos tipos de objetos modelo en una estructura ordenada. Por ejemplo, usar el método tidy() en el modelo lineal produce:\n\ntidy(lm_form_fit)\n## # A tibble: 3 × 5\n##   term        estimate std.error statistic  p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)  -303.      14.4       -21.0 3.64e-90\n## 2 Longitude      -2.07     0.129     -16.1 1.40e-55\n## 3 Latitude        2.71     0.180      15.0 9.29e-49\n\nLos nombres de las columnas están estandarizados en todos los modelos y no contienen ningún dato adicional (como el tipo de prueba estadística). Los datos que antes estaban contenidos en los nombres de las filas ahora están en una columna llamada term (término). Un principio importante en el ecosistema de tidymodels es que una función debe devolver valores que sean predecibles, consistentes y no sorprendentes.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Creando Modelos Con parsnip</span>"
    ]
  },
  {
    "objectID": "06-fitting-models.html#sec-parsnip-predictions",
    "href": "06-fitting-models.html#sec-parsnip-predictions",
    "title": "6  Creando Modelos Con parsnip",
    "section": "6.3 Hacer Predicciones",
    "text": "6.3 Hacer Predicciones\nOtra área donde parsnip difiere de las funciones de modelado R convencionales es el formato de los valores devueltos por predict(). Para las predicciones, parsnip siempre se ajusta a las siguientes reglas:\n\nLos resultados son siempre un tibble.\nLos nombres de las columnas del tibble siempre son predecibles.\nSiempre hay tantas filas en el tibble como en el conjunto de datos de entrada.\n\nPor ejemplo, cuando se predicen datos numéricos:\n\names_test_small &lt;- ames_test %&gt;% slice(1:5)\npredict(lm_form_fit, new_data = ames_test_small)\n## # A tibble: 5 × 1\n##   .pred\n##   &lt;dbl&gt;\n## 1  5.22\n## 2  5.21\n## 3  5.28\n## 4  5.27\n## 5  5.28\n\nEl orden de las filas de las predicciones es siempre el mismo que el de los datos originales.\n\n¿Por qué el punto inicial en algunos de los nombres de las columnas? Algunos argumentos y valores de retorno de tidyverse y tidymodels contienen puntos. Esto es para proteger contra la fusión de datos con nombres duplicados. ¡Hay algunos conjuntos de datos que contienen predictores llamados “pred”!\n\nEstas tres reglas facilitan la combinación de predicciones con los datos originales:\n\names_test_small %&gt;% \n  select(Sale_Price) %&gt;% \n  bind_cols(predict(lm_form_fit, ames_test_small)) %&gt;% \n  # Agregue intervalos de predicción del 95% a los resultados:\n  bind_cols(predict(lm_form_fit, ames_test_small, type = \"pred_int\")) \n## # A tibble: 5 × 4\n##   Sale_Price .pred .pred_lower .pred_upper\n##        &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n## 1       5.02  5.22        4.91        5.54\n## 2       5.39  5.21        4.90        5.53\n## 3       5.28  5.28        4.97        5.60\n## 4       5.28  5.27        4.96        5.59\n## 5       5.28  5.28        4.97        5.60\n\nLa motivación para la primera regla proviene de algunos paquetes de R que producen tipos de datos diferentes a partir de funciones de predicción. Por ejemplo, el paquete ranger es una excelente herramienta para calcular modelos forestales aleatorios. Sin embargo, en lugar de devolver un marco de datos o un vector como salida, devuelve un objeto especializado que tiene múltiples valores incrustados (incluidos los valores predichos). Este es solo un paso más que el analista de datos debe solucionar en sus scripts. Como otro ejemplo, el modelo nativo glmnet puede devolver al menos cuatro tipos de salida diferentes para predicciones, dependiendo de los detalles del modelo y las características de los datos. Estos se muestran en Tabla 6.3.\n\n\n\nTabla 6.3: Diferentes valores de retorno para tipos de predicción glmnet.\n\n\n\n\n\n\nTipo de Predicción\nDevuelve:\n\n\n\n\nnumérica\nmatriz numérica\n\n\nclase\nmatriz de texto\n\n\nprobabilidad (2 classes)\nmatriz numérica (solo 2do nivel)\n\n\nprobabilidad (3+ classes)\narreglo numérico 3D (todos los niveles)\n\n\n\n\n\n\n\n\n\nAdemás, los nombres de las columnas de los resultados contienen valores codificados que se asignan a un vector llamado lambda dentro del objeto del modelo glmnet. Este excelente método estadístico puede resultar desalentador en la práctica debido a todos los casos especiales que un analista puede encontrar y que requieren código adicional para ser útil.\nPara la segunda regla de predicción de tidymodels, los nombres de columnas predecibles para diferentes tipos de predicciones se muestran en Tabla 6.4.\n\n\n\nTabla 6.4: El mapeo de tidymodels de tipos de predicción y nombres de columnas.\n\n\n\n\n\n\ntipo de valor\nnombre(s) de columna(s)\n\n\n\n\nnumeric\n.pred\n\n\nclass\n.pred_class\n\n\nprob\n.pred_{class levels}\n\n\nconf_int\n.pred_lower, .pred_upper\n\n\npred_int\n.pred_lower, .pred_upper\n\n\n\n\n\n\n\n\n\nLa tercera regla con respecto al número de filas en la salida es crítica. Por ejemplo, si alguna fila de los datos nuevos contiene valores faltantes, la salida se completará con los resultados faltantes para esas filas. Una ventaja principal de estandarizar la interfaz del modelo y los tipos de predicción en parsnip es que, cuando se utilizan diferentes modelos, la sintaxis es idéntica. Supongamos que utilizamos un árbol de decisión para modelar los datos de Ames. Fuera de la especificación del modelo, no hay diferencias significativas en la canalización del código:\n\ntree_model &lt;- \n  decision_tree(min_n = 2) %&gt;% \n  set_engine(\"rpart\") %&gt;% \n  set_mode(\"regression\")\n\ntree_fit &lt;- \n  tree_model %&gt;% \n  fit(Sale_Price ~ Longitude + Latitude, data = ames_train)\n\names_test_small %&gt;% \n  select(Sale_Price) %&gt;% \n  bind_cols(predict(tree_fit, ames_test_small))\n## # A tibble: 5 × 2\n##   Sale_Price .pred\n##        &lt;dbl&gt; &lt;dbl&gt;\n## 1       5.02  5.15\n## 2       5.39  5.15\n## 3       5.28  5.32\n## 4       5.28  5.32\n## 5       5.28  5.32\n\nEsto demuestra el beneficio de homogeneizar el proceso de análisis de datos y la sintaxis en diferentes modelos. Permite a los usuarios dedicar su tiempo a los resultados y la interpretación en lugar de tener que centrarse en las diferencias sintácticas entre los paquetes de R.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Creando Modelos Con parsnip</span>"
    ]
  },
  {
    "objectID": "06-fitting-models.html#paquetes-de-extensión-de-parsnip",
    "href": "06-fitting-models.html#paquetes-de-extensión-de-parsnip",
    "title": "6  Creando Modelos Con parsnip",
    "section": "6.4 Paquetes De Extensión De parsnip",
    "text": "6.4 Paquetes De Extensión De parsnip\nEl paquete parsnip en sí contiene interfaces para varios modelos. Sin embargo, para facilitar la instalación y el mantenimiento del paquete, existen otros paquetes tidymodels que tienen definiciones de modelo parsnip para otros conjuntos de modelos. El paquete discrim tiene definiciones de modelos para el conjunto de técnicas de clasificación llamadas métodos de análisis discriminante (como análisis discriminante lineal o cuadrático). De esta manera, se reducen las dependencias de paquetes necesarias para instalar parsnip. Puede encontrar una lista de todos los modelos que se pueden usar con parsnip (en diferentes paquetes que están en CRAN) en https://www.tidymodels.org/find/.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Creando Modelos Con parsnip</span>"
    ]
  },
  {
    "objectID": "06-fitting-models.html#sec-parsnip-addin",
    "href": "06-fitting-models.html#sec-parsnip-addin",
    "title": "6  Creando Modelos Con parsnip",
    "section": "6.5 Crear Especificaciones De Modelo",
    "text": "6.5 Crear Especificaciones De Modelo\nPuede resultar tedioso escribir muchas especificaciones de modelos o recordar cómo escribir el código para generarlas. El paquete parsnip incluye un complemento RStudio3 que puede ayudar. Ya sea eligiendo este complemento en el menú de la barra de herramientas Addins o ejecutando el código:\n\nparsnip_addin()\n\nabrirá una ventana en el panel Viewer de RStudio IDE con una lista de posibles modelos para cada modo de modelo. Estos se pueden escribir en el panel de código fuente.\nLa lista de modelos incluye modelos de los paquetes de extensión parsnip y parsnip que están en CRAN.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Creando Modelos Con parsnip</span>"
    ]
  },
  {
    "objectID": "06-fitting-models.html#sec-models-summary",
    "href": "06-fitting-models.html#sec-models-summary",
    "title": "6  Creando Modelos Con parsnip",
    "section": "6.6 Resumen Del Capítulo",
    "text": "6.6 Resumen Del Capítulo\nEste capítulo presentó el paquete parsnip, que proporciona una interfaz común para modelos en todos los paquetes R utilizando una sintaxis estándar. La interfaz y los objetos resultantes tienen una estructura predecible.\nEl código para modelar los datos de Ames que usaremos en el futuro es:\n\nlibrary(tidymodels)\ndata(ames)\names &lt;- mutate(ames, Sale_Price = log10(Sale_Price))\n\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\nlm_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\n\n\n\n\nFriedman, J, T Hastie, y R Tibshirani. 2010. «Regularization paths for generalized linear models via coordinate descent». Journal of statistical Software 33 (1): 1.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Creando Modelos Con parsnip</span>"
    ]
  },
  {
    "objectID": "06-fitting-models.html#footnotes",
    "href": "06-fitting-models.html#footnotes",
    "title": "6  Creando Modelos Con parsnip",
    "section": "",
    "text": "Tenga en cuenta que parsnip restringe la columna de resultados de un modelo de clasificación para que se codifique como un factor; el uso de valores numéricos binarios generará un error.↩︎\n¿Cuáles son las diferencias entre fit() y fit_xy()? La función fit_xy() siempre pasa los datos tal cual a la función del modelo subyacente. No creará variables ficticias/indicadoras antes de hacerlo. Cuando se usa fit() con una especificación de modelo, esto casi siempre significa que se crearán variables ficticias a partir de predictores cualitativos. Si la función subyacente requiere una matriz (como glmnet), creará la matriz. Sin embargo, si la función subyacente usa una fórmula, fit() simplemente pasa la fórmula a esa función. Estimamos que el 99% de las funciones de modelado que utilizan fórmulas generan variables ficticias. El otro 1% incluye métodos basados ​​en árboles que no requieren predictores puramente numéricos. Consulte Sección 7.4 para obtener más información sobre el uso de fórmulas en tidymodels.↩︎\nhttps://rstudio.github.io/rstudioaddins/↩︎",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Creando Modelos Con parsnip</span>"
    ]
  },
  {
    "objectID": "07-the-model-workflow.html",
    "href": "07-the-model-workflow.html",
    "title": "7  Un Flujo De Modelado",
    "section": "",
    "text": "7.1 ¿Dónde Comienza Y Termina El Modelo?\nHasta ahora, cuando hemos utilizado el término “el modelo”, nos referimos a una ecuación estructural que relaciona algunos predictores con uno o más resultados. Consideremos nuevamente la regresión lineal como ejemplo. Los datos de resultado se indican como \\(y_i\\), donde hay \\(i = 1 \\ldots n\\) muestras en el conjunto de entrenamiento. Supongamos que hay \\(p\\) predictores \\(x_{i1}, \\ldots, x_{ip}\\) que se utilizan en el modelo. La regresión lineal produce una ecuación modelo de\n\\[ \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_{i1} + \\ldots + \\hat{\\beta}_px_{ip} \\]\nSi bien este es un modelo lineal, es lineal sólo en los parámetros. Los predictores podrían ser términos no lineales (como el \\(\\log(x_i)\\)).\nPara algunos conjuntos de datos sencillos, ajustar el modelo en sí puede ser todo el proceso. Sin embargo, a menudo se presentan una variedad de opciones y pasos adicionales antes de que el modelo se ajuste:\nSi bien estos ejemplos están relacionados con pasos que ocurren antes de que el modelo se ajuste, también puede haber operaciones que ocurren después de que se crea el modelo. Cuando se crea un modelo de clasificación donde el resultado es binario (por ejemplo, “evento” y “no evento”), se acostumbra utilizar un límite de probabilidad del 50% para crear una predicción de clase discreta, también conocida como predicción dura. Por ejemplo, un modelo de clasificación podría estimar que la probabilidad de un evento era del 62%. Usando el valor predeterminado típico, la predicción difícil sería “evento”. Sin embargo, es posible que el modelo deba centrarse más en reducir los resultados falsos positivos (es decir, donde los verdaderos no eventos se clasifican como eventos). Una forma de hacerlo es elevar el límite del 50% a un valor mayor. Esto aumenta el nivel de evidencia requerido para llamar evento a una nueva muestra. Si bien esto reduce la tasa de verdaderos positivos (lo cual es malo), puede tener un efecto más dramático en la reducción de falsos positivos. La elección del valor de corte debe optimizarse utilizando datos. Este es un ejemplo de un paso de posprocesamiento que tiene un efecto significativo en el funcionamiento del modelo, aunque no esté incluido en el paso de ajuste del modelo.\nEs importante centrarse en el proceso de modelado más amplio, en lugar de ajustar únicamente el modelo específico utilizado para estimar los parámetros. Este proceso más amplio incluye cualquier paso de preprocesamiento, el ajuste del modelo en sí mismo, así como posibles actividades de posprocesamiento. En este libro, nos referiremos a este concepto más completo como flujo de modelado y resaltaremos cómo manejar todos sus componentes para producir una ecuación de modelo final.\nUnir los componentes analíticos del análisis de datos es importante por otra razón. Los capítulos futuros demostrarán cómo medir con precisión el rendimiento, así como también cómo optimizar los parámetros estructurales (es decir, ajuste del modelo). Para cuantificar correctamente el rendimiento del modelo en el conjunto de entrenamiento, [Capítulo @ sec-resampling] recomienda el uso de métodos de remuestreo. Para hacer esto correctamente, no se debe excluir de la validación ninguna parte del análisis basada en datos. Para ello, el flujo de trabajo debe incluir todos los pasos de estimación importantes.\nA modo de ejemplo, considere la extracción de señales del análisis de componentes principales (PCA). Hablaremos más sobre esto en Sección 8.4 y en el Capítulo 16; PCA es una forma de reemplazar predictores correlacionados con nuevas características artificiales que no están correlacionadas y capturan la mayor parte de la información del conjunto original. Las nuevas características podrían usarse como predictores y la regresión de mínimos cuadrados podría usarse para estimar los parámetros del modelo.\nHay dos formas de pensar sobre el flujo de trabajo del modelo. Figura 7.1 ilustra el método incorrecto: pensar que el paso de preprocesamiento de PCA no forma parte del flujo de trabajo de modelado.\nFigura 7.1: Modelo mental incorrecto de dónde ocurre la estimación del modelo en el proceso de análisis de datos\nLa falacia aquí es que, aunque PCA realiza cálculos importantes para producir los componentes, se supone que sus operaciones no tienen incertidumbre asociada con ellos. Los componentes de PCA se tratan como conocidos y, si no se incluyen en el flujo de trabajo del modelo, el efecto de PCA no se podría medir adecuadamente.\nFigura 7.2 muestra un enfoque apropiado.\nFigura 7.2: Modelo mental correcto de dónde ocurre la estimación del modelo en el proceso de análisis de datos.\nDe esta forma, el preprocesamiento PCA se considera parte del proceso de modelado.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un Flujo De Modelado</span>"
    ]
  },
  {
    "objectID": "07-the-model-workflow.html#sec-begin-model-end",
    "href": "07-the-model-workflow.html#sec-begin-model-end",
    "title": "7  Un Flujo De Modelado",
    "section": "",
    "text": "La forma convencional de pensar sobre el proceso de modelado es que sólo incluye el ajuste del modelo.\n\n\n\nSi bien nuestro modelo de ejemplo tiene predictores \\(p\\), es común comenzar con más predictores candidatos \\(p\\). Mediante un análisis de datos exploratorio o utilizando el conocimiento del dominio, algunos de los predictores pueden excluirse del análisis. En otros casos, se puede utilizar un algoritmo de selección de características para realizar una elección basada en datos para el conjunto de predictores mínimo para el modelo.\nHay ocasiones en las que falta el valor de un predictor importante. En lugar de eliminar esta muestra del conjunto de datos, el valor faltante podría imputarse utilizando otros valores de los datos. Por ejemplo, si faltara \\(x_1\\) pero estuviera correlacionado con los predictores \\(x_2\\) y \\(x_3\\), un método de imputación podría estimar la observación faltante de \\(x_1\\) a partir de los valores de \\(x_2\\) y \\(x_3\\).\nPuede resultar beneficioso transformar la escala de un predictor. Si no hay información a priori sobre cuál debería ser la nueva escala, podemos estimar la escala adecuada utilizando una técnica de transformación estadística, los datos existentes y algún criterio de optimización. Otras transformaciones, como PCA, toman grupos de predictores y los transforman en nuevas características que se utilizan como predictores.\n\n\n\n\nEn otro software, como Python o Spark, colecciones similares de pasos se denominan pipelines. En tidymodels, el término “pipeline” ya connota una secuencia de operaciones encadenadas con un operador de pipe (como %&gt;% de magrittr o el nativo más nuevo |&gt;). En lugar de utilizar terminología ambigua en este contexto, llamamos a la secuencia de operaciones computacionales relacionadas con el modelado flujos de trabajo.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un Flujo De Modelado</span>"
    ]
  },
  {
    "objectID": "07-the-model-workflow.html#conceptos-básicos-del-flujo-de-trabajo",
    "href": "07-the-model-workflow.html#conceptos-básicos-del-flujo-de-trabajo",
    "title": "7  Un Flujo De Modelado",
    "section": "7.2 Conceptos Básicos Del Flujo De Trabajo",
    "text": "7.2 Conceptos Básicos Del Flujo De Trabajo\nEl paquete workflows permite al usuario vincular objetos de modelado y preprocesamiento. Empecemos de nuevo con los datos de Ames y un modelo lineal simple:\n\nlibrary(tidymodels)  # Incluye el paquete de flujos de trabajo.\ntidymodels_prefer()\n\nlm_model &lt;- \n  linear_reg() %&gt;% \n  set_engine(\"lm\")\n\nUn flujo de trabajo siempre requiere un objeto modelo parsnip:\n\nlm_wflow &lt;- \n  workflow() %&gt;% \n  add_model(lm_model)\n\nlm_wflow\n## ══ Workflow ═════════════════════════════════════════════════════════════════════════\n## Preprocessor: None\n## Model: linear_reg()\n## \n## ── Model ────────────────────────────────────────────────────────────────────────────\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm\n\nTenga en cuenta que aún no hemos especificado cómo este flujo de trabajo debe preprocesar los datos: Preprocessor: None.\nSi nuestro modelo es muy simple, se puede utilizar una fórmula R estándar como preprocesador:\n\nlm_wflow &lt;- \n  lm_wflow %&gt;% \n  add_formula(Sale_Price ~ Longitude + Latitude)\n\nlm_wflow\n## ══ Workflow ═════════════════════════════════════════════════════════════════════════\n## Preprocessor: Formula\n## Model: linear_reg()\n## \n## ── Preprocessor ─────────────────────────────────────────────────────────────────────\n## Sale_Price ~ Longitude + Latitude\n## \n## ── Model ────────────────────────────────────────────────────────────────────────────\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm\n\nLos flujos de trabajo tienen un método fit() que se puede utilizar para crear el modelo. Usando los objetos creados en Sección 6.6:\n\nlm_fit &lt;- fit(lm_wflow, ames_train)\nlm_fit\n## ══ Workflow [trained] ═══════════════════════════════════════════════════════════════\n## Preprocessor: Formula\n## Model: linear_reg()\n## \n## ── Preprocessor ─────────────────────────────────────────────────────────────────────\n## Sale_Price ~ Longitude + Latitude\n## \n## ── Model ────────────────────────────────────────────────────────────────────────────\n## \n## Call:\n## stats::lm(formula = ..y ~ ., data = data)\n## \n## Coefficients:\n## (Intercept)    Longitude     Latitude  \n##     -302.97        -2.07         2.71\n\nTambién podemos predict() en el flujo de trabajo ajustado:\n\npredict(lm_fit, ames_test %&gt;% slice(1:3))\n## # A tibble: 3 × 1\n##   .pred\n##   &lt;dbl&gt;\n## 1  5.22\n## 2  5.21\n## 3  5.28\n\nEl método predict() sigue las mismas reglas y convenciones de nomenclatura que describimos para el paquete parsnip en Sección 6.3.\nTanto el modelo como el preprocesador se pueden eliminar o actualizar:\n\nlm_fit %&gt;% update_formula(Sale_Price ~ Longitude)\n## ══ Workflow ═════════════════════════════════════════════════════════════════════════\n## Preprocessor: Formula\n## Model: linear_reg()\n## \n## ── Preprocessor ─────────────────────────────────────────────────────────────────────\n## Sale_Price ~ Longitude\n## \n## ── Model ────────────────────────────────────────────────────────────────────────────\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm\n\nTenga en cuenta que, en este nuevo objeto, el resultado muestra que el modelo ajustado anterior se eliminó ya que la nueva fórmula es inconsistente con el ajuste del modelo anterior.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un Flujo De Modelado</span>"
    ]
  },
  {
    "objectID": "07-the-model-workflow.html#agregar-variables-sin-procesar-al-workflow",
    "href": "07-the-model-workflow.html#agregar-variables-sin-procesar-al-workflow",
    "title": "7  Un Flujo De Modelado",
    "section": "7.3 Agregar Variables Sin Procesar Al workflow()",
    "text": "7.3 Agregar Variables Sin Procesar Al workflow()\nHay otra interfaz para pasar datos al modelo, la función add_variables(), que usa una sintaxis similar a dplyr para elegir variables. La función tiene dos argumentos principales: outcomes y predictors. Estos utilizan un enfoque de selección similar al backend tidyselect de los paquetes tidyverse para capturar múltiples selectores usando c().\n\nlm_wflow &lt;- \n  lm_wflow %&gt;% \n  remove_formula() %&gt;% \n  add_variables(outcome = Sale_Price, predictors = c(Longitude, Latitude))\nlm_wflow\n## ══ Workflow ═════════════════════════════════════════════════════════════════════════\n## Preprocessor: Variables\n## Model: linear_reg()\n## \n## ── Preprocessor ─────────────────────────────────────────────────────────────────────\n## Outcomes: Sale_Price\n## Predictors: c(Longitude, Latitude)\n## \n## ── Model ────────────────────────────────────────────────────────────────────────────\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm\n\nLos predictores también podrían haberse especificado utilizando un selector más general, como\n\npredictors = c(ends_with(\"tude\"))\n\nUna ventaja es que cualquier columna de resultados especificada accidentalmente en el argumento de los predictores se eliminará silenciosamente. Esto facilita el uso de:\n\npredictors = everything()\n\nCuando el modelo se ajusta, la especificación reúne estos datos, sin modificar, en un marco de datos y los pasa a la función subyacente:\n\nfit(lm_wflow, ames_train)\n## ══ Workflow [trained] ═══════════════════════════════════════════════════════════════\n## Preprocessor: Variables\n## Model: linear_reg()\n## \n## ── Preprocessor ─────────────────────────────────────────────────────────────────────\n## Outcomes: Sale_Price\n## Predictors: c(Longitude, Latitude)\n## \n## ── Model ────────────────────────────────────────────────────────────────────────────\n## \n## Call:\n## stats::lm(formula = ..y ~ ., data = data)\n## \n## Coefficients:\n## (Intercept)    Longitude     Latitude  \n##     -302.97        -2.07         2.71\n\nSi desea que el método de modelado subyacente haga lo que normalmente haría con los datos, add_variables() puede ser una interfaz útil. Como veremos en Sección 7.4.1, también facilita especificaciones de modelado más complejas. Sin embargo, como mencionamos en la siguiente sección, modelos como glmnet y xgboost esperan que el usuario cree variables indicadoras a partir de predictores de factores. En estos casos, una interfaz de receta o fórmula suele ser una mejor opción.\nEn el próximo capítulo, veremos un preprocesador más potente (llamado recipe) que también se puede agregar a un flujo de trabajo.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un Flujo De Modelado</span>"
    ]
  },
  {
    "objectID": "07-the-model-workflow.html#sec-workflow-encoding",
    "href": "07-the-model-workflow.html#sec-workflow-encoding",
    "title": "7  Un Flujo De Modelado",
    "section": "7.4 ¿Cómo Utiliza Un workflow() La Fórmula?",
    "text": "7.4 ¿Cómo Utiliza Un workflow() La Fórmula?\nRecuerde de Sección 3.2 que el método de fórmula en R tiene múltiples propósitos (lo discutiremos más a fondo en el Capítulo 8). Uno de ellos es codificar correctamente los datos originales en un formato listo para el análisis. Esto puede implicar la ejecución de transformaciones en línea (por ejemplo, log(x)), la creación de columnas de variables ficticias, la creación de interacciones u otras expansiones de columnas, etc. Sin embargo, muchos métodos estadísticos requieren diferentes tipos de codificaciones:\n\nLa mayoría de los paquetes para modelos basados en árboles utilizan la interfaz de fórmulas pero no codifican los predictores categóricos como variables ficticias.\nLos paquetes pueden utilizar funciones en línea especiales que le indican a la función del modelo cómo tratar el predictor en el análisis. Por ejemplo, en los modelos de análisis de supervivencia, un término de fórmula como strata(site) indicaría que la columna site es una variable de estratificación. Esto significa que no debe tratarse como un predictor regular y no tiene una estimación de parámetro de ubicación correspondiente en el modelo.\nAlgunos paquetes de R han ampliado la fórmula de manera que las funciones básicas de R no pueden analizar ni ejecutar. En modelos multinivel (por ejemplo, modelos mixtos o modelos bayesianos jerárquicos), un término de modelo como (semana | sujeto) indica que la columna semana es un efecto aleatorio que tiene diferentes estimaciones de parámetros de pendiente para cada valor de la columna sujeto.\n\nUn flujo de trabajo es una interfaz de propósito general. Cuando se utiliza add_formula(), ¿cómo debería el flujo de trabajo preprocesar los datos? Dado que el preprocesamiento depende del modelo, workflows intenta emular lo que haría el modelo subyacente siempre que sea posible. Si no es posible, el procesamiento de la fórmula no debe afectar las columnas utilizadas en la fórmula. Veamos esto con más detalle.\n\nModelos basados en árboles\nCuando ajustamos un árbol a los datos, el paquete parsnip comprende lo que haría la función de modelado. Por ejemplo, si un modelo de bosque aleatorio se ajusta usando los paquetes ranger o randomForest, el flujo de trabajo sabe que las columnas de predictores que son factores deben dejarse como están.\nComo contraejemplo, un árbol potenciado creado con el paquete xgboost requiere que el usuario cree variables ficticias a partir de predictores de factores (ya que xgboost::xgb.train() no lo hará). Este requisito está integrado en el objeto de especificación del modelo y un flujo de trabajo que utiliza xgboost creará las columnas indicadoras para este motor. También tenga en cuenta que un motor diferente para árboles potenciados, C5.0, no requiere variables ficticias, por lo que el flujo de trabajo no crea ninguna.\nEsta determinación se realiza para cada combinación de modelo y motor.\n\n\n7.4.1 Fórmulas especiales y funciones en línea.\nVarios modelos multinivel se han estandarizado según una especificación de fórmula diseñada en el paquete lme4. Por ejemplo, para ajustar un modelo de regresión que tenga efectos aleatorios para los sujetos, usaríamos la siguiente fórmula:\nlibrary(lme4)\nlmer(distance ~ Sex + (age | Subject), data = Orthodont)\nEl efecto de esto es que cada sujeto tendrá un parámetro estimado de intersección y pendiente para la “edad”.\nEl problema es que los métodos estándar de R no pueden procesar adecuadamente esta fórmula:\n\nmodel.matrix(distance ~ Sex + (age | Subject), data = Orthodont)\n## Warning in Ops.ordered(age, Subject): '|' is not meaningful for ordered factors\n##      (Intercept) SexFemale age | SubjectTRUE\n## attr(,\"assign\")\n## [1] 0 1 2\n## attr(,\"contrasts\")\n## attr(,\"contrasts\")$Sex\n## [1] \"contr.treatment\"\n## \n## attr(,\"contrasts\")$`age | Subject`\n## [1] \"contr.treatment\"\n\nEl resultado es un marco de datos de cero filas.\n\nEl problema es que la fórmula especial debe ser procesada por el código del paquete subyacente, no por el enfoque estándar model.matrix().\n\nIncluso si esta fórmula pudiera usarse con model.matrix(), esto aún presentaría un problema ya que la fórmula también especifica los atributos estadísticos del modelo.\nLa solución en workflows es una fórmula de modelo suplementaria opcional que se puede pasar a add_model(). La especificación add_variables() proporciona los nombres de las columnas básicas, y luego la fórmula real dada al modelo se establece dentro de add_model():\n\nlibrary(multilevelmod)\n\nmultilevel_spec &lt;- linear_reg() %&gt;% set_engine(\"lmer\")\n\nmultilevel_workflow &lt;- \n  workflow() %&gt;% \n  # Pase los datos tal cual:\n  add_variables(outcome = distance, predictors = c(Sex, age, Subject)) %&gt;% \n  add_model(multilevel_spec, \n            # Esta fórmula se le da al modelo.\n            formula = distance ~ Sex + (age | Subject))\n\nmultilevel_fit &lt;- fit(multilevel_workflow, data = Orthodont)\nmultilevel_fit\n## ══ Workflow [trained] ═══════════════════════════════════════════════════════════════\n## Preprocessor: Variables\n## Model: linear_reg()\n## \n## ── Preprocessor ─────────────────────────────────────────────────────────────────────\n## Outcomes: distance\n## Predictors: c(Sex, age, Subject)\n## \n## ── Model ────────────────────────────────────────────────────────────────────────────\n## Linear mixed model fit by REML ['lmerMod']\n## Formula: distance ~ Sex + (age | Subject)\n##    Data: data\n## REML criterion at convergence: 471.2\n## Random effects:\n##  Groups   Name        Std.Dev. Corr \n##  Subject  (Intercept) 7.391         \n##           age         0.694    -0.97\n##  Residual             1.310         \n## Number of obs: 108, groups:  Subject, 27\n## Fixed Effects:\n## (Intercept)    SexFemale  \n##       24.52        -2.15\n\nIncluso podemos usar la función strata() mencionada anteriormente del paquete survival para el análisis de supervivencia:\n\nlibrary(censored)\n\nparametric_spec &lt;- survival_reg()\n\nparametric_workflow &lt;- \n  workflow() %&gt;% \n  add_variables(outcome = c(fustat, futime), predictors = c(age, rx)) %&gt;% \n  add_model(parametric_spec, \n            formula = Surv(futime, fustat) ~ age + strata(rx))\n\nparametric_fit &lt;- fit(parametric_workflow, data = ovarian)\nparametric_fit\n## ══ Workflow [trained] ═══════════════════════════════════════════════════════════════\n## Preprocessor: Variables\n## Model: survival_reg()\n## \n## ── Preprocessor ─────────────────────────────────────────────────────────────────────\n## Outcomes: c(fustat, futime)\n## Predictors: c(age, rx)\n## \n## ── Model ────────────────────────────────────────────────────────────────────────────\n## Call:\n## survival::survreg(formula = Surv(futime, fustat) ~ age + strata(rx), \n##     data = data, model = TRUE)\n## \n## Coefficients:\n## (Intercept)         age \n##     12.8734     -0.1034 \n## \n## Scale:\n##   rx=1   rx=2 \n## 0.7696 0.4704 \n## \n## Loglik(model)= -89.4   Loglik(intercept only)= -97.1\n##  Chisq= 15.36 on 1 degrees of freedom, p= 9e-05 \n## n= 26\n\nObserve cómo en ambas convocatorias se utilizó la fórmula específica del modelo.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un Flujo De Modelado</span>"
    ]
  },
  {
    "objectID": "07-the-model-workflow.html#sec-workflow-sets-intro",
    "href": "07-the-model-workflow.html#sec-workflow-sets-intro",
    "title": "7  Un Flujo De Modelado",
    "section": "7.5 Crear Múltiples Flujos De Trabajo A La Vez",
    "text": "7.5 Crear Múltiples Flujos De Trabajo A La Vez\nEn algunas situaciones, los datos requieren numerosos intentos para encontrar un modelo apropiado. Por ejemplo:\n\nPara los modelos predictivos, es aconsejable evaluar una variedad de tipos de modelos diferentes. Esto requiere que el usuario cree múltiples especificaciones de modelo.\nLas pruebas secuenciales de modelos suelen comenzar con un conjunto ampliado de predictores. Este “modelo completo” se compara con una secuencia del mismo modelo que elimina cada predictor por turno. Utilizando métodos básicos de prueba de hipótesis o validación empírica, se puede aislar y evaluar el efecto de cada predictor.\n\nEn estas situaciones, así como en otras, puede resultar tedioso u oneroso crear muchos flujos de trabajo a partir de diferentes conjuntos de preprocesadores y/o especificaciones de modelo. Para solucionar este problema, el paquete workflowset crea combinaciones de componentes de flujo de trabajo. Una lista de preprocesadores (por ejemplo, fórmulas, selectores dplyr u objetos de recetas de ingeniería de características que se analizan en el siguiente capítulo) se puede combinar con una lista de especificaciones de modelo, lo que da como resultado un conjunto de flujos de trabajo.\nComo ejemplo, digamos que queremos centrarnos en las diferentes formas en que se representa la ubicación de la casa en los datos de Ames. Podemos crear un conjunto de fórmulas que capturen estos predictores:\n\nlocation &lt;- list(\n  longitude = Sale_Price ~ Longitude,\n  latitude = Sale_Price ~ Latitude,\n  coords = Sale_Price ~ Longitude + Latitude,\n  neighborhood = Sale_Price ~ Neighborhood\n)\n\nEstas representaciones se pueden cruzar con uno o más modelos usando la función workflow_set(). Simplemente usaremos la especificación del modelo lineal anterior para demostrar:\n\nlibrary(workflowsets)\nlocation_models &lt;- workflow_set(preproc = location, models = list(lm = lm_model))\nlocation_models\n## # A workflow set/tibble: 4 × 4\n##   wflow_id        info             option    result    \n##   &lt;chr&gt;           &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n## 1 longitude_lm    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n## 2 latitude_lm     &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n## 3 coords_lm       &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n## 4 neighborhood_lm &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\nlocation_models$info[[1]]\n## # A tibble: 1 × 4\n##   workflow   preproc model      comment\n##   &lt;list&gt;     &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;  \n## 1 &lt;workflow&gt; formula linear_reg \"\"\nextract_workflow(location_models, id = \"coords_lm\")\n## ══ Workflow ═════════════════════════════════════════════════════════════════════════\n## Preprocessor: Formula\n## Model: linear_reg()\n## \n## ── Preprocessor ─────────────────────────────────────────────────────────────────────\n## Sale_Price ~ Longitude + Latitude\n## \n## ── Model ────────────────────────────────────────────────────────────────────────────\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm\n\nLos conjuntos de flujos de trabajo están diseñados principalmente para funcionar con remuestreo, lo cual se analiza en el Capítulo 10. Las columnas “opción” y “resultado” deben completarse con tipos específicos de objetos que resultan del remuestreo. Demostraremos esto con más detalle en los Capítulos 11 y 15.\nMientras tanto, creemos ajustes de modelo para cada fórmula y guárdelos en una nueva columna llamada fit. Usaremos las operaciones básicas dplyr y purrr:\n\nlocation_models &lt;-\n   location_models %&gt;%\n   mutate(fit = map(info, ~ fit(.x$workflow[[1]], ames_train)))\nlocation_models\n## # A workflow set/tibble: 4 × 5\n##   wflow_id        info             option    result     fit       \n##   &lt;chr&gt;           &lt;list&gt;           &lt;list&gt;    &lt;list&gt;     &lt;list&gt;    \n## 1 longitude_lm    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; &lt;workflow&gt;\n## 2 latitude_lm     &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; &lt;workflow&gt;\n## 3 coords_lm       &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; &lt;workflow&gt;\n## 4 neighborhood_lm &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; &lt;workflow&gt;\nlocation_models$fit[[1]]\n## ══ Workflow [trained] ═══════════════════════════════════════════════════════════════\n## Preprocessor: Formula\n## Model: linear_reg()\n## \n## ── Preprocessor ─────────────────────────────────────────────────────────────────────\n## Sale_Price ~ Longitude\n## \n## ── Model ────────────────────────────────────────────────────────────────────────────\n## \n## Call:\n## stats::lm(formula = ..y ~ ., data = data)\n## \n## Coefficients:\n## (Intercept)    Longitude  \n##     -184.40        -2.02\n\nUsamos una función purrr aquí para mapear nuestros modelos, pero existe un enfoque mejor y más fácil para ajustar conjuntos de flujo de trabajo que se presentará en Sección 11.1.\n\nEn general, ¡hay mucho más en los conjuntos de flujos de trabajo! Si bien hemos cubierto los conceptos básicos aquí, los matices y ventajas de los conjuntos de flujo de trabajo no se ilustrarán hasta el Capítulo 15.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un Flujo De Modelado</span>"
    ]
  },
  {
    "objectID": "07-the-model-workflow.html#evaluación-del-conjunto-de-prueba",
    "href": "07-the-model-workflow.html#evaluación-del-conjunto-de-prueba",
    "title": "7  Un Flujo De Modelado",
    "section": "7.6 Evaluación Del Conjunto De Prueba",
    "text": "7.6 Evaluación Del Conjunto De Prueba\nDigamos que hemos concluido el desarrollo de nuestro modelo y nos hemos decidido por un modelo final. Hay una función de conveniencia llamada last_fit() que ajustará el modelo a todo el conjunto de entrenamiento y lo evaluará con el conjunto de prueba.\nUsando lm_wflow como ejemplo, podemos pasar el modelo y la división inicial de entrenamiento/prueba a la función:\n\nfinal_lm_res &lt;- last_fit(lm_wflow, ames_split)\nfinal_lm_res\n## # Resampling results\n## # Manual resampling \n## # A tibble: 1 × 6\n##   splits             id               .metrics .notes   .predictions .workflow \n##   &lt;list&gt;             &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n## 1 &lt;split [2342/588]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\nObserve que last_fit() toma una división de datos como entrada, no un marco de datos. Esta función utiliza la división para generar los conjuntos de entrenamiento y prueba para el ajuste y la evaluación finales.\n\nLa columna .workflow contiene el flujo de trabajo ajustado y se puede extraer de los resultados usando:\n\nfitted_lm_wflow &lt;- extract_workflow(final_lm_res)\n\nDe manera similar, collect_metrics() y collect_predictions() proporcionan acceso a las métricas de rendimiento y predicciones, respectivamente.\n\ncollect_metrics(final_lm_res)\ncollect_predictions(final_lm_res) %&gt;% slice(1:5)\n\nVeremos más sobre last_fit() en acción y cómo usarlo nuevamente en Sección 16.6.\n\nCuando se usan conjuntos de validación, last_fit() tiene un argumento llamado add_validation_set para especificar si debemos entrenar el modelo final únicamente en el conjunto de entrenamiento (el predeterminado) o la combinación de los conjuntos de entrenamiento y validación.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un Flujo De Modelado</span>"
    ]
  },
  {
    "objectID": "07-the-model-workflow.html#sec-workflows-summary",
    "href": "07-the-model-workflow.html#sec-workflows-summary",
    "title": "7  Un Flujo De Modelado",
    "section": "7.7 Resumen Del capítulo",
    "text": "7.7 Resumen Del capítulo\nEn este capítulo, aprendió que el proceso de modelado abarca más que simplemente estimar los parámetros de un algoritmo que conecta los predictores con un resultado. Este proceso también incluye pasos de preprocesamiento y operaciones realizadas después de que se ajusta un modelo. Introdujimos un concepto llamado flujo de trabajo modelo que puede capturar los componentes importantes del proceso de modelado. También se pueden crear múltiples flujos de trabajo dentro de un conjunto de flujos de trabajo. La función last_fit() es conveniente para ajustar un modelo final al conjunto de entrenamiento y evaluar con el conjunto de prueba.\nPara los datos de Ames, el código relacionado que veremos usado nuevamente es:\n\nlibrary(tidymodels)\ndata(ames)\n\names &lt;- mutate(ames, Sale_Price = log10(Sale_Price))\n\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\nlm_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\nlm_wflow &lt;- \n  workflow() %&gt;% \n  add_model(lm_model) %&gt;% \n  add_variables(outcome = Sale_Price, predictors = c(Longitude, Latitude))\n\nlm_fit &lt;- fit(lm_wflow, ames_train)",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Un Flujo De Modelado</span>"
    ]
  },
  {
    "objectID": "08-feature-engineering.html",
    "href": "08-feature-engineering.html",
    "title": "8  Ingeniería De Características Con recipes",
    "section": "",
    "text": "8.1 Una recipe() Simple Para Los Datos De Vivienda De Ames\nEn esta sección, nos centraremos en un pequeño subconjunto de los predictores disponibles en los datos de vivienda de Ames:\nSupongamos que se ajustara un modelo de regresión lineal ordinaria inicial a estos datos. Recordando que, en el Capítulo 4, los precios de venta estaban registrados previamente, una llamada estándar a lm() podría verse así:\nlm(Sale_Price ~ Neighborhood + log10(Gr_Liv_Area) + Year_Built + Bldg_Type, data = ames)\nCuando se ejecuta esta función, los datos se convierten de un marco de datos a una matriz de diseño numérica (también llamada matriz de modelo) y luego se utiliza el método de mínimos cuadrados para estimar los parámetros. En Sección 3.2 enumeramos los múltiples propósitos de la fórmula del modelo R; Centrémonos sólo en los aspectos de manipulación de datos por ahora. Lo que hace esta fórmula se puede descomponer en una serie de pasos:\nAs mentioned in Chapter 3, the formula method will apply these data manipulations to any data, including new data, that are passed to the predict() function.\nUna receta también es un objeto que define una serie de pasos para el procesamiento de datos. A diferencia del método de fórmula dentro de una función de modelado, la receta define los pasos mediante funciones step_*() sin ejecutarlas inmediatamente; es sólo una especificación de lo que se debe hacer. Aquí hay una receta equivalente a la fórmula anterior que se basa en el resumen del código en Sección 5.5:\nlibrary(tidymodels) # Incluye el paquete de recetas.\ntidymodels_prefer()\n\nsimple_ames &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,\n         data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_dummy(all_nominal_predictors())\nsimple_ames\n## \n## ── Recipe ───────────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:   1\n## predictor: 4\n## \n## ── Operations\n## • Log transformation on: Gr_Liv_Area\n## • Dummy variables from: all_nominal_predictors()\nAnalicemos esto:\nLa función all_nominal_predictors() captura los nombres de cualquier columna predictora que actualmente sea de naturaleza factor o carácter (es decir, nominal). Esta es una función selectora similar a dplyr similar a starts_with() o matches() pero que solo se puede usar dentro de una receta.\n¿Cuál es la ventaja de utilizar una receta sobre una fórmula o predictores sin procesar? Hay algunos, que incluyen:",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Ingeniería De Características Con recipes</span>"
    ]
  },
  {
    "objectID": "08-feature-engineering.html#una-recipe-simple-para-los-datos-de-vivienda-de-ames",
    "href": "08-feature-engineering.html#una-recipe-simple-para-los-datos-de-vivienda-de-ames",
    "title": "8  Ingeniería De Características Con recipes",
    "section": "",
    "text": "El vecindario (cualitativo, con vecindarios 29 en el conjunto de entrenamiento)\nLa superficie habitable bruta sobre el nivel del suelo (continua, denominada Gr_Liv_Area)\nEl año de construcción (Year_Built)\nEl tipo de edificio (Bldg_Type con valores OneFam (\\(n = 1,936\\)), TwoFmCon (\\(n =    50\\)), Duplex (\\(n =    88\\)), Twnhs (\\(n =    77\\)), and TwnhsE (\\(n =   191\\)))\n\n\n\n\n\nEl precio de venta se define como el resultado, mientras que las variables de vecindario, superficie habitable bruta, año de construcción y tipo de edificio se definen como predictores.\nSe aplica una transformación logarítmica al predictor de superficie habitable bruta.\nLas columnas de vecindario y tipo de edificio se convierten de un formato no numérico a un formato numérico (ya que los mínimos cuadrados requieren predictores numéricos).\n\n\n\n\n\n\nLa llamada a recipe() (recipe es receta en español) con una fórmula le dice a la receta las funciones de los “ingredientes” o variables (por ejemplo, predictor, resultado). Solo utiliza los datos ames_train para determinar los tipos de datos de las columnas.\nstep_log() declara que Gr_Liv_Area debe transformarse en registros.\nstep_dummy() especifica qué variables deben convertirse de un formato cualitativo a un formato cuantitativo, en este caso, utilizando variables ficticias o indicadoras. Un indicador o variable ficticia es una variable numérica binaria (una columna de unos y ceros) que codifica información cualitativa; Profundizaremos en este tipo de variables en Sección 8.4.1.\n\n\n\nOtros selectores específicos del paquete recipes son: all_numeric_predictors(), all_numeric(), all_predictors() y all_outcomes(). Al igual que con dplyr, se pueden usar una o más expresiones sin comillas, separadas por comas, para seleccionar qué columnas se ven afectadas por cada paso.\n\n\n\nEstos cálculos se pueden reciclar entre modelos, ya que no están estrechamente vinculados a la función de modelado.\nUna receta permite un conjunto más amplio de opciones de procesamiento de datos que las que pueden ofrecer las fórmulas.\nLa sintaxis puede ser muy compacta. Por ejemplo, all_nominal_predictors() se puede utilizar para capturar muchas variables para tipos específicos de procesamiento, mientras que una fórmula requeriría que cada una de ellas se enumere explícitamente.\nTodo el procesamiento de datos se puede capturar en un único objeto R en lugar de en scripts que se repiten o incluso se distribuyen en diferentes archivos.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Ingeniería De Características Con recipes</span>"
    ]
  },
  {
    "objectID": "08-feature-engineering.html#sec-using-recipes",
    "href": "08-feature-engineering.html#sec-using-recipes",
    "title": "8  Ingeniería De Características Con recipes",
    "section": "8.2 Usando Recetas",
    "text": "8.2 Usando Recetas\nComo comentamos en el Capítulo 7, las opciones de preprocesamiento y la ingeniería de características normalmente deben considerarse parte de un flujo de trabajo de modelado, no una tarea separada. El paquete workflows contiene funciones de alto nivel para manejar diferentes tipos de preprocesadores. Nuestro flujo de trabajo anterior (lm_wflow) usaba un conjunto simple de selectores dplyr. Para mejorar ese enfoque con ingeniería de características más compleja, usemos la receta simple_ames para preprocesar datos para modelar.\nEste objeto se puede adjuntar al flujo de trabajo:\n\nlm_wflow %&gt;% \n  add_recipe(simple_ames)\n## Error in `add_recipe()`:\n## ! A recipe cannot be added when variables already exist.\n\n¡Eso no funcionó! Solo podemos tener un método de preprocesamiento a la vez, por lo que debemos eliminar el preprocesador existente antes de agregar la receta.\n\nlm_wflow &lt;- \n  lm_wflow %&gt;% \n  remove_variables() %&gt;% \n  add_recipe(simple_ames)\nlm_wflow\n## ══ Workflow ═════════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ─────────────────────────────────────────────────────────────────────\n## 2 Recipe Steps\n## \n## • step_log()\n## • step_dummy()\n## \n## ── Model ────────────────────────────────────────────────────────────────────────────\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm\n\nEstimemos tanto la receta como el modelo usando una simple llamada a fit():\n\nlm_fit &lt;- fit(lm_wflow, ames_train)\n\nEl método predict() aplica el mismo preprocesamiento que se usó en el conjunto de entrenamiento a los nuevos datos antes de pasarlos al método predict() del modelo:\n\npredict(lm_fit, ames_test %&gt;% slice(1:3))\n## # A tibble: 3 × 1\n##   .pred\n##   &lt;dbl&gt;\n## 1  5.08\n## 2  5.32\n## 3  5.28\n\nSi necesitamos el objeto del modelo básico o la receta, existen funciones extract_* que pueden recuperarlos:\n\n# Obtén la receta una vez estimada:\nlm_fit %&gt;% \n  extract_recipe(estimated = TRUE)\n## \n## ── Recipe ───────────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:   1\n## predictor: 4\n## \n## ── Training information\n## Training data contained 2342 data points and no incomplete rows.\n## \n## ── Operations\n## • Log transformation on: Gr_Liv_Area | Trained\n## • Dummy variables from: Neighborhood and Bldg_Type | Trained\n\n# Para ordenar el ajuste del modelo:\nlm_fit %&gt;% \n  # Esto devuelve el objeto parsnip:\n  extract_fit_parsnip() %&gt;% \n  # Ahora ordena el objeto del modelo lineal:\n  tidy() %&gt;% \n  slice(1:5)\n## # A tibble: 5 × 5\n##   term                       estimate std.error statistic   p.value\n##   &lt;chr&gt;                         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)                -0.669    0.231        -2.90 3.80e-  3\n## 2 Gr_Liv_Area                 0.620    0.0143       43.2  2.63e-299\n## 3 Year_Built                  0.00200  0.000117     17.1  6.16e- 62\n## 4 Neighborhood_College_Creek  0.0178   0.00819       2.17 3.02e-  2\n## 5 Neighborhood_Old_Town      -0.0330   0.00838      -3.93 8.66e-  5\n\n\nLas herramientas para usar (y depurar) recetas fuera de los objetos del flujo de trabajo se describen en Sección 16.4.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Ingeniería De Características Con recipes</span>"
    ]
  },
  {
    "objectID": "08-feature-engineering.html#cómo-se-utilizan-los-datos-en-recipe",
    "href": "08-feature-engineering.html#cómo-se-utilizan-los-datos-en-recipe",
    "title": "8  Ingeniería De Características Con recipes",
    "section": "8.3 Cómo Se Utilizan Los Datos En recipe()",
    "text": "8.3 Cómo Se Utilizan Los Datos En recipe()\nLos datos se pasan a recetas en diferentes etapas.\nPrimero, al llamar a recipe(..., data), el conjunto de datos se usa para determinar los tipos de datos de cada columna, de modo que se puedan usar selectores como all_numeric() o all_numeric_predictors().\nEn segundo lugar, cuando se preparan los datos usando “fit(workflow, data)”, los datos de entrenamiento se utilizan para todas las operaciones de estimación, incluida una receta que puede ser parte del “workflow”, desde determinar los niveles de los factores hasta calcular los componentes de PCA y todo lo demás.\n\nTodos los pasos de preprocesamiento e ingeniería de funciones utilizan solo los datos de entrenamiento. De lo contrario, la fuga de información puede afectar negativamente el rendimiento del modelo cuando se utiliza con datos nuevos.\n\nFinalmente, cuando se usa predict(workflow, new_data), ningún modelo o parámetro de preprocesador como los de las recetas se reestima usando los valores en new_data. Tome el centrado y el escalado usando step_normalize() como ejemplo. Con este paso, las medias y las desviaciones estándar de las columnas apropiadas se determinan a partir del conjunto de entrenamiento; Las nuevas muestras en el momento de la predicción se estandarizan utilizando estos valores del entrenamiento cuando se invoca predict().",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Ingeniería De Características Con recipes</span>"
    ]
  },
  {
    "objectID": "08-feature-engineering.html#sec-example-steps",
    "href": "08-feature-engineering.html#sec-example-steps",
    "title": "8  Ingeniería De Características Con recipes",
    "section": "8.4 Ejemplos De Pasos De Recetas",
    "text": "8.4 Ejemplos De Pasos De Recetas\nAntes de continuar, hagamos un recorrido extenso por las capacidades de recipes y exploremos algunas de las funciones más importantes de step_*(). Cada una de estas funciones de pasos de recetas especifica un posible paso específico en un proceso de ingeniería de características, y diferentes pasos de recetas pueden tener diferentes efectos en las columnas de datos.\n\n8.4.1 Codificación de datos cualitativos en formato numérico\nUna de las tareas de ingeniería de características más comunes es transformar datos nominales o cualitativos (factores o caracteres) para que puedan codificarse o representarse numéricamente. A veces podemos alterar los niveles de factores de una columna cualitativa de manera útil antes de dicha transformación. Por ejemplo, step_unknown() se puede utilizar para cambiar los valores faltantes a un nivel de factor dedicado. De manera similar, si anticipamos que se puede encontrar un nuevo nivel de factor en datos futuros, step_novel() puede asignar un nuevo nivel para este propósito.\nAdemás, step_other() se puede utilizar para analizar las frecuencias de los niveles de factores en el conjunto de entrenamiento y convertir valores que ocurren con poca frecuencia a un nivel general de “otro”, con un umbral que se puede especificar. Un buen ejemplo es el predictor Neighborhood de nuestros datos, que se muestra en Figura 8.1.\n\n\n\n\n\n\n\n\nFigura 8.1: Frecuencias de barrios en el conjunto de entrenamiento de Ames\n\n\n\n\n\nAquí vemos que dos vecindarios tienen menos de cinco propiedades en los datos de entrenamiento (Landmark y Green Hills); en este caso, no se incluyó ninguna casa en el vecindario Landmark en el conjunto de pruebas. Para algunos modelos, puede resultar problemático tener variables ficticias con una única entrada distinta de cero en la columna. Como mínimo, es muy improbable que estas características sean importantes para un modelo. Si agregamos step_other(Neighborhood, umbral = 0.01) a nuestra receta, el 1% inferior de los vecindarios se agrupará en un nuevo nivel llamado “otros”. En este conjunto de entrenamiento, esto capturará los vecindarios seven.\nPara los datos de Ames, podemos modificar la receta para usar:\n\nsimple_ames &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,\n         data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors())\n\n\nMuchos, pero no todos, los cálculos del modelo subyacente requieren que los valores predictores se codifiquen como números. Las excepciones notables incluyen modelos basados en árboles, modelos basados en reglas y modelos ingenuos de Bayes.\n\nEl método más común para convertir un factor predictivo a un formato numérico es crear variables ficticias o indicadoras. Tomemos el predictor en los datos de Ames para el tipo de edificio, que es una variable factorial con cinco niveles (ver Tabla 8.1). Para variables ficticias, la única columna Bldg_Type se reemplazaría con cuatro columnas numéricas cuyos valores son cero o uno. Estas variables binarias representan valores de nivel de factor específicos. En R, la convención es excluir una columna para el primer nivel de factor (OneFam, en este caso). La columna Bldg_Type se reemplazaría con una columna llamada TwoFmCon que es uno cuando la fila tiene ese valor y cero en caso contrario. Se crean otras tres columnas de manera similar:\n\n\n\nTabla 8.1: Ilustración de codificaciones binarias (es decir, variables ficticias) para un predictor cualitativo.\n\n\n\n\n\n\nRaw Data\nTwoFmCon\nDuplex\nTwnhs\nTwnhsE\n\n\n\n\nOneFam\n0\n0\n0\n0\n\n\nTwoFmCon\n1\n0\n0\n0\n\n\nDuplex\n0\n1\n0\n0\n\n\nTwnhs\n0\n0\n1\n0\n\n\nTwnhsE\n0\n0\n0\n1\n\n\n\n\n\n\n\n\n\n¿Por qué no los cinco? La razón más básica es la simplicidad; Si conoce el valor de estas cuatro columnas, puede determinar el último valor porque son categorías mutuamente excluyentes. Más técnicamente, la justificación clásica es que varios modelos, incluida la regresión lineal ordinaria, tienen problemas numéricos cuando existen dependencias lineales entre columnas. Si se incluyen las cinco columnas de indicadores de tipo de edificio, se sumarían a la columna de intersección (si hay una). Esto causaría un problema, o quizás un error total, en el álgebra matricial subyacente.\nEl conjunto completo de codificaciones se puede utilizar para algunos modelos. Esto se denomina tradicionalmente codificación one-hot y se puede lograr utilizando el argumento one_hot de step_dummy().\nUna característica útil de step_dummy() es que hay más control sobre cómo se nombran las variables ficticias resultantes. En base R, los nombres de variables ficticias combinan el nombre de la variable con el nivel, lo que da como resultado nombres como NeighborhoodVeenker. Las recetas, de forma predeterminada, usan un guión bajo como separador entre el nombre y el nivel (por ejemplo, Neighborhood_Veenker) y existe una opción para usar formato personalizado para los nombres. La convención de nomenclatura predeterminada en recipes hace que sea más fácil capturar esas nuevas columnas en pasos futuros usando un selector, como starts_with(\"Neighborhood_\").\nLas variables ficticias tradicionales requieren que se conozcan todas las categorías posibles para crear un conjunto completo de características numéricas. Existen otros métodos para realizar esta transformación a un formato numérico. Los métodos de hashing de características solo consideran el valor de la categoría para asignarlo a un grupo predefinido de variables ficticias. Las codificaciones de efecto o probabilidad reemplazan los datos originales con una única columna numérica que mide el efecto de esos datos. Tanto el hash de características como la codificación de efectos pueden manejar sin problemas situaciones en las que se encuentra un nivel de factor novedoso en los datos. El Capítulo 17 explora estos y otros métodos para codificar datos categóricos, más allá de simples variables ficticias o indicadoras.\n\nLos diferentes pasos de una receta se comportan de manera diferente cuando se aplican a variables de los datos. Por ejemplo, step_log() modifica una columna en su lugar sin cambiar el nombre. Otros pasos, como step_dummy(), eliminan la columna de datos original y la reemplazan con una o más columnas con nombres diferentes. El efecto de un paso de receta depende del tipo de transformación de ingeniería de características que se realiza.\n\n\n\n8.4.2 Términos de interacción\nLos efectos de interacción involucran dos o más predictores. Tal efecto ocurre cuando un predictor tiene un efecto sobre el resultado que depende de uno o más predictores. Por ejemplo, si intenta predecir cuánto tráfico habrá durante su viaje, dos posibles predictores podrían ser la hora específica del día en que viaja y el clima. Sin embargo, la relación entre la cantidad de tráfico y el mal tiempo es diferente según el momento del día. En este caso, podría agregar un término de interacción entre los dos predictores al modelo junto con los dos predictores originales (que se denominan efectos principales). Numéricamente, un término de interacción entre predictores se codifica como su producto. Las interacciones se definen en términos de su efecto sobre el resultado y pueden ser combinaciones de diferentes tipos de datos (por ejemplo, numéricos, categóricos, etc.). Capítulo 7 de Kuhn y Johnson (2020) analiza las interacciones y cómo detectarlas con mayor detalle.\nDespués de explorar el conjunto de entrenamiento de Ames, podríamos encontrar que las pendientes de regresión para el área habitable bruta difieren para diferentes tipos de edificios, como se muestra en Figura 8.2.\n\nggplot(ames_train, aes(x = Gr_Liv_Area, y = 10^Sale_Price)) + \n  geom_point(alpha = .2) + \n  facet_wrap(~ Bldg_Type) + \n  geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = \"lightblue\") + \n  scale_x_log10() + \n  scale_y_log10() + \n  labs(x = \"Área Habitable Bruta\", y = \"Precio de Venta (USD)\")\n\n\n\n\n\n\n\n\n\nFigura 8.2: Superficie habitable bruta (en unidades log-10) versus precio de venta (también en unidades log-10) para cinco tipos diferentes de edificios\n\n\n\n\n\n¿Cómo se especifican las interacciones en una receta? Una fórmula base R requeriría una interacción usando :, por lo que usaríamos:\nSale_Price ~ Neighborhood + log10(Gr_Liv_Area) + Bldg_Type + \n  log10(Gr_Liv_Area):Bldg_Type\n# o\nSale_Price ~ Neighborhood + log10(Gr_Liv_Area) * Bldg_Type \ndonde * expande esas columnas a los efectos principales y al término de interacción. Nuevamente, el método de fórmula hace muchas cosas simultáneamente y comprende que una variable de factor (como Bldg_Type) debe expandirse primero a variables ficticias y que la interacción debe involucrar a todas las columnas binarias resultantes.\nLas recetas son más explícitas y secuenciales y te dan más control. Con la receta actual, step_dummy() ya ha creado variables ficticias. ¿Cómo los combinaríamos para una interacción? El paso adicional se vería así step_interact(~ términos de interacción) donde los términos en el lado derecho de la tilde son las interacciones. Estos pueden incluir selectores, por lo que sería apropiado utilizar:\n\nsimple_ames &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,\n         data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  # Gr_Liv_Area está en la escala logarítmica de un paso anterior\n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") )\n\nSe pueden especificar interacciones adicionales en esta fórmula separándolas por +. También tenga en cuenta que la receta sólo utilizará interacciones entre diferentes variables; si la fórmula usa var_1:var_1, este término se ignorará.\nSupongamos que, en una receta, todavía no hemos creado variables ficticias para los tipos de edificios. Sería inapropiado incluir una columna de factores en este paso, como por ejemplo:\n step_interact( ~ Gr_Liv_Area:Bldg_Type )\nEsto le dice al código subyacente (base R) utilizado por step_interact() para crear variables ficticias y luego formar las interacciones. De hecho, si esto ocurre, una advertencia indica que esto podría generar resultados inesperados.\n\nEste comportamiento le brinda más control, pero es diferente de la fórmula del modelo estándar de R.\n\nAl igual que con el nombre de variables ficticias, recipes proporciona nombres más coherentes para los términos de interacción. En este caso, la interacción se denomina Gr_Liv_Area_x_Bldg_Type_Duplex en lugar de Gr_Liv_Area:Bldg_TypeDuplex (que no es un nombre de columna válido para un marco de datos).\n\nRecuerda que el orden importa. La superficie habitable bruta se transforma logarítmicamente antes del término de interacción. Las interacciones posteriores con esta variable también utilizarán la escala logarítmica.\n\n\n\n8.4.3 Funciones splines\nCuando un predictor tiene una relación no lineal con el resultado, algunos tipos de modelos predictivos pueden aproximarse adaptativamente a esta relación durante el entrenamiento. Sin embargo, lo más simple suele ser mejor y no es raro intentar utilizar un modelo simple, como un ajuste lineal, y agregar características no lineales específicas para los predictores que puedan necesitarlas, como la longitud y la latitud para los datos de vivienda de Ames. Un método común para hacer esto es usar funciones spline para representar los datos. Los splines reemplazan el predictor numérico existente con un conjunto de columnas que permiten que un modelo emule una relación flexible y no lineal. A medida que se agregan más términos spline a los datos, aumenta la capacidad de representar la relación de forma no lineal. Desafortunadamente, también puede aumentar la probabilidad de detectar tendencias de datos que ocurren por casualidad (es decir, sobreajuste).\nSi alguna vez usó geom_smooth() dentro de un ggplot, probablemente haya usado una representación spline de los datos. Por ejemplo, cada panel en Figura 8.3 utiliza un número diferente de splines suaves para el predictor de latitud:\n\nlibrary(patchwork)\nlibrary(splines)\n\nplot_smoother &lt;- function(deg_free) {\n  ggplot(ames_train, aes(x = Latitude, y = 10^Sale_Price)) + \n    geom_point(alpha = .2) + \n    scale_y_log10() +\n    geom_smooth(\n      method = lm,\n      formula = y ~ ns(x, df = deg_free),\n      color = \"lightblue\",\n      se = FALSE\n    ) +\n    labs(title = paste(deg_free, \"Términos Spline\"),\n         y = \"Precio de Venta (USD)\")\n}\n\n( plot_smoother(2) + plot_smoother(5) ) / ( plot_smoother(20) + plot_smoother(100) )\n\n\n\n\n\n\n\n\n\nFigura 8.3: Precio de venta versus latitud, con líneas de tendencia que utilizan splines naturales con diferentes grados de libertad\n\n\n\n\n\nLa función ns() en el paquete splines genera columnas de características usando funciones llamadas splines naturales.\nAlgunos paneles en Figura 8.3 claramente no encajan bien; dos términos no se ajustan bien a los datos, mientras que 100 términos se ajustan demasiado. Los paneles con veinticinco términos parecen ajustes razonablemente suaves que captan los patrones principales de los datos. Esto indica que la cantidad adecuada de “no linealidad” es importante. El número de términos spline podría entonces considerarse un parámetro de ajuste para este modelo. Estos tipos de parámetros se exploran en el Capítulo 12.\nEn recipes, varios pasos pueden crear este tipo de términos. Para agregar una representación spline natural para este predictor:\n\nrecipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Latitude,\n         data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;% \n  step_ns(Latitude, deg_free = 20)\n\nEl usuario necesitaría determinar si tanto la vecindad como la latitud deberían estar en el modelo, ya que ambos representan los mismos datos subyacentes de diferentes maneras.\n\n\n8.4.4 Extracción de características\nOtro método común para representar múltiples características a la vez se llama extracción de características. La mayoría de estas técnicas crean nuevas características a partir de los predictores que capturan la información en un conjunto más amplio en su conjunto. Por ejemplo, el análisis de componentes principales (PCA) intenta extraer la mayor cantidad posible de información original en el conjunto de predictores utilizando un número menor de características. PCA es un método de extracción lineal, lo que significa que cada característica nueva es una combinación lineal de los predictores originales. Un aspecto interesante de PCA es que cada una de las nuevas características, llamadas componentes principales o puntuaciones de PCA, no están correlacionadas entre sí. Debido a esto, PCA puede resultar muy eficaz para reducir la correlación entre predictores. Tenga en cuenta que PCA sólo conoce los predictores; Es posible que las nuevas funciones de PCA no estén asociadas con el resultado.\nEn los datos de Ames, varios predictores miden el tamaño de la propiedad, como el tamaño total del sótano (Total_Bsmt_SF), el tamaño del primer piso (First_Flr_SF), la superficie habitable bruta (Gr_Liv_Area), etc. PCA podría ser una opción para representar estas variables potencialmente redundantes como un conjunto de características más pequeño. Además del área habitable bruta, estos predictores tienen el sufijo “SF” en sus nombres (para pies cuadrados), por lo que un paso de receta para PCA podría verse así:\n  # Utilice una expresión regular para capturar predictores del tamaño de la casa:\n  step_pca(matches(\"(SF$)|(Gr_Liv)\"))\nTenga en cuenta que todas estas columnas se miden en pies cuadrados. PCA supone que todos los predictores están en la misma escala. Eso es cierto en este caso, pero a menudo este paso puede ir precedido de step_normalize(), que centrará y escalará cada columna.\nExisten pasos de recetas para otros métodos de extracción, como: análisis de componentes independientes (ICA), factorización matricial no negativa (NNMF), escalamiento multidimensional (MDS), aproximación y proyección de variedades uniformes (UMAP) y otros.\n\n\n8.4.5 Pasos de muestreo de filas\nLos pasos de una receta también pueden afectar las filas de un conjunto de datos. Por ejemplo, las técnicas de submuestreo para desequilibrios de clases cambian las proporciones de clases en los datos que se proporcionan al modelo; estas técnicas a menudo no mejoran el rendimiento general, pero pueden generar distribuciones de mejor comportamiento de las probabilidades de clase predichas. Estos son enfoques que puede probar al submuestrear sus datos con desequilibrio de clases:\n\nReducción de resolución los datos mantienen la clase minoritaria y toman una muestra aleatoria de la clase mayoritaria para que las frecuencias de las clases estén equilibradas.\nUpsampling replica muestras de la clase minoritaria para equilibrar las clases. Algunas técnicas hacen esto sintetizando nuevas muestras que se asemejan a los datos de la clase minoritaria, mientras que otros métodos simplemente agregan las mismas muestras minoritarias repetidamente.\nLos métodos híbridos hacen una combinación de ambos.\n\nEl paquete themis tiene pasos de receta que se pueden usar para abordar el desequilibrio de clases mediante submuestreo. Para una reducción de resolución simple, usaríamos:\n  step_downsample(outcome_column_name)\n\nSólo el conjunto de entrenamiento debería verse afectado por estas técnicas. El conjunto de prueba u otras muestras reservadas deben dejarse como están cuando se procesan utilizando la receta. Por esta razón, todos los pasos de submuestreo tienen por defecto el argumento “skip” para que tenga un valor de “TRUE” (Sección 8.5).\n\nOtras funciones de pasos también están basadas en filas: step_filter(), step_sample(), step_slice() y step_arrange(). En casi todos los usos de estos pasos, el argumento skip debe establecerse en TRUE.\n\n\n8.4.6 Transformaciones generales\nReflejando la operación original dplyr, step_mutate() se puede utilizar para realizar una variedad de operaciones básicas con los datos. Se utiliza mejor para transformaciones sencillas como calcular una proporción de dos variables, como Bedroom_AbvGr / Full_Bath, la proporción entre dormitorios y baños para los datos de vivienda de Ames.\n\nAl utilizar este paso flexible, tenga especial cuidado para evitar la fuga de datos en el preprocesamiento. Considere, por ejemplo, la transformación x = w &gt; mean(w). Cuando se aplica a datos nuevos o datos de prueba, esta transformación usaría la media de “w” de los datos nuevos, no la media de “w” de los datos de entrenamiento.\n\n\n\n8.4.7 Procesamiento natural del lenguaje\nLas recetas también pueden manejar datos que no están en la estructura tradicional donde las columnas son características. Por ejemplo, el paquete textrecipes puede aplicar métodos de procesamiento de lenguaje natural a los datos. La columna de entrada suele ser una cadena de texto y se pueden utilizar diferentes pasos para tokenizar los datos (por ejemplo, dividir el texto en palabras separadas), filtrar tokens y crear nuevas características apropiadas para el modelado.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Ingeniería De Características Con recipes</span>"
    ]
  },
  {
    "objectID": "08-feature-engineering.html#sec-skip-equals-true",
    "href": "08-feature-engineering.html#sec-skip-equals-true",
    "title": "8  Ingeniería De Características Con recipes",
    "section": "8.5 Saltarse Pasos Para Obtener Nuevos Datos",
    "text": "8.5 Saltarse Pasos Para Obtener Nuevos Datos\nLos datos del precio de venta ya están transformados logarítmicamente en el marco de datos “ames”. ¿Por qué no utilizar?\n step_log(Sale_Price, base = 10)\nEsto provocará un fallo cuando la receta se aplique a propiedades nuevas con un precio de venta desconocido. Dado que lo que intentamos predecir es el precio, probablemente no habrá una columna en los datos para esta variable. De hecho, para evitar la fuga de información, muchos paquetes de tidymodels aíslan los datos que se utilizan al realizar predicciones. Esto significa que el conjunto de entrenamiento y las columnas de resultados no están disponibles para su uso en el momento de la predicción.\n\nPara transformaciones simples de la(s) columna(s) de resultados, sugerimos encarecidamente que esas operaciones se realicen fuera de la receta.\n\nSin embargo, hay otras circunstancias en las que esta no es una solución adecuada. Por ejemplo, en modelos de clasificación en los que existe un desequilibrio de clases grave, es común realizar un submuestreo de los datos que se proporcionan a la función de modelado. Por ejemplo, supongamos que hubiera dos clases y una tasa de eventos del 10 %. Un enfoque simple, aunque controvertido, sería reducir la muestra de los datos para que el modelo reciba todos los eventos y un 10% aleatorio de las muestras sin eventos.\nEl problema es que no se debe aplicar el mismo proceso de submuestreo a los datos que se predicen. Como resultado, cuando utilizamos una receta, necesitamos un mecanismo para garantizar que algunas operaciones se apliquen solo a los datos proporcionados al modelo. Cada función de paso tiene una opción llamada skip que, cuando se establece en TRUE, será ignorada por la función predict(). De esta manera, puede aislar los pasos que afectan los datos del modelado sin causar errores cuando se aplican a nuevas muestras. Sin embargo, todos los pasos se aplican cuando se usa fit().\nAl momento de escribir este artículo, las funciones de paso en los paquetes recipes y themis que solo se aplican a los datos de entrenamiento son: step_adasyn(), step_bsmote(), step_downsample(), step_filter(), step_naomit(), step_nearmiss(), step_rose(), step_sample(), step_slice(), step_smote(), step_smotenc(), step_tomek(), and step_upsample().",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Ingeniería De Características Con recipes</span>"
    ]
  },
  {
    "objectID": "08-feature-engineering.html#ordenar-recipe",
    "href": "08-feature-engineering.html#ordenar-recipe",
    "title": "8  Ingeniería De Características Con recipes",
    "section": "8.6 Ordenar recipe()",
    "text": "8.6 Ordenar recipe()\nEn ?sec-tidines-modeling, introdujimos el verbo tidy() para objetos estadísticos. También hay un método tidy() para recetas, así como pasos de recetas individuales. Antes de continuar, creemos una receta extendida para los datos de Ames usando algunos de los nuevos pasos que hemos discutido en este capítulo:\n\names_rec &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;% \n  step_ns(Latitude, Longitude, deg_free = 20)\n\nEl método tidy(), cuando se llama con el objeto de receta, ofrece un resumen de los pasos de la receta:\n\ntidy(ames_rec)\n## # A tibble: 5 × 6\n##   number operation type     trained skip  id            \n##    &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;    &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;         \n## 1      1 step      log      FALSE   FALSE log_66JTU     \n## 2      2 step      other    FALSE   FALSE other_ePfcw   \n## 3      3 step      dummy    FALSE   FALSE dummy_Z18Cl   \n## 4      4 step      interact FALSE   FALSE interact_JLU36\n## 5      5 step      ns       FALSE   FALSE ns_rvsqQ\n\nEste resultado puede ser útil para identificar pasos individuales, quizás para luego poder ejecutar el método tidy() en un paso específico.\nPodemos especificar el argumento id en cualquier llamada de función de paso; de lo contrario, se genera utilizando un sufijo aleatorio. Establecer este valor puede resultar útil si se agrega el mismo tipo de paso a la receta más de una vez. Especifiquemos el id con anticipación para step_other(), ya que queremos tidy():\n\names_rec &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.01, id = \"my_id\") %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;% \n  step_ns(Latitude, Longitude, deg_free = 20)\n\nReacondicionaremos el flujo de trabajo con esta nueva receta:\n\nlm_wflow &lt;- \n  workflow() %&gt;% \n  add_model(lm_model) %&gt;% \n  add_recipe(ames_rec)\n\nlm_fit &lt;- fit(lm_wflow, ames_train)\n\nEl método tidy() se puede volver a llamar junto con el identificador id que especificamos para obtener nuestros resultados al aplicar step_other():\n\nestimated_recipe &lt;- \n  lm_fit %&gt;% \n  extract_recipe(estimated = TRUE)\n\ntidy(estimated_recipe, id = \"my_id\")\n## # A tibble: 22 × 3\n##   terms        retained           id   \n##   &lt;chr&gt;        &lt;chr&gt;              &lt;chr&gt;\n## 1 Neighborhood North_Ames         my_id\n## 2 Neighborhood College_Creek      my_id\n## 3 Neighborhood Old_Town           my_id\n## 4 Neighborhood Edwards            my_id\n## 5 Neighborhood Somerset           my_id\n## 6 Neighborhood Northridge_Heights my_id\n## # ℹ 16 more rows\n\nLos resultados de tidy() que vemos aquí al usar step_other() muestran qué niveles de factor se retuvieron, es decir, no se agregaron a la nueva categoría “otros”.\nEl método tidy() también se puede llamar con el identificador number, si sabemos qué paso de la receta necesitamos:\n\ntidy(estimated_recipe, number = 2)\n## # A tibble: 22 × 3\n##   terms        retained           id   \n##   &lt;chr&gt;        &lt;chr&gt;              &lt;chr&gt;\n## 1 Neighborhood North_Ames         my_id\n## 2 Neighborhood College_Creek      my_id\n## 3 Neighborhood Old_Town           my_id\n## 4 Neighborhood Edwards            my_id\n## 5 Neighborhood Somerset           my_id\n## 6 Neighborhood Northridge_Heights my_id\n## # ℹ 16 more rows\n\nCada método tidy() devuelve la información relevante sobre ese paso. Por ejemplo, el método tidy() para step_dummy() devuelve una columna con las variables que se convirtieron en variables ficticias y otra columna con todos los niveles conocidos para cada columna.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Ingeniería De Características Con recipes</span>"
    ]
  },
  {
    "objectID": "08-feature-engineering.html#roles-de-columna",
    "href": "08-feature-engineering.html#roles-de-columna",
    "title": "8  Ingeniería De Características Con recipes",
    "section": "8.7 Roles De Columna",
    "text": "8.7 Roles De Columna\nCuando se usa una fórmula con la llamada inicial a receta(), asigna roles a cada una de las columnas, dependiendo de en qué lado de la tilde se encuentran. Esos roles son \"predictor\" o \"outcome\". Sin embargo, se pueden asignar otros roles según sea necesario.\nPor ejemplo, en nuestro conjunto de datos de Ames, los datos originales sin procesar contenían una columna para la dirección.1 Puede resultar útil mantener esa columna en los datos para que, después de realizar las predicciones, se obtengan resultados problemáticos. se puede investigar en detalle. En otras palabras, la columna podría ser importante incluso cuando no sea un predictor o un resultado.\nPara resolver esto, las funciones add_role(), remove_role() y update_role() pueden resultar útiles. Por ejemplo, para los datos del precio de la vivienda, la función de la columna de dirección postal podría modificarse usando:\names_rec %&gt;% update_role(address, new_role = \"street address\")\nDespués de este cambio, la columna address en el marco de datos ya no será un predictor sino una street address de acuerdo con la receta. Se puede utilizar cualquier cadena de caracteres como rol. Además, las columnas pueden tener múltiples roles (se agregan roles adicionales mediante add_role()) para que puedan seleccionarse en más de un contexto.\nEsto puede resultar útil cuando los datos se vuelven a muestrear. Ayuda a mantener las columnas que no están involucradas con el modelo encajadas en el mismo marco de datos (en lugar de en un vector externo). El remuestreo, descrito en el Capítulo 10, crea versiones alternativas de los datos principalmente mediante submuestreo de filas. Si la dirección postal estuviera en otra columna, se requeriría un submuestreo adicional y podría generar un código más complejo y una mayor probabilidad de errores.\nFinalmente, todas las funciones de paso tienen un campo “rol” que puede asignar roles a los resultados del paso. En muchos casos, las columnas afectadas por un paso conservan su función actual. Por ejemplo, las llamadas step_log() a nuestro objeto ames_rec afectaron la columna Gr_Liv_Area. Para ese paso, el comportamiento predeterminado es mantener la función existente para esta columna, ya que no se crea ninguna columna nueva. Como contraejemplo, el paso para producir splines establece de forma predeterminada que las nuevas columnas tengan una función de \"predictor\" ya que así es como normalmente se usan las columnas spline en un modelo. La mayoría de los pasos tienen valores predeterminados razonables pero, dado que los valores predeterminados pueden ser diferentes, asegúrese de consultar la página de documentación para comprender qué roles se asignarán.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Ingeniería De Características Con recipes</span>"
    ]
  },
  {
    "objectID": "08-feature-engineering.html#sec-recipes-summary",
    "href": "08-feature-engineering.html#sec-recipes-summary",
    "title": "8  Ingeniería De Características Con recipes",
    "section": "8.8 Resumen Del Capítulo",
    "text": "8.8 Resumen Del Capítulo\nEn este capítulo, aprendió a usar recipes para ingeniería de funciones flexible y preprocesamiento de datos, desde la creación de variables ficticias hasta el manejo del desequilibrio de clases y más. La ingeniería de características es una parte importante del proceso de modelado donde puede ocurrir fácilmente una fuga de información y se deben adoptar buenas prácticas. Entre el paquete recipes y otros paquetes que amplían recetas, hay más de 100 pasos disponibles. Todos los pasos posibles de la receta se enumeran en tidymodels.org/find. El marco recipes proporciona un rico entorno de manipulación de datos para preprocesar y transformar datos antes del modelado. Además, tidymodels.org/learn/develop/recipes/ muestra cómo se pueden crear pasos personalizados.\nNuestro trabajo aquí ha utilizado recetas únicamente dentro de un objeto de flujo de trabajo. Para el modelado, ese es el uso recomendado porque la ingeniería de características debe estimarse junto con un modelo. Sin embargo, para la visualización y otras actividades, un flujo de trabajo puede no ser apropiado; Es posible que se requieran funciones más específicas de recetas. El Capítulo 16 analiza las API de nivel inferior para adaptar, usar y solucionar problemas de recetas.\nEl código que usaremos en capítulos posteriores es:\n\nlibrary(tidymodels)\ndata(ames)\names &lt;- mutate(ames, Sale_Price = log10(Sale_Price))\n\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\names_rec &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;% \n  step_ns(Latitude, Longitude, deg_free = 20)\n  \nlm_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\nlm_wflow &lt;- \n  workflow() %&gt;% \n  add_model(lm_model) %&gt;% \n  add_recipe(ames_rec)\n\nlm_fit &lt;- fit(lm_wflow, ames_train)\n\n\n\n\n\nKuhn, M, y K Johnson. 2020. Feature engineering and selection: A practical approach for predictive models. CRC Press.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Ingeniería De Características Con recipes</span>"
    ]
  },
  {
    "objectID": "08-feature-engineering.html#footnotes",
    "href": "08-feature-engineering.html#footnotes",
    "title": "8  Ingeniería De Características Con recipes",
    "section": "",
    "text": "Nuestra versión de estos datos no contiene esa columna.↩︎",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Ingeniería De Características Con recipes</span>"
    ]
  },
  {
    "objectID": "09-judging-model-effectiveness.html",
    "href": "09-judging-model-effectiveness.html",
    "title": "9  Juzgar La Eficacia Del Modelo",
    "section": "",
    "text": "9.1 Métricas De Rendimiento E Inferencia\nLa eficacia de cualquier modelo depende de cómo se utilizará. Un modelo inferencial se utiliza principalmente para comprender las relaciones y normalmente enfatiza la elección (y validez) de distribuciones probabilísticas y otras cualidades generativas que definen el modelo. Por el contrario, para un modelo utilizado principalmente para la predicción, la fuerza predictiva es de primordial importancia y otras preocupaciones sobre las cualidades estadísticas subyacentes pueden ser menos importantes. La fuerza predictiva generalmente está determinada por qué tan cerca están nuestras predicciones de los datos observados, es decir, la fidelidad de las predicciones del modelo a los resultados reales. Este capítulo se centra en funciones que se pueden utilizar para medir la fuerza predictiva. Sin embargo, nuestro consejo para quienes desarrollan modelos inferenciales es utilizar estas técnicas incluso cuando el modelo no se utilizará con el objetivo principal de predicción.\nUn problema de larga data con la práctica de la estadística inferencial es que, centrándose exclusivamente en la inferencia, es difícil evaluar la credibilidad de un modelo. Por ejemplo, considere los datos sobre la enfermedad de Alzheimer de Craig–Schapiro et al. (2011) cuando se estudiaron pacientes con 333 para determinar los factores que influyen en el deterioro cognitivo. Un análisis podría tomar los factores de riesgo conocidos y construir un modelo de regresión logística donde el resultado sea binario (deteriorado/no deteriorado). Consideremos los predictores de edad, sexo y genotipo de apolipoproteína E. Esta última es una variable categórica con las seis combinaciones posibles de las tres variantes principales de este gen. Se sabe que la apolipoproteína E tiene una asociación con la demencia (Jungsu, Basak, y Holtzman 2009).\nUn enfoque superficial, pero no infrecuente, para este análisis sería ajustar un modelo grande con efectos e interacciones principales y luego utilizar pruebas estadísticas para encontrar el conjunto mínimo de términos del modelo que sean estadísticamente significativos en algún nivel predefinido. Si se utilizara un modelo completo con los tres factores y sus interacciones de dos y tres vías, una fase inicial sería probar las interacciones utilizando pruebas de índice de probabilidad secuencial (Hosmer y Lemeshow 2000). Analicemos este tipo de enfoque para el ejemplo de datos sobre la enfermedad de Alzheimer:\nSi bien superficial, esta estrategia de análisis es común tanto en la práctica como en la literatura. Esto es especialmente cierto si el profesional tiene una formación formal limitada en análisis de datos.\nUn dato que falta en este enfoque es qué tan cerca se ajusta este modelo a los datos reales. Usando métodos de remuestreo, discutidos en el Capítulo 10, podemos estimar que la precisión de este modelo es aproximadamente 73.4%. La precisión es a menudo una mala medida del rendimiento del modelo; Lo usamos aquí porque se entiende comúnmente. Si el modelo tiene 73.4% de fidelidad a los datos, ¿deberíamos confiar en las conclusiones que produce? Podríamos pensar así hasta que nos demos cuenta de que la tasa inicial de pacientes no deteriorados en los datos es 72.7%. Esto significa que, a pesar de nuestro análisis estadístico, el modelo de dos factores parece ser sólo 0.8% mejor que una simple heurística que siempre predice que los pacientes no sufrirán daños, independientemente de los datos observados.\nEn el resto de este capítulo, discutiremos enfoques generales para evaluar modelos mediante validación empírica. Estos enfoques se agrupan según la naturaleza de los datos de resultados: puramente numéricos, clases binarias y tres o más niveles de clase.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Juzgar La Eficacia Del Modelo</span>"
    ]
  },
  {
    "objectID": "09-judging-model-effectiveness.html#métricas-de-rendimiento-e-inferencia",
    "href": "09-judging-model-effectiveness.html#métricas-de-rendimiento-e-inferencia",
    "title": "9  Juzgar La Eficacia Del Modelo",
    "section": "",
    "text": "Al comparar el modelo con todas las interacciones de dos vías con uno con la interacción de tres vías adicional, las pruebas de razón de verosimilitud producen un valor p de 0.888. Esto implica que no hay evidencia de que los términos del modelo adicionales four asociados con la interacción de tres vías expliquen suficiente variación en los datos para mantenerlos en el modelo.\nA continuación, las interacciones bidireccionales se evalúan de manera similar con respecto al modelo sin interacciones. El valor p aquí es 0.0382. Esto es algo dudoso, pero, dado el pequeño tamaño de la muestra, sería prudente concluir que hay evidencia de que algunas de las posibles interacciones bidireccionales 10 son importantes para la modelo.\nA partir de aquí, construiríamos alguna explicación de los resultados. Sería particularmente importante discutir las interacciones, ya que pueden generar hipótesis fisiológicas o neurológicas interesantes que se explorarán más a fondo.\n\n\n\n\nEl objetivo de este análisis es demostrar la idea de que la optimización de las características estadísticas del modelo no implica que el modelo se ajuste bien a los datos. Incluso para modelos puramente inferenciales, alguna medida de fidelidad a los datos debería acompañar a los resultados inferenciales. Con esto, los consumidores de los análisis pueden calibrar sus expectativas sobre los resultados.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Juzgar La Eficacia Del Modelo</span>"
    ]
  },
  {
    "objectID": "09-judging-model-effectiveness.html#métricas-de-regresión",
    "href": "09-judging-model-effectiveness.html#métricas-de-regresión",
    "title": "9  Juzgar La Eficacia Del Modelo",
    "section": "9.2 Métricas De Regresión",
    "text": "9.2 Métricas De Regresión\nRecuerde de Sección 6.3 que las funciones de predicción de tidymodels producen tibbles con columnas para los valores predichos. Estas columnas tienen nombres consistentes y las funciones en el paquete yardstick que producen métricas de rendimiento tienen interfaces consistentes. Las funciones están basadas en marcos de datos, a diferencia de vectores, con la sintaxis general de:\nfunction(data, truth, ...)\ndonde data es un marco de datos o tibble y truth es la columna con los valores de resultados observados. Las elipses u otros argumentos se utilizan para especificar las columnas que contienen las predicciones.\nPara ilustrar, tomemos el modelo de Sección 8.8. Este modelo lm_wflow_fit combina un modelo de regresión lineal con un conjunto de predictores complementado con una interacción y funciones spline para longitud y latitud. Fue creado a partir de un conjunto de entrenamiento (llamado ames_train). Aunque no recomendamos utilizar el conjunto de pruebas en este punto del proceso de modelado, se utilizará aquí para ilustrar la funcionalidad y la sintaxis. El marco de datos ames_test consta de las propiedades 588. Para empezar, hagamos predicciones:\n\names_test_res &lt;- predict(lm_fit, new_data = ames_test %&gt;% select(-Sale_Price))\names_test_res\n## # A tibble: 588 × 1\n##   .pred\n##   &lt;dbl&gt;\n## 1  5.07\n## 2  5.31\n## 3  5.28\n## 4  5.33\n## 5  5.30\n## 6  5.24\n## # ℹ 582 more rows\n\nEl resultado numérico previsto por el modelo de regresión se denomina .pred. Hagamos coincidir los valores predichos con sus correspondientes valores de resultado observados:\n\names_test_res &lt;- bind_cols(ames_test_res, ames_test %&gt;% select(Sale_Price))\names_test_res\n## # A tibble: 588 × 2\n##   .pred Sale_Price\n##   &lt;dbl&gt;      &lt;dbl&gt;\n## 1  5.07       5.02\n## 2  5.31       5.39\n## 3  5.28       5.28\n## 4  5.33       5.28\n## 5  5.30       5.28\n## 6  5.24       5.26\n## # ℹ 582 more rows\n\nVemos que estos valores en su mayoría parecen cercanos, pero aún no tenemos una comprensión cuantitativa de cómo funciona el modelo porque no hemos calculado ninguna métrica de rendimiento. Tenga en cuenta que tanto los resultados previstos como los observados están en unidades log-10. Es una buena práctica analizar las predicciones en la escala transformada (si se usara una), incluso si las predicciones se informan utilizando las unidades originales.\nTrazamos los datos en Figura 9.2 antes de calcular las métricas:\n\nggplot(ames_test_res, aes(x = Sale_Price, y = .pred)) + \n  # Crear una línea diagonal:\n  geom_abline(lty = 2) + \n  geom_point(alpha = 0.5) + \n  labs(y = \"Precio de Venta Predecido (log10)\", x = \"Precio de Venta (log10)\") +\n  # Escale y dimensione los ejes x e y de manera uniforme:\n  coord_obs_pred()\n\n\n\n\n\n\n\n\n\nFigura 9.2: Valores observados versus valores predichos para un modelo de regresión de Ames, con unidades log-10 en ambos ejes\n\n\n\n\n\nHay una propiedad de bajo precio que está sustancialmente sobreestimada, es decir, bastante por encima de la línea discontinua.\nCalculemos la raíz del error cuadrático medio para este modelo usando la función rmse():\n\nrmse(ames_test_res, truth = Sale_Price, estimate = .pred)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 rmse    standard      0.0736\n\nEsto nos muestra el formato estándar de salida de las funciones yardstick. Las métricas para resultados numéricos suelen tener un valor “estándar” para la columna .estimator. En las siguientes secciones se muestran ejemplos con diferentes valores para esta columna.\nPara calcular varias métricas a la vez, podemos crear un conjunto de métricas. Sumemos \\(R^2\\) y el error absoluto medio:\n\names_metrics &lt;- metric_set(rmse, rsq, mae)\names_metrics(ames_test_res, truth = Sale_Price, estimate = .pred)\n## # A tibble: 3 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 rmse    standard      0.0736\n## 2 rsq     standard      0.836 \n## 3 mae     standard      0.0549\n\nEste formato de datos ordenado apila las métricas verticalmente. Las métricas del error cuadrático medio y del error absoluto medio están en la escala del resultado (por lo tanto, log10(Sale_Price) para nuestro ejemplo) y miden la diferencia entre los valores previstos y observados. El valor de \\(R^2\\) mide la correlación al cuadrado entre los valores previstos y observados, por lo que los valores más cercanos a uno son mejores.\n\nEl paquete yardstick no contiene una función para \\(R^2\\) ajustado. Esta modificación del coeficiente de determinación se utiliza comúnmente cuando los mismos datos utilizados para ajustar el modelo se utilizan para evaluar el modelo. Esta métrica no es totalmente compatible con tidymodels porque siempre es un mejor enfoque para calcular el rendimiento en un conjunto de datos separado que el utilizado para ajustar el modelo.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Juzgar La Eficacia Del Modelo</span>"
    ]
  },
  {
    "objectID": "09-judging-model-effectiveness.html#métricas-de-clasificación-binaria",
    "href": "09-judging-model-effectiveness.html#métricas-de-clasificación-binaria",
    "title": "9  Juzgar La Eficacia Del Modelo",
    "section": "9.3 Métricas De Clasificación Binaria",
    "text": "9.3 Métricas De Clasificación Binaria\nPara ilustrar otras formas de medir el rendimiento del modelo, cambiaremos a un ejemplo diferente. El paquete modeldata (otro de los paquetes tidymodels) contiene predicciones de ejemplo de un conjunto de datos de prueba con dos clases (“Class1” y “Class2”):\n\ndata(two_class_example)\ntibble(two_class_example)\n## # A tibble: 500 × 4\n##   truth   Class1   Class2 predicted\n##   &lt;fct&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;    \n## 1 Class2 0.00359 0.996    Class2   \n## 2 Class1 0.679   0.321    Class1   \n## 3 Class2 0.111   0.889    Class2   \n## 4 Class1 0.735   0.265    Class1   \n## 5 Class2 0.0162  0.984    Class2   \n## 6 Class1 0.999   0.000725 Class1   \n## # ℹ 494 more rows\n\nLa segunda y tercera columnas son las probabilidades de clase predichas para el conjunto de prueba, mientras que predicted son las predicciones discretas.\nPara las predicciones de clases difíciles, una variedad de funciones yardstick son útiles:\n\n# Una matriz de confusión:\nconf_mat(two_class_example, truth = truth, estimate = predicted)\n##           Truth\n## Prediction Class1 Class2\n##     Class1    227     50\n##     Class2     31    192\n\n# Exactitud:\naccuracy(two_class_example, truth, predicted)\n## # A tibble: 1 × 3\n##   .metric  .estimator .estimate\n##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n## 1 accuracy binary         0.838\n\n# Coeficiente de correlación de Matthews:\nmcc(two_class_example, truth, predicted)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 mcc     binary         0.677\n\n# Métrica F1:\nf_meas(two_class_example, truth, predicted)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 f_meas  binary         0.849\n\n# Combinando estas tres métricas de clasificación juntas\nclassification_metrics &lt;- metric_set(accuracy, mcc, f_meas)\nclassification_metrics(two_class_example, truth = truth, estimate = predicted)\n## # A tibble: 3 × 3\n##   .metric  .estimator .estimate\n##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n## 1 accuracy binary         0.838\n## 2 mcc      binary         0.677\n## 3 f_meas   binary         0.849\n\nEl coeficiente de correlación de Matthews y la puntuación F1 resumen la matriz de confusión, pero en comparación con mcc(), que mide la calidad de ejemplos tanto positivos como negativos, la métrica f_meas() enfatiza la clase positiva, es decir, el evento de interés. Para conjuntos de datos de clasificación binaria como este ejemplo, las funciones yardstick tienen un argumento estándar llamado event_level para distinguir los niveles positivos y negativos. El valor predeterminado (que utilizamos en este código) es que el primer nivel del factor de resultado es el evento de interés.\n\nExiste cierta heterogeneidad en las funciones R a este respecto; algunos utilizan el primer nivel y otros el segundo para denotar el evento de interés. Consideramos más intuitivo que el primer nivel es el más importante. La lógica de segundo nivel surge de codificar el resultado como 0/1 (en cuyo caso el segundo valor es el evento) y desafortunadamente permanece en algunos paquetes. Sin embargo, tidymodels (junto con muchos otros paquetes de R) requieren que se codifique un resultado categórico como factor y, por esta razón, la justificación heredada para el segundo nivel como evento se vuelve irrelevante.\n\nComo ejemplo donde el segundo nivel es el evento:\n\nf_meas(two_class_example, truth, predicted, event_level = \"second\")\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 f_meas  binary         0.826\n\nEn este resultado, el valor .estimator de “binario” indica que se utilizará la fórmula estándar para clases binarias.\nExisten numerosas métricas de clasificación que utilizan las probabilidades predichas como entradas en lugar de las predicciones de clase estrictas. Por ejemplo, la curva de características operativas del receptor (ROC) calcula la sensibilidad y la especificidad sobre un continuo de diferentes umbrales de eventos. La columna de clase prevista no se utiliza. Hay dos funciones yardstick para este método: roc_curve() calcula los puntos de datos que forman la curva ROC y roc_auc() calcula el área bajo la curva.\nLas interfaces para estos tipos de funciones métricas utilizan el marcador de posición del argumento ... para pasar la columna de probabilidad de clase apropiada. Para problemas de dos clases, la columna de probabilidad del evento de interés se pasa a la función:\n\ntwo_class_curve &lt;- roc_curve(two_class_example, truth, Class1)\ntwo_class_curve\n## # A tibble: 502 × 3\n##   .threshold specificity sensitivity\n##        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n## 1 -Inf           0                 1\n## 2    1.79e-7     0                 1\n## 3    4.50e-6     0.00413           1\n## 4    5.81e-6     0.00826           1\n## 5    5.92e-6     0.0124            1\n## 6    1.22e-5     0.0165            1\n## # ℹ 496 more rows\n\nroc_auc(two_class_example, truth, Class1)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 roc_auc binary         0.939\n\nEl objeto two_class_curve se puede usar en una llamada ggplot para visualizar la curva, como se muestra en Figura 9.3. Existe un método autoplot() que se encargará de los detalles:\n\nautoplot(two_class_curve)\n\n\n\n\n\n\n\n\n\nFigura 9.3: Ejemplo de curva ROC\n\n\n\n\n\nSi la curva estuviera cerca de la línea diagonal, entonces las predicciones del modelo no serían mejores que las conjeturas aleatorias. Dado que la curva está arriba en la esquina superior izquierda, vemos que nuestro modelo funciona bien en diferentes umbrales.\nHay otras funciones que utilizan estimaciones de probabilidad, incluidas gain_curve(), lift_curve() y pr_curve().",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Juzgar La Eficacia Del Modelo</span>"
    ]
  },
  {
    "objectID": "09-judging-model-effectiveness.html#métricas-de-clasificación-multiclase",
    "href": "09-judging-model-effectiveness.html#métricas-de-clasificación-multiclase",
    "title": "9  Juzgar La Eficacia Del Modelo",
    "section": "9.4 Métricas De Clasificación Multiclase",
    "text": "9.4 Métricas De Clasificación Multiclase\n¿Qué pasa con los datos con tres o más clases? Para demostrarlo, exploremos un conjunto de datos de ejemplo diferente que tiene cuatro clases:\n\ndata(hpc_cv)\ntibble(hpc_cv)\n## # A tibble: 3,467 × 7\n##   obs   pred     VF      F       M          L Resample\n##   &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;   \n## 1 VF    VF    0.914 0.0779 0.00848 0.0000199  Fold01  \n## 2 VF    VF    0.938 0.0571 0.00482 0.0000101  Fold01  \n## 3 VF    VF    0.947 0.0495 0.00316 0.00000500 Fold01  \n## 4 VF    VF    0.929 0.0653 0.00579 0.0000156  Fold01  \n## 5 VF    VF    0.942 0.0543 0.00381 0.00000729 Fold01  \n## 6 VF    VF    0.951 0.0462 0.00272 0.00000384 Fold01  \n## # ℹ 3,461 more rows\n\nComo antes, hay factores para los resultados observados y previstos junto con otras cuatro columnas de probabilidades previstas para cada clase. (Estos datos también incluyen una columna Resample. Estos resultados hpc_cv son para predicciones fuera de muestra asociadas con una validación cruzada de 10 veces. Por el momento, esta columna se ignorará y discutiremos el remuestreo en profundidad en Capítulo 10.)\nLas funciones para las métricas que utilizan predicciones de clases discretas son idénticas a sus contrapartes binarias:\n\naccuracy(hpc_cv, obs, pred)\n## # A tibble: 1 × 3\n##   .metric  .estimator .estimate\n##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n## 1 accuracy multiclass     0.709\n\nmcc(hpc_cv, obs, pred)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 mcc     multiclass     0.515\n\nTenga en cuenta que, en estos resultados, aparece un .estimator “multiclase”. Al igual que “binario”, esto indica que se utilizó la fórmula para resultados con tres o más niveles de clase. El coeficiente de correlación de Matthews se diseñó originalmente para dos clases, pero se ha extendido a casos con más niveles de clase.\nExisten métodos para tomar métricas diseñadas para manejar resultados con solo dos clases y extenderlas para resultados con más de dos clases. Por ejemplo, una métrica como la sensibilidad mide la tasa de verdaderos positivos que, por definición, es específica de dos clases (es decir, “evento” y “no evento”). ¿Cómo se puede utilizar esta métrica en nuestros datos de ejemplo?\nExisten métodos contenedores que se pueden utilizar para aplicar sensibilidad a nuestro resultado de cuatro clases. Estas opciones son promedio macro, promedio macroponderado y micropromedio:\n\nEl promedio macro calcula un conjunto de métricas de uno contra todos utilizando las estadísticas estándar de dos clases. Estos están promediados.\nEl promedio macroponderado hace lo mismo, pero el promedio se pondera según el número de muestras de cada clase.\nEl micropromedio calcula la contribución de cada clase, las agrega y luego calcula una única métrica a partir de los agregados.\n\nConsulte Wu y Zhou (2017) y Opitz y Burst (2019) para obtener más información sobre cómo ampliar las métricas de clasificación a resultados con más de dos clases.\nUsando la sensibilidad como ejemplo, el cálculo habitual de dos clases es la relación entre el número de eventos predichos correctamente dividido por el número de eventos verdaderos. Los cálculos manuales para estos métodos de promediación son:\n\nclass_totals &lt;- \n  count(hpc_cv, obs, name = \"totals\") %&gt;% \n  mutate(class_wts = totals / sum(totals))\nclass_totals\n##   obs totals class_wts\n## 1  VF   1769   0.51024\n## 2   F   1078   0.31093\n## 3   M    412   0.11883\n## 4   L    208   0.05999\n\ncell_counts &lt;- \n  hpc_cv %&gt;% \n  group_by(obs, pred) %&gt;% \n  count() %&gt;% \n  ungroup()\n\n# Calcule las cuatro sensibilidades usando 1 contra todos\none_versus_all &lt;- \n  cell_counts %&gt;% \n  filter(obs == pred) %&gt;% \n  full_join(class_totals, by = \"obs\") %&gt;% \n  mutate(sens = n / totals)\none_versus_all\n## # A tibble: 4 × 6\n##   obs   pred      n totals class_wts  sens\n##   &lt;fct&gt; &lt;fct&gt; &lt;int&gt;  &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n## 1 VF    VF     1620   1769    0.510  0.916\n## 2 F     F       647   1078    0.311  0.600\n## 3 M     M        79    412    0.119  0.192\n## 4 L     L       111    208    0.0600 0.534\n\n# Tres estimaciones diferentes:\none_versus_all %&gt;% \n  summarize(\n    macro = mean(sens), \n    macro_wts = weighted.mean(sens, class_wts),\n    micro = sum(n) / sum(totals)\n  )\n## # A tibble: 1 × 3\n##   macro macro_wts micro\n##   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n## 1 0.560     0.709 0.709\n\nAfortunadamente, no es necesario implementar manualmente estos métodos de promedio. En cambio, las funciones yardstick pueden aplicar automáticamente estos métodos a través del argumento estimator:\n\nsensitivity(hpc_cv, obs, pred, estimator = \"macro\")\n## # A tibble: 1 × 3\n##   .metric     .estimator .estimate\n##   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n## 1 sensitivity macro          0.560\nsensitivity(hpc_cv, obs, pred, estimator = \"macro_weighted\")\n## # A tibble: 1 × 3\n##   .metric     .estimator     .estimate\n##   &lt;chr&gt;       &lt;chr&gt;              &lt;dbl&gt;\n## 1 sensitivity macro_weighted     0.709\nsensitivity(hpc_cv, obs, pred, estimator = \"micro\")\n## # A tibble: 1 × 3\n##   .metric     .estimator .estimate\n##   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n## 1 sensitivity micro          0.709\n\nCuando se trata de estimaciones de probabilidad, existen algunas métricas con análogos multiclase. Por ejemplo, Hand y Till (2001) determinó una técnica multiclase para curvas ROC. En este caso, todas las columnas de probabilidad de clase deben asignarse a la función:\n\nroc_auc(hpc_cv, obs, VF, F, M, L)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 roc_auc hand_till      0.829\n\nEl promedio macroponderado también está disponible como opción para aplicar esta métrica a un resultado multiclase:\n\nroc_auc(hpc_cv, obs, VF, F, M, L, estimator = \"macro_weighted\")\n## # A tibble: 1 × 3\n##   .metric .estimator     .estimate\n##   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;\n## 1 roc_auc macro_weighted     0.868\n\nFinalmente, todas estas métricas de rendimiento se pueden calcular utilizando agrupaciones dplyr. Recuerde que estos datos tienen una columna para los grupos de remuestreo. Aún no hemos analizado el remuestreo en detalle, pero observe cómo podemos pasar un marco de datos agrupados a la función métrica para calcular las métricas para cada grupo:\n\nhpc_cv %&gt;% \n  group_by(Resample) %&gt;% \n  accuracy(obs, pred)\n## # A tibble: 10 × 4\n##   Resample .metric  .estimator .estimate\n##   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n## 1 Fold01   accuracy multiclass     0.726\n## 2 Fold02   accuracy multiclass     0.712\n## 3 Fold03   accuracy multiclass     0.758\n## 4 Fold04   accuracy multiclass     0.712\n## 5 Fold05   accuracy multiclass     0.712\n## 6 Fold06   accuracy multiclass     0.697\n## # ℹ 4 more rows\n\nLas agrupaciones también se traducen a los métodos autoplot(), y los resultados se muestran en Figura 9.4.\n\n# Cuatro curvas ROC 1 contra todos para cada pliegue\nhpc_cv %&gt;% \n  group_by(Resample) %&gt;% \n  roc_curve(obs, VF, F, M, L) %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\nFigura 9.4: Curvas ROC remuestreadas para cada una de las cuatro clases de resultados.\n\n\n\n\n\nEsta visualización nos muestra que todos los diferentes grupos se desempeñan más o menos igual, pero que la clase VF se predice mejor que las clases F o M, ya que las curvas ROC VF están más en la esquina superior izquierda. . Este ejemplo utiliza remuestreos como grupos, pero se puede utilizar cualquier agrupación de sus datos. Este método autoplot() puede ser un método de visualización rápida para la efectividad del modelo en todas las clases y/o grupos de resultados.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Juzgar La Eficacia Del Modelo</span>"
    ]
  },
  {
    "objectID": "09-judging-model-effectiveness.html#sec-performance-summary",
    "href": "09-judging-model-effectiveness.html#sec-performance-summary",
    "title": "9  Juzgar La Eficacia Del Modelo",
    "section": "9.5 Resumen Del Capítulo",
    "text": "9.5 Resumen Del Capítulo\nDiferentes métricas miden diferentes aspectos del ajuste de un modelo, por ejemplo, RMSE mide la precisión mientras que \\(R^2\\) mide la correlación. Medir el rendimiento del modelo es importante incluso cuando un modelo determinado no se utilizará principalmente para predicción; el poder predictivo también es importante para los modelos inferenciales o descriptivos. Las funciones del paquete yardstick miden la efectividad de un modelo utilizando datos. La interfaz principal de tidymodels utiliza principios de tidyverse y marcos de datos (en lugar de tener argumentos vectoriales). Diferentes métricas son apropiadas para las métricas de regresión y clasificación y, dentro de ellas, a veces hay diferentes formas de estimar las estadísticas, como para resultados multiclase.\n\n\n\n\nCraig–Schapiro, R, M Kuhn, C Xiong, E Pickering, J Liu, T Misko, R Perrin, et al. 2011. «Multiplexed immunoassay panel identifies novel CSF biomarkers for Alzheimer’s disease diagnosis and prognosis». PLoS ONE 6 (4): e18850.\n\n\nHand, D, y R Till. 2001. «A simple generalisation of the area under the ROC curve for multiple class classification problems». Machine Learning 45 (agosto): 171-86.\n\n\nHosmer, D, y Sy Lemeshow. 2000. Applied Logistic Regression. New York: John Wiley; Sons.\n\n\nJungsu, K, D Basak, y D Holtzman. 2009. «The role of Apolipoprotein E in Alzheimer’s disease». Neuron 63 (3): 287-303.\n\n\nOpitz, J, y S Burst. 2019. «Macro F1 and Macro F1». https://arxiv.org/abs/1911.03347.\n\n\nWu, X, y Z Zhou. 2017. «A unified view of multi-label performance measures». En International Conference on Machine Learning, 3780-88.",
    "crumbs": [
      "FUNDAMENTOS DE MODELADO",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Juzgar La Eficacia Del Modelo</span>"
    ]
  },
  {
    "objectID": "10-resampling.html",
    "href": "10-resampling.html",
    "title": "10  Remuestrear Para Evaluar El Rendimiento",
    "section": "",
    "text": "10.1 El Enfoque De La Resustitución\nCuando medimos el rendimiento con los mismos datos que utilizamos para el entrenamiento (a diferencia de datos nuevos o datos de prueba), decimos que hemos resustituido los datos. Utilicemos nuevamente los datos de vivienda de Ames para demostrar estos conceptos. Sección 8.8 resume el estado actual de nuestro análisis de Ames. Incluye un objeto de receta llamado ames_rec, un modelo lineal y un flujo de trabajo que usa esa receta y modelo llamado lm_wflow. Este flujo de trabajo se ajustó al conjunto de entrenamiento, lo que dio como resultado lm_fit.\nPara comparar con este modelo lineal, también podemos ajustar un tipo diferente de modelo. Los random forest son un método de conjunto de árboles que opera creando una gran cantidad de árboles de decisión a partir de versiones ligeramente diferentes del conjunto de entrenamiento (Breiman 2001). Esta colección de árboles conforma el conjunto. Al predecir una nueva muestra, cada miembro del conjunto realiza una predicción por separado. Estos se promedian para crear la predicción conjunta final para el nuevo punto de datos.\nLos modelos de random forest son muy potentes y pueden emular muy fielmente los patrones de datos subyacentes. Si bien este modelo puede requerir un uso intensivo de computación, requiere muy poco mantenimiento; se requiere muy poco preprocesamiento (como se documenta en Apéndice A).\nUsando el mismo conjunto de predictores que el modelo lineal (sin los pasos de preprocesamiento adicionales), podemos ajustar un modelo de random forest al conjunto de entrenamiento a través del motor \"ranger\" (que usa el paquete R ranger para cálculo). Este modelo no requiere procesamiento previo, por lo que se puede utilizar una fórmula simple:\nrf_model &lt;- \n  rand_forest(trees = 1000) %&gt;% \n  set_engine(\"ranger\") %&gt;% \n  set_mode(\"regression\")\n\nrf_wflow &lt;- \n  workflow() %&gt;% \n  add_formula(\n    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n      Latitude + Longitude) %&gt;% \n  add_model(rf_model) \n\nrf_fit &lt;- rf_wflow %&gt;% fit(data = ames_train)\n¿Cómo deberíamos comparar los modelos forestales lineales y aleatorios? A modo de demostración, predeciremos que el conjunto de entrenamiento producirá lo que se conoce como métrica aparente o métrica de resustitución. Esta función crea predicciones y formatea los resultados:\nestimate_perf &lt;- function(model, dat) {\n  # Captura los nombres de los objetos `model` y `dat`\n  cl &lt;- match.call()\n  obj_name &lt;- as.character(cl$model)\n  data_name &lt;- as.character(cl$dat)\n  data_name &lt;- gsub(\"ames_\", \"\", data_name)\n  \n  # Estima estas metricas\n  reg_metrics &lt;- metric_set(rmse, rsq)\n  \n  model %&gt;%\n    predict(dat) %&gt;%\n    bind_cols(dat %&gt;% select(Sale_Price)) %&gt;%\n    reg_metrics(Sale_Price, .pred) %&gt;%\n    select(-.estimator) %&gt;%\n    mutate(object = obj_name, data = data_name)\n}\nSe calculan tanto RMSE como \\(R^2\\). Las estadísticas de resustitución son:\nestimate_perf(rf_fit, ames_train)\n## # A tibble: 2 × 4\n##   .metric .estimate object data \n##   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n## 1 rmse       0.0365 rf_fit train\n## 2 rsq        0.960  rf_fit train\nestimate_perf(lm_fit, ames_train)\n## # A tibble: 2 × 4\n##   .metric .estimate object data \n##   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n## 1 rmse       0.0754 lm_fit train\n## 2 rsq        0.816  lm_fit train\nEn base a estos resultados, el random forest es mucho más capaz de predecir los precios de venta; la estimación de RMSE es two, veces mejor que la regresión lineal. Si tuviéramos que elegir entre estos dos modelos para este problema de predicción de precios, probablemente elegiríamos el random fores porque, en la escala logarítmica que estamos usando, su RMSE es aproximadamente la mitad. El siguiente paso aplica el modelo de random forest al conjunto de prueba para la verificación final:\nestimate_perf(rf_fit, ames_test)\n## # A tibble: 2 × 4\n##   .metric .estimate object data \n##   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n## 1 rmse       0.0704 rf_fit test \n## 2 rsq        0.852  rf_fit test\n¡La estimación RMSE del conjunto de prueba, 0.0704, es much worse than the training set valor de 0.0365! ¿Por qué pasó esto?\nMuchos modelos predictivos son capaces de aprender tendencias complejas a partir de los datos. En estadística, estos se conocen comúnmente como modelos de bajo sesgo.\nPara un modelo de bajo sesgo, el alto grado de capacidad predictiva a veces puede hacer que el modelo casi memorice los datos del conjunto de entrenamiento. Como ejemplo obvio, considere un modelo de 1 vecino más cercano. Siempre proporcionará predicciones perfectas para el conjunto de entrenamiento, sin importar qué tan bien funcione para otros conjuntos de datos. Los modelos de random forest son similares; Repredecir el conjunto de entrenamiento siempre dará como resultado una estimación artificialmente optimista del rendimiento.\nPara ambos modelos, Tabla 10.1 resume la estimación de RMSE para los conjuntos de entrenamiento y prueba:\nTabla 10.1: Estadísticas de rendimiento para conjuntos de entrenamiento y prueba.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimaciones de RMSE\n\n\n\nobject\ntrain\ntest\n\n\n\n\n&lt;tt&gt;lm_fit&lt;/tt&gt;\n0.0754\n0.0736\n\n\n&lt;tt&gt;rf_fit&lt;/tt&gt;\n0.0365\n0.0704\nObserve que el modelo de regresión lineal es consistente entre el entrenamiento y las pruebas, debido a su complejidad limitada.2\nSi el conjunto de prueba no se debe utilizar de inmediato y repredecir el conjunto de entrenamiento es una mala idea, ¿qué se debe hacer? Los métodos de remuestreo, como la validación cruzada o los conjuntos de validación, son la solución.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Remuestrear Para Evaluar El Rendimiento</span>"
    ]
  },
  {
    "objectID": "10-resampling.html#sec-resampling-resubstition",
    "href": "10-resampling.html#sec-resampling-resubstition",
    "title": "10  Remuestrear Para Evaluar El Rendimiento",
    "section": "",
    "text": "En este contexto, sesgo es la diferencia entre el patrón o las relaciones verdaderas en los datos y los tipos de patrones que el modelo puede emular. Muchos modelos de aprendizaje automático de caja negra tienen un sesgo bajo, lo que significa que pueden reproducir relaciones complejas. Otros modelos (como la regresión lineal/logística, el análisis discriminante y otros) no son tan adaptables y se consideran modelos de alto sesgo.1\n\n\n\n\n\n\nLa principal conclusión de este ejemplo es que repredecir el conjunto de entrenamiento dará como resultado una estimación del rendimiento artificialmente optimista. Es una mala idea para la mayoría de los modelos.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Remuestrear Para Evaluar El Rendimiento</span>"
    ]
  },
  {
    "objectID": "10-resampling.html#métodos-de-remuestreo",
    "href": "10-resampling.html#métodos-de-remuestreo",
    "title": "10  Remuestrear Para Evaluar El Rendimiento",
    "section": "10.2 Métodos De Remuestreo",
    "text": "10.2 Métodos De Remuestreo\nLos métodos de remuestreo son sistemas de simulación empíricos que emulan el proceso de utilizar algunos datos para modelar y diferentes datos para evaluación. La mayoría de los métodos de remuestreo son iterativos, lo que significa que este proceso se repite varias veces. El diagrama en Figura 10.1 ilustra cómo funcionan generalmente los métodos de remuestreo.\n\n\n\n\n\n\n\n\nFigura 10.1: Esquema de división de datos desde la división de datos inicial hasta el remuestreo\n\n\n\n\n\nEl remuestreo se realiza solo en el conjunto de entrenamiento, como se ve en Figura 10.1. El equipo de prueba no está involucrado. Para cada iteración de remuestreo, los datos se dividen en dos submuestras:\n\nEl modelo se ajusta a la conjunto de análisis.\nEl modelo se evalúa con el conjunto de evaluación.\n\nEstas dos submuestras son algo análogas a los conjuntos de entrenamiento y prueba. Nuestro lenguaje de análisis y evaluación evita confusiones con la división inicial de los datos. Estos conjuntos de datos son mutuamente excluyentes. El esquema de partición utilizado para crear los conjuntos de análisis y evaluación suele ser la característica definitoria del método.\nSupongamos que se realizan 20 iteraciones de remuestreo. Esto significa que se ajustan 20 modelos separados a los conjuntos de análisis y los conjuntos de evaluación correspondientes producen 20 conjuntos de estadísticas de desempeño. La estimación final del rendimiento de un modelo es el promedio de las 20 réplicas de las estadísticas. Este promedio tiene muy buenas propiedades de generalización y es mucho mejor que las estimaciones de resustitución.\nLa siguiente sección define varios métodos de remuestreo comúnmente utilizados y analiza sus ventajas y desventajas.\n\n10.2.1 Validación cruzada\nLa validación cruzada es un método de remuestreo bien establecido. Si bien existen varias variaciones, el método de validación cruzada más común es la validación cruzada V. Los datos se dividen aleatoriamente en V conjuntos de tamaño aproximadamente igual (llamados pliegues). A modo de ilustración, V = 3 se muestra en Figura 10.2 para un conjunto de datos de 30 puntos de configuración de entrenamiento con asignaciones de pliegues aleatorios. El número dentro de los símbolos es el número de muestra.\n\n\n\n\n\n\n\n\nFigura 10.2: La validación cruzada de pliegues en V asigna aleatoriamente datos a los pliegues\n\n\n\n\n\nEl color de los símbolos en Figura 10.2 representa sus pliegues asignados aleatoriamente. El muestreo estratificado también es una opción para asignar pliegues (analizado anteriormente en Sección 5.1).\nPara una validación cruzada triple, las tres iteraciones de remuestreo se ilustran en Figura 10.3. Para cada iteración, se reserva un pliegue para las estadísticas de evaluación y los pliegues restantes son sustrato para el modelo. Este proceso continúa en cada pliegue, de modo que tres modelos producen tres conjuntos de estadísticas de rendimiento.\n\n\n\n\n\n\n\n\nFigura 10.3: Uso de datos de validación cruzada V-fold\n\n\n\n\n\nCuando V = 3, los conjuntos de análisis son 2/3 del conjunto de entrenamiento y cada conjunto de evaluación es un 1/3 distinto. La estimación final del remuestreo del rendimiento promedia cada una de las V réplicas.\nUsar V = 3 es una buena opción para ilustrar la validación cruzada, pero en la práctica es una mala opción porque es demasiado baja para generar estimaciones confiables. En la práctica, los valores de V suelen ser 5 o 10; Generalmente preferimos la validación cruzada 10 veces como valor predeterminado porque es lo suficientemente grande para obtener buenos resultados en la mayoría de las situaciones.\n\n¿Cuáles son los efectos de cambiar V? Los valores más grandes dan como resultado estimaciones de remuestreo con un sesgo pequeño pero una varianza sustancial. Los valores más pequeños de V tienen un sesgo grande pero una varianza baja. Preferimos 10 veces, ya que la replicación reduce el ruido, pero no el sesgo.3\n\nLa entrada principal es el marco de datos del conjunto de entrenamiento, así como el número de pliegues (por defecto, 10):\n\nset.seed(1001)\names_folds &lt;- vfold_cv(ames_train, v = 10)\names_folds\n## #  10-fold cross-validation \n## # A tibble: 10 × 2\n##   splits             id    \n##   &lt;list&gt;             &lt;chr&gt; \n## 1 &lt;split [2107/235]&gt; Fold01\n## 2 &lt;split [2107/235]&gt; Fold02\n## 3 &lt;split [2108/234]&gt; Fold03\n## 4 &lt;split [2108/234]&gt; Fold04\n## 5 &lt;split [2108/234]&gt; Fold05\n## 6 &lt;split [2108/234]&gt; Fold06\n## # ℹ 4 more rows\n\nLa columna denominada splits contiene información sobre cómo dividir los datos (similar al objeto utilizado para crear la partición de entrenamiento/prueba inicial). Si bien cada fila de “divisiones” tiene una copia incrustada de todo el conjunto de entrenamiento, R es lo suficientemente inteligente como para no hacer copias de los datos en la memoria.4 El método de impresión dentro del tibble muestra la frecuencia de cada: [2107/235] indica que alrededor de dos mil muestras están en el conjunto de análisis y 235 están en ese conjunto de evaluación en particular.\nEstos objetos también contienen siempre una columna de caracteres llamada id que etiqueta la partición.5\nPara recuperar manualmente los datos particionados, las funciones analysis() y assessment() devuelven los marcos de datos correspondientes:\n\n# Para el primer pliegue:\names_folds$splits[[1]] %&gt;% analysis() %&gt;% dim()\n## [1] 2107   74\n\nLos paquetes tidymodels, como tune, contienen interfaces de usuario de alto nivel para que funciones como analysis() no sean generalmente necesarias para el trabajo diario. Sección 10.3 demuestra una función para ajustar un modelo sobre estos remuestreos.\nExiste una variedad de variaciones de validación cruzada; repasaremos los más importantes.\n\n\nValidación cruzada repetida\nLa variación más importante de la validación cruzada es la validación cruzada repetida V veces. Dependiendo del tamaño de los datos u otras características, la estimación de remuestreo producida por la validación cruzada V veces puede ser excesivamente ruidosa.6 Como ocurre con muchos problemas estadísticos, una forma de reducir el ruido es recopilar más datos . Para la validación cruzada, esto significa promediar más de V estadísticas.\nPara crear repeticiones R de validación cruzada de pliegues V, se realiza el mismo proceso de generación de pliegues R veces para generar colecciones R de particiones V. Ahora, en lugar de promediar las estadísticas V, las estadísticas \\(V \\times R\\) producen la estimación final del remuestreo. Debido al teorema del límite central, las estadísticas resumidas de cada modelo tienden a una distribución normal, siempre que tengamos muchos datos relativos a \\(V \\times R\\).\nConsideremos los datos de Ames. En promedio, la validación cruzada 10 veces utiliza conjuntos de evaluación que contienen aproximadamente propiedades 234. Si RMSE es la estadística elegida, podemos denotar la desviación estándar de esa estimación como \\(\\sigma\\). Con una validación cruzada simple de 10 veces, el error estándar del RMSE medio es \\(\\sigma/\\sqrt{10}\\). Si esto es demasiado ruidoso, las repeticiones reducen el error estándar a \\(\\sigma/\\sqrt{10R}\\). Para una validación cruzada de 10 veces con réplicas de \\(R\\), el gráfico en Figura 10.4 muestra qué tan rápido disminuye el error estándar7 con las réplicas.\n\n\n\n\n\n\n\n\nFigura 10.4: Relación entre la varianza relativa en las estimaciones de desempeño versus el número de repeticiones de validación cruzada\n\n\n\n\n\nUn mayor número de réplicas tiende a tener menos impacto en el error estándar. Sin embargo, si el valor de referencia de \\(\\sigma\\) es imprácticamente grande, los rendimientos decrecientes de la replicación aún pueden valer los costos computacionales adicionales.\nPara crear repeticiones, invoque vfold_cv() con un argumento adicional repeats:\n\nvfold_cv(ames_train, v = 10, repeats = 5)\n## #  10-fold cross-validation repeated 5 times \n## # A tibble: 50 × 3\n##   splits             id      id2   \n##   &lt;list&gt;             &lt;chr&gt;   &lt;chr&gt; \n## 1 &lt;split [2107/235]&gt; Repeat1 Fold01\n## 2 &lt;split [2107/235]&gt; Repeat1 Fold02\n## 3 &lt;split [2108/234]&gt; Repeat1 Fold03\n## 4 &lt;split [2108/234]&gt; Repeat1 Fold04\n## 5 &lt;split [2108/234]&gt; Repeat1 Fold05\n## 6 &lt;split [2108/234]&gt; Repeat1 Fold06\n## # ℹ 44 more rows\n\n\n\nValidación cruzada de dejar uno fuera\nUna variación de la validación cruzada es la validación cruzada de dejar uno fuera (LOO). Si hay \\(n\\) muestras de conjuntos de entrenamiento, \\(n\\) modelos se ajustan usando \\(n-1\\) filas del conjunto de entrenamiento. Cada modelo predice el único punto de datos excluido. Al final del remuestreo, las predicciones \\(n\\) se combinan para producir una única estadística de rendimiento.\nLos métodos de dejar uno fuera son deficientes en comparación con casi cualquier otro método. Para cualquier cosa que no sea una muestra patológicamente pequeña, LOO es computacionalmente excesivo y puede que no tenga buenas propiedades estadísticas. Aunque el paquete rsample contiene una función loo_cv(), estos objetos generalmente no están integrados en los marcos más amplios de tidymodels.\n\n\nValidación cruzada de Monte Carlo\nOtra variante de la validación cruzada V es la validación cruzada de Monte Carlo (MCCV, Xu y Liang (2001)). Al igual que la validación cruzada V veces, asigna una proporción fija de datos a los conjuntos de evaluación. La diferencia entre MCCV y la validación cruzada regular es que, para MCCV, esta proporción de datos se selecciona aleatoriamente cada vez. Esto da como resultado conjuntos de evaluación que no son mutuamente excluyentes. Para crear estos objetos de remuestreo:\n\nmc_cv(ames_train, prop = 9/10, times = 20)\n## # Monte Carlo cross-validation (0.9/0.1) with 20 resamples  \n## # A tibble: 20 × 2\n##   splits             id        \n##   &lt;list&gt;             &lt;chr&gt;     \n## 1 &lt;split [2107/235]&gt; Resample01\n## 2 &lt;split [2107/235]&gt; Resample02\n## 3 &lt;split [2107/235]&gt; Resample03\n## 4 &lt;split [2107/235]&gt; Resample04\n## 5 &lt;split [2107/235]&gt; Resample05\n## 6 &lt;split [2107/235]&gt; Resample06\n## # ℹ 14 more rows\n\n\n\n10.2.2 Conjuntos de validación\nEn Sección 5.2, analizamos brevemente el uso de un conjunto de validación, una única partición que se reserva para estimar el rendimiento separada del conjunto de prueba. Cuando se utiliza un conjunto de validación, el conjunto de datos inicial disponible se divide en un conjunto de entrenamiento, un conjunto de validación y un conjunto de prueba (consulte Figura 10.5).\n\n\n\n\n\n\n\n\nFigura 10.5: Una división inicial de tres vías en conjuntos de entrenamiento, pruebas y validación.\n\n\n\n\n\nLos conjuntos de validación se utilizan a menudo cuando el conjunto de datos original es muy grande. En este caso, una única partición grande puede ser adecuada para caracterizar el rendimiento del modelo sin tener que realizar múltiples iteraciones de remuestreo.\nCon el paquete rsample, un conjunto de validación es como cualquier otro objeto de remuestreo; este tipo se diferencia únicamente en que tiene una única iteración.8 Figura 10.6 muestra este esquema.\n\n\n\n\n\n\n\n\nFigura 10.6: Una división inicial bidireccional en entrenamiento y pruebas con un conjunto de validación adicional dividido en el conjunto de entrenamiento\n\n\n\n\n\nPara construir sobre el código de Sección 5.2, la función validation_set() puede tomar los resultados de initial_validation_split() y convertirlos en un objeto rset similar a los producidos. mediante funciones como vfold_cv():\n\n# Previamente:\n\nset.seed(52)\n# Para dedicar el 60 % al entrenamiento, el 20 % a la validación y el 20 % a las pruebas:\names_val_split &lt;- initial_validation_split(ames, prop = c(0.6, 0.2))\names_val_split\n## &lt;Training/Validation/Testing/Total&gt;\n## &lt;1758/586/586/2930&gt;\n\n# Objeto utilizado para remuestreo: \nval_set &lt;- validation_set(ames_val_split)\nval_set\n## # A tibble: 1 × 2\n##   splits             id        \n##   &lt;list&gt;             &lt;chr&gt;     \n## 1 &lt;split [1758/586]&gt; validation\n\nComo verá en ?sec-resampling-rendimiento, la función fit_resamples() se utilizará para calcular estimaciones correctas de rendimiento mediante el remuestreo. El objeto val_set se puede utilizar en esta y otras funciones aunque sea una única “muestra” de los datos.\n\n\n10.2.3 Bootstrapping\nEl remuestreo Bootstrap se inventó originalmente como un método para aproximar la distribución muestral de estadísticas cuyas propiedades teóricas son intratables (Davison y Hinkley 1997). Usarlo para estimar el desempeño del modelo es una aplicación secundaria del método.\nUna muestra bootstrap del conjunto de entrenamiento es una muestra que tiene el mismo tamaño que el conjunto de entrenamiento pero que se extrae con reemplazo. Esto significa que algunos puntos de datos del conjunto de entrenamiento se seleccionan varias veces para el conjunto de análisis. Cada punto de datos tiene una probabilidad 63.2% de ser incluido en el conjunto de entrenamiento al menos una vez. El conjunto de evaluación contiene todas las muestras del conjunto de entrenamiento que no fueron seleccionadas para el conjunto de análisis (en promedio, con 36.8% del conjunto de entrenamiento). Al realizar el arranque, el conjunto de evaluación a menudo se denomina muestra fuera de la bolsa.\nPara un conjunto de entrenamiento de 30 muestras, en la siguiente figura se muestra un esquema de tres muestras bootstrap Figura 10.7.\n\n\n\n\n\n\n\n\nFigura 10.7: Uso de datos Bootstraping\n\n\n\n\n\nTenga en cuenta que los tamaños de los conjuntos de evaluación varían.\nUsando el paquete rsample, podemos crear tales remuestreos de bootstrap:\n\nbootstraps(ames_train, times = 5)\n## # Bootstrap sampling \n## # A tibble: 5 × 2\n##   splits             id        \n##   &lt;list&gt;             &lt;chr&gt;     \n## 1 &lt;split [2342/867]&gt; Bootstrap1\n## 2 &lt;split [2342/869]&gt; Bootstrap2\n## 3 &lt;split [2342/859]&gt; Bootstrap3\n## 4 &lt;split [2342/858]&gt; Bootstrap4\n## 5 &lt;split [2342/873]&gt; Bootstrap5\n\nLas muestras Bootstrap producen estimaciones de rendimiento que tienen una varianza muy baja (a diferencia de la validación cruzada) pero tienen un sesgo pesimista significativo. Esto significa que, si la precisión real de un modelo es del 90%, el bootstrap tendería a estimar que el valor es inferior al 90%. La cantidad de sesgo no se puede determinar empíricamente con suficiente precisión. Además, la cantidad de sesgo cambia a lo largo de la escala de la métrica de desempeño. Por ejemplo, es probable que el sesgo sea diferente cuando la precisión es del 90 % y cuando es del 70 %.\nEl bootstrap también se utiliza en muchos modelos. Por ejemplo, el modelo de random forest mencionado anteriormente contenía 1000 árboles de decisión individuales. Cada árbol fue producto de una muestra de bootstrap diferente del conjunto de entrenamiento.\n\n\n10.2.4 Remuestreo continuo del origen de la previsión\nCuando los datos tienen un fuerte componente temporal, un método de remuestreo debería respaldar el modelado para estimar tendencias estacionales y otras tendencias temporales dentro de los datos. Una técnica que muestrea aleatoriamente valores del conjunto de entrenamiento puede alterar la capacidad del modelo para estimar estos patrones.\nEl remuestreo continuo del origen del pronóstico (Hyndman y Athanasopoulos 2018) proporciona un método que emula cómo los datos de series de tiempo a menudo se dividen en la práctica, estimando el modelo con datos históricos y evaluándolo con los datos más recientes. Para este tipo de remuestreo, se especifica el tamaño de los conjuntos de análisis y evaluación iniciales. La primera iteración de remuestreo utiliza estos tamaños, comenzando desde el principio de la serie. La segunda iteración utiliza los mismos tamaños de datos pero cambia en un número determinado de muestras.\nA modo de ilustración, se volvió a muestrear un conjunto de entrenamiento de quince muestras con un tamaño de análisis de ocho muestras y un tamaño de conjunto de evaluación de tres. La segunda iteración descarta la primera muestra del conjunto de entrenamiento y ambos conjuntos de datos avanzan uno. Esta configuración da como resultado cinco remuestreos, como se muestra en Figura 10.8.\n\n\n\n\n\n\n\n\nFigura 10.8: Uso de datos para el remuestreo continuo del origen de la previsión\n\n\n\n\n\nAquí hay dos configuraciones diferentes de este método:\n\nEl conjunto de análisis puede crecer acumulativamente (en lugar de permanecer del mismo tamaño). Después del primer conjunto de análisis inicial, se pueden acumular nuevas muestras sin descartar los datos anteriores.\nLos remuestreos no necesitan incrementarse en uno. Por ejemplo, para conjuntos de datos grandes, el bloque incremental podría ser una semana o un mes en lugar de un día.\n\nPara los datos de un año, supongamos que seis conjuntos de bloques de 30 días definen el conjunto de análisis. Para conjuntos de evaluación de 30 días con un salto de 29 días, podemos usar el paquete rsample para especificar:\n\ntime_slices &lt;- \n  tibble(x = 1:365) %&gt;% \n  rolling_origin(initial = 6 * 30, assess = 30, skip = 29, cumulative = FALSE)\n\ndata_range &lt;- function(x) {\n  summarize(x, first = min(x), last = max(x))\n}\n\nmap_dfr(time_slices$splits, ~   analysis(.x) %&gt;% data_range())\n## # A tibble: 6 × 2\n##   first  last\n##   &lt;int&gt; &lt;int&gt;\n## 1     1   180\n## 2    31   210\n## 3    61   240\n## 4    91   270\n## 5   121   300\n## 6   151   330\nmap_dfr(time_slices$splits, ~ assessment(.x) %&gt;% data_range())\n## # A tibble: 6 × 2\n##   first  last\n##   &lt;int&gt; &lt;int&gt;\n## 1   181   210\n## 2   211   240\n## 3   241   270\n## 4   271   300\n## 5   301   330\n## 6   331   360",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Remuestrear Para Evaluar El Rendimiento</span>"
    ]
  },
  {
    "objectID": "10-resampling.html#sec-resampling-performance",
    "href": "10-resampling.html#sec-resampling-performance",
    "title": "10  Remuestrear Para Evaluar El Rendimiento",
    "section": "10.3 Estimación Del Rendimiento",
    "text": "10.3 Estimación Del Rendimiento\nCualquiera de los métodos de remuestreo discutidos en este capítulo se puede utilizar para evaluar el proceso de modelado (incluido el preprocesamiento, el ajuste del modelo, etc.). Estos métodos son eficaces porque se utilizan diferentes grupos de datos para entrenar el modelo y evaluarlo. Para reiterar, el proceso para utilizar el remuestreo es:\n\nDurante el remuestreo, el conjunto de análisis se utiliza para preprocesar los datos, aplicar el preprocesamiento a sí mismo y utilizar estos datos procesados para ajustarse al modelo.\nLas estadísticas de preprocesamiento producidas por el conjunto de análisis se aplican al conjunto de evaluación. Las predicciones del conjunto de evaluación estiman el rendimiento con datos nuevos.\n\nEsta secuencia se repite para cada nuevo muestreo. Si hay B remuestreos, hay B réplicas de cada una de las métricas de rendimiento. La estimación final del remuestreo es el promedio de estas estadísticas B. Si B = 1, como ocurre con un conjunto de validación, las estadísticas individuales representan el rendimiento general.\nReconsideremos el modelo de random forest anterior contenido en el objeto rf_wflow. La función fit_resamples() es análoga a fit(), pero en lugar de tener un argumento data, fit_resamples() tiene resamples, que espera un objeto rset como los que se muestran en este capítulo Las posibles interfaces para la función son:\n\nmodel_spec %&gt;% fit_resamples(formula,  resamples, ...)\nmodel_spec %&gt;% fit_resamples(recipe,   resamples, ...)\nworkflow   %&gt;% fit_resamples(          resamples, ...)\n\nHay una serie de otros argumentos opcionales, como por ejemplo:\n\nmetrics: Un conjunto de métricas de estadísticas de rendimiento para calcular. De forma predeterminada, los modelos de regresión utilizan RMSE y \\(R^2\\), mientras que los modelos de clasificación calculan el área bajo la curva ROC y la precisión general. Tenga en cuenta que esta elección también define qué predicciones se producen durante la evaluación del modelo. Para la clasificación, si solo se solicita precisión, no se generan estimaciones de probabilidad de clase para el conjunto de evaluación (ya que no son necesarias).\ncontrol: Una lista creada por control_resamples() con varias opciones.\n\nLos argumentos de control incluyen:\n\nverbose: Una lógica para imprimir el registro.\nextract: Una función para retener objetos de cada iteración del modelo (que se analiza más adelante en este capítulo).\nsave_pred: Una lógica para guardar las predicciones del conjunto de evaluación.\n\nPara nuestro ejemplo, guardemos las predicciones para visualizar el ajuste y los residuos del modelo:\n\nkeep_pred &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE)\n\nset.seed(1003)\nrf_res &lt;- \n  rf_wflow %&gt;% \n  fit_resamples(resamples = ames_folds, control = keep_pred)\nrf_res\n## # Resampling results\n## # 10-fold cross-validation \n## # A tibble: 10 × 5\n##   splits             id     .metrics         .notes           .predictions      \n##   &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;            \n## 1 &lt;split [2107/235]&gt; Fold01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [235 × 4]&gt;\n## 2 &lt;split [2107/235]&gt; Fold02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [235 × 4]&gt;\n## 3 &lt;split [2108/234]&gt; Fold03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [234 × 4]&gt;\n## 4 &lt;split [2108/234]&gt; Fold04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [234 × 4]&gt;\n## 5 &lt;split [2108/234]&gt; Fold05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [234 × 4]&gt;\n## 6 &lt;split [2108/234]&gt; Fold06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [234 × 4]&gt;\n## # ℹ 4 more rows\n\nEl valor de retorno es un tibble similar a las muestras de entrada, junto con algunas columnas adicionales:\n\n.metrics es una columna de lista de tibbles que contiene las estadísticas de rendimiento del conjunto de evaluación.\n.notes Hay otra columna de lista de tibbles que cataloga cualquier advertencia o error generado durante el remuestreo. Tenga en cuenta que los errores no detendrán la ejecución posterior del remuestreo.\n.predictions está presente cuando save_pred = TRUE. Esta columna de lista contiene tibbles con predicciones fuera de muestra.\n\nSi bien estas columnas de lista pueden parecer desalentadoras, se pueden reconfigurar fácilmente usando tidyr o con las funciones convenientes que proporciona tidymodels. Por ejemplo, para devolver las métricas de rendimiento en un formato más utilizable:\n\ncollect_metrics(rf_res)\n## # A tibble: 2 × 6\n##   .metric .estimator   mean     n std_err .config             \n##   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n## 1 rmse    standard   0.0721    10 0.00305 Preprocessor1_Model1\n## 2 rsq     standard   0.831     10 0.0108  Preprocessor1_Model1\n\nEstas son las estimaciones de remuestreo promediadas sobre las réplicas individuales. Para obtener las métricas para cada remuestreo, use la opción summarize = FALSE.\n¡Observe cuánto más realistas son las estimaciones de rendimiento que las estimaciones de resustitución de Sección 10.1!\nPara obtener las predicciones del conjunto de evaluación:\n\nassess_res &lt;- collect_predictions(rf_res)\nassess_res\n## # A tibble: 2,342 × 5\n##   .pred id      .row Sale_Price .config             \n##   &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;               \n## 1  5.10 Fold01    10       5.09 Preprocessor1_Model1\n## 2  4.92 Fold01    27       4.90 Preprocessor1_Model1\n## 3  5.21 Fold01    47       5.08 Preprocessor1_Model1\n## 4  5.13 Fold01    52       5.10 Preprocessor1_Model1\n## 5  5.13 Fold01    59       5.10 Preprocessor1_Model1\n## 6  5.13 Fold01    63       5.11 Preprocessor1_Model1\n## # ℹ 2,336 more rows\n\nLos nombres de las columnas de predicción siguen las convenciones analizadas para los modelos parsnip en el Capítulo 6, para mayor coherencia y facilidad de uso. La columna de resultados observados siempre utiliza el nombre de la columna original de los datos de origen. La columna .row es un número entero que coincide con la fila del conjunto de entrenamiento original para que estos resultados puedan organizarse y unirse adecuadamente con los datos originales.\n\nPara algunos métodos de remuestreo, como el bootstrap o la validación cruzada repetida, habrá múltiples predicciones por fila del conjunto de entrenamiento original. Para obtener valores resumidos (promedios de las predicciones replicadas), utilice collect_predictions(object, summarize = TRUE).\n\nDado que este análisis utilizó una validación cruzada de 10 veces, existe una predicción única para cada muestra del conjunto de entrenamiento. Estos datos pueden generar gráficos útiles del modelo para comprender dónde potencialmente falló. Por ejemplo, ?fig-ames-resampled-dance compara los valores observados y predichos retenidos (análogo a ?fig-ames-rendimiento-plot):\n\nassess_res %&gt;% \n  ggplot(aes(x = Sale_Price, y = .pred)) + \n  geom_point(alpha = .15) +\n  geom_abline(color = \"red\") + \n  coord_obs_pred() + \n  ylab(\"Predichos\") +\n  xlab(\"Precio de Venta\")\n\n\n\n\n\n\n\n\n\nFigura 10.9: Valores observados versus predichos fuera de la muestra para un modelo de regresión de Ames, utilizando unidades log-10 en ambos ejes\n\n\n\n\n\nHay dos casas en el conjunto de entrenamiento con un precio de venta bajo observado que el modelo sobreestima significativamente. ¿Qué casas son estas? Averigüemos por el resultado de assess_res:\n\nover_predicted &lt;- \n  assess_res %&gt;% \n  mutate(residual = Sale_Price - .pred) %&gt;% \n  arrange(desc(abs(residual))) %&gt;% \n  slice(1:2)\nover_predicted\n## # A tibble: 2 × 6\n##   .pred id      .row Sale_Price .config              residual\n##   &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;\n## 1  4.97 Fold09    32       4.11 Preprocessor1_Model1   -0.858\n## 2  4.93 Fold08   317       4.12 Preprocessor1_Model1   -0.815\n\names_train %&gt;% \n  slice(over_predicted$.row) %&gt;% \n  select(Gr_Liv_Area, Neighborhood, Year_Built, Bedroom_AbvGr, Full_Bath)\n## # A tibble: 2 × 5\n##   Gr_Liv_Area Neighborhood           Year_Built Bedroom_AbvGr Full_Bath\n##         &lt;int&gt; &lt;fct&gt;                       &lt;int&gt;         &lt;int&gt;     &lt;int&gt;\n## 1         832 Old_Town                     1923             2         1\n## 2         733 Iowa_DOT_and_Rail_Road       1952             2         1\n\nIdentificar ejemplos como estos con un desempeño especialmente pobre puede ayudarnos a realizar un seguimiento e investigar por qué estas predicciones específicas son tan pobres.\nVolvamos a las casas en general. ¿Cómo podemos utilizar un conjunto de validación en lugar de validación cruzada? De nuestro objeto anterior rsample:\n\nval_res &lt;- rf_wflow %&gt;% fit_resamples(resamples = val_set)\nval_res\n## # Resampling results\n## # Validation Set (0.75/0.25) \n## # A tibble: 1 × 4\n##   splits             id         .metrics         .notes          \n##   &lt;list&gt;             &lt;chr&gt;      &lt;list&gt;           &lt;list&gt;          \n## 1 &lt;split [1758/586]&gt; validation &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n\ncollect_metrics(val_res)\n## # A tibble: 2 × 6\n##   .metric .estimator   mean     n std_err .config             \n##   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n## 1 rmse    standard   0.0728     1      NA Preprocessor1_Model1\n## 2 rsq     standard   0.822      1      NA Preprocessor1_Model1\n\nEstos resultados también están mucho más cerca de los resultados del conjunto de pruebas que las estimaciones de rendimiento de resustitución.\n\nEn estos análisis, los resultados del remuestreo son muy cercanos a los resultados del conjunto de pruebas. Los dos tipos de estimaciones tienden a estar bien correlacionados. Sin embargo, esto podría deberse al azar. Un valor inicial de “55” fijó los números aleatorios antes de crear las nuevas muestras. Intente cambiar este valor y vuelva a ejecutar los análisis para investigar si las estimaciones remuestreadas también coinciden con los resultados del conjunto de pruebas.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Remuestrear Para Evaluar El Rendimiento</span>"
    ]
  },
  {
    "objectID": "10-resampling.html#sec-parallel",
    "href": "10-resampling.html#sec-parallel",
    "title": "10  Remuestrear Para Evaluar El Rendimiento",
    "section": "10.4 Procesamiento En Paralelo",
    "text": "10.4 Procesamiento En Paralelo\nLos modelos creados durante el remuestreo son independientes entre sí. Los cálculos de este tipo a veces se denominan vergonzosamente paralelos; cada modelo podría adaptarse simultáneamente sin problemas.9 El paquete tune usa foreach paquete para facilitar los cálculos paralelos. Estos cálculos podrían dividirse entre procesadores de la misma computadora o entre computadoras diferentes, según la tecnología elegida.\nPara cálculos realizados en una sola computadora, el número de procesos de trabajo posibles está determinado por el paquete parallel:\n\n# La cantidad de núcleos físicos en el hardware:\nparallel::detectCores(logical = FALSE)\n## [1] 4\n\n# El número de posibles procesadors independientes que pueden\n# ser utilizados simultáneamente:  \nparallel::detectCores(logical = TRUE)\n## [1] 4\n\nLa diferencia entre estos dos valores está relacionada con el procesador de la computadora. Por ejemplo, la mayoría de los procesadores Intel utilizan hyperthreading, que crea dos núcleos virtuales para cada núcleo físico. Si bien estos recursos adicionales pueden mejorar el rendimiento, la mayoría de las aceleraciones producidas por el procesamiento paralelo ocurren cuando el procesamiento utiliza menos núcleos físicos.\nPara fit_resamples() y otras funciones en tune, el procesamiento paralelo ocurre cuando el usuario registra un paquete backend paralelo. Estos paquetes de R definen cómo ejecutar el procesamiento paralelo. En los sistemas operativos Unix y macOS, un método para dividir los cálculos es bifurcar subprocesos. Para habilitar esto, cargue el paquete doMC y registre el número de núcleos paralelos con foreach:\n\n# Solo Unix y macOS\nlibrary(doMC)\nregisterDoMC(cores = 2)\n\n# Ahora ejecute fit_resamples()...\n\nEsto indica a fit_resamples() que ejecute la mitad de los cálculos en cada uno de los dos núcleos. Para restablecer los cálculos al procesamiento secuencial:\n\nregisterDoSEQ()\n\nAlternativamente, un enfoque diferente para paralelizar los cálculos utiliza sockets de red. El paquete doParallel habilita este método (utilizable en todos los sistemas operativos):\n\n# Todos los sistemas operativos\nlibrary(doParallel)\n\n# Cree un objeto de clúster y luego regístrelo: \ncl &lt;- makePSOCKcluster(2)\nregisterDoParallel(cl)\n\n# Ahora ejecute fit_resamples()`...\n\nstopCluster(cl)\n\nOtro paquete de R que facilita el procesamiento paralelo es el paquete future. Al igual que foreach, proporciona un marco para el paralelismo. Este paquete se usa junto con foreach a través del paquete doFuture.\n\nLos paquetes R con backends paralelos para foreach comienzan con el prefijo \"do\".\n\nEl procesamiento paralelo con el paquete tune tiende a proporcionar aceleraciones lineales para los primeros núcleos. Esto significa que, con dos núcleos, los cálculos son el doble de rápidos. Dependiendo de los datos y del tipo de modelo, la aceleración lineal se deteriora después de cuatro o cinco núcleos. El uso de más núcleos seguirá reduciendo el tiempo necesario para completar la tarea; simplemente hay rendimientos decrecientes para los núcleos adicionales.\nConcluyamos con una nota final sobre el paralelismo. Para cada una de estas tecnologías, los requisitos de memoria se multiplican por cada núcleo adicional utilizado. Por ejemplo, si el conjunto de datos actual tiene 2 GB de memoria y se utilizan tres núcleos, el requisito total de memoria es de 8 GB (2 para cada proceso de trabajo más el original). El uso de demasiados núcleos puede hacer que los cálculos (y la computadora) se ralenticen considerablemente.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Remuestrear Para Evaluar El Rendimiento</span>"
    ]
  },
  {
    "objectID": "10-resampling.html#sec-extract",
    "href": "10-resampling.html#sec-extract",
    "title": "10  Remuestrear Para Evaluar El Rendimiento",
    "section": "10.5 Guardar Los Objetos Remuestreados",
    "text": "10.5 Guardar Los Objetos Remuestreados\nLos modelos creados durante el remuestreo no se conservan. Estos modelos se entrenan con el fin de evaluar el desempeño y, por lo general, no los necesitamos después de haber calculado las estadísticas de desempeño. Si un enfoque de modelado particular resulta ser la mejor opción para nuestro conjunto de datos, entonces la mejor opción es ajustar nuevamente todo el conjunto de entrenamiento para que los parámetros del modelo puedan estimarse con más datos.\nSi bien estos modelos creados durante el remuestreo no se conservan, existe un método para conservarlos o algunos de sus componentes. La opción extract de control_resamples() especifica una función que toma un solo argumento; Usaremos x. Cuando se ejecuta, x da como resultado un objeto de flujo de trabajo ajustado, independientemente de si proporcionó a fit_resamples() un flujo de trabajo. Recuerde que el paquete workflows tiene funciones que pueden extraer los diferentes componentes de los objetos (por ejemplo, el modelo, la receta, etc.).\nAjustemos un modelo de regresión lineal usando la receta que desarrollamos en el Capítulo 8:\n\names_rec &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_other(Neighborhood, threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;% \n  step_ns(Latitude, Longitude, deg_free = 20)\n\nlm_wflow &lt;-  \n  workflow() %&gt;% \n  add_recipe(ames_rec) %&gt;% \n  add_model(linear_reg() %&gt;% set_engine(\"lm\")) \n\nlm_fit &lt;- lm_wflow %&gt;% fit(data = ames_train)\n\n# Selecciona la receta:\nextract_recipe(lm_fit, estimated = TRUE)\n## \n## ── Recipe ───────────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:   1\n## predictor: 6\n## \n## ── Training information\n## Training data contained 2342 data points and no incomplete rows.\n## \n## ── Operations\n## • Collapsing factor levels for: Neighborhood | Trained\n## • Dummy variables from: Neighborhood and Bldg_Type | Trained\n## • Interactions with: Gr_Liv_Area:(Bldg_Type_TwoFmCon + Bldg_Type_Duplex +\n##   Bldg_Type_Twnhs + Bldg_Type_TwnhsE) | Trained\n## • Natural splines on: Latitude and Longitude | Trained\n\nPodemos guardar los coeficientes del modelo lineal para un objeto de modelo ajustado desde un flujo de trabajo:\n\nget_model &lt;- function(x) {\n  extract_fit_parsnip(x) %&gt;% tidy()\n}\n\n# Pruébelo usando:\n# get_model(lm_fit)\n\nAhora apliquemos esta función a los diez ajustes remuestreados. Los resultados de la función de extracción se envuelven en un objeto de lista y se devuelven en un tibble:\n\nctrl &lt;- control_resamples(extract = get_model)\n\nlm_res &lt;- lm_wflow %&gt;%  fit_resamples(resamples = ames_folds, control = ctrl)\nlm_res\n## # Resampling results\n## # 10-fold cross-validation \n## # A tibble: 10 × 5\n##   splits             id     .metrics         .notes           .extracts       \n##   &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;           &lt;list&gt;           &lt;list&gt;          \n## 1 &lt;split [2107/235]&gt; Fold01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n## 2 &lt;split [2107/235]&gt; Fold02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n## 3 &lt;split [2108/234]&gt; Fold03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n## 4 &lt;split [2108/234]&gt; Fold04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n## 5 &lt;split [2108/234]&gt; Fold05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n## 6 &lt;split [2108/234]&gt; Fold06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n## # ℹ 4 more rows\n\nAhora hay una columna .extracts con tibbles anidados. ¿Qué contienen estos? Averigüemos subconjuntos.\n\nlm_res$.extracts[[1]]\n## # A tibble: 1 × 2\n##   .extracts         .config             \n##   &lt;list&gt;            &lt;chr&gt;               \n## 1 &lt;tibble [73 × 5]&gt; Preprocessor1_Model1\n\n# Para obtener los resultados\nlm_res$.extracts[[1]][[1]]\n## [[1]]\n## # A tibble: 73 × 5\n##   term                        estimate  std.error statistic   p.value\n##   &lt;chr&gt;                          &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)                 1.48     0.320         4.62   4.11e-  6\n## 2 Gr_Liv_Area                 0.000158 0.00000476   33.2    9.72e-194\n## 3 Year_Built                  0.00180  0.000149     12.1    1.57e- 32\n## 4 Neighborhood_College_Creek -0.00163  0.0373       -0.0438 9.65e-  1\n## 5 Neighborhood_Old_Town      -0.0757   0.0138       -5.47   4.92e-  8\n## 6 Neighborhood_Edwards       -0.109    0.0310       -3.53   4.21e-  4\n## # ℹ 67 more rows\n\nEsto podría parecer un método complicado para guardar los resultados del modelo. Sin embargo, extract es flexible y no supone que el usuario solo guardará un tibble por remuestreo. Por ejemplo, el método tidy() podría ejecutarse tanto en la receta como en el modelo. En este caso, se devolverá una lista de dos tibbles.\nPara nuestro ejemplo más simple, todos los resultados se pueden aplanar y recopilar usando:\n\nall_coef &lt;- map_dfr(lm_res$.extracts, ~ .x[[1]][[1]])\n# Muestre las réplicas de un único predictor:\nfilter(all_coef, term == \"Year_Built\")\n## # A tibble: 10 × 5\n##   term       estimate std.error statistic  p.value\n##   &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 Year_Built  0.00180  0.000149      12.1 1.57e-32\n## 2 Year_Built  0.00180  0.000151      12.0 6.45e-32\n## 3 Year_Built  0.00185  0.000150      12.3 1.00e-33\n## 4 Year_Built  0.00183  0.000147      12.5 1.90e-34\n## 5 Year_Built  0.00184  0.000150      12.2 2.47e-33\n## 6 Year_Built  0.00180  0.000150      12.0 3.35e-32\n## # ℹ 4 more rows\n\nLos capítulos 13 y 14 analizan un conjunto de funciones para ajustar modelos. Sus interfaces son similares a fit_resamples() y muchas de las características descritas aquí se aplican a esas funciones.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Remuestrear Para Evaluar El Rendimiento</span>"
    ]
  },
  {
    "objectID": "10-resampling.html#sec-resampling-summary",
    "href": "10-resampling.html#sec-resampling-summary",
    "title": "10  Remuestrear Para Evaluar El Rendimiento",
    "section": "10.6 Resumen Del Capítulo",
    "text": "10.6 Resumen Del Capítulo\nEste capítulo describe una de las herramientas fundamentales del análisis de datos, la capacidad de medir el rendimiento y la variación en los resultados del modelo. El remuestreo nos permite determinar qué tan bien funciona el modelo sin utilizar el conjunto de prueba.\nSe introdujo una función importante del paquete tune, llamada fit_resamples(). La interfaz para esta función también se utiliza en capítulos futuros que describen las herramientas de ajuste de modelos.\nEl código de análisis de datos, hasta ahora, para los datos de Ames es:\n\nlibrary(tidymodels)\ndata(ames)\names &lt;- mutate(ames, Sale_Price = log10(Sale_Price))\n\nset.seed(502)\names_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train &lt;- training(ames_split)\names_test  &lt;-  testing(ames_split)\n\names_rec &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;% \n  step_ns(Latitude, Longitude, deg_free = 20)\n\nlm_model &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\nlm_wflow &lt;- \n  workflow() %&gt;% \n  add_model(lm_model) %&gt;% \n  add_recipe(ames_rec)\n\nlm_fit &lt;- fit(lm_wflow, ames_train)\n\nrf_model &lt;- \n  rand_forest(trees = 1000) %&gt;% \n  set_engine(\"ranger\") %&gt;% \n  set_mode(\"regression\")\n\nrf_wflow &lt;- \n  workflow() %&gt;% \n  add_formula(\n    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n      Latitude + Longitude) %&gt;% \n  add_model(rf_model) \n\nset.seed(1001)\names_folds &lt;- vfold_cv(ames_train, v = 10)\n\nkeep_pred &lt;- control_resamples(save_pred = TRUE, save_workflow = TRUE)\n\nset.seed(1003)\nrf_res &lt;- rf_wflow %&gt;% fit_resamples(resamples = ames_folds, control = keep_pred)\n\n\n\n\n\nBreiman, L. 2001. «Random forests». Machine learning 45 (1): 5-32.\n\n\nDavison, A, y D Hinkley. 1997. Bootstrap methods and their application. Vol. 1. Cambridge university press.\n\n\nHyndman, R, y G Athanasopoulos. 2018. Forecasting: principles and practice. OTexts.\n\n\nKuhn, M, y K Johnson. 2020. Feature engineering and selection: A practical approach for predictive models. CRC Press.\n\n\nSchmidberger, M, M Morgan, D Eddelbuettel, H Yu, L Tierney, y U Mansmann. 2009. «State of the art in parallel computing with R». Journal of Statistical Software 31 (1): 1-27. https://www.jstatsoft.org/v031/i01.\n\n\nXu, Q, y Y Liang. 2001. «Monte Carlo cross validation». Chemometrics and Intelligent Laboratory Systems 56 (1): 1-11.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Remuestrear Para Evaluar El Rendimiento</span>"
    ]
  },
  {
    "objectID": "10-resampling.html#footnotes",
    "href": "10-resampling.html#footnotes",
    "title": "10  Remuestrear Para Evaluar El Rendimiento",
    "section": "",
    "text": "Consulte la Sección 1.2.5 de Kuhn y Johnson (2020) para una discusión: https://bookdown.org/max/FES/important-concepts.html#model-bias-and-variance↩︎\nEs posible que un modelo lineal casi memorice el conjunto de entrenamiento, como lo hizo el modelo de bosque aleatorio. En el objeto ames_rec, cambie el número de términos spline para longitud y latitud a un número grande (digamos 1000). Esto produciría un modelo ajustado con un RMSE de resustitución muy pequeño y un RMSE del conjunto de prueba que es mucho más grande.↩︎\nConsulte la Sección 3.4 de Kuhn y Johnson (2020) para obtener una descripción más detallada de los resultados del cambio de V: https://bookdown.org/max/FES/resampling.html↩︎\nPara comprobarlo usted mismo, intente ejecutar lobstr::obj_size(ames_folds) y lobstr::obj_size(ames_train). El tamaño del objeto de remuestreo es mucho menor que diez veces el tamaño de los datos originales.↩︎\nAlgunos métodos de remuestreo requieren múltiples campos id.↩︎\nPara obtener más detalles, consulte la Sección 3.4.6 de Kuhn y Johnson (2020): https://bookdown.org/max/FES/resampling.html#resample-var-bias.↩︎\nEstos son errores estándar aproximados. Como se analizará en el próximo capítulo, existe una correlación dentro de las réplicas que es típica de los resultados remuestreados. Al ignorar este componente adicional de variación, los cálculos simples que se muestran en este gráfico son sobreestimaciones de la reducción del ruido en los errores estándar.↩︎\nEn esencia, un conjunto de validación puede considerarse una iteración única de la validación cruzada de Monte Carlo.↩︎\nSchmidberger et al. (2009) ofrece una descripción técnica de estas tecnologías.↩︎",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Remuestrear Para Evaluar El Rendimiento</span>"
    ]
  },
  {
    "objectID": "11-comparing-models.html",
    "href": "11-comparing-models.html",
    "title": "11  Comparación De Modelos Con Remuestreo",
    "section": "",
    "text": "11.1 Crear Múltiples Modelos Con Conjuntos De Flujo De Trabajo\nEn Sección 7.5 describimos la idea de un conjunto de flujo de trabajo donde se pueden generar combinatoriamente diferentes preprocesadores y/o modelos. En el Capítulo 10, utilizamos una receta para los datos de Ames que incluía un término de interacción así como funciones spline para longitud y latitud. Para demostrar más con los conjuntos de flujos de trabajo, creemos tres modelos lineales diferentes que agreguen estos pasos de preprocesamiento de forma incremental; Podemos probar si estos términos adicionales mejoran los resultados del modelo. Crearemos tres recetas y luego las combinaremos en un conjunto de flujo de trabajo:\nlibrary(tidymodels)\ntidymodels_prefer()\n\nbasic_rec &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = 0.01) %&gt;% \n  step_dummy(all_nominal_predictors())\n\ninteraction_rec &lt;- \n  basic_rec %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) \n\nspline_rec &lt;- \n  interaction_rec %&gt;% \n  step_ns(Latitude, Longitude, deg_free = 50)\n\npreproc &lt;- \n  list(basic = basic_rec, \n       interact = interaction_rec, \n       splines = spline_rec\n  )\n\nlm_models &lt;- workflow_set(preproc, list(lm = linear_reg()), cross = FALSE)\nlm_models\n## # A workflow set/tibble: 3 × 4\n##   wflow_id    info             option    result    \n##   &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n## 1 basic_lm    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n## 2 interact_lm &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n## 3 splines_lm  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\nNos gustaría volver a muestrear cada uno de estos modelos por turno. Para hacerlo, usaremos una función similar a purrr llamada workflow_map(). Esta función toma un argumento inicial de la función para aplicarlo a los flujos de trabajo, seguido de opciones para esa función. También configuramos un argumento verbose que imprimirá el progreso, así como un argumento seed que garantiza que cada modelo utilice el mismo flujo de números aleatorios que los demás.\nlm_models &lt;- \n  lm_models %&gt;% \n  workflow_map(\"fit_resamples\", \n               # Options to `workflow_map()`: \n               seed = 1101, verbose = TRUE,\n               # Options to `fit_resamples()`: \n               resamples = ames_folds, control = keep_pred)\n## i 1 of 3 resampling: basic_lm\n## ✔ 1 of 3 resampling: basic_lm (623ms)\n## i 2 of 3 resampling: interact_lm\n## ✔ 2 of 3 resampling: interact_lm (674ms)\n## i 3 of 3 resampling: splines_lm\n## ✔ 3 of 3 resampling: splines_lm (1.1s)\nlm_models\n## # A workflow set/tibble: 3 × 4\n##   wflow_id    info             option    result   \n##   &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n## 1 basic_lm    &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n## 2 interact_lm &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n## 3 splines_lm  &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\nObserve que las columnas option y result ahora están completas. La primera incluye las opciones para fit_resamples() que se dieron (para reproducibilidad), y la última columna contiene los resultados producidos por fit_resamples().\nHay algunas funciones convenientes para conjuntos de flujos de trabajo, incluida collect_metrics() para recopilar las estadísticas de rendimiento. Podemos filtrar() a cualquier métrica específica que nos interese:\ncollect_metrics(lm_models) %&gt;% \n  filter(.metric == \"rmse\")\n## # A tibble: 3 × 9\n##   wflow_id    .config          preproc model .metric .estimator   mean     n std_err\n##   &lt;chr&gt;       &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n## 1 basic_lm    Preprocessor1_M… recipe  line… rmse    standard   0.0803    10 0.00264\n## 2 interact_lm Preprocessor1_M… recipe  line… rmse    standard   0.0799    10 0.00272\n## 3 splines_lm  Preprocessor1_M… recipe  line… rmse    standard   0.0785    10 0.00282\n¿Qué pasa con el modelo de bosque aleatorio del capítulo anterior? Podemos agregarlo al conjunto convirtiéndolo primero a su propio conjunto de flujo de trabajo y luego vinculando filas. Esto requiere que, cuando se volvió a muestrear el modelo, se haya configurado la opción save_workflow = TRUE en la función de control.\nfour_models &lt;- \n  as_workflow_set(random_forest = rf_res) %&gt;% \n  bind_rows(lm_models)\nfour_models\n## # A workflow set/tibble: 4 × 4\n##   wflow_id      info             option    result   \n##   &lt;chr&gt;         &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n## 1 random_forest &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;rsmp[+]&gt;\n## 2 basic_lm      &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n## 3 interact_lm   &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\n## 4 splines_lm    &lt;tibble [1 × 4]&gt; &lt;opts[2]&gt; &lt;rsmp[+]&gt;\nEl método autoplot(), con salida en Figura 11.1, muestra intervalos de confianza para cada modelo en orden de mejor a peor. En este capítulo, nos centraremos en el coeficiente de determinación (también conocido como \\(R^2\\)) y usaremos metric = \"rsq\" en la llamada para configurar nuestro gráfico:\nlibrary(ggrepel)\nautoplot(four_models, metric = \"rsq\") +\n  geom_text_repel(aes(label = wflow_id), nudge_x = 1/8, nudge_y = 1/100) +\n  theme(legend.position = \"none\")\nFigura 11.1: Intervalos de confianza para el coeficiente de determinación utilizando cuatro modelos diferentes\nEn este gráfico de intervalos de confianza \\(R^2\\), podemos ver que el método de random forest está funcionando mejor y hay mejoras menores en los modelos lineales a medida que agregamos más pasos a la receta.\nAhora que tenemos 10 estimaciones de rendimiento remuestreadas para cada uno de los cuatro modelos, estas estadísticas resumidas se pueden utilizar para realizar comparaciones entre modelos.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Comparación De Modelos Con Remuestreo</span>"
    ]
  },
  {
    "objectID": "11-comparing-models.html#sec-resampled-stats",
    "href": "11-comparing-models.html#sec-resampled-stats",
    "title": "11  Comparación De Modelos Con Remuestreo",
    "section": "11.2 Comparación De Estadísticas De Rendimiento Remuestreadas",
    "text": "11.2 Comparación De Estadísticas De Rendimiento Remuestreadas\nTeniendo en cuenta los resultados anteriores para los tres modelos lineales, parece que los términos adicionales no mejoran profundamente las estadísticas medias de RMSE o \\(R^2\\) para los modelos lineales. La diferencia es pequeña, pero podría ser mayor que el ruido experimental en el sistema, es decir, considerarse estadísticamente significativa. Podemos probar formalmente la hipótesis de que los términos adicionales aumentan \\(R^2\\).\n\nAntes de hacer comparaciones entre modelos, es importante para nosotros analizar la correlación dentro de la muestra para las estadísticas de remuestreo. Cada modelo se midió con los mismos pliegues de validación cruzada y los resultados para la misma nueva muestra tienden a ser similares.\n\nEn otras palabras, hay algunos remuestreos en los que el rendimiento de todos los modelos tiende a ser bajo y otros en los que tiende a ser alto. En estadística, esto se denomina componente de variación remuestreo a remuestreo.\nPara ilustrar, recopilemos las estadísticas de remuestreo individuales para los modelos lineales y el bosque aleatorio. Nos centraremos en la estadística \\(R^2\\) para cada modelo, que mide la correlación entre los precios de venta observados y previstos para cada casa. Vamos a filtrar() para mantener solo las métricas \\(R^2\\), remodelar los resultados y calcular cómo se correlacionan las métricas entre sí.\n\nrsq_indiv_estimates &lt;- \n  collect_metrics(four_models, summarize = FALSE) %&gt;% \n  filter(.metric == \"rsq\") \n\nrsq_wider &lt;- \n  rsq_indiv_estimates %&gt;% \n  select(wflow_id, .estimate, id) %&gt;% \n  pivot_wider(id_cols = \"id\", names_from = \"wflow_id\", values_from = \".estimate\")\n\ncorrr::correlate(rsq_wider %&gt;% select(-id), quiet = TRUE)\n## # A tibble: 4 × 5\n##   term          random_forest basic_lm interact_lm splines_lm\n##   &lt;chr&gt;                 &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n## 1 random_forest        NA        0.887       0.888      0.889\n## 2 basic_lm              0.887   NA           0.993      0.997\n## 3 interact_lm           0.888    0.993      NA          0.987\n## 4 splines_lm            0.889    0.997       0.987     NA\n\nEstas correlaciones son altas e indican que, entre modelos, existen grandes correlaciones dentro de la muestra. Para ver esto visualmente en Figura 11.2, las estadísticas \\(R^2\\) se muestran para cada modelo con líneas que conectan los remuestreos:\n\nrsq_indiv_estimates %&gt;% \n  mutate(wflow_id = reorder(wflow_id, .estimate)) %&gt;% \n  ggplot(aes(x = wflow_id, y = .estimate, group = id, color = id)) + \n  geom_line(alpha = .5, linewidth = 1.25) + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nFigura 11.2: Remuestrear estadísticas entre modelos\n\n\n\n\n\nSi el efecto de remuestreo a remuestreo no fuera real, no habría líneas paralelas. Una prueba estadística de las correlaciones evalúa si las magnitudes de estas correlaciones no son simplemente ruido. Para los modelos lineales:\n\nrsq_wider %&gt;% \n  with( cor.test(basic_lm, splines_lm) ) %&gt;% \n  tidy() %&gt;% \n  select(estimate, starts_with(\"conf\"))\n## # A tibble: 1 × 3\n##   estimate conf.low conf.high\n##      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1    0.997    0.987     0.999\n\nLos resultados de la prueba de correlación (la “estimación”, estimate, de la correlación y los intervalos de confianza) nos muestran que la correlación dentro de la muestra parece ser real.\n¿Qué efecto tiene la correlación adicional en nuestro análisis? Considere la varianza de una diferencia de dos variables:\n\\[\\operatorname{Var}[X - Y] = \\operatorname{Var}[X] + \\operatorname{Var}[Y]  - 2 \\operatorname{Cov}[X, Y]\\]\nEl último término es la covarianza entre dos elementos. Si hay una covarianza positiva significativa, entonces cualquier prueba estadística de esta diferencia carecería de potencia crítica al comparar la diferencia en dos modelos. En otras palabras, ignorar el efecto de remuestreo a remuestreo sesgaría nuestras comparaciones de modelos hacia la ausencia de diferencias entre los modelos.\n\nEsta característica de las estadísticas de remuestreo entrará en juego en las dos secciones siguientes.\n\nAntes de realizar comparaciones de modelos o observar los resultados del remuestreo, puede resultar útil definir un tamaño del efecto práctico relevante. Dado que estos análisis se centran en las estadísticas de \\(R^2\\), el tamaño del efecto práctico es el cambio en \\(R^2\\) que consideraríamos una diferencia realista que importa. Por ejemplo, podríamos pensar que dos modelos no son prácticamente diferentes si sus valores de \\(R^2\\) están dentro de \\(\\pm 2\\)%. Si este fuera el caso, las diferencias menores al 2% no se consideran importantes incluso si son estadísticamente significativas.\nLa importancia práctica es subjetiva; dos personas pueden tener ideas muy diferentes en el umbral de la importancia. Sin embargo, más adelante mostraremos que esta consideración puede resultar muy útil a la hora de decidir entre modelos.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Comparación De Modelos Con Remuestreo</span>"
    ]
  },
  {
    "objectID": "11-comparing-models.html#métodos-simples-de-prueba-de-hipótesis",
    "href": "11-comparing-models.html#métodos-simples-de-prueba-de-hipótesis",
    "title": "11  Comparación De Modelos Con Remuestreo",
    "section": "11.3 Métodos Simples De Prueba De Hipótesis",
    "text": "11.3 Métodos Simples De Prueba De Hipótesis\nPodemos utilizar pruebas de hipótesis simples para hacer comparaciones formales entre modelos. Considere el conocido modelo estadístico lineal:\n\\[y_{ij} = \\beta_0 + \\beta_1x_{i1} + \\ldots + \\beta_px_{ip} + \\epsilon_{ij}\\]\nEste modelo versátil se utiliza para crear modelos de regresión, además de ser la base de la popular técnica de análisis de varianza (ANOVA) para comparar grupos. Con el modelo ANOVA, los predictores (\\(x_{ij}\\)) son variables binarias ficticias para diferentes grupos. A partir de esto, los parámetros \\(\\beta\\) estiman si dos o más grupos son diferentes entre sí utilizando técnicas de prueba de hipótesis.\nEn nuestra situación específica, el ANOVA también puede realizar comparaciones de modelos. Supongamos que las estadísticas individuales \\(R^2\\) remuestreadas sirven como datos de resultado (es decir, \\(y_{ij}\\)) y los modelos como predictores en el modelo ANOVA. Se muestra una muestra de esta estructura de datos en Tabla 11.1.\n\n\n\nTabla 11.1: Modelar estadísticas de rendimiento como un conjunto de datos para análisis.\n\n\n\n\n\n\nY = rsq\nmodel\nX1\nX2\nX3\nid\n\n\n\n\n0.8108\nbasic_lm\n0\n0\n0\nFold01\n\n\n0.8134\ninteract_lm\n1\n0\n0\nFold01\n\n\n0.8615\nrandom_forest\n0\n1\n0\nFold01\n\n\n0.8217\nsplines_lm\n0\n0\n1\nFold01\n\n\n0.8045\nbasic_lm\n0\n0\n0\nFold02\n\n\n0.8103\ninteract_lm\n1\n0\n0\nFold02\n\n\n\n\n\n\n\n\n\nLas columnas X1, X2 y X3 de la tabla son indicadores de los valores de la columna modelo. Su orden se definió de la misma manera que R los definiría, ordenados alfabéticamente por modelo.\nPara nuestra comparación de modelos, el modelo ANOVA específico es:\n\\[y_{ij} = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\beta_3x_{i3} + \\epsilon_{ij}\\]\ndonde\n\n\\(\\beta_0\\) es la estimación del estadístico \\(R^2\\) medio para los modelos lineales básicos (es decir, sin splines ni interacciones),\n\\(\\beta_1\\) es el cambio en la media \\(R^2\\) cuando se agregan interacciones al modelo lineal básico,\n\\(\\beta_2\\) es el cambio en la media \\(R^2\\) entre el modelo lineal básico y el modelo de random forest, y\n\\(\\beta_3\\) es el cambio en la media \\(R^2\\) entre el modelo lineal básico y uno con interacciones y splines.\n\nA partir de estos parámetros del modelo, se generan pruebas de hipótesis y valores p para comparar modelos estadísticamente, pero debemos lidiar con cómo manejar el efecto de remuestreo a remuestre. Históricamente, los grupos de remuestreo se consideraban un efecto de bloque y se agregaba un término apropiado al modelo. Alternativamente, el efecto de remuestreo podría considerarse un efecto aleatorio en el que estas remuestras particulares se extrajeron al azar de una población más grande de posibles remuestras. Sin embargo, no estamos realmente interesados en estos efectos; sólo queremos ajustarlos en el modelo para que las varianzas de las diferencias interesantes se estimen adecuadamente.\nTratar las remuestras como efectos aleatorios es teóricamente atractivo. Los métodos para ajustar un modelo ANOVA con este tipo de efecto aleatorio podrían incluir el modelo lineal mixto (Faraway 2016) o un modelo jerárquico bayesiano (que se muestra en la siguiente sección).\nUn método simple y rápido para comparar dos modelos a la vez es usar las diferencias en los valores de \\(R^2\\) como datos de resultado en el modelo ANOVA. Dado que los resultados coinciden mediante remuestreo, las diferencias no contienen el efecto de remuestreo a remuestre y, por esta razón, el modelo ANOVA estándar es apropiado. Para ilustrar, esta llamada a lm() prueba la diferencia entre dos de los modelos de regresión lineal:\n\ncompare_lm &lt;- \n  rsq_wider %&gt;% \n  mutate(difference = splines_lm - basic_lm)\n\nlm(difference ~ 1, data = compare_lm) %&gt;% \n  tidy(conf.int = TRUE) %&gt;% \n  select(estimate, p.value, starts_with(\"conf\"))\n## # A tibble: 1 × 4\n##   estimate   p.value conf.low conf.high\n##      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1  0.00913 0.0000256  0.00650    0.0118\n\n# Alternativamente, también se podría utilizar una prueba t pareada: \nrsq_wider %&gt;% \n  with( t.test(splines_lm, basic_lm, paired = TRUE) ) %&gt;%\n  tidy() %&gt;% \n  select(estimate, p.value, starts_with(\"conf\"))\n## # A tibble: 1 × 4\n##   estimate   p.value conf.low conf.high\n##      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1  0.00913 0.0000256  0.00650    0.0118\n\nPodríamos evaluar cada diferencia por pares de esta manera. Tenga en cuenta que el valor p indica una señal estadísticamente significativa; la colección de términos splines para longitud y latitud parece tener un efecto. Sin embargo, la diferencia en \\(R^2\\) se estima en 0.91%. Si nuestro tamaño de efecto práctico fuera del 2%, es posible que no consideráramos que valga la pena incluir estos términos en el modelo.\n\nYa hemos mencionado brevemente los valores p, pero ¿qué son realmente? De Wasserstein y Lazar (2016): “Informalmente, un valor p es la probabilidad, según un modelo estadístico específico, de que un resumen estadístico de los datos (por ejemplo, la diferencia de medias muestrales entre dos grupos comparados) sea igual o más extremo que su valor observado.”\nEn otras palabras, si este análisis se repitiera una gran cantidad de veces bajo la hipótesis nula de que no hay diferencias, el valor p refleja cuán extremos serían nuestros resultados observados en comparación.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Comparación De Modelos Con Remuestreo</span>"
    ]
  },
  {
    "objectID": "11-comparing-models.html#sec-tidyposterior",
    "href": "11-comparing-models.html#sec-tidyposterior",
    "title": "11  Comparación De Modelos Con Remuestreo",
    "section": "11.4 Métodos Bayesianos",
    "text": "11.4 Métodos Bayesianos\nSimplemente utilizamos pruebas de hipótesis para comparar modelos formalmente, pero también podemos adoptar un enfoque más general para realizar estas comparaciones formales utilizando efectos aleatorios y estadísticas bayesianas (McElreath 2020). Si bien el modelo es más complejo que el método ANOVA, la interpretación es más simple y directa que el método del valor p. El modelo ANOVA anterior tenía la forma:\n\\[y_{ij} = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\beta_3x_{i3} + \\epsilon_{ij}\\]\ndonde se supone que los residuales \\(\\epsilon_{ij}\\) son independientes y siguen una distribución gaussiana con media cero y desviación estándar constante de \\(\\sigma\\). A partir de este supuesto, la teoría estadística muestra que los parámetros de regresión estimados siguen una distribución gaussiana multivariada y, de ahí, se derivan los valores p y los intervalos de confianza.\nUn modelo lineal bayesiano hace suposiciones adicionales. Además de especificar una distribución para los residuos, requerimos especificaciones de distribución previa para los parámetros del modelo ( \\(\\beta_j\\) y \\(\\sigma\\) ). Estas son distribuciones de los parámetros que asume el modelo antes de ser expuesto a los datos observados. Por ejemplo, un conjunto simple de distribuciones previas para nuestro modelo podría ser:\n\\[\\begin{align}\n\\epsilon_{ij} &\\sim N(0, \\sigma) \\notag \\\\\n\\beta_j &\\sim N(0, 10) \\notag \\\\\n\\sigma &\\sim \\text{exponential}(1) \\notag\n\\end{align}\\]\nEstos antecedentes establecen los rangos posibles/probables de los parámetros del modelo y no tienen parámetros desconocidos. Por ejemplo, el valor anterior de \\(\\sigma\\) indica que los valores deben ser mayores que cero, están muy sesgados a la derecha y tienen valores que generalmente son menores que 3 o 4.\nTenga en cuenta que los parámetros de regresión tienen una distribución a priori bastante amplia, con una desviación estándar de 10. En muchos casos, es posible que no tengamos una opinión sólida sobre el a priori más allá de que sea simétrico y tenga forma de campana. La gran desviación estándar implica un previo bastante poco informativo; no es demasiado restrictivo en términos de los posibles valores que podrían adoptar los parámetros. Esto permite que los datos tengan más influencia durante la estimación de parámetros.\nDados los datos observados y las especificaciones de distribución previas, se pueden estimar los parámetros del modelo. Las distribuciones finales de los parámetros del modelo son combinaciones de las estimaciones previas y de probabilidad. Estas distribuciones posteriores de los parámetros son las distribuciones clave de interés. Son una descripción probabilística completa de los parámetros estimados del modelo.\n\nUn modelo de intersección aleatoria\nPara adaptar nuestro modelo ANOVA bayesiano de modo que las remuestras se modelen adecuadamente, consideramos un modelo de intersección aleatoria. Aquí, asumimos que los remuestreos impactan el modelo solo al cambiar la intersección. Tenga en cuenta que esto impide que los remuestreos tengan un impacto diferencial en los parámetros de regresión \\(\\beta_j\\); Se supone que estos tienen la misma relación entre remuestreos. Esta ecuación modelo es:\n\\[y_{ij} = (\\beta_0 + b_{i}) + \\beta_1x_{i1} + \\beta_2x_{i2} + \\beta_3x_{i3} + \\epsilon_{ij}\\]\nEste no es un modelo irrazonable para estadísticas remuestreadas que, cuando se trazan entre modelos como en Figura 11.2, tienden a tener efectos bastante paralelos entre modelos (es decir, poco cruce de líneas).\nPara la configuración de este modelo, se hace un supuesto adicional para la distribución previa de efectos aleatorios. Una suposición razonable para esta distribución es otra distribución simétrica, como otra curva en forma de campana. Dado el tamaño de muestra efectivo de 10 en nuestro resumen de datos estadísticos, usemos una distribución previa que sea más amplia que una distribución normal estándar. Usaremos una distribución t con un solo grado de libertad (es decir, \\(b_i \\sim t(1)\\)), que tiene colas más pesadas que una distribución gaussiana análoga.\nEl paquete tidyposterior tiene funciones para ajustarse a dichos modelos bayesianos con el fin de comparar modelos remuestreados. La función principal se llama perf_mod() y está configurada para “simplemente funcionar” para diferentes tipos de objetos:\n\nPara conjuntos de flujos de trabajo, crea un modelo ANOVA donde los grupos corresponden a los flujos de trabajo. Nuestros modelos existentes no optimizaron ningún parámetro de ajuste (consulte los siguientes tres capítulos). Si uno de los flujos de trabajo del conjunto tenía datos sobre parámetros de ajuste, en el análisis bayesiano se utilizan los mejores parámetros de ajuste establecidos para cada flujo de trabajo. En otras palabras, a pesar de la presencia de parámetros de ajuste, perf_mod() se centra en hacer comparaciones entre flujos de trabajo.\nPara los objetos que contienen un único modelo que ha sido ajustado usando remuestreo, perf_mod() hace comparaciones dentro del modelo. En esta situación, las variables de agrupación probadas en el modelo ANOVA bayesiano son los submodelos definidos por los parámetros de ajuste.\nLa función perf_mod() también puede tomar un marco de datos producido por rsample que tiene columnas de métricas de rendimiento asociadas con dos o más resultados de modelo/flujo de trabajo. Estos podrían haberse generado por medios no estándar.\n\nA partir de cualquiera de estos tipos de objetos, la función perf_mod() determina un modelo bayesiano apropiado y lo ajusta con las estadísticas de remuestreo. Para nuestro ejemplo, modelará los cuatro conjuntos de estadísticas \\(R^2\\) asociadas con los flujos de trabajo.\nEl paquete tidyposterior utiliza el software Stan para especificar y ajustar los modelos a través del paquete rstanarm. Las funciones dentro de ese paquete tienen prioridades predeterminadas (consulte ?priors para obtener más detalles). El siguiente modelo utiliza los valores previos predeterminados para todos los parámetros excepto para las intersecciones aleatorias (que siguen una distribución t). El proceso de estimación utiliza números aleatorios para que la semilla se establezca dentro de la llamada a la función. El proceso de estimación es iterativo y se replica varias veces en colecciones llamadas cadenas. El parámetro iter le dice a la función cuánto tiempo ejecutar el proceso de estimación en cada cadena. Cuando se utilizan varias cadenas, sus resultados se combinan (supongamos que esto está validado mediante evaluaciones diagnósticas).\n\nlibrary(tidyposterior)\nlibrary(rstanarm)\n\n# El paquete rstanarm genera grandes cantidades de producción; esos resultados\n# no se muestran aquí, pero vale la pena inspeccionarlos para detectar posibles problemas. El\n# opción `refresh = 0` se puede utilizar para eliminar el registro.. \nrsq_anova &lt;-\n  perf_mod(\n    four_models,\n    metric = \"rsq\",\n    prior_intercept = rstanarm::student_t(df = 1),\n    chains = 4,\n    iter = 5000,\n    seed = 1102\n  )\n\nEl objeto resultante tiene información sobre el proceso de remuestreo, así como el objeto Stan incrustado en él (en un elemento llamado “stan”). Lo que más nos interesa son las distribuciones posteriores de los parámetros de regresión. El paquete tidyposterior tiene un método tidy() que extrae estas distribuciones posteriores en un tibble:\n\nmodel_post &lt;- \n  rsq_anova %&gt;% \n  # Tomaa una muestra aleatoria de la distribución posterior\n  # así que se configura la semilla nuevamente para que sea reproducible. \n  tidy(seed = 1103) \n\nglimpse(model_post)\n## Rows: 40,000\n## Columns: 2\n## $ model     &lt;chr&gt; \"random_forest\", \"basic_lm\", \"interact_lm\", \"splines_lm\", \"rando…\n## $ posterior &lt;dbl&gt; 0.8287, 0.7898, 0.7921, 0.8000, 0.8338, 0.7944, 0.7970, 0.8002, …\n\nLas cuatro distribuciones posteriores se visualizan en Figura 11.3.\n\nmodel_post %&gt;% \n  mutate(model = forcats::fct_inorder(model)) %&gt;%\n  ggplot(aes(x = posterior)) + \n  geom_histogram(bins = 50, color = \"white\", fill = \"blue\", alpha = 0.4) + \n  facet_wrap(~ model, ncol = 1)\n\n\n\n\n\n\n\n\n\nFigura 11.3: Distribuciones posteriores del coeficiente de determinación utilizando cuatro modelos diferentes.\n\n\n\n\n\nEstos histogramas describen las distribuciones de probabilidad estimadas del valor medio \\(R^2\\) para cada modelo. Existe cierta superposición, especialmente para los tres modelos lineales.\nTambién hay un método básico autoplot() para los resultados del modelo, que se muestra en Figura 11.4, así como el objeto ordenado que muestra gráficos de densidad superpuestos.\n\nautoplot(rsq_anova) +\n  geom_text_repel(aes(label = workflow), nudge_x = 1/8, nudge_y = 1/100) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nFigura 11.4: Intervalos creíbles derivados de las distribuciones posteriores del modelo.\n\n\n\n\n\nUn aspecto maravilloso del uso del remuestreo con modelos bayesianos es que, una vez que tenemos los parámetros posteriores, es trivial obtener las distribuciones posteriores para las combinaciones de los parámetros. Por ejemplo, para comparar los dos modelos de regresión lineal, nos interesa la diferencia de medias. La parte posterior de esta diferencia se calcula tomando muestras de las partes posteriores individuales y tomando las diferencias. La función contrast_models() puede hacer esto. Para especificar las comparaciones a realizar, los parámetros list_1 y list_2 toman vectores de caracteres y calculan las diferencias entre los modelos en esas listas (parametrizados como list_1 - list_2).\nPodemos comparar dos de los modelos lineales y visualizar los resultados en Figura 11.5.\n\nrqs_diff &lt;-\n  contrast_models(rsq_anova,\n                  list_1 = \"splines_lm\",\n                  list_2 = \"basic_lm\",\n                  seed = 1104)\n\nrqs_diff %&gt;% \n  as_tibble() %&gt;% \n  ggplot(aes(x = difference)) + \n  geom_vline(xintercept = 0, lty = 2) + \n  geom_histogram(bins = 50, color = \"white\", fill = \"red\", alpha = 0.4)\n\n\n\n\n\n\n\n\n\nFigura 11.5: Distribución posterior de la diferencia en el coeficiente de determinación.\n\n\n\n\n\nLa parte posterior muestra que el centro de la distribución es mayor que cero (lo que indica que el modelo con splines normalmente tenía valores más grandes) pero se superpone con cero hasta cierto punto. El método summary() para este objeto calcula la media de la distribución así como los intervalos creíbles, el análogo bayesiano de los intervalos de confianza.\n\nsummary(rqs_diff) %&gt;% \n  select(-starts_with(\"pract\"))\n## # A tibble: 1 × 6\n##   contrast               probability    mean   lower  upper  size\n##   &lt;chr&gt;                        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1 splines_lm vs basic_lm        1.00 0.00917 0.00509 0.0132     0\n\nLa columna probability refleja la proporción del posterior que es mayor que cero. Ésta es la probabilidad de que la diferencia positiva sea real. El valor no es cercano a cero, lo que proporciona un fuerte argumento a favor de la significancia estadística, es decir, la idea de que estadísticamente la diferencia real no es cero.\nSin embargo, la estimación de la diferencia de medias es bastante cercana a cero. Recuerde que el tamaño del efecto práctico que sugerimos anteriormente es del 2%. Con una distribución posterior, también podemos calcular la probabilidad de que sea prácticamente significativo. En el análisis bayesiano, esta es una estimación ROPE (para Región de equivalencia práctica, Kruschke y Liddell (2018)). Para estimar esto, se utiliza la opción size de la función de summary:\n\nsummary(rqs_diff, size = 0.02) %&gt;% \n  select(contrast, starts_with(\"pract\"))\n## # A tibble: 1 × 4\n##   contrast               pract_neg pract_equiv pract_pos\n##   &lt;chr&gt;                      &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n## 1 splines_lm vs basic_lm         0           1         0\n\nLa columna pract_equiv es la proporción del posterior que está dentro de [-size, size] (las columnas pract_neg y pract_pos son las proporciones que están debajo y encima de este intervalo). Este gran valor indica que, para nuestro tamaño del efecto, existe una probabilidad abrumadora de que los dos modelos sean prácticamente iguales. Aunque el gráfico anterior mostró que nuestra diferencia probablemente sea distinta de cero, la prueba de equivalencia sugiere que es lo suficientemente pequeña como para no ser significativa en la práctica.\nSe podría utilizar el mismo proceso para comparar el modelo de random forest con una o ambas regresiones lineales que se volvieron a muestrear. De hecho, cuando se usa perf_mod() con un conjunto de flujo de trabajo, el método autoplot() puede mostrar los resultados de pract_equiv que comparan cada flujo de trabajo con el mejor actual (el modelo de bosque aleatorio, en este caso).\n\nautoplot(rsq_anova, type = \"ROPE\", size = 0.02) +\n  geom_text_repel(aes(label = workflow)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nFigura 11.6: Probabilidad de equivalencia práctica para un tamaño del efecto del 2%\n\n\n\n\n\nFigura 11.6 nos muestra que ninguno de los modelos lineales se acerca al modelo de random forest cuando se utiliza un tamaño de efecto práctico del 2%.\n\n\nEl efecto de la cantidad de remuestreo.\n¿Cómo afecta el número de remuestreos a estos tipos de comparaciones bayesianas formales? Más remuestreos aumentan la precisión de la estimación general del remuestreo; esa precisión se propaga a este tipo de análisis. A modo de ilustración, se agregaron nuevas muestras adicionales mediante validación cruzada repetida. ¿Cómo cambió la distribución posterior? Figura 11.7 muestra los intervalos de credibilidad del 90% con hasta 100 remuestreos (generados a partir de 10 repeticiones de validación cruzada 10 veces).1\n\nggplot(intervals,\n       aes(x = resamples, y = mean)) +\n  geom_path() +\n  geom_ribbon(aes(ymin = lower, ymax = upper), fill = \"red\", alpha = .1) +\n  labs(x = \"Número de remuestras (validación cruzada repetida 10 veces)\")\n\n\n\n\n\n\n\n\n\nFigura 11.7: Probabilidad de equivalencia práctica con el modelo de bosque aleatorio.\n\n\n\n\n\nEl ancho de los intervalos disminuye a medida que se agregan más muestras. Claramente, pasar de diez remuestreos a treinta tiene un impacto mayor que pasar de ochenta a 100. Hay rendimientos decrecientes por utilizar un número “grande” de remuestreos (“grande” será diferente para diferentes conjuntos de datos).",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Comparación De Modelos Con Remuestreo</span>"
    ]
  },
  {
    "objectID": "11-comparing-models.html#sec-compare-summary",
    "href": "11-comparing-models.html#sec-compare-summary",
    "title": "11  Comparación De Modelos Con Remuestreo",
    "section": "11.5 Resumen Del Capítulo",
    "text": "11.5 Resumen Del Capítulo\nEste capítulo describió métodos estadísticos formales para probar diferencias en el rendimiento entre modelos. Demostramos el efecto dentro de la nueva muestra, donde los resultados para la misma nueva muestra tienden a ser similares; este aspecto de las estadísticas resumidas remuestreadas requiere un análisis apropiado para realizar comparaciones de modelos válidas. Además, aunque la significación estadística y la significancia práctica son conceptos importantes para las comparaciones de modelos, son diferentes.\n\n\n\n\nFaraway, J. 2016. Extending the linear model with R: generalized linear, mixed effects and nonparametric regression models. CRC press.\n\n\nKruschke, J, y T Liddell. 2018. «The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective». Psychonomic Bulletin and Review 25 (1): 178-206.\n\n\nMcElreath, R. 2020. Statistical rethinking: A Bayesian course with examples in R and Stan. CRC press.\n\n\nWasserstein, R, y N Lazar. 2016. «The ASA statement on p-values: Context, process, and purpose». The American Statistician 70 (2): 129-33.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Comparación De Modelos Con Remuestreo</span>"
    ]
  },
  {
    "objectID": "11-comparing-models.html#footnotes",
    "href": "11-comparing-models.html#footnotes",
    "title": "11  Comparación De Modelos Con Remuestreo",
    "section": "",
    "text": "El código para generar intervalos está disponible en https://github.com/tidymodels/TMwR/blob/main/extras/ames_posterior_intervals.R.↩︎",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Comparación De Modelos Con Remuestreo</span>"
    ]
  },
  {
    "objectID": "12-tuning-parameters.html",
    "href": "12-tuning-parameters.html",
    "title": "12  Ajuste De Modelos Y Los Peligros Del Sobreajuste",
    "section": "",
    "text": "12.1 Parámetros Del Modelo\nEn la regresión lineal ordinaria, hay dos parámetros \\(\\beta_0\\) y \\(\\beta_1\\) del modelo:\n\\[ y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\]\nCuando tenemos los datos del resultado (\\(y\\)) y del predictor (\\(x\\)), podemos estimar los dos parámetros \\(\\beta_0\\) y \\(\\beta_1\\):\n\\[\\hat \\beta_1 = \\frac{\\sum_i (y_i-\\bar{y})(x_i-\\bar{x})}{\\sum_i(x_i-\\bar{x})^2}\\]\ny\n\\[\\hat \\beta_0 = \\bar{y}-\\hat \\beta_1 \\bar{x}.\\]\nPodemos estimar directamente estos valores a partir de los datos de este modelo de ejemplo porque son analíticamente manejables; si tenemos los datos, entonces podemos estimar estos parámetros del modelo.\nPara el modelo KNN, la ecuación de predicción para un nuevo valor \\(x_0\\) es\n\\[\\hat y = \\frac{1}{K}\\sum_{\\ell = 1}^K x_\\ell^*\\]\ndonde \\(K\\) es el número de vecinos y \\(x_\\ell^*\\) son los valores de \\(K\\) más cercanos a \\(x_0\\) en el conjunto de entrenamiento. El modelo en sí no está definido por una ecuación modelo; en cambio, la ecuación de predicción anterior lo define. Esta característica, junto con la posible intratabilidad de la medida de distancia, hace imposible crear un conjunto de ecuaciones que puedan resolverse para \\(K\\) (de forma iterativa o no). El número de vecinos tiene un profundo impacto en el modelo; gobierna la flexibilidad de la frontera de clase. Para valores pequeños de \\(K\\), el límite es muy elaborado, mientras que para valores grandes, puede ser bastante suave.\nEl número de vecinos más cercanos es un buen ejemplo de parámetro de ajuste o hiperparámetro que no se puede estimar directamente a partir de los datos.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Ajuste De Modelos Y Los Peligros Del Sobreajuste</span>"
    ]
  },
  {
    "objectID": "12-tuning-parameters.html#parámetros-del-modelo",
    "href": "12-tuning-parameters.html#parámetros-del-modelo",
    "title": "12  Ajuste De Modelos Y Los Peligros Del Sobreajuste",
    "section": "",
    "text": "Hay muchas situaciones en las que un modelo tiene parámetros que no pueden estimarse directamente a partir de los datos.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Ajuste De Modelos Y Los Peligros Del Sobreajuste</span>"
    ]
  },
  {
    "objectID": "12-tuning-parameters.html#sec-tuning-parameter-examples",
    "href": "12-tuning-parameters.html#sec-tuning-parameter-examples",
    "title": "12  Ajuste De Modelos Y Los Peligros Del Sobreajuste",
    "section": "12.2 Parámetros De Ajuste Para Diferentes Tipos De Modelos",
    "text": "12.2 Parámetros De Ajuste Para Diferentes Tipos De Modelos\nHay muchos ejemplos de ajuste de parámetros o hiperparámetros en diferentes modelos estadísticos y de aprendizaje automático:\n\nBoosting es un método conjunto que combina una serie de modelos base, cada uno de los cuales se crea secuencialmente y depende de los modelos anteriores. El número de iteraciones de impulso es un parámetro de ajuste importante que normalmente requiere optimización.\nEn la clásica red neuronal artificial de una sola capa (también conocida como perceptrón multicapa), los predictores se combinan utilizando dos o más unidades ocultas. Las unidades ocultas son combinaciones lineales de los predictores que se capturan en una función de activación (normalmente una función no lineal, como una sigmoidea). Las unidades ocultas se conectan luego con las unidades resultantes; se utiliza una unidad de resultado para los modelos de regresión y se requieren múltiples unidades de resultado para la clasificación. El número de unidades ocultas y el tipo de función de activación son parámetros de ajuste estructural importantes.\nLos métodos modernos de descenso de gradientes se mejoran al encontrar los parámetros de optimización adecuados. Ejemplos de tales hiperparámetros son las tasas de aprendizaje, el impulso y el número de iteraciones/épocas de optimización (Goodfellow, Bengio, y Courville 2016). Las redes neuronales y algunos modelos de conjuntos utilizan el descenso de gradiente para estimar los parámetros del modelo. Si bien los parámetros de ajuste asociados con el descenso del gradiente no son parámetros estructurales, a menudo requieren ajuste.\n\nEn algunos casos, las técnicas de preprocesamiento requieren ajustes:\n\nEn el análisis de componentes principales, o su primo supervisado llamado mínimos cuadrados parciales, los predictores se reemplazan con características nuevas y artificiales que tienen mejores propiedades relacionadas con la colinealidad. Se puede ajustar el número de componentes extraídos.\nLos métodos de imputación estiman los valores predictivos faltantes utilizando los valores completos de uno o más predictores. Una herramienta de imputación eficaz utiliza \\(K\\) vecinos más cercanos de las columnas completas para predecir el valor faltante. El número de vecinos modula la cantidad de promedio y se puede ajustar.\n\nAlgunos modelos estadísticos clásicos también tienen parámetros estructurales:\n\nEn la regresión binaria, el enlace logit se usa comúnmente (es decir, regresión logística). También están disponibles otras funciones de enlace, como probit y log-log complementario (Dobson 1999). Este ejemplo se describe con más detalle en Sección 12.3.\nLos modelos de medidas repetidas y longitudinales no bayesianos requieren una especificación para la estructura de covarianza o correlación de los datos. Las opciones incluyen compuesto simétrico (también conocido como intercambiable), autorregresivo, Toeplitz y otros (Littell, Pendergast, y Natarajan 2000).\n\nUn contraejemplo en el que no es apropiado ajustar un parámetro es la distribución previa requerida para el análisis bayesiano. Lo anterior resume la creencia del analista sobre la distribución de una cantidad antes de que se tengan en cuenta la evidencia o los datos. Por ejemplo, en Sección 11.4, utilizamos un modelo ANOVA bayesiano y no teníamos claro cuál debería ser el valor previo para los parámetros de regresión (más allá de ser una distribución simétrica). Elegimos una distribución t con un grado de libertad para la anterior ya que tiene colas más pesadas; esto refleja nuestra incertidumbre adicional. Nuestras creencias previas no deberían estar sujetas a optimización. Los parámetros de ajuste generalmente se optimizan para el rendimiento, mientras que los anteriores no deben modificarse para obtener “los resultados correctos”.\n\nOtro contraejemplo (quizás más discutible) de un parámetro que no necesita ser ajustado es el número de árboles en un randm forest o en un modelo de embolsado. En cambio, este valor debe elegirse para que sea lo suficientemente grande como para garantizar la estabilidad numérica en los resultados; ajustarlo no puede mejorar el rendimiento siempre que el valor sea lo suficientemente grande como para producir resultados confiables. Para random forest, este valor suele ser de miles, mientras que la cantidad de árboles necesarios para el embolsado es de alrededor de 50 a 100.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Ajuste De Modelos Y Los Peligros Del Sobreajuste</span>"
    ]
  },
  {
    "objectID": "12-tuning-parameters.html#sec-what-to-optimize",
    "href": "12-tuning-parameters.html#sec-what-to-optimize",
    "title": "12  Ajuste De Modelos Y Los Peligros Del Sobreajuste",
    "section": "12.3 ¿Qué Optimizamos?",
    "text": "12.3 ¿Qué Optimizamos?\n¿Cómo debemos evaluar los modelos cuando optimizamos los parámetros de ajuste? Depende del modelo y del propósito del modelo.\nPara los casos en los que las propiedades estadísticas del parámetro de ajuste son manejables, se pueden utilizar propiedades estadísticas comunes como función objetivo. Por ejemplo, en el caso de la regresión logística binaria, la función de enlace se puede elegir maximizando la probabilidad o los criterios de información. Sin embargo, es posible que estas propiedades estadísticas no se alineen con los resultados obtenidos utilizando propiedades orientadas a la precisión. Como ejemplo, Friedman (2001) optimizó la cantidad de árboles en un conjunto de árboles mejorado y encontró resultados diferentes al maximizar la probabilidad y la precisión:\n\ndegradar la probabilidad mediante el sobreajuste en realidad mejora la tasa de error de clasificación errónea. Aunque quizás sea contradictorio, esto no es una contradicción; la probabilidad y la tasa de error miden diferentes aspectos de la calidad del ajuste.\n\nPara demostrarlo, considere los datos de clasificación que se muestran en Figura 12.1 con dos predictores, dos clases y un conjunto de entrenamiento de puntos de datos 593.\n\n\n\n\n\n\n\n\nFigura 12.1: Un ejemplo de conjunto de datos de clasificación de dos clases con dos predictores\n\n\n\n\n\nPodríamos comenzar ajustando un límite de clase lineal a estos datos. El método más común para hacer esto es utilizar un modelo lineal generalizado en forma de regresión logística. Este modelo relaciona las log odds de que una muestra sea Clase 1 usando la transformación logit:\n\\[ \\log\\left(\\frac{\\pi}{1 - \\pi}\\right) = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_px_p\\]\nEn el contexto de los modelos lineales generalizados, la función logit es la función de enlace entre el resultado (\\(\\pi\\)) y los predictores. Hay otras funciones de enlace que incluyen el modelo probit:\n\\[\\Phi^{-1}(\\pi) = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_px_p\\]\ndonde \\(\\Phi\\) es la función normal estándar acumulativa, así como el modelo log-log complementario:\n\\[\\log(-\\log(1-\\pi)) = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_px_p\\]\nCada uno de estos modelos da como resultado límites de clase lineales. ¿Cuál deberíamos usar? Dado que, para estos datos, el número de parámetros del modelo no varía, el enfoque estadístico es calcular la probabilidad (logaritmo) de cada modelo y determinar el modelo con el valor más grande. Tradicionalmente, la probabilidad se calcula utilizando los mismos datos que se utilizaron para estimar los parámetros, sin utilizar enfoques como la división de datos o el remuestreo de los Capítulos 5 y 10.\nPara un marco de datos training_set, creemos una función para calcular los diferentes modelos y extraer las estadísticas de probabilidad para el conjunto de entrenamiento (usando broom::glance()):\n\nlibrary(tidymodels)\ntidymodels_prefer()\n\nllhood &lt;- function(...) {\n  logistic_reg() %&gt;% \n    set_engine(\"glm\", ...) %&gt;% \n    fit(Class ~ ., data = training_set) %&gt;% \n    glance() %&gt;% \n    select(logLik)\n}\n\nbind_rows(\n  llhood(),\n  llhood(family = binomial(link = \"probit\")),\n  llhood(family = binomial(link = \"cloglog\"))\n) %&gt;% \n  mutate(link = c(\"logit\", \"probit\", \"c-log-log\"))  %&gt;% \n  arrange(desc(logLik))\n## # A tibble: 3 × 2\n##   logLik link     \n##    &lt;dbl&gt; &lt;chr&gt;    \n## 1  -258. logit    \n## 2  -262. probit   \n## 3  -270. c-log-log\n\nSegún estos resultados, el modelo logístico tiene las mejores propiedades estadísticas.\nA partir de la escala de los valores de probabilidad logarítmica, es difícil entender si estas diferencias son importantes o insignificantes. Una forma de mejorar este análisis es volver a muestrear las estadísticas y separar los datos del modelado de los datos utilizados para la estimación del desempeño. Con este pequeño conjunto de datos, la validación cruzada repetida 10 veces es una buena opción para el remuestreo. En el paquete yardstick, la función mn_log_loss() se usa para estimar la probabilidad logarítmica negativa, y nuestros resultados se muestran en Figura 12.2.\n\nset.seed(1201)\nrs &lt;- vfold_cv(training_set, repeats = 10)\n\n# Devuelva las estimaciones de rendimiento individuales remuestreadas:\nlloss &lt;- function(...) {\n  perf_meas &lt;- metric_set(roc_auc, mn_log_loss)\n    \n  logistic_reg() %&gt;% \n    set_engine(\"glm\", ...) %&gt;% \n    fit_resamples(Class ~ A + B, rs, metrics = perf_meas) %&gt;% \n    collect_metrics(summarize = FALSE) %&gt;%\n    select(id, id2, .metric, .estimate)\n}\n\nresampled_res &lt;- \n  bind_rows(\n    lloss()                                    %&gt;% mutate(model = \"logistic\"),\n    lloss(family = binomial(link = \"probit\"))  %&gt;% mutate(model = \"probit\"),\n    lloss(family = binomial(link = \"cloglog\")) %&gt;% mutate(model = \"c-log-log\")     \n  ) %&gt;%\n  # Convierta la pérdida logarítmica en probabilidad logarítmica:\n  mutate(.estimate = ifelse(.metric == \"mn_log_loss\", -.estimate, .estimate)) %&gt;% \n  group_by(model, .metric) %&gt;% \n  summarize(\n    mean = mean(.estimate, na.rm = TRUE),\n    std_err = sd(.estimate, na.rm = TRUE) / sqrt(n()), \n    .groups = \"drop\"\n  )\n\nresampled_res %&gt;% \n  filter(.metric == \"mn_log_loss\") %&gt;% \n  ggplot(aes(x = mean, y = model)) + \n  geom_point() + \n  geom_errorbar(aes(xmin = mean - 1.64 * std_err, xmax = mean + 1.64 * std_err),\n                width = .1) + \n  labs(y = NULL, x = \"log-likelihood\")\n\n\n\n\n\n\n\n\n\nFigura 12.2: Medias e intervalos de confianza aproximados del 90% para la probabilidad logarítmica binomial remuestreada con tres funciones de enlace diferentes\n\n\n\n\n\n\nLa escala de estos valores es diferente a los valores anteriores ya que se calculan en un conjunto de datos más pequeño; el valor producido por broom::glance() es una suma mientras que yardstick::mn_log_loss() es un promedio.\n\nEstos resultados muestran evidencia de que la elección de la función de enlace es algo importante. Aunque existe una superposición en los intervalos de confianza, el modelo logístico tiene los mejores resultados.\n¿Qué tal una métrica diferente? También calculamos el área bajo la curva ROC para cada remuestreo. Estos resultados, que reflejan la capacidad discriminativa de los modelos a través de numerosos umbrales de probabilidad, muestran una falta de diferencia en Figura 12.3.\n\n\n\n\n\n\n\n\nFigura 12.3: Medias e intervalos de confianza aproximados del 90% para el área remuestreada bajo la curva ROC con tres funciones de enlace diferentes\n\n\n\n\n\nDada la superposición de los intervalos, así como la escala del eje x, se podría utilizar cualquiera de estas opciones. Vemos esto nuevamente cuando los límites de clase para los tres modelos se superponen en el conjunto de prueba de puntos de datos 198 en Figura 12.4.\n\n\n\n\n\n\n\n\nFigura 12.4: El límite de clase lineal se ajusta a tres funciones de enlace.\n\n\n\n\n\n\nEste ejercicio enfatiza que diferentes métricas pueden conducir a diferentes decisiones sobre la elección de los valores de los parámetros de ajuste. En este caso, una métrica indica que los modelos son algo diferentes mientras que otra métrica no muestra ninguna diferencia.\n\nThomas y Uminsky (2020) analiza exhaustivamente la optimización de métricas y explora varios temas, incluido el juego de métricas. Advierten que:\n\nLa eficacia irrazonable de la optimización de métricas en los enfoques actuales de IA es un desafío fundamental para el campo y genera una contradicción inherente: la optimización exclusiva de las métricas conduce a resultados que distan mucho de ser óptimos.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Ajuste De Modelos Y Los Peligros Del Sobreajuste</span>"
    ]
  },
  {
    "objectID": "12-tuning-parameters.html#sec-overfitting-bad",
    "href": "12-tuning-parameters.html#sec-overfitting-bad",
    "title": "12  Ajuste De Modelos Y Los Peligros Del Sobreajuste",
    "section": "12.4 Las Consecuencias De Las Malas Estimaciones De Parámetros",
    "text": "12.4 Las Consecuencias De Las Malas Estimaciones De Parámetros\nMuchos parámetros de ajuste modulan la cantidad de complejidad del modelo. Una mayor complejidad a menudo implica una mayor maleabilidad en los patrones que un modelo puede emular. Por ejemplo, como se muestra en Sección 8.4.3, agregar grados de libertad en una función spline aumenta la complejidad de la ecuación de predicción. Si bien esto es una ventaja cuando los motivos subyacentes de los datos son complejos, también puede conducir a una sobreinterpretación de patrones aleatorios que no se reproducirían en datos nuevos. Sobreajuste es la situación en la que un modelo se adapta demasiado a los datos de entrenamiento; funciona bien con los datos utilizados para construir el modelo, pero mal con datos nuevos.\n\nDado que ajustar los parámetros del modelo puede aumentar la complejidad del modelo, las malas elecciones pueden provocar un sobreajuste.\n\nRecuerde el modelo de red neuronal de una sola capa descrito en Sección 12.2. Con una única unidad oculta y funciones de activación sigmoideas, una red neuronal para clasificación es, para todos los efectos, solo una regresión logística. Sin embargo, a medida que aumenta el número de unidades ocultas, también aumenta la complejidad del modelo. De hecho, cuando el modelo de red utiliza unidades de activación sigmoidales, Cybenko (1989) demostró que el modelo es un aproximador de función universal siempre que haya suficientes unidades ocultas.\nAjustamos modelos de clasificación de redes neuronales a los mismos datos de dos clases de la sección anterior, variando el número de unidades ocultas. Utilizando el área bajo la curva ROC como métrica de rendimiento, la efectividad del modelo en el conjunto de entrenamiento aumenta a medida que se agregan más unidades ocultas. El modelo de red aprende completa y meticulosamente el conjunto de entrenamiento. Si el modelo se juzga a sí mismo por el valor ROC del conjunto de entrenamiento, prefiere muchas unidades ocultas para poder casi eliminar los errores.\nLos capítulos 5 y 10 demostraron que simplemente repredecir el conjunto de entrenamiento es un enfoque deficiente para la evaluación de modelos. Aquí, la red neuronal muy rápidamente comienza a sobreinterpretar los patrones que ve en el conjunto de entrenamiento. Compare estos tres límites de clase de ejemplo (desarrollados con el conjunto de entrenamiento) superpuestos en los conjuntos de entrenamiento y prueba en Figura 12.5.\n\n\n\n\n\n\n\n\nFigura 12.5: Límites de clase para tres modelos con un número creciente de unidades ocultas. Los límites se ajustan al conjunto de entrenamiento y se muestran para los conjuntos de entrenamiento y prueba.\n\n\n\n\n\nEl modelo unitario no se adapta con mucha flexibilidad a los datos (ya que está obligado a ser lineal). Un modelo con cuatro unidades ocultas comienza a mostrar signos de sobreajuste con un límite poco realista para valores alejados de la corriente principal de datos. Esto se debe a un único punto de datos de la primera clase en la esquina superior derecha de los datos. Con 20 unidades ocultas, el modelo comienza a memorizar el conjunto de entrenamiento, creando pequeñas islas alrededor de esos datos para minimizar la tasa de error de resustitución. Estos patrones no se repiten en el conjunto de prueba. Este último panel es la mejor ilustración de cómo se deben modular los parámetros de ajuste que controlan la complejidad para que el modelo sea efectivo. Para un modelo de 20 unidades, el conjunto de entrenamiento ROC AUC es 0.944 pero el valor del conjunto de prueba es 0.855.\nEsta ocurrencia de sobreajuste es obvia con dos predictores que podemos trazar. Sin embargo, en general, debemos utilizar un enfoque cuantitativo para detectar el sobreajuste.\n\nLa solución para detectar cuándo un modelo está exagerando el conjunto de entrenamiento es utilizar datos fuera de la muestra.\n\nEn lugar de utilizar el conjunto de prueba, se requiere algún tipo de remuestreo. Esto podría significar un enfoque iterativo (por ejemplo, una validación cruzada de 10 veces) o una única fuente de datos (por ejemplo, un conjunto de validación).",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Ajuste De Modelos Y Los Peligros Del Sobreajuste</span>"
    ]
  },
  {
    "objectID": "12-tuning-parameters.html#dos-estrategias-generales-para-la-optimización.",
    "href": "12-tuning-parameters.html#dos-estrategias-generales-para-la-optimización.",
    "title": "12  Ajuste De Modelos Y Los Peligros Del Sobreajuste",
    "section": "12.5 Dos Estrategias Generales Para La Optimización.",
    "text": "12.5 Dos Estrategias Generales Para La Optimización.\nLa optimización de los parámetros de ajuste generalmente se divide en una de dos categorías: búsqueda en cuadrícula y búsqueda iterativa.\nBúsqueda de cuadrícula es cuando predefinimos un conjunto de valores de parámetros para evaluar. Las principales opciones involucradas en la búsqueda de cuadrículas son cómo hacer la cuadrícula y cuántas combinaciones de parámetros evaluar. La búsqueda de cuadrícula a menudo se considera ineficiente ya que la cantidad de puntos de cuadrícula necesarios para cubrir el espacio de parámetros puede volverse inmanejable debido a la maldición de la dimensionalidad. Hay algo de verdad en esta preocupación, pero es más cierta cuando el proceso no está optimizado. Esto se analiza con más detalle en el Capítulo 13.\nBúsqueda iterativa o búsqueda secuencial es cuando descubrimos secuencialmente nuevas combinaciones de parámetros en función de resultados anteriores. Casi cualquier método de optimización no lineal es apropiado, aunque algunos son más eficientes que otros. En algunos casos, se requiere un conjunto inicial de resultados para una o más combinaciones de parámetros para iniciar el proceso de optimización. La búsqueda iterativa se analiza con más detalle en el Capítulo 14.\nFigura 12.6 muestra dos paneles que demuestran estos dos enfoques para una situación con dos parámetros de ajuste que oscilan entre cero y uno. En cada uno, un conjunto de contornos muestra la relación verdadera (simulada) entre los parámetros y el resultado. Los resultados óptimos se encuentran en las esquinas superiores derechas.\n\n\n\n\n\n\n\n\nFigura 12.6: Ejemplos de ajuste de cuadrícula predefinido y método de búsqueda iterativo. Las líneas representan contornos de una métrica de desempeño; es mejor en el lado superior derecho de la trama.\n\n\n\n\n\nEl panel izquierdo de Figura 12.6 muestra un tipo de cuadrícula llamada diseño de relleno de espacio. Este es un tipo de diseño experimental diseñado para cubrir el espacio de parámetros de manera que las combinaciones de parámetros de ajuste no estén cercanas entre sí. Los resultados de este diseño no colocan ningún punto exactamente en la ubicación verdaderamente óptima. Sin embargo, un punto está en la vecindad general y probablemente tendría resultados de métricas de rendimiento que estén dentro del ruido del valor más óptimo.\nEl panel derecho de Figura 12.6 ilustra los resultados de un método de búsqueda global: el método simplex de Nelder-Mead (Olsson y Nelson 1975). El punto de partida está en la parte inferior izquierda del espacio de parámetros. La búsqueda serpentea por el espacio hasta llegar a la ubicación óptima, donde se esfuerza por acercarse lo más posible al mejor valor numérico. Este método de búsqueda en particular, aunque eficaz, no es conocido por su eficiencia; requiere muchas evaluaciones de funciones, especialmente cerca de los valores óptimos. El Capítulo 14 analiza algoritmos de búsqueda más eficientes.\n\nLas estrategias híbridas también son una opción y pueden funcionar bien. Después de una búsqueda inicial de cuadrícula, puede comenzar una optimización secuencial a partir de la mejor combinación de cuadrícula.\n\nEn los dos capítulos siguientes se analizan en detalle ejemplos de estas estrategias. Antes de continuar, aprendamos cómo trabajar con objetos de parámetros de ajuste en tidymodels, usando el paquete dials.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Ajuste De Modelos Y Los Peligros Del Sobreajuste</span>"
    ]
  },
  {
    "objectID": "12-tuning-parameters.html#sec-tuning-params-tidymodels",
    "href": "12-tuning-parameters.html#sec-tuning-params-tidymodels",
    "title": "12  Ajuste De Modelos Y Los Peligros Del Sobreajuste",
    "section": "12.6 Ajuste De Parámetros En tidymodels",
    "text": "12.6 Ajuste De Parámetros En tidymodels\nYa hemos tratado un buen número de argumentos que corresponden a parámetros de ajuste para especificaciones de recetas y modelos en capítulos anteriores. Es posible sintonizar:\n\nel umbral para combinar vecindarios en una categoría “otra” (con el nombre de argumento threshold) discutido en Sección 8.4.1\nel número de grados de libertad en un spline natural (deg_free, Sección 8.4.3)\nel número de puntos de datos necesarios para ejecutar una división en un modelo basado en árbol (min_n, Sección 6.1)\nel importe de la regularización en los modelos penalizados (penalty, Sección 6.1)\n\nPara las especificaciones del modelo parsnip, hay dos tipos de argumentos de parámetros. Los argumentos principales son aquellos que con mayor frecuencia están optimizados para el rendimiento y están disponibles en múltiples motores. Los principales parámetros de ajuste son argumentos de nivel superior para la función de especificación del modelo. Por ejemplo, la función rand_forest() tiene argumentos principales trees, min_n y mtry, ya que estos se especifican u optimizan con mayor frecuencia.\nUn conjunto secundario de parámetros de ajuste son específicos del motor. Estos se optimizan con poca frecuencia o son específicos solo para ciertos motores. Nuevamente usando random forest como ejemplo, el paquete ranger contiene algunos argumentos que no son utilizados por otros paquetes. Un ejemplo es la penalización de ganancia, que regulariza la selección de predictores en el proceso de inducción del árbol. Este parámetro puede ayudar a modular el equilibrio entre la cantidad de predictores utilizados en el conjunto y el rendimiento (Wundervald, Parnell, y Domijan 2020). El nombre de este argumento en ranger() es regularization.factor. Para especificar un valor mediante una especificación de modelo parsnip, se agrega como argumento complementario a set_engine():\n\nrand_forest(trees = 2000, min_n = 10) %&gt;%                   # &lt;- argumentos principales\n  set_engine(\"ranger\", regularization.factor = 0.5)         # &lt;- específico del motor\n\n\nLos argumentos principales utilizan un sistema de nombres armonizado para eliminar inconsistencias entre motores, mientras que los argumentos específicos del motor no lo hacen.\n\n¿Cómo podemos indicar a las funciones de tidymodels qué argumentos deben optimizarse? Los parámetros se marcan para su ajuste asignándoles un valor de tune(). Para la red neuronal de una sola capa utilizada en Sección 12.4, la cantidad de unidades ocultas se designa para ajustar usando:\n\nneural_net_spec &lt;- \n  mlp(hidden_units = tune()) %&gt;%\n  set_mode(\"regression\") %&gt;%\n  set_engine(\"keras\")\n\nLa función tune() no ejecuta ningún valor de parámetro en particular; solo devuelve una expresión:\n\ntune()\n## tune()\n\nIncrustar este valor tune() en un argumento etiquetará el parámetro para su optimización. Las funciones de ajuste del modelo que se muestran en los dos capítulos siguientes analizan la especificación y/o receta del modelo para descubrir los parámetros etiquetados. Estas funciones pueden configurar y procesar automáticamente estos parámetros ya que comprenden sus características (por ejemplo, el rango de valores posibles, etc.).\nPara enumerar los parámetros de ajuste de un objeto, utilice la función extract_parameter_set_dials():\n\nextract_parameter_set_dials(neural_net_spec)\n## Collection of 1 parameters for tuning\n## \n##    identifier         type    object\n##  hidden_units hidden_units nparam[+]\n\nLos resultados muestran un valor de nparam[+], lo que indica que el número de unidades ocultas es un parámetro numérico.\nHay un argumento de identificación opcional que asocia un nombre con los parámetros. Esto puede resultar útil cuando se ajusta el mismo tipo de parámetro en diferentes lugares. Por ejemplo, con los datos de vivienda de Ames de Sección 10.6, la receta codificó tanto la longitud como la latitud con funciones spline. Si queremos ajustar las dos funciones spline para que potencialmente tengan diferentes niveles de suavidad, llamamos a step_ns() dos veces, una para cada predictor. Para que los parámetros sean identificables, el argumento de identificación puede tomar cualquier cadena de caracteres:\n\names_rec &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train)  %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_other(Neighborhood, threshold = tune()) %&gt;% \n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;% \n  step_ns(Longitude, deg_free = tune(\"longitude df\")) %&gt;% \n  step_ns(Latitude,  deg_free = tune(\"latitude df\"))\n\nrecipes_param &lt;- extract_parameter_set_dials(ames_rec)\nrecipes_param\n## Collection of 3 parameters for tuning\n## \n##    identifier      type    object\n##     threshold threshold nparam[+]\n##  longitude df  deg_free nparam[+]\n##   latitude df  deg_free nparam[+]\n\nTenga en cuenta que las columnas identifier y type no son las mismas para ambos parámetros de spline.\nCuando se combinan una receta y una especificación de modelo mediante un flujo de trabajo, se muestran ambos conjuntos de parámetros:\n\nwflow_param &lt;- \n  workflow() %&gt;% \n  add_recipe(ames_rec) %&gt;% \n  add_model(neural_net_spec) %&gt;% \n  extract_parameter_set_dials()\nwflow_param\n## Collection of 4 parameters for tuning\n## \n##    identifier         type    object\n##  hidden_units hidden_units nparam[+]\n##     threshold    threshold nparam[+]\n##  longitude df     deg_free nparam[+]\n##   latitude df     deg_free nparam[+]\n\n\nLas redes neuronales son exquisitamente capaces de emular patrones no lineales. No es necesario agregar términos splines a este tipo de modelo; combinamos este modelo y receta solo con fines ilustrativos.\n\nCada argumento de parámetro de ajuste tiene una función correspondiente en el paquete dials. En la gran mayoría de los casos, la función tiene el mismo nombre que el argumento del parámetro:\n\nhidden_units()\n## # Hidden Units (quantitative)\n## Range: [1, 10]\nthreshold()\n## Threshold (quantitative)\n## Range: [0, 1]\n\nEl parámetro deg_free es un contraejemplo; la noción de grados de libertad surge en una variedad de contextos diferentes. Cuando se usa con splines, hay una función especializada dials llamada spline_title() que, de forma predeterminada, se invoca para splines:\n\nspline_degree()\n## Spline Degrees of Freedom (quantitative)\n## Range: [1, 10]\n\nEl paquete dials también tiene una función conveniente para extraer un objeto de parámetro particular:\n\n# identifique el parámetro usando el valor de id:\nwflow_param %&gt;% extract_parameter_dials(\"threshold\")\n## Threshold (quantitative)\n## Range: [0, 0.1]\n\nDentro del conjunto de parámetros, el rango de parámetros también se puede actualizar en el lugar:\n\nextract_parameter_set_dials(ames_rec) %&gt;% \n  update(threshold = threshold(c(0.8, 1.0)))\n## Collection of 3 parameters for tuning\n## \n##    identifier      type    object\n##     threshold threshold nparam[+]\n##  longitude df  deg_free nparam[+]\n##   latitude df  deg_free nparam[+]\n\nLos conjuntos de parámetros creados por extract_parameter_set_dials() son consumidos por las funciones de ajuste de tidymodels (cuando sea necesario). Si los valores predeterminados de los objetos de parámetros de ajuste requieren modificación, se pasa un conjunto de parámetros modificado a la función de ajuste adecuada.\n\nAlgunos parámetros de ajuste dependen de las dimensiones de los datos. Por ejemplo, el número de vecinos más cercanos debe estar entre uno y el número de filas de los datos.\n\nEn algunos casos, es fácil tener valores predeterminados razonables para el rango de valores posibles. En otros casos, el rango de parámetros es crítico y no se puede asumir. El principal parámetro de ajuste para los modelos de random forest es el número de columnas predictoras que se muestrean aleatoriamente para cada división del árbol, normalmente denominada “mtry()”. Sin conocer la cantidad de predictores, este rango de parámetros no se puede preconfigurar y requiere finalización.\n\nrf_spec &lt;- \n  rand_forest(mtry = tune()) %&gt;% \n  set_engine(\"ranger\", regularization.factor = tune(\"regularization\")) %&gt;%\n  set_mode(\"regression\")\n\nrf_param &lt;- extract_parameter_set_dials(rf_spec)\nrf_param\n## Collection of 2 parameters for tuning\n## \n##      identifier                  type    object\n##            mtry                  mtry nparam[?]\n##  regularization regularization.factor nparam[+]\n## \n## Model parameters needing finalization:\n##    # Randomly Selected Predictors ('mtry')\n## \n## See `?dials::finalize` or `?dials::update.parameters` for more information.\n\nLos objetos de parámetros completos tienen [+] en su resumen; un valor de [?] indica que falta al menos un extremo del rango posible. Hay dos métodos para manejar esto. La primera es usar update(), para agregar un rango basado en lo que sabes sobre las dimensiones de datos:\n\nrf_param %&gt;% \n  update(mtry = mtry(c(1, 70)))\n## Collection of 2 parameters for tuning\n## \n##      identifier                  type    object\n##            mtry                  mtry nparam[+]\n##  regularization regularization.factor nparam[+]\n\nSin embargo, es posible que este enfoque no funcione si se adjunta una receta a un flujo de trabajo que utiliza pasos que suman o restan columnas. Si esos pasos no están programados para ajustarse, la función finalize() puede ejecutar la receta una vez para obtener las dimensiones:\n\npca_rec &lt;- \n  recipe(Sale_Price ~ ., data = ames_train) %&gt;% \n  # Seleccione los predictores de pies cuadrados y extraiga sus componentes PCA:\n  step_normalize(contains(\"SF\")) %&gt;% \n  # Seleccione el número de componentes necesarios para capturar el 95% de\n  # la varianza en los predictores. \n  step_pca(contains(\"SF\"), threshold = .95)\n  \nupdated_param &lt;- \n  workflow() %&gt;% \n  add_model(rf_spec) %&gt;% \n  add_recipe(pca_rec) %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  finalize(ames_train)\nupdated_param\n## Collection of 2 parameters for tuning\n## \n##      identifier                  type    object\n##            mtry                  mtry nparam[+]\n##  regularization regularization.factor nparam[+]\nupdated_param %&gt;% extract_parameter_dials(\"mtry\")\n## # Randomly Selected Predictors (quantitative)\n## Range: [1, 74]\n\nCuando se prepara la receta, la función finalize() aprende a establecer el rango superior de mtry en 74 predictores.\nAdemás, los resultados de extract_parameter_set_dials() incluirán parámetros específicos del motor (si los hay). Se descubren de la misma manera que los argumentos principales y se incluyen en el conjunto de parámetros. El paquete dials contiene funciones de parámetros para todos los parámetros específicos del motor potencialmente ajustables:\n\nrf_param\n## Collection of 2 parameters for tuning\n## \n##      identifier                  type    object\n##            mtry                  mtry nparam[?]\n##  regularization regularization.factor nparam[+]\n## \n## Model parameters needing finalization:\n##    # Randomly Selected Predictors ('mtry')\n## \n## See `?dials::finalize` or `?dials::update.parameters` for more information.\nregularization_factor()\n## Gain Penalization (quantitative)\n## Range: [0, 1]\n\nFinalmente, algunos parámetros de ajuste se asocian mejor con las transformaciones. Un buen ejemplo de esto es el parámetro de penalización asociado con muchos modelos de regresión regularizados. Este parámetro no es negativo y es común variar sus valores en unidades logarítmicas. El objeto de parámetro principal dials indica que se utiliza una transformación de forma predeterminada:\n\npenalty()\n## Amount of Regularization (quantitative)\n## Transformer: log-10 [1e-100, Inf]\n## Range (transformed scale): [-10, 0]\n\nEs importante saber esto, especialmente al modificar el rango. Los valores del nuevo rango deben estar en las unidades transformadas:\n\n# método correcto para tener valores de penalización entre 0,1 y 1,0\npenalty(c(-1, 0)) %&gt;% value_sample(1000) %&gt;% summary()\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   0.101   0.181   0.327   0.400   0.589   0.999\n\n# incorrecto:\npenalty(c(0.1, 1.0)) %&gt;% value_sample(1000) %&gt;% summary()\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    1.26    2.21    3.68    4.26    5.89   10.00\n\nLa escala se puede cambiar si se desea con el argumento trans. Puedes usar unidades naturales pero del mismo rango:\n\npenalty(trans = NULL, range = 10^c(-10, 0))\n## Amount of Regularization (quantitative)\n## Range: [1e-10, 1]",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Ajuste De Modelos Y Los Peligros Del Sobreajuste</span>"
    ]
  },
  {
    "objectID": "12-tuning-parameters.html#resumen-del-capítulo",
    "href": "12-tuning-parameters.html#resumen-del-capítulo",
    "title": "12  Ajuste De Modelos Y Los Peligros Del Sobreajuste",
    "section": "12.7 Resumen Del Capítulo",
    "text": "12.7 Resumen Del Capítulo\nEste capítulo presentó el proceso de ajuste de los hiperparámetros del modelo que no se pueden estimar directamente a partir de los datos. Ajustar dichos parámetros puede conducir a un sobreajuste, a menudo al permitir que un modelo se vuelva demasiado complejo, por lo que es importante utilizar conjuntos de datos remuestreados junto con métricas apropiadas para la evaluación. Hay dos estrategias generales para determinar los valores correctos, la búsqueda en cuadrícula y la búsqueda iterativa, que exploraremos en profundidad en los dos capítulos siguientes. En tidymodels, la función tune() se usa para identificar parámetros para optimización, y las funciones del paquete dials pueden extraer e interactuar con objetos de parámetros de ajuste.\n\n\n\n\nCybenko, G. 1989. «Approximation by superpositions of a sigmoidal function». Mathematics of Control, Signals and Systems 2 (4): 303-14.\n\n\nDobson, A. 1999. An introduction to generalized linear models. Chapman; Hall: Boca Raton.\n\n\nFriedman, J. 2001. «Greedy Function Approximation: A Gradient Boosting Machine». Annals of Statistics 29 (5): 1189-1232.\n\n\nGoodfellow, I, Y Bengio, y A Courville. 2016. Deep Learning. MIT Press.\n\n\nLittell, R, J Pendergast, y R Natarajan. 2000. «Modelling covariance structure in the analysis of repeated measures data». Statistics in Medicine 19 (13): 1793-1819.\n\n\nOlsson, D, y L Nelson. 1975. «The Nelder-Mead Simplex Procedure for Function Minimization». Technometrics 17 (1): 45-51.\n\n\nThomas, R, y D Uminsky. 2020. «The Problem with Metrics is a Fundamental Problem for AI». https://arxiv.org/abs/2002.08512.\n\n\nWundervald, B, A Parnell, y K Domijan. 2020. «Generalizing Gain Penalization for Feature Selection in Tree-based Models». https://arxiv.org/abs/2006.07515.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Ajuste De Modelos Y Los Peligros Del Sobreajuste</span>"
    ]
  },
  {
    "objectID": "13-grid-search.html",
    "href": "13-grid-search.html",
    "title": "13  Búsqueda De Cuadrícula",
    "section": "",
    "text": "13.1 Cuadrículas Regulares Y No Regulares\nHay dos tipos principales de cuadrículas. Una cuadrícula regular combina cada parámetro (con su correspondiente conjunto de valores posibles) factorialmente, es decir, utilizando todas las combinaciones de los conjuntos. Alternativamente, una cuadrícula no regular es aquella en la que las combinaciones de parámetros no se forman a partir de un pequeño conjunto de puntos.\nAntes de analizar cada tipo con más detalle, consideremos un modelo de ejemplo: el modelo de perceptrón multicapa (también conocido como red neuronal artificial de una sola capa). Los parámetros marcados para tuning son:\nUsando parsnip, la especificación para un modelo de clasificación que se ajusta usando el paquete nnet es:\nlibrary(tidymodels)\ntidymodels_prefer()\n\nmlp_spec &lt;- \n  mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %&gt;% \n  set_engine(\"nnet\", trace = 0) %&gt;% \n  set_mode(\"classification\")\nEl argumento trace = 0 evita el registro adicional del proceso de entrenamiento. Como se muestra en Sección 12.6, la función extract_parameter_set_dials() puede extraer el conjunto de argumentos con valores desconocidos y establece sus objetos dials:\nmlp_param &lt;- extract_parameter_set_dials(mlp_spec)\nmlp_param %&gt;% extract_parameter_dials(\"hidden_units\")\n## # Hidden Units (quantitative)\n## Range: [1, 10]\nmlp_param %&gt;% extract_parameter_dials(\"penalty\")\n## Amount of Regularization (quantitative)\n## Transformer: log-10 [1e-100, Inf]\n## Range (transformed scale): [-10, 0]\nmlp_param %&gt;% extract_parameter_dials(\"epochs\")\n## # Epochs (quantitative)\n## Range: [10, 1000]\nEsta salida indica que los objetos de parámetros están completos e imprime sus rangos predeterminados. Estos valores se utilizarán para demostrar cómo crear diferentes tipos de cuadrículas de parámetros.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Búsqueda De Cuadrícula</span>"
    ]
  },
  {
    "objectID": "13-grid-search.html#sec-grids",
    "href": "13-grid-search.html#sec-grids",
    "title": "13  Búsqueda De Cuadrícula",
    "section": "",
    "text": "el número de unidades ocultas\nel número de épocas/iteraciones de ajuste en el entrenamiento del modelo\nla cantidad de penalización por pérdida de peso\n\n\nHistóricamente, el número de épocas estuvo determinado por la detención temprana; un conjunto de validación separado determinó la duración del entrenamiento en función de la tasa de error, ya que volver a predecir el conjunto de entrenamiento conducía a un sobreajuste. En nuestro caso, el uso de una penalización por disminución de peso debería prohibir el sobreajuste, y hay poco daño en ajustar la penalización y el número de épocas.\n\n\n\n\n\n\n\nCuadrículas regulares\nLas cuadrículas regulares son combinaciones de conjuntos separados de valores de parámetros. Primero, el usuario crea un conjunto distinto de valores para cada parámetro. El número de valores posibles no tiene por qué ser el mismo para cada parámetro. La función tidyr crossing() es una forma de crear una cuadrícula regular:\n\ncrossing(\n  hidden_units = 1:3,\n  penalty = c(0.0, 0.1),\n  epochs = c(100, 200)\n)\n## # A tibble: 12 × 3\n##   hidden_units penalty epochs\n##          &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n## 1            1     0      100\n## 2            1     0      200\n## 3            1     0.1    100\n## 4            1     0.1    200\n## 5            2     0      100\n## 6            2     0      200\n## # ℹ 6 more rows\n\nEl objeto de parámetro conoce los rangos de los parámetros. El paquete dials contiene un conjunto de funciones grid_*() que toman el objeto parámetro como entrada para producir diferentes tipos de cuadrículas. Por ejemplo:\n\ngrid_regular(mlp_param, levels = 2)\n## # A tibble: 8 × 3\n##   hidden_units      penalty epochs\n##          &lt;int&gt;        &lt;dbl&gt;  &lt;int&gt;\n## 1            1 0.0000000001     10\n## 2           10 0.0000000001     10\n## 3            1 1                10\n## 4           10 1                10\n## 5            1 0.0000000001   1000\n## 6           10 0.0000000001   1000\n## # ℹ 2 more rows\n\nEl argumento levels es el número de niveles por parámetro a crear. También puede tomar un vector de valores con nombre:\n\nmlp_param %&gt;% \n  grid_regular(levels = c(hidden_units = 3, penalty = 2, epochs = 2))\n## # A tibble: 12 × 3\n##   hidden_units      penalty epochs\n##          &lt;int&gt;        &lt;dbl&gt;  &lt;int&gt;\n## 1            1 0.0000000001     10\n## 2            5 0.0000000001     10\n## 3           10 0.0000000001     10\n## 4            1 1                10\n## 5            5 1                10\n## 6           10 1                10\n## # ℹ 6 more rows\n\nExisten técnicas para crear cuadrículas regulares que no utilizan todos los valores posibles de cada conjunto de parámetros. Estos diseños factoriales fraccionales (Box, Hunter, y Hunter 2005) también podrían usarse. Para obtener más información, consulte la Vista de tareas de CRAN para el diseño experimental.1\n\nEl uso de cuadrículas regulares puede ser costoso desde el punto de vista computacional, especialmente cuando hay una cantidad media a grande de parámetros de ajuste. Esto es cierto para muchos modelos pero no para todos. Como se analiza en ?sec-ficient-grids a continuación, ¡hay muchos modelos cuyo tiempo de ajuste disminuye con una cuadrícula normal!\n\nUna ventaja de utilizar una cuadrícula regular es que las relaciones y patrones entre los parámetros de ajuste y las métricas del modelo se entienden fácilmente. La naturaleza factorial de estos diseños permite examinar cada parámetro por separado con poca confusión entre los parámetros.\n\n\nRejillas irregulares\nHay varias opciones para crear cuadrículas no regulares. La primera es utilizar un muestreo aleatorio en toda la gama de parámetros. La función grid_random() genera números aleatorios uniformes independientes en todos los rangos de parámetros. Si el objeto parámetro tiene una transformación asociada (como la que tenemos para “penalización”, penalty), los números aleatorios se generan en la escala transformada. Creemos una cuadrícula aleatoria para los parámetros de nuestra red neuronal de ejemplo:\n\nset.seed(1301)\nmlp_param %&gt;% \n  grid_random(size = 1000) %&gt;% # 'size' es el número de combinaciones\n  summary()\n##   hidden_units      penalty           epochs   \n##  Min.   : 1.00   Min.   :0.0000   Min.   : 10  \n##  1st Qu.: 3.00   1st Qu.:0.0000   1st Qu.:266  \n##  Median : 5.00   Median :0.0000   Median :497  \n##  Mean   : 5.38   Mean   :0.0437   Mean   :510  \n##  3rd Qu.: 8.00   3rd Qu.:0.0027   3rd Qu.:761  \n##  Max.   :10.00   Max.   :0.9814   Max.   :999\n\nPara la “penalización”, penalty, los números aleatorios son uniformes en la escala logarítmica (base-10), pero los valores en la cuadrícula están en unidades naturales.\nEl problema con las cuadrículas aleatorias es que, en las cuadrículas pequeñas y medianas, los valores aleatorios pueden dar como resultado combinaciones de parámetros superpuestos. Además, la cuadrícula aleatoria debe cubrir todo el espacio de parámetros, pero la probabilidad de una buena cobertura aumenta con el número de valores de la cuadrícula. Incluso para una muestra de 15 puntos candidatos, Figura 13.1 muestra cierta superposición entre puntos para nuestro perceptrón multicapa de ejemplo.\n\nlibrary(ggforce)\nset.seed(1302)\nmlp_param %&gt;% \n  # La opción 'original = FALSE' mantiene la penalización en log10 unidades\n  grid_random(size = 20, original = FALSE) %&gt;% \n  ggplot(aes(x = .panel_x, y = .panel_y)) + \n  geom_point() +\n  geom_blank() +\n  facet_matrix(vars(hidden_units, penalty, epochs), layer.diag = 2) + \n  labs(title = \"Diseño aleatorio con 20 candidatos\")\n\n\n\n\n\n\n\n\n\nFigura 13.1: Tres parámetros de ajuste con 15 puntos generados al azar\n\n\n\n\n\nUn enfoque mucho mejor es utilizar un conjunto de diseños experimentales llamados diseños de relleno de espacio. Si bien los diferentes métodos de diseño tienen objetivos ligeramente diferentes, generalmente encuentran una configuración de puntos que cubre el espacio de parámetros con la menor posibilidad de superposición o valores redundantes. Ejemplos de tales diseños son los hipercubos latinos (McKay, Beckman, y Conover 1979), los diseños de máxima entropía (Shewry y Wynn 1987), los diseños de máxima proyección (Joseph, Gul, y Ba 2015) y otros. Consulte Santner et al. (2003) para obtener una descripción general.\nEl paquete dials contiene funciones para diseños de hipercubo latino y de máxima entropía. Al igual que con grid_random(), las entradas principales son el número de combinaciones de parámetros y un objeto de parámetro. Comparemos un diseño aleatorio con un diseño de hipercubo latino para 20 valores de parámetros candidatos en Figura 13.2.\n\nset.seed(1303)\nmlp_param %&gt;% \n  grid_latin_hypercube(size = 20, original = FALSE) %&gt;% \n  ggplot(aes(x = .panel_x, y = .panel_y)) + \n  geom_point() +\n  geom_blank() +\n  facet_matrix(vars(hidden_units, penalty, epochs), layer.diag = 2) + \n  labs(title = \"Diseño de Hipercubo Latino con 20 candidatos\")\n\n\n\n\n\n\n\n\n\nFigura 13.2: Tres parámetros de ajuste con 20 puntos generados mediante un diseño que llena el espacio\n\n\n\n\n\nSi bien no es perfecto, este diseño de hipercubo latino aleja los puntos entre sí y permite una mejor exploración del espacio de hiperparámetros.\nLos diseños que llenan el espacio pueden ser muy eficaces para representar el espacio de parámetros. El diseño predeterminado utilizado por el paquete tune es el diseño de máxima entropía. Estos tienden a producir cuadrículas que cubren bien el espacio candidato y aumentan drásticamente las posibilidades de encontrar buenos resultados.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Búsqueda De Cuadrícula</span>"
    ]
  },
  {
    "objectID": "13-grid-search.html#sec-evaluating-grid",
    "href": "13-grid-search.html#sec-evaluating-grid",
    "title": "13  Búsqueda De Cuadrícula",
    "section": "13.2 Evaluación De La Cuadrícula",
    "text": "13.2 Evaluación De La Cuadrícula\nPara elegir la mejor combinación de parámetros de ajuste, cada conjunto candidato se evalúa utilizando datos que no se utilizaron para entrenar ese modelo. Los métodos de remuestreo o un único conjunto de validación funcionan bien para este propósito. El proceso (y la sintaxis) se parece mucho al enfoque en Sección 10.3 que utilizó la función fit_resamples() del paquete tune.\nDespués del remuestreo, el usuario selecciona el conjunto de parámetros candidatos más apropiado. Podría tener sentido elegir la mejor combinación de parámetros empíricamente o sesgar la elección hacia otros aspectos del ajuste del modelo, como la simplicidad.\nUsamos un conjunto de datos de clasificación para demostrar el ajuste del modelo en este y el próximo capítulo. Los datos provienen de Hill et al. (2007), quien desarrolló una herramienta de laboratorio de microscopía automatizada para la investigación del cáncer. Los datos consisten en 56 mediciones de imágenes de 2019 células de cáncer de mama humano. Estos predictores representan características de forma e intensidad de diferentes partes de las células (por ejemplo, el núcleo, el límite celular, etc.). Existe un alto grado de correlación entre los predictores. Por ejemplo, existen varios predictores diferentes que miden el tamaño y la forma del núcleo y el límite celular. Además, individualmente, muchos predictores tienen distribuciones sesgadas.\nCada celda pertenece a una de dos clases. Dado que esto es parte de una prueba de laboratorio automatizada, la atención se centró en la capacidad de predicción en lugar de la inferencia.\nLos datos están incluidos en el paquete modeldata. Eliminemos una columna que no es necesaria para el análisis (caso):\n\nlibrary(tidymodels)\ndata(cells)\ncells &lt;- cells %&gt;% select(-case)\n\nDadas las dimensiones de los datos, podemos calcular métricas de rendimiento utilizando una validación cruzada de 10 veces:\n\nset.seed(1304)\ncell_folds &lt;- vfold_cv(cells)\n\nDebido al alto grado de correlación entre los predictores, tiene sentido utilizar la extracción de características PCA para descorrelacionar los predictores. La siguiente receta contiene pasos para transformar los predictores para aumentar la simetría, normalizarlos para que estén en la misma escala y luego realizar la extracción de características. También se ajusta la cantidad de componentes PCA que se conservarán, junto con los parámetros del modelo.\n\nSi bien los componentes PCA resultantes están técnicamente en la misma escala, los componentes de rango inferior tienden a tener un rango más amplio que los componentes de rango superior. Por esta razón, volvemos a normalizar para obligar a los predictores a tener la misma media y varianza.\n\nMuchos de los predictores tienen distribuciones sesgadas. Dado que el PCA se basa en la varianza, los valores extremos pueden tener un efecto perjudicial en estos cálculos. Para contrarrestar esto, agreguemos un paso de receta que estime una transformación de Yeo-Johnson para cada predictor (Yeo y Johnson 2000). Si bien originalmente se pensó como una transformación del resultado, también se puede utilizar para estimar transformaciones que fomentan distribuciones más simétricas. Este paso step_YeoJohnson() ocurre en la receta justo antes de la normalización inicial mediante step_normalize(). Luego, combinemos esta receta de ingeniería de características con nuestra especificación de modelo de red neuronal mlp_spec.\n\nmlp_rec &lt;-\n  recipe(class ~ ., data = cells) %&gt;%\n  step_YeoJohnson(all_numeric_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors()) %&gt;% \n  step_pca(all_numeric_predictors(), num_comp = tune()) %&gt;% \n  step_normalize(all_numeric_predictors())\n\nmlp_wflow &lt;- \n  workflow() %&gt;% \n  add_model(mlp_spec) %&gt;% \n  add_recipe(mlp_rec)\n\nCreemos un objeto de parámetro mlp_param para ajustar algunos de los rangos predeterminados. Podemos cambiar el número de épocas para tener un rango más pequeño (50 a 200 épocas). Además, el rango predeterminado para num_comp() es un rango muy estrecho (de uno a cuatro componentes); podemos aumentar el rango a 40 componentes y establecer el valor mínimo en cero:\n\nmlp_param &lt;- \n  mlp_wflow %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  Matrix::update(\n    epochs = epochs(c(50, 200)),\n    num_comp = num_comp(c(0, 40))\n  )\n\n\nEn step_pca(), usar cero componentes PCA es un atajo para omitir la extracción de funciones. De esta manera, los predictores originales se pueden comparar directamente con los resultados que incluyen componentes PCA.\n\nLa función tune_grid() es la función principal para realizar búsquedas en la cuadrícula. Su funcionalidad es muy similar a fit_resamples() de Sección 10.3, aunque tiene argumentos adicionales relacionados con la cuadrícula:\n\ngrid: Un número entero o marco de datos. Cuando se utiliza un número entero, la función crea un diseño que llena el espacio con un número de grid de combinaciones de parámetros candidatos. Si existen combinaciones de parámetros específicas, el parámetro grid se utiliza para pasarlas a la función.\nparam_info: Un argumento opcional para definir los rangos de parámetros. El argumento es más útil cuando grid es un número entero.\n\nDe lo contrario, la interfaz para tune_grid() es la misma que fit_resamples(). El primer argumento es una especificación del modelo o un flujo de trabajo. Cuando se da un modelo, el segundo argumento puede ser una receta o una fórmula. El otro argumento requerido es un objeto de remuestreo rsample (como cell_folds). La siguiente llamada también pasa un conjunto de métricas para que el área bajo la curva ROC se mida durante el remuestreo.\nPara comenzar, evaluemos una cuadrícula regular con tres niveles en los remuestreos:\n\nroc_res &lt;- metric_set(roc_auc)\nset.seed(1305)\nmlp_reg_tune &lt;-\n  mlp_wflow %&gt;%\n  tune_grid(\n    cell_folds,\n    grid = mlp_param %&gt;% grid_regular(levels = 3),\n    metrics = roc_res\n  )\nmlp_reg_tune\n## # Tuning results\n## # 10-fold cross-validation \n## # A tibble: 10 × 4\n##   splits             id     .metrics          .notes          \n##   &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n## 1 &lt;split [1817/202]&gt; Fold01 &lt;tibble [81 × 8]&gt; &lt;tibble [0 × 3]&gt;\n## 2 &lt;split [1817/202]&gt; Fold02 &lt;tibble [81 × 8]&gt; &lt;tibble [0 × 3]&gt;\n## 3 &lt;split [1817/202]&gt; Fold03 &lt;tibble [81 × 8]&gt; &lt;tibble [0 × 3]&gt;\n## 4 &lt;split [1817/202]&gt; Fold04 &lt;tibble [81 × 8]&gt; &lt;tibble [0 × 3]&gt;\n## 5 &lt;split [1817/202]&gt; Fold05 &lt;tibble [81 × 8]&gt; &lt;tibble [0 × 3]&gt;\n## 6 &lt;split [1817/202]&gt; Fold06 &lt;tibble [81 × 8]&gt; &lt;tibble [0 × 3]&gt;\n## # ℹ 4 more rows\n\nExisten funciones de conveniencia de alto nivel que podemos utilizar para comprender los resultados. Primero, el método autoplot() para cuadrículas regulares muestra los perfiles de rendimiento en todos los parámetros de ajuste en Figura 13.3.\n\nautoplot(mlp_reg_tune) + \n  scale_color_viridis_d(direction = -1) + \n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nFigura 13.3: Los resultados regulares de la cuadrícula.\n\n\n\n\n\nSegún estos datos, el monto de la penalización tiene el mayor impacto en el área bajo la curva ROC. El número de épocas no parece tener un efecto pronunciado en el rendimiento. El cambio en el número de unidades ocultas parece ser más importante cuando la cantidad de regularización es baja (y perjudica el rendimiento). Hay varias configuraciones de parámetros que tienen un rendimiento aproximadamente equivalente, como se ve usando la función show_best():\n\nshow_best(mlp_reg_tune) %&gt;% select(-.estimator)\n## Warning in show_best(mlp_reg_tune): No value of `metric` was given; \"roc_auc\" will\n## be used.\n## # A tibble: 5 × 9\n##   hidden_units penalty epochs num_comp .metric  mean     n std_err .config          \n##          &lt;int&gt;   &lt;dbl&gt;  &lt;int&gt;    &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;            \n## 1            5       1     50        0 roc_auc 0.897    10 0.00857 Preprocessor1_Mo…\n## 2           10       1    125        0 roc_auc 0.895    10 0.00898 Preprocessor1_Mo…\n## 3           10       1     50        0 roc_auc 0.894    10 0.00960 Preprocessor1_Mo…\n## 4            5       1    200        0 roc_auc 0.894    10 0.00784 Preprocessor1_Mo…\n## 5            5       1    125        0 roc_auc 0.892    10 0.00822 Preprocessor1_Mo…\n\nCon base en estos resultados, tendría sentido realizar otra ejecución de búsqueda de cuadrícula con valores mayores de penalización por disminución de peso.\nPara utilizar un diseño que llene el espacio, al argumento grid se le puede dar un número entero o una de las funciones grid_*() puede producir un marco de datos. Para evaluar el mismo rango utilizando un diseño de máxima entropía con 20 valores candidatos:\n\nset.seed(1306)\nmlp_sfd_tune &lt;-\n  mlp_wflow %&gt;%\n  tune_grid(\n    cell_folds,\n    grid = 20,\n    # Pase el objeto de parámetro para usar el rango apropiado: \n    param_info = mlp_param,\n    metrics = roc_res\n  )\nmlp_sfd_tune\n## # Tuning results\n## # 10-fold cross-validation \n## # A tibble: 10 × 4\n##   splits             id     .metrics          .notes          \n##   &lt;list&gt;             &lt;chr&gt;  &lt;list&gt;            &lt;list&gt;          \n## 1 &lt;split [1817/202]&gt; Fold01 &lt;tibble [20 × 8]&gt; &lt;tibble [0 × 3]&gt;\n## 2 &lt;split [1817/202]&gt; Fold02 &lt;tibble [20 × 8]&gt; &lt;tibble [0 × 3]&gt;\n## 3 &lt;split [1817/202]&gt; Fold03 &lt;tibble [20 × 8]&gt; &lt;tibble [0 × 3]&gt;\n## 4 &lt;split [1817/202]&gt; Fold04 &lt;tibble [20 × 8]&gt; &lt;tibble [0 × 3]&gt;\n## 5 &lt;split [1817/202]&gt; Fold05 &lt;tibble [20 × 8]&gt; &lt;tibble [0 × 3]&gt;\n## 6 &lt;split [1817/202]&gt; Fold06 &lt;tibble [20 × 8]&gt; &lt;tibble [0 × 3]&gt;\n## # ℹ 4 more rows\n\nEl método autoplot() también funcionará con estos diseños, aunque el formato de los resultados será diferente. Figura 13.4 se produjo usando autoplot(mlp_sfd_tune).\n\n\n\n\n\n\n\n\nFigura 13.4: El método autoplot() resulta cuando se usa con un diseño que llena el espacio\n\n\n\n\n\nEste gráfico de efectos marginales (Figura 13.4) muestra la relación de cada parámetro con la métrica de rendimiento.\n\nTenga cuidado al examinar esta trama; dado que no se utiliza una cuadrícula normal, los valores de los demás parámetros de ajuste pueden afectar a cada panel.\n\nEl parámetro de penalización parece dar como resultado un mejor rendimiento con menores cantidades de pérdida de peso. Esto es lo opuesto a los resultados de la cuadrícula normal. Dado que cada punto de cada panel se comparte con los otros tres parámetros de ajuste, las tendencias de un panel pueden verse afectadas por los demás. Utilizando una cuadrícula regular, cada punto de cada panel se promedia por igual con respecto a los demás parámetros. Por esta razón, el efecto de cada parámetro se aísla mejor con cuadrículas regulares.\nAl igual que con la cuadrícula normal, show_best() puede informar sobre los mejores resultados numéricamente:\n\nshow_best(mlp_sfd_tune) %&gt;% select(-.estimator)\n## Warning in show_best(mlp_sfd_tune): No value of `metric` was given; \"roc_auc\" will\n## be used.\n## # A tibble: 5 × 9\n##   hidden_units       penalty epochs num_comp .metric  mean     n std_err .config    \n##          &lt;int&gt;         &lt;dbl&gt;  &lt;int&gt;    &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n## 1            8 0.594             97       22 roc_auc 0.880    10 0.00998 Preprocess…\n## 2            3 0.00000000649    135        8 roc_auc 0.878    10 0.00952 Preprocess…\n## 3            9 0.141            177       11 roc_auc 0.873    10 0.0104  Preprocess…\n## 4            8 0.0000000103      74        9 roc_auc 0.869    10 0.00761 Preprocess…\n## 5            6 0.00581          129       15 roc_auc 0.865    10 0.00658 Preprocess…\n\nGeneralmente, es una buena idea evaluar los modelos según múltiples métricas para tener en cuenta diferentes aspectos del ajuste del modelo. Además, a menudo tiene sentido elegir una combinación de parámetros ligeramente subóptima asociada con un modelo más simple. Para este modelo, la simplicidad corresponde a valores de penalización mayores y/o menos unidades ocultas.\nAl igual que con los resultados de fit_resamples(), normalmente no tiene ningún valor conservar los ajustes del modelo intermediario entre los remuestreos y los parámetros de ajuste. Sin embargo, como antes, la opción extraer de control_grid() permite conservar los modelos y/o recetas ajustados. Además, configurar la opción save_pred en TRUE conserva las predicciones del conjunto de evaluación y se puede acceder a ellas usando collect_predictions().",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Búsqueda De Cuadrícula</span>"
    ]
  },
  {
    "objectID": "13-grid-search.html#finalizando-el-modelo",
    "href": "13-grid-search.html#finalizando-el-modelo",
    "title": "13  Búsqueda De Cuadrícula",
    "section": "13.3 Finalizando El Modelo",
    "text": "13.3 Finalizando El Modelo\nSi uno de los conjuntos de posibles parámetros del modelo encontrados mediante show_best() fuera una opción final atractiva para estos datos, es posible que deseemos evaluar qué tan bien funciona en el conjunto de prueba. Sin embargo, los resultados de tune_grid() solo proporcionan el sustrato para elegir los parámetros de ajuste apropiados. La función no se ajusta a un modelo final.\nPara ajustar un modelo final, se debe determinar un conjunto final de valores de parámetros. Hay dos métodos para hacerlo:\n\nseleccionar manualmente los valores que parezcan apropiados o\nutilice una función select_*().\n\nPor ejemplo, select_best() elegirá los parámetros con los mejores resultados numéricamente. Volvamos a nuestros resultados habituales de la cuadrícula y veamos cuál es mejor:\n\nselect_best(mlp_reg_tune, metric = \"roc_auc\")\n## # A tibble: 1 × 5\n##   hidden_units penalty epochs num_comp .config              \n##          &lt;int&gt;   &lt;dbl&gt;  &lt;int&gt;    &lt;int&gt; &lt;chr&gt;                \n## 1            5       1     50        0 Preprocessor1_Model08\n\nMirando hacia atrás en Figura 13.3, podemos ver que un modelo con una sola unidad oculta entrenada durante 125 épocas en los predictores originales con una gran cantidad de penalización tiene un rendimiento competitivo con esta opción y es más simple. ¡Esto es básicamente una regresión logística penalizada! Para especificar manualmente estos parámetros, podemos crear un tibble con estos valores y luego usar una función de finalización para unir los valores nuevamente en el flujo de trabajo:\n\nlogistic_param &lt;- \n  tibble(\n    num_comp = 0,\n    epochs = 125,\n    hidden_units = 1,\n    penalty = 1\n  )\n\nfinal_mlp_wflow &lt;- \n  mlp_wflow %&gt;% \n  finalize_workflow(logistic_param)\nfinal_mlp_wflow\n## ══ Workflow ═════════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: mlp()\n## \n## ── Preprocessor ─────────────────────────────────────────────────────────────────────\n## 4 Recipe Steps\n## \n## • step_YeoJohnson()\n## • step_normalize()\n## • step_pca()\n## • step_normalize()\n## \n## ── Model ────────────────────────────────────────────────────────────────────────────\n## Single Layer Neural Network Model Specification (classification)\n## \n## Main Arguments:\n##   hidden_units = 1\n##   penalty = 1\n##   epochs = 125\n## \n## Engine-Specific Arguments:\n##   trace = 0\n## \n## Computational engine: nnet\n\nNo se incluyen más valores de tune() en este flujo de trabajo finalizado. Ahora el modelo se puede ajustar a todo el conjunto de entrenamiento:\n\nfinal_mlp_fit &lt;- \n  final_mlp_wflow %&gt;% \n  fit(cells)\n\nEste objeto ahora se puede utilizar para hacer predicciones futuras sobre nuevos datos.\nSi no usó un flujo de trabajo, la finalización de un modelo y/o receta se realiza usando finalize_model() y finalize_recipe().",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Búsqueda De Cuadrícula</span>"
    ]
  },
  {
    "objectID": "13-grid-search.html#sec-tuning-usemodels",
    "href": "13-grid-search.html#sec-tuning-usemodels",
    "title": "13  Búsqueda De Cuadrícula",
    "section": "13.4 Herramientas Para Crear Especificaciones De Ajuste",
    "text": "13.4 Herramientas Para Crear Especificaciones De Ajuste\nEl paquete usemodels puede tomar un marco de datos y una fórmula de modelo, luego escribir código R para ajustar el modelo. El código también crea una receta adecuada cuyos pasos dependen del modelo solicitado, así como de los datos del predictor.\nPor ejemplo, para los datos de vivienda de Ames, el código de modelado xgboost podría crearse con:\n\nlibrary(usemodels)\n\nuse_xgboost(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n              Latitude + Longitude, \n            data = ames_train,\n            # Agregue comentarios que expliquen parte del código:\n            verbose = TRUE)\n\nEl código resultante es el siguiente:\n\nxgboost_recipe &lt;- \n  recipe(formula = Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n    Latitude + Longitude, data = ames_train) %&gt;% \n  step_novel(all_nominal_predictors()) %&gt;% \n  ## Este modelo requiere que los predictores sean numéricos. \n  ## El método más común para convertir predictores cualitativos \n  ## en numéricos es crear variables indicadoras binarias \n  ## (también conocidas como variables ficticias) a partir de estos \n  ## predictores. Sin embargo, para este modelo, se pueden crear \n  ## variables indicadoras binarias para cada uno de los niveles de \n  ## los factores (lo que se conoce como \"codificación one-hot\").\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) %&gt;% \n  step_zv(all_predictors()) \n\nxgboost_spec &lt;- \n  boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(), \n    loss_reduction = tune(), sample_size = tune()) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"xgboost\") \n\nxgboost_workflow &lt;- \n  workflow() %&gt;% \n  add_recipe(xgboost_recipe) %&gt;% \n  add_model(xgboost_spec) \n\nset.seed(69305)\nxgboost_tune &lt;-\n  tune_grid(xgboost_workflow, \n            resamples = stop(\"agregue su objeto rsample\"), \n            grid = stop(\"agregar número de puntos candidatos\"))\n\nSegún lo que entiende usemodels sobre los datos, este código es el preprocesamiento mínimo requerido. Para otros modelos, se agregan operaciones como step_normalize() para satisfacer las necesidades básicas del modelo. Tenga en cuenta que es nuestra responsabilidad, como practicante del modelado, elegir qué remuestras, resamples, usar para la afinación, así como qué tipo de cuadrícula, grid.\n\nEl paquete usemodels también se puede utilizar para crear código de ajuste de modelo sin ajuste estableciendo el argumento tune = FALSE.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Búsqueda De Cuadrícula</span>"
    ]
  },
  {
    "objectID": "13-grid-search.html#sec-efficient-grids",
    "href": "13-grid-search.html#sec-efficient-grids",
    "title": "13  Búsqueda De Cuadrícula",
    "section": "13.5 Herramientas Para Una Búsqueda Eficiente En La Cuadrícula",
    "text": "13.5 Herramientas Para Una Búsqueda Eficiente En La Cuadrícula\nEs posible hacer que la búsqueda en cuadrícula sea más eficiente desde el punto de vista computacional aplicando algunos trucos y optimizaciones diferentes. Esta sección describe varias técnicas.\n\n13.5.1 Optimización del submodelo\nHay tipos de modelos en los que, a partir de un único ajuste de modelo, se pueden evaluar múltiples parámetros de ajuste sin necesidad de reajustarlos.\nPor ejemplo, los mínimos cuadrados parciales (PLS) son una versión supervisada del análisis de componentes principales (Geladi y Kowalski 1986). Crea componentes que maximizan la variación en los predictores (como PCA) pero simultáneamente intenta maximizar la correlación entre estos predictores y el resultado. Exploraremos más PLS en el Capítulo 16. Un parámetro de ajuste es el número de componentes PLS que se conservarán. Supongamos que un conjunto de datos con 100 predictores se ajusta mediante PLS. El número de posibles componentes a conservar puede oscilar entre uno y cincuenta. Sin embargo, en muchas implementaciones, un único ajuste de modelo puede calcular valores predichos en muchos valores de num_comp. Como resultado, un modelo PLS creado con 100 componentes también puede hacer predicciones para cualquier num_comp &lt;= 100. Esto ahorra tiempo ya que, en lugar de crear ajustes de modelo redundantes, se puede utilizar un ajuste único para evaluar muchos submodelos.\nSi bien no todos los modelos pueden aprovechar esta característica, muchos de los más utilizados sí lo hacen.\n\nLos modelos de impulso normalmente pueden hacer predicciones en múltiples valores para el número de iteraciones de impulso.\nLos métodos de regularización, como el modelo glmnet, pueden realizar predicciones simultáneas sobre la cantidad de regularización utilizada para ajustar el modelo.\nLos splines de regresión adaptativa multivariada (MARS) añaden un conjunto de características no lineales a los modelos de regresión lineal (Friedman 1991). El número de términos a retener es un parámetro de ajuste y es computacionalmente rápido hacer predicciones sobre muchos valores de este parámetro a partir de un único ajuste de modelo.\n\nEl paquete tune aplica automáticamente este tipo de optimización cada vez que se ajusta un modelo aplicable.\nPor ejemplo, si se ajustó un modelo de clasificación C5.0 mejorado (M. Kuhn y Johnson 2013) a los datos de la celda, podemos ajustar el número de iteraciones de refuerzo (“árboles”). Con todos los demás parámetros establecidos en sus valores predeterminados, podemos evaluar iteraciones de 1 a 100 en las mismas muestras utilizadas anteriormente:\n\nc5_spec &lt;- \n  boost_tree(trees = tune()) %&gt;% \n  set_engine(\"C5.0\") %&gt;% \n  set_mode(\"classification\")\n\nset.seed(1307)\nc5_spec %&gt;%\n  tune_grid(\n    class ~ .,\n    resamples = cell_folds,\n    grid = data.frame(trees = 1:100),\n    metrics = roc_res\n  )\n\nSin la optimización del submodelo, la llamada a tune_grid() usó 62.2 minutos para volver a muestrear 100 submodelos. Con la optimización, la misma llamada tomó 100 segundos (una aceleración de 37). El tiempo reducido es la diferencia entre tune_grid() que ajusta 1000 modelos frente a 10 modelos.\n\nAunque ajustamos el modelo con y sin el truco de predicción del submodelo, esta optimización se aplica automáticamente mediante parsnip.\n\n\n\n13.5.2 Procesamiento en paralelo\nComo se mencionó anteriormente en Sección 10.4, el procesamiento paralelo es un método eficaz para disminuir el tiempo de ejecución al volver a muestrear modelos. Esta ventaja se transmite al ajuste del modelo mediante búsqueda en cuadrícula, aunque existen consideraciones adicionales.\nConsideremos dos esquemas de procesamiento paralelo diferentes.\nAl ajustar modelos mediante búsqueda de cuadrícula, hay dos bucles distintos: uno sobre remuestreos y otro sobre combinaciones únicas de parámetros de ajuste. En pseudocódigo, este proceso se vería así:\n\nfor (rs in resamples) {\n  # Crear conjuntos de análisis y evaluación.\n  # Preprocesar datos (por ejemplo, fórmula o receta)\n  for (mod in configurations) {\n    # Ajustar el modelo {mod} al conjunto de análisis {rs}\n    # Predecir el conjunto de evaluación {rs}\n  }\n}\n\nDe forma predeterminada, el paquete tune paraleliza solo sobre remuestreos (el bucle externo), a diferencia de los bucles externo e interno.\nEste es el escenario óptimo cuando el método de preprocesamiento es costoso. Sin embargo, este enfoque tiene dos posibles desventajas:\n\nLimita las aceleraciones alcanzables cuando el preprocesamiento no es caro.\nEl número de trabajadores paralelos está limitado por el número de remuestras. Por ejemplo, con una validación cruzada de 10 veces, puede utilizar solo 10 trabajadores paralelos incluso cuando la computadora tiene más de 10 núcleos.\n\nPara ilustrar cómo funciona el procesamiento paralelo, usaremos un caso en el que hay 7 valores de parámetros de ajuste del modelo, con validación cruzada quíntuple. Figura 13.5 muestra cómo se asignan las tareas a los procesos de trabajo.\n\n\n\n\n\n\n\n\nFigura 13.5: Procesos de trabajo cuando el procesamiento paralelo hace coincidir los remuestreos con un proceso de trabajo específico\n\n\n\n\n\nTenga en cuenta que cada pliegue se asigna a su propio proceso de trabajo y, dado que solo se están ajustando los parámetros del modelo, el preprocesamiento se realiza una vez por pliegue/trabajador. Si se utilizaran menos de cinco procesos de trabajo, algunos trabajadores recibirían múltiples pliegues.\nEn las funciones de control para las funciones tune_*(), el argumento parallel_over controla cómo se ejecuta el proceso. Para utilizar la estrategia de paralelización anterior, el argumento es parallel_over = \"resamples\".\nEn lugar de procesar los remuestreos en paralelo, un esquema alternativo combina los bucles sobre los remuestreos y los modelos en un solo bucle. En pseudocódigo, este proceso se vería así:\n\nall_tasks &lt;- crossing(resamples, configurations)\n\nfor (iter in all_tasks) {                           \n  # Crear conjuntos de análisis y evaluación para {iter}\n  # Preprocesar datos (e.j. formula or recipe)\n  # Ajustar el modelo {iter} al conjunto de análisis {iter}\n  # Predecir el conjunto de evaluación {iter}\n}\n\nEn este caso, la paralelización se produce ahora en un bucle único. Por ejemplo, si utilizamos validación cruzada quíntuple con valores de parámetros de ajuste \\(M\\), el bucle se ejecuta en \\(5\\times M\\) iteraciones. Esto aumenta el número de trabajadores potenciales que se pueden utilizar. Sin embargo, el trabajo relacionado con el preprocesamiento de datos se repite varias veces. Si esas medidas son costosas, este enfoque será ineficiente.\nEn tidymodels, los conjuntos de validación se tratan como un único remuestreo. En estos casos, este esquema de paralelización sería el mejor.\nFigura 13.6 ilustra la delegación de tareas a los trabajadores en este plan; se utiliza el mismo ejemplo pero con 10 trabajadores.\n\n\n\n\n\n\n\n\nFigura 13.6: Procesos de trabajo cuando las tareas de preprocesamiento y modelado se distribuyen a muchos trabajadores\n\n\n\n\n\nAquí, cada proceso de trabajo maneja múltiples pliegues y el preprocesamiento se repite innecesariamente. Por ejemplo, para el primer pliegue, el preprocesamiento se calculó seven veces en lugar de una vez.\nPara este esquema, el argumento de la función de control es parallel_over = \"everything\".\n\n\n13.5.3 Evaluación comparativa de árboles potenciados\nPara comparar diferentes esquemas de paralelización posibles, ajustamos un árbol potenciado con el motor xgboost utilizando un conjunto de datos de 4000 muestras, con validación cruzada quíntuple y 10 modelos candidatos. Estos datos requirieron algún procesamiento previo de referencia que no requirió ninguna estimación. El preprocesamiento se manejó de tres maneras diferentes:\n\nPreprocese los datos antes de modelar utilizando una canalización dplyr (etiquetada como “ninguna” en los gráficos posteriores).\nRealice el mismo preprocesamiento mediante una receta (que se muestra como preprocesamiento “ligero”).\nCon una receta, agregar un paso adicional que tenga un alto costo computacional (etiquetado como “caro”).\n\nLa primera y segunda opciones de preprocesamiento están diseñadas para comparar, para medir el costo computacional de la receta en la segunda opción. La tercera opción mide el costo de realizar cálculos redundantes con parallel_over = \"everything\".\nEvaluamos este proceso usando números variables de procesos de trabajo y usando las dos opciones parallel_over, en una computadora con 10 núcleos físicos y 20 núcleos virtuales (mediante hyper-threading).\nPrimero, consideremos los tiempos de ejecución sin procesar en Figura 13.7.\n\n\n\n\n\n\n\n\nFigura 13.7: Tiempos de ejecución para el ajuste del modelo versus la cantidad de trabajadores que utilizan diferentes esquemas de delegación\n\n\n\n\n\nDado que solo hubo cinco remuestreos, el número de núcleos utilizados cuando parallel_over = \"resamples\" está limitado a cinco.\nComparando las curvas en los dos primeros paneles para “none” y “light”:\n\nHay poca diferencia en los tiempos de ejecución entre los paneles. Esto indica que, para estos datos, no existe una penalización computacional real por realizar los pasos de preprocesamiento en una receta.\nHay algunos beneficios al usar parallel_over = \"everything\" con muchos núcleos. Sin embargo, como se muestra en la figura, la mayor parte del beneficio del procesamiento paralelo ocurre en los primeros cinco trabajadores.\n\nCon el costoso paso de preprocesamiento, existe una diferencia considerable en los tiempos de ejecución. Usar parallel_over = \"everything\" es problemático ya que, incluso usando todos los núcleos, nunca alcanza el tiempo de ejecución que parallel_over = \"resamples\" logra con solo cinco núcleos. Esto se debe a que el costoso paso de preprocesamiento se repite innecesariamente en el esquema computacional.\nTambién podemos ver estos datos en términos de aceleraciones en Figura 13.8.\n\n\n\n\n\n\n\n\nFigura 13.8: Aceleraciones en el ajuste del modelo frente al número de trabajadores que utilizan diferentes esquemas de delegación. La línea negra diagonal indica una aceleración lineal donde la adición de un nuevo proceso de trabajo tiene el máximo efecto.\n\n\n\n\n\nLas mejores aceleraciones, para estos datos, ocurren cuando parallel_over = \"resamples\" y cuando los cálculos son costosos. Sin embargo, en este último caso, recuerde que el análisis anterior indica que los ajustes generales del modelo son más lentos.\n¿Cuál es el beneficio de utilizar el método de optimización de submodelos junto con el procesamiento paralelo? El modelo de clasificación C5.0 que se muestra en Sección 13.5.1 también se ejecutó en paralelo con diez trabajadores. Los cálculos paralelos tomaron 13,3 segundos para acelerar 7.5 (ambas ejecuciones utilizaron el truco de optimización del submodelo). Entre el truco de optimización del submodelo y el procesamiento paralelo, hubo una aceleración total de 282 sobre el código de búsqueda de cuadrícula más básico.\n\nEn general, tenga en cuenta que el aumento de los ahorros computacionales variará de un modelo a otro y también se verá afectado por el tamaño de la cuadrícula, la cantidad de remuestreos, etc. Es posible que un modelo muy eficiente desde el punto de vista computacional no se beneficie tanto del procesamiento paralelo.\n\n\n\n13.5.4 Acceso a variables globales\nCuando se utilizan tidymodels, es posible utilizar valores en su entorno local (normalmente el entorno global) en los objetos del modelo.\n\n¿Qué entendemos aquí por “medio ambiente”? Piense en un entorno en R como un lugar para almacenar variables con las que puede trabajar. Consulte el capítulo “Entornos” de Wickham (2019) para obtener más información.\n\nSi definimos una variable para usar como parámetro del modelo y luego la pasamos a una función como linear_reg(), la variable normalmente se define en el entorno global.\n\ncoef_penalty &lt;- 0.1\nspec &lt;- linear_reg(penalty = coef_penalty) %&gt;% set_engine(\"glmnet\")\nspec\n## Linear Regression Model Specification (regression)\n## \n## Main Arguments:\n##   penalty = coef_penalty\n## \n## Computational engine: glmnet\n\nLos modelos creados con el paquete parsnip guardan argumentos como estos como quosures; se trata de objetos que rastrean tanto el nombre del objeto como el entorno donde vive:\n\nspec$args$penalty\n## &lt;quosure&gt;\n## expr: ^coef_penalty\n## env:  global\n\nObserve que tenemos env: global porque esta variable se creó en el entorno global. La especificación del modelo definida por spec funciona correctamente cuando se ejecuta en la sesión normal de un usuario porque esa sesión también utiliza el entorno global; R puede encontrar fácilmente el objeto coef_penalty.\n\nCuando un modelo de este tipo se evalúa con trabajadores paralelos, puede fallar. Dependiendo de la tecnología particular que se utilice para el procesamiento paralelo, es posible que los trabajadores no tengan acceso al entorno global.\n\nAl escribir código que se ejecutará en paralelo, es una buena idea insertar los datos reales en los objetos en lugar de la referencia al objeto. Los paquetes rlang y dplyr pueden resultar muy útiles para esto. Por ejemplo, el operador !! puede unir un único valor en un objeto:\n\nspec &lt;- linear_reg(penalty = !!coef_penalty) %&gt;% set_engine(\"glmnet\")\nspec$args$penalty\n## &lt;quosure&gt;\n## expr: ^0.1\n## env:  empty\n\nAhora la salida es ^0.1, lo que indica que el valor está ahí en lugar de la referencia al objeto. Cuando tiene varios valores externos para insertar en un objeto, el operador !!! puede ayudar:\n\nmcmc_args &lt;- list(chains = 3, iter = 1000, cores = 3)\n\nlinear_reg() %&gt;% set_engine(\"stan\", !!!mcmc_args)\n## Linear Regression Model Specification (regression)\n## \n## Engine-Specific Arguments:\n##   chains = 3\n##   iter = 1000\n##   cores = 3\n## \n## Computational engine: stan\n\nLos selectores de recetas son otro lugar donde es posible que desees acceder a variables globales. Suponga que tiene un paso de receta que debería utilizar todos los predictores en los datos de la celda que se midieron utilizando el segundo canal óptico. Podemos crear un vector de estos nombres de columnas:\n\nlibrary(stringr)\nch_2_vars &lt;- str_subset(names(cells), \"ch_2\")\nch_2_vars\n## [1] \"avg_inten_ch_2\"   \"total_inten_ch_2\"\n\nPodríamos codificarlos en un paso de receta, pero sería mejor hacer referencia a ellos mediante programación en caso de que los datos cambien. Dos formas de hacer esto son:\n\n# Todavía utiliza una referencia a datos globales. (~_~;)\nrecipe(class ~ ., data = cells) %&gt;% \n  step_spatialsign(all_of(ch_2_vars))\n## \n## ── Recipe ───────────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:    1\n## predictor: 56\n## \n## ── Operations\n## • Spatial sign on: all_of(ch_2_vars)\n\n# Inserta los valores en el paso. ヽ(•‿•)ノ\nrecipe(class ~ ., data = cells) %&gt;% \n  step_spatialsign(!!!ch_2_vars)\n## \n## ── Recipe ───────────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:    1\n## predictor: 56\n## \n## ── Operations\n## • Spatial sign on: \"avg_inten_ch_2\" and \"total_inten_ch_2\"\n\nEste último es mejor para el procesamiento paralelo porque toda la información necesaria está integrada en el objeto de receta.\n\n\n13.5.5 Métodos de carrera\nUn problema con la búsqueda de cuadrícula es que todos los modelos deben ajustarse en todas las muestras antes de poder evaluar cualquier parámetro de ajuste. Sería útil si, en cambio, en algún momento durante el ajuste, se pudiera realizar un análisis intermedio para eliminar cualquier parámetro candidato realmente terrible. Esto sería similar al análisis de inutilidad en ensayos clínicos. Si un nuevo fármaco tiene un rendimiento excesivamente malo (o bueno), es potencialmente poco ético esperar hasta que finalice el ensayo para tomar una decisión.\nEn el aprendizaje automático, el conjunto de técnicas llamadas métodos de carrera proporcionan una función similar (Maron y Moore 1994). Aquí, el proceso de ajuste evalúa todos los modelos en un subconjunto inicial de remuestreos. Según sus métricas de rendimiento actuales, algunos conjuntos de parámetros no se consideran en remuestreos posteriores.\nComo ejemplo, en el proceso de ajuste del perceptrón multicapa con una cuadrícula regular explorado en este capítulo, ¿cómo se verían los resultados después de solo los primeros tres pliegues? Usando técnicas similares a las que se muestran en el Capítulo 11, podemos ajustar un modelo donde el resultado es el área remuestreada bajo la curva ROC y el predictor es un indicador para la combinación de parámetros. El modelo tiene en cuenta el efecto de remuestreo a remuestreo y produce estimaciones puntuales y de intervalo para cada configuración de parámetro. Los resultados del modelo son intervalos de confianza unilaterales del 95% que miden la pérdida del valor ROC en relación con los parámetros de mejor rendimiento actualmente, como se muestra en Figura 13.9.\n\n\n\n\n\n\n\n\nFigura 13.9: El proceso de carrera para 20 parámetros de afinación y 10 resampleos.\n\n\n\n\n\nCualquier conjunto de parámetros cuyo intervalo de confianza incluya cero carecería de evidencia de que su desempeño sea estadísticamente diferente de los mejores resultados. Mantenemos la configuración de 9; estos se vuelven a muestrear más. Los submodelos restantes 11 ya no se consideran.\n\n\n\nEl proceso continúa para cada nueva muestra; después del siguiente conjunto de métricas de rendimiento, se ajusta un nuevo modelo a estas estadísticas y potencialmente se descartan más submodelos.2\n\nLos métodos de carrera pueden ser más eficientes que la búsqueda básica en la cuadrícula siempre que el análisis intermedio sea rápido y algunas configuraciones de parámetros tengan un rendimiento deficiente. También es más útil cuando el modelo no tiene la capacidad de explotar las predicciones del submodelo.\n\nEl paquete finetune contiene funciones para carreras. La función tune_race_anova() realiza un modelo ANOVA para probar la significancia estadística de las diferentes configuraciones del modelo. La sintaxis para reproducir el filtrado mostrado anteriormente es:\n\nlibrary(finetune)\n\nset.seed(1308)\nmlp_sfd_race &lt;-\n  mlp_wflow %&gt;%\n  tune_race_anova(\n    cell_folds,\n    grid = 20,\n    param_info = mlp_param,\n    metrics = roc_res,\n    control = control_race(verbose_elim = TRUE)\n  )\n\nLos argumentos reflejan los de tune_grid(). La función control_race() tiene opciones para el procedimiento de eliminación.\nComo se muestra en la animación anterior, se estaban considerando combinaciones de parámetros de ajuste four una vez que se evaluó el conjunto completo de remuestreos. show_best() devuelve los mejores modelos (clasificados por rendimiento) pero solo devuelve las configuraciones que nunca fueron eliminadas:\n\nshow_best(mlp_sfd_race, n = 10)\n## Warning in show_best(mlp_sfd_race, n = 10): No value of `metric` was given;\n## \"roc_auc\" will be used.\n## # A tibble: 4 × 10\n##   hidden_units  penalty epochs num_comp .metric .estimator  mean     n std_err\n##          &lt;int&gt;    &lt;dbl&gt;  &lt;int&gt;    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n## 1            8 8.14e- 1    177       15 roc_auc binary     0.885    10 0.00988\n## 2            3 4.02e- 2    151       10 roc_auc binary     0.881    10 0.00995\n## 3            2 7.91e- 4    164        7 roc_auc binary     0.880    10 0.00813\n## 4            5 1.30e-10     89        5 roc_auc binary     0.879    10 0.00967\n## # ℹ 1 more variable: .config &lt;chr&gt;\n\nExisten otras técnicas de análisis intermedio para descartar configuraciones. Por ejemplo, Krueger, Panknin, y Braun (2015) usa métodos de análisis secuencial tradicionales, mientras que Max Kuhn (2014) trata los datos como una competencia deportiva y usa el modelo Bradley-Terry (Bradley y Terry 1952) para medir la capacidad ganadora de la configuración de parámetros.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Búsqueda De Cuadrícula</span>"
    ]
  },
  {
    "objectID": "13-grid-search.html#sec-grid-summary",
    "href": "13-grid-search.html#sec-grid-summary",
    "title": "13  Búsqueda De Cuadrícula",
    "section": "13.6 Resumen Del Capítulo",
    "text": "13.6 Resumen Del Capítulo\nEste capítulo analizó las dos clases principales de búsqueda de cuadrículas (regular y no regular) que se pueden usar para ajustar el modelo y demostró cómo construir estas cuadrículas, ya sea manualmente o usando la familia de funciones grid_*(). La función tune_grid() puede evaluar estos conjuntos candidatos de parámetros del modelo mediante remuestreo. El capítulo también mostró cómo finalizar un modelo, receta o flujo de trabajo para actualizar los valores de los parámetros para el ajuste final. La búsqueda en cuadrícula puede ser costosa desde el punto de vista computacional, pero las decisiones bien pensadas en el diseño experimental de dichas búsquedas pueden hacerlas manejables.\nEl código de análisis de datos que se reutilizará en el próximo capítulo es:\n\nlibrary(tidymodels)\n\ndata(cells)\ncells &lt;- cells %&gt;% select(-case)\n\nset.seed(1304)\ncell_folds &lt;- vfold_cv(cells)\n\nroc_res &lt;- metric_set(roc_auc)\n\n\n\n\n\nBox, GEP, W Hunter, y J Hunter. 2005. Statistics for Experimenters: An Introduction to Design, Data Analysis, and Model Building. Wiley.\n\n\nBradley, R, y M Terry. 1952. «Rank analysis of incomplete block designs: I. The method of paired comparisons». Biometrika 39 (3/4): 324-45.\n\n\nFriedman, J. 1991. «Multivariate Adaptive Regression Splines». The Annals of Statistics 19 (1): 1-141.\n\n\nGeladi, P., y B Kowalski. 1986. «Partial Least-Squares Regression: A Tutorial». Analytica Chimica Acta 185: 1-17.\n\n\nHill, A, P LaPan, Y Li, y S Haney. 2007. «Impact of Image Segmentation on High-Content Screening Data Quality for SK-BR-3 Cells». BMC Bioinformatics 8 (1): 340.\n\n\nJoseph, V, E Gul, y S Ba. 2015. «Maximum projection designs for computer experiments». Biometrika 102 (2): 371-80.\n\n\nKrueger, T, D Panknin, y M Braun. 2015. «Fast Cross-Validation via Sequential Testing». Journal of Machine Learning Research 16 (33): 1103-55.\n\n\nKuhn, Max. 2014. «Futility Analysis in the Cross-Validation of Machine Learning Models». https://arxiv.org/abs/1405.6974.\n\n\nKuhn, M, y K Johnson. 2013. Applied Predictive Modeling. Springer.\n\n\nMaron, O, y A Moore. 1994. «Hoeffding races: Accelerating model selection search for classification and function approximation». En Advances in neural information processing systems, 59-66.\n\n\nMcKay, M, R Beckman, y W Conover. 1979. «A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code». Technometrics 21 (2): 239-45.\n\n\nSantner, T, B Williams, W Notz, y B Williams. 2003. The design and analysis of computer experiments. Springer.\n\n\nShewry, M, y H Wynn. 1987. «Maximum entropy sampling». Journal of Applied Statistics 14 (2): 165-70.\n\n\nWickham, H. 2019. Advanced R. 2nd ed. Chapman & Hall/CRC The R Series. Taylor & Francis. https://doi.org/10.1201/9781351201315.\n\n\nYeo, I-K, y R Johnson. 2000. «A new family of power transformations to improve normality or symmetry». Biometrika 87 (4): 954-59.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Búsqueda De Cuadrícula</span>"
    ]
  },
  {
    "objectID": "13-grid-search.html#footnotes",
    "href": "13-grid-search.html#footnotes",
    "title": "13  Búsqueda De Cuadrícula",
    "section": "",
    "text": "https://CRAN.R-project.org/view=ExperimentalDesign↩︎\nConsulte Max Kuhn (2014) para obtener más detalles sobre los aspectos computacionales de este enfoque.↩︎",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Búsqueda De Cuadrícula</span>"
    ]
  },
  {
    "objectID": "14-iterative-search.html",
    "href": "14-iterative-search.html",
    "title": "14  Búsqueda Iterativa",
    "section": "",
    "text": "14.1 Un Modelo De Máquina De Vectores De Soporte\nUna vez más utilizamos los datos de segmentación de celdas, descritos en Sección 13.2, para modelar, con un modelo de máquina de vectores de soporte (SVM) para demostrar métodos de ajuste secuencial. Consulte Kuhn y Johnson (2013) para obtener más información sobre este modelo. Los dos parámetros de ajuste a optimizar son el valor del costo de SVM y el parámetro del núcleo de la función de base radial \\(\\sigma\\). Ambos parámetros pueden tener un efecto profundo en la complejidad y el rendimiento del modelo.\nEl modelo SVM utiliza un producto escalar y, por este motivo, es necesario centrar y escalar los predictores. Al igual que el modelo de perceptrón multicapa, este modelo se beneficiaría del uso de la extracción de características PCA. Sin embargo, no utilizaremos este tercer parámetro de ajuste en este capítulo para que podamos visualizar el proceso de búsqueda en dos dimensiones.\nJunto con los objetos utilizados anteriormente (que se muestran en Sección 13.6), los objetos tidymodels svm_rec, svm_spec y svm_wflow definen el proceso del modelo:\nlibrary(tidymodels)\ntidymodels_prefer()\n\nsvm_rec &lt;- \n  recipe(class ~ ., data = cells) %&gt;%\n  step_YeoJohnson(all_numeric_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\nsvm_spec &lt;- \n  svm_rbf(cost = tune(), rbf_sigma = tune()) %&gt;% \n  set_engine(\"kernlab\") %&gt;% \n  set_mode(\"classification\")\n\nsvm_wflow &lt;- \n  workflow() %&gt;% \n  add_model(svm_spec) %&gt;% \n  add_recipe(svm_rec)\nLos rangos de parámetros predeterminados para los dos parámetros de ajuste cost y rbf_sigma son:\ncost()\n## Cost (quantitative)\n## Transformer: log-2 [1e-100, Inf]\n## Range (transformed scale): [-10, 5]\nrbf_sigma()\n## Radial Basis Function sigma (quantitative)\n## Transformer: log-10 [1e-100, Inf]\n## Range (transformed scale): [-10, 0]\nA modo de ilustración, cambiemos ligeramente el rango de parámetros del kernel para mejorar las visualizaciones de la búsqueda:\nsvm_param &lt;- \n  svm_wflow %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  Matrix::update(rbf_sigma = rbf_sigma(c(-7, -1)))\nAntes de analizar detalles específicos sobre la búsqueda iterativa y cómo funciona, exploremos la relación entre los dos parámetros de ajuste de SVM y el área bajo la curva ROC para este conjunto de datos específico. Construimos una cuadrícula regular muy grande, compuesta por 2500 valores candidatos, y evaluamos la cuadrícula mediante remuestreo. Obviamente, esto es poco práctico en el análisis de datos regular y tremendamente ineficiente. Sin embargo, aclara el camino que debe tomar el proceso de búsqueda y dónde ocurren los valores numéricamente óptimos.\nFigura 14.1 muestra los resultados de la evaluación de esta cuadrícula, donde el color más claro corresponde a un mayor (mejor) rendimiento del modelo. Hay una gran franja en la diagonal inferior del espacio de parámetros que es relativamente plana con un rendimiento deficiente. En la parte superior derecha del espacio se produce una cresta de mejor rendimiento. El punto negro indica la mejor configuración. La transición desde la meseta de los malos resultados a la cima del mejor desempeño es muy pronunciada. También hay una fuerte caída en el área bajo la curva ROC justo a la derecha de la cresta.\nFigura 14.1: Mapa de calor del área media bajo la curva ROC para una cuadrícula de alta densidad de valores de parámetros de ajuste. El mejor punto es un punto sólido en la esquina superior derecha.\nLos siguientes procedimientos de búsqueda requieren al menos algunas estadísticas de rendimiento remuestreadas antes de continuar. Para ello, el siguiente código crea una pequeña cuadrícula regular que reside en la parte plana del espacio de parámetros. La función tune_grid() vuelve a muestrear esta cuadrícula:\nset.seed(1401)\nstart_grid &lt;- \n  svm_param %&gt;% \n  Matrix::update(\n    cost = cost(c(-6, 1)),\n    rbf_sigma = rbf_sigma(c(-6, -4))\n  ) %&gt;% \n  grid_regular(levels = 2)\n\nset.seed(1402)\nsvm_initial &lt;- \n  svm_wflow %&gt;% \n  tune_grid(resamples = cell_folds, grid = start_grid, metrics = roc_res)\n\ncollect_metrics(svm_initial)\n## # A tibble: 4 × 8\n##     cost rbf_sigma .metric .estimator  mean     n std_err .config             \n##    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n## 1 0.0156  0.000001 roc_auc binary     0.864    10 0.00864 Preprocessor1_Model1\n## 2 2       0.000001 roc_auc binary     0.863    10 0.00867 Preprocessor1_Model2\n## 3 0.0156  0.0001   roc_auc binary     0.863    10 0.00862 Preprocessor1_Model3\n## 4 2       0.0001   roc_auc binary     0.866    10 0.00855 Preprocessor1_Model4\nEsta cuadrícula inicial muestra resultados bastante equivalentes, sin que ningún punto individual sea mucho mejor que los demás. Estos resultados pueden ser absorbidos por las funciones de ajuste iterativas analizadas en las siguientes secciones para usarse como valores iniciales.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Búsqueda Iterativa</span>"
    ]
  },
  {
    "objectID": "14-iterative-search.html#optimización-bayesiana",
    "href": "14-iterative-search.html#optimización-bayesiana",
    "title": "14  Búsqueda Iterativa",
    "section": "14.2 Optimización Bayesiana",
    "text": "14.2 Optimización Bayesiana\nLas técnicas de optimización bayesiana analizan los resultados del remuestreo actual y crean un modelo predictivo para sugerir valores de parámetros de ajuste que aún no se han evaluado. A continuación se vuelve a muestrear la combinación de parámetros sugerida. Estos resultados luego se utilizan en otro modelo predictivo que recomienda más valores candidatos para realizar pruebas, y así sucesivamente. El proceso continúa durante un número determinado de iteraciones o hasta que no se produzcan más mejoras. Shahriari et al. (2016) y Frazier (2018) son buenas introducciones a la optimización bayesiana.\nCuando se utiliza la optimización bayesiana, las principales preocupaciones son cómo crear el modelo y cómo seleccionar los parámetros recomendados por ese modelo. Primero, consideremos la técnica más comúnmente utilizada para la optimización bayesiana, el modelo de proceso gaussiano.\n\n14.2.1 Un modelo de proceso gaussiano\nLos modelos de proceso gaussiano (GP) (Schulz, Speekenbrink, y Krause 2018) son técnicas estadísticas bien conocidas que tienen una historia en la estadística espacial (bajo el nombre de métodos de kriging). Se pueden derivar de múltiples formas, incluso como modelo bayesiano; consulte Rasmussen y Williams (2006) para obtener una excelente referencia.\nMatemáticamente, un GP es una colección de variables aleatorias cuya distribución de probabilidad conjunta es gaussiana multivariada. En el contexto de nuestra aplicación, esta es la colección de métricas de rendimiento para los valores candidatos de los parámetros de ajuste. Para la cuadrícula inicial anterior de cuatro muestras, la realización de estas cuatro variables aleatorias fue 0.8639, 0.8625, 0.8627, and 0.8659. Se supone que están distribuidos como gaussianos multivariados. Las entradas que definen las variables/predictores independientes para el modelo GP son los valores de los parámetros de ajuste correspondientes (que se muestran en Tabla 14.1).\n\n\n\nTabla 14.1: Estadísticas de remuestreo utilizadas como sustrato inicial del modelo de proceso gaussiano.\n\n\n\n\n\n\n\n\n\n\n\n\noutcome\n\n\npredictors\n\n\n\nROC\ncost\nrbf_sigma\n\n\n\n\n0.8639\n0.01562\n0.000001\n\n\n0.8625\n2.00000\n0.000001\n\n\n0.8627\n0.01562\n0.000100\n\n\n0.8659\n2.00000\n0.000100\n\n\n\n\n\n\n\n\n\nLos modelos de procesos gaussianos se especifican por sus funciones de media y covarianza, aunque esta última tiene el mayor efecto sobre la naturaleza del modelo GP. La función de covarianza a menudo se parametriza en términos de los valores de entrada (denotados como \\(x\\)). Como ejemplo, una función de covarianza comúnmente utilizada es la función exponencial al cuadrado1:\n\\[\\operatorname{cov}(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\exp\\left(-\\frac{1}{2}|\\boldsymbol{x}_i - \\boldsymbol{x}_j|^2\\right) + \\sigma^2_{ij}\\]\ndonde \\(\\sigma^2_{ij}\\) es un término de varianza de error constante que es cero cuando \\(i=j\\). Esta ecuación se traduce en:\n\nA medida que aumenta la distancia entre dos combinaciones de parámetros de ajuste, la covarianza entre las métricas de rendimiento aumenta exponencialmente.\n\nLa naturaleza de la ecuación también implica que la variación de la métrica del resultado se minimiza en los puntos que ya se han observado (es decir, cuando \\(|\\boldsymbol{x}_i - \\boldsymbol{x}_j|^2\\) es cero) .\nLa naturaleza de esta función de covarianza permite que el proceso gaussiano represente relaciones altamente no lineales entre el rendimiento del modelo y los parámetros de ajuste incluso cuando sólo existe una pequeña cantidad de datos.\n\nSin embargo, ajustar estos modelos puede resultar difícil en algunos casos y el modelo se vuelve más costoso computacionalmente a medida que aumenta el número de combinaciones de parámetros de ajuste.\n\nUna virtud importante de este modelo es que, dado que se especifica un modelo de probabilidad total, las predicciones de nuevos insumos pueden reflejar la distribución completa del resultado. En otras palabras, se pueden predecir nuevas estadísticas de desempeño tanto en términos de media como de varianza.\nSupongamos que se estuvieran considerando dos nuevos parámetros de ajuste. En Tabla 14.2, el candidato A tiene un valor ROC medio ligeramente mejor que el candidato B (el mejor actual es 0.8659). Sin embargo, su varianza es cuatro veces mayor que B. ¿Esto es bueno o malo? Elegir la opción A es más riesgoso pero tiene un rendimiento potencialmente mayor. El aumento en la varianza también refleja que este nuevo valor está más alejado de los datos existentes que B. La siguiente sección considera con más detalle estos aspectos de las predicciones de GP para la optimización bayesiana.\n\n\n\n\nTabla 14.2: Dos ejemplos de parámetros de ajuste considerados para un muestreo posterior.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicción GP de ROC AUC\n\n\n\ncandidate\nmean\nvariance\n\n\n\n\nA\n0.90\n0.000400\n\n\nB\n0.89\n0.000025\n\n\n\n\n\n\n\n\n\n\n\n\nLa optimización bayesiana es un proceso iterativo.\n\nCon base en la cuadrícula inicial de cuatro resultados, se ajusta el modelo GP, se predicen los candidatos y se selecciona una quinta combinación de parámetros de ajuste. Calculamos estimaciones de rendimiento para la nueva configuración, el GP se reajusta con los cinco resultados existentes (y así sucesivamente).\n\n\n14.2.2 Funciones de adquisición\nUna vez que el proceso gaussiano se ajusta a los datos actuales, ¿cómo se utiliza? Nuestro objetivo es elegir la siguiente combinación de parámetros de ajuste que tenga más probabilidades de tener “mejores resultados” que los mejores actuales. Un enfoque para hacer esto es crear un gran conjunto de candidatos (quizás usando un diseño que llene el espacio) y luego hacer predicciones de media y varianza para cada uno. Utilizando esta información, elegimos el valor del parámetro de ajuste más ventajoso.\nUna clase de funciones objetivo, llamadas funciones de adquisición, facilitan el equilibrio entre media y varianza. Recuerde que la varianza prevista de los modelos GP depende principalmente de qué tan lejos están de los datos existentes. El equilibrio entre la media y la varianza previstas para nuevos candidatos se ve frecuentemente a través del lente de la exploración y la explotación:\n\nLa Exploración sesga la selección hacia regiones donde hay menos (si es que hay alguno) modelos candidatos observados. Esto tiende a dar más peso a los candidatos con mayor variación y se centra en encontrar nuevos resultados.\nLa explotación se basa principalmente en la predicción media para encontrar el mejor valor (medio). Se centra en los resultados existentes.\n\nPara demostrarlo, veamos un ejemplo de juguete con un único parámetro que tiene valores entre [0, 1] y la métrica de rendimiento es \\(R^2\\). La función verdadera se muestra en Figura 14.2, junto con los valores candidatos five que tienen resultados existentes como puntos.\n\n\n\n\n\n\n\n\nFigura 14.2: Perfil de rendimiento real hipotético sobre un parámetro de ajuste arbitrario, con cinco puntos estimados\n\n\n\n\n\nPara estos datos, el ajuste del modelo GP se muestra en Figura 14.3. La región sombreada indica el error estándar medio \\(\\pm\\) 1. Las dos líneas verticales indican dos puntos candidatos que se examinan con más detalle más adelante.\nLa región de confianza sombreada demuestra la función de varianza exponencial al cuadrado; se vuelve muy grande entre puntos y converge a cero en los puntos de datos existentes.\n\n\n\n\n\n\n\n\nFigura 14.3: Perfil de rendimiento estimado generado por el modelo de proceso gaussiano. La región sombreada muestra límites de error estándar.\n\n\n\n\n\nEsta tendencia no lineal pasa por cada punto observado, pero el modelo no es perfecto. No se observan puntos cercanos al verdadero ajuste óptimo y, en esta región, el ajuste podría ser mucho mejor. A pesar de esto, el modelo GP puede orientarnos efectivamente en la dirección correcta.\nDesde un punto de vista puramente de explotación, la mejor opción sería seleccionar el valor del parámetro que tenga la mejor predicción media. Aquí, esto sería un valor de 0.106, justo a la derecha del punto mejor observado existente en 0,09.\nComo forma de fomentar la exploración, un enfoque simple (pero no utilizado con frecuencia) es encontrar el parámetro de ajuste asociado con el intervalo de confianza más grande. Por ejemplo, al usar una única desviación estándar para el límite de confianza \\(R^2\\), el siguiente punto a muestrear sería 0.236. Esto es un poco más en la región sin resultados observados. Aumentar el número de desviaciones estándar utilizadas en el límite superior empujaría la selección hacia regiones vacías.\nUna de las funciones de adquisición más utilizadas es mejora esperada. La noción de mejora requiere un valor para los mejores resultados actuales (a diferencia del enfoque de confianza). Dado que el médico de cabecera puede describir un nuevo punto candidato utilizando una distribución, podemos ponderar las partes de la distribución que muestran una mejora utilizando la probabilidad de que se produzca la mejora.\nPor ejemplo, considere dos valores de parámetros candidatos de 0,10 y 0,25 (indicados por las líneas verticales en Figura 14.3). Utilizando el modelo GP ajustado, sus distribuciones \\(R^2\\) previstas se muestran en Figura 14.4 junto con una línea de referencia para los mejores resultados actuales.\n\n\n\n\n\n\n\n\nFigura 14.4: Distribuciones de rendimiento previstas para dos valores de parámetros de ajuste muestreados\n\n\n\n\n\nCuando solo se considera la predicción media de \\(R^2\\), un valor de parámetro de 0,10 es la mejor opción (consulte Tabla 14.3). Se prevé, en promedio, que la recomendación del parámetro de ajuste de 0,25 será peor que el mejor nivel actual. Sin embargo, dado que tiene una mayor varianza, tiene más área de probabilidad general por encima del mejor nivel actual. Como resultado, tiene una mejora esperada mayor:\n\n\n\nTabla 14.3: Mejora esperada para los dos parámetros de ajuste candidatos.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicciones\n\n\n\nParameter Value\nMean\nStd Dev\nExpected Improvment\n\n\n\n\n0.10\n0.8679\n0.0004317\n0.000190\n\n\n0.25\n0.8671\n0.0039301\n0.001216\n\n\n\n\n\n\n\n\n\nCuando la mejora esperada se calcula en todo el rango del parámetro de ajuste, el punto de muestreo recomendado está mucho más cerca de 0,25 que de 0,10, como se muestra en Figura 14.5.\n\n\n\n\n\n\n\n\nFigura 14.5: El perfil de rendimiento estimado generado por el modelo de proceso gaussiano (panel superior) y la mejora esperada (panel inferior). La línea vertical indica el punto de máxima mejora.\n\n\n\n\n\nSe han propuesto y discutido numerosas funciones de adquisición; en tidymodels, la mejora esperada es la predeterminada.\n\n\n14.2.3 La función tune_bayes()\nPara implementar la búsqueda iterativa mediante optimización bayesiana, utilice la función tune_bayes(). Su sintaxis es muy similar a tune_grid() pero con varios argumentos adicionales:\n\niter es el número máximo de iteraciones de búsqueda.\ninitial Puede ser un número entero, un objeto producido usando tune_grid() o una de las funciones de carrera. El uso de un número entero especifica el tamaño de un diseño de relleno de espacio que se muestrea antes del primer modelo GP.\nobjective es un argumento para qué función de adquisición se debe utilizar. El paquete tune contiene funciones para pasar aquí, como exp_improve() o conf_bound().\nEl argumento param_info, en este caso, especifica el rango de los parámetros así como cualquier transformación que se utilice. Se utilizan para definir el espacio de búsqueda. En situaciones en las que los objetos de parámetros predeterminados son insuficientes, se utiliza param_info para anular los valores predeterminados.\n\nEl argumento control ahora usa los resultados de control_bayes(). Algunos argumentos útiles son:\n\nno_improve es un número entero que detendrá la búsqueda si no se descubren parámetros mejorados dentro de las iteraciones no_improve.\nuncertain también es un número entero (o Inf) que tomará una muestra de incertidumbre si no hay mejora dentro de las iteraciones inciertas. Esto seleccionará al siguiente candidato que tenga una gran variación. Tiene el efecto de exploración pura ya que no considera la predicción media.\nverbose es un método lógico que imprimirá información de registro a medida que avanza la búsqueda.\n\nUsemos los primeros resultados de SVM de Sección 14.1 como sustrato inicial para el modelo de proceso gaussiano. Recuerde que, para esta aplicación, queremos maximizar el área bajo la curva ROC. Nuestro código es:\n\nctrl &lt;- control_bayes(verbose = TRUE)\n\nset.seed(1403)\nsvm_bo &lt;-\n  svm_wflow %&gt;%\n  tune_bayes(\n    resamples = cell_folds,\n    metrics = roc_res,\n    initial = svm_initial,\n    param_info = svm_param,\n    iter = 25,\n    control = ctrl\n  )\n\nEl proceso de búsqueda comienza con un mejor valor inicial de 0.8659 para el área bajo la curva ROC. Un modelo de proceso gaussiano utiliza estas estadísticas four para crear un modelo. El gran conjunto de candidatos se genera y califica automáticamente utilizando la función de adquisición de mejora esperada. La primera iteración no logró mejorar el resultado con un valor ROC de 0.86315. Después de ajustar otro modelo de proceso gaussiano con el nuevo valor de resultado, la segunda iteración tampoco logró producir una mejora.\nEl registro de las dos primeras iteraciones, generado por la opción “detallado”, fue:\nLa búsqueda continúa. Hubo un total de 9 mejoras en el resultado a lo largo del camino en las iteraciones 3, 4, 5, 6, 8, 13, 22, 23, and 24. El mejor resultado se produjo en la iteración 24 con un área bajo la curva ROC de 0.8986.\nEl último paso fue:\nLas funciones que se utilizan para interrogar los resultados son las mismas que se utilizan para la búsqueda en cuadrícula (por ejemplo, collect_metrics(), etc.). Por ejemplo:\n\nshow_best(svm_bo)\n## Warning in show_best(svm_bo): No value of `metric` was given; \"roc_auc\" will be\n## used.\n## # A tibble: 5 × 9\n##    cost rbf_sigma .metric .estimator  mean     n std_err .config .iter\n##   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;int&gt;\n## 1  31.8   0.00160 roc_auc binary     0.899    10 0.00785 Iter24     24\n## 2  30.8   0.00191 roc_auc binary     0.899    10 0.00791 Iter23     23\n## 3  31.4   0.00166 roc_auc binary     0.899    10 0.00784 Iter22     22\n## 4  31.8   0.00153 roc_auc binary     0.899    10 0.00783 Iter13     13\n## 5  30.8   0.00163 roc_auc binary     0.899    10 0.00782 Iter15     15\n\nLa función autoplot() tiene varias opciones para métodos de búsqueda iterativos. Figura 14.6 muestra cómo cambió el resultado durante la búsqueda usando autoplot(svm_bo, type = \"performance\").\n\n\n\n\n\n\n\n\nFigura 14.6: El progreso de la optimización bayesiana producido cuando se utiliza el método autoplot() con type = \"performance\"\n\n\n\n\n\nUn tipo adicional de gráfico utiliza type = \"parameters\" que muestra los valores de los parámetros en iteraciones.\nLa siguiente animación visualiza los resultados de la búsqueda. Los valores negros \\(\\times\\) muestran los valores iniciales contenidos en svm_initial. El panel azul superior izquierdo muestra el valor medio previsto del área bajo la curva ROC. El panel rojo en la parte superior derecha muestra la variación prevista en los valores de ROC, mientras que el gráfico inferior visualiza la mejora esperada. En cada panel, los colores más oscuros indican valores menos atractivos (por ejemplo, valores medios pequeños, variación grande y mejoras pequeñas).\n\n\n\nLa superficie de la superficie media prevista es muy inexacta en las primeras iteraciones de la búsqueda. A pesar de esto, ayuda a guiar el proceso hacia la región de buen desempeño. En otras palabras, el modelo del proceso gaussiano es incorrecto pero resulta muy útil. Dentro de las primeras diez iteraciones, la búsqueda realiza un muestreo cerca de la ubicación óptima.\nSi bien la mejor combinación de parámetros de ajuste se encuentra en el límite del espacio de parámetros, la optimización bayesiana a menudo elegirá nuevos puntos en otros lados del límite. Si bien podemos ajustar la proporción de exploración y explotación, la búsqueda tiende a muestrear puntos fronterizos desde el principio.\n\nSi la búsqueda se basa en una cuadrícula inicial, un diseño que llene el espacio probablemente sería una mejor opción que un diseño normal. Muestra valores más únicos del espacio de parámetros y mejoraría las predicciones de la desviación estándar en las primeras iteraciones.\n\nFinalmente, si el usuario interrumpe los cálculos de tune_bayes(), la función devuelve los resultados actuales (en lugar de generar un error).",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Búsqueda Iterativa</span>"
    ]
  },
  {
    "objectID": "14-iterative-search.html#recocido-simulado",
    "href": "14-iterative-search.html#recocido-simulado",
    "title": "14  Búsqueda Iterativa",
    "section": "14.3 Recocido Simulado",
    "text": "14.3 Recocido Simulado\nRecocido simulado (SA) (Kirkpatrick, Gelatt, y Vecchi 1983; Van Laarhoven y Aarts 1987) es una rutina de búsqueda no lineal general inspirada en el proceso en el que se enfría el metal. Es un método de búsqueda global que puede navegar eficazmente por muchos tipos diferentes de entornos de búsqueda, incluidas funciones discontinuas. A diferencia de la mayoría de las rutinas de optimización basadas en gradientes, el recocido simulado puede reevaluar soluciones anteriores.\n\n14.3.1 Proceso de búsqueda de recocido simulado\nEl proceso de uso del recocido simulado comienza con un valor inicial y se embarca en un recorrido aleatorio controlado a través del espacio de parámetros. Cada nuevo valor de parámetro candidato es una pequeña perturbación del valor anterior que mantiene el nuevo punto dentro de una vecindad local.\nEl punto candidato se vuelve a muestrear para obtener su valor de rendimiento correspondiente. Si con este se logran mejores resultados que los parámetros anteriores, se acepta como el nuevo mejor y el proceso continúa. Si los resultados son peores que el valor anterior, el procedimiento de búsqueda aún puede usar este parámetro para definir pasos adicionales. Esto depende de dos factores. Primero, la probabilidad de aceptar un mal resultado disminuye a medida que el desempeño empeora. En otras palabras, un resultado ligeramente peor tiene más posibilidades de aceptación que uno con una gran caída en el rendimiento. El otro factor es el número de iteraciones de búsqueda. El recocido simulado quiere aceptar menos valores subóptimos a medida que avanza la búsqueda. A partir de estos dos factores, la probabilidad de aceptación de un mal resultado puede formalizarse como:\n\\[\\operatorname{Pr}[\\text{accept suboptimal parameters at iteration } i] = \\exp(c\\times D_i \\times i)\\]\ndonde \\(i\\) es el número de iteración, \\(c\\) es una constante especificada por el usuario y \\(D_i\\) es la diferencia porcentual entre los valores antiguos y nuevos (donde los valores negativos implican peores resultados). Para un mal resultado, determinamos la probabilidad de aceptación y la comparamos con un número uniforme aleatorio. Si el número aleatorio es mayor que el valor de probabilidad, la búsqueda descarta los parámetros actuales y la siguiente iteración crea su valor candidato en la vecindad del valor anterior. De lo contrario, la siguiente iteración forma el siguiente conjunto de parámetros en función de los valores actuales (subóptimos).\n\nLas probabilidades de aceptación del recocido simulado permiten que la búsqueda avance en la dirección equivocada, al menos a corto plazo, con el potencial de encontrar una región mucho mejor del espacio de parámetros a largo plazo.\n\n¿Cómo se ven influenciadas las probabilidades de aceptación? El mapa de calor en Figura 14.7 muestra cómo la probabilidad de aceptación puede cambiar a lo largo de las iteraciones, el rendimiento y el coeficiente especificado por el usuario.\n\n\n\n\n\n\n\n\nFigura 14.7: Mapa de calor de las probabilidades de aceptación del recocido simuladas para diferentes valores de coeficientes\n\n\n\n\n\nEl usuario puede ajustar los coeficientes para encontrar un perfil de probabilidad que se adapte a sus necesidades. En finetune::control_sim_anneal(), el valor predeterminado para este argumento cooling_coef es 0.02. Disminuir este coeficiente fomentará que la búsqueda sea más indulgente con los malos resultados.\nEste proceso continúa durante una cantidad determinada de iteraciones, pero puede detenerse si no se obtienen los mejores resultados globales dentro de un número predeterminado de iteraciones. Sin embargo, puede resultar muy útil establecer un umbral de reinicio. Si hay una serie de fallas, esta función revisa la última configuración de parámetros globalmente mejor y comienza de nuevo.\nEl principal detalle importante es definir cómo perturbar los parámetros de ajuste de una iteración a otra. Hay una variedad de métodos en la literatura para esto. Seguimos el método dado en Bohachevsky, Johnson, y Stein (1986) llamado recocido simulado generalizado. Para parámetros de ajuste continuo, definimos un radio pequeño para especificar el “vecindario” local. Por ejemplo, supongamos que hay dos parámetros de ajuste y cada uno está limitado por cero y uno. El proceso de recocido simulado genera valores aleatorios en el radio circundante y elige aleatoriamente uno como valor candidato actual.\nEn nuestra implementación, la vecindad se determina escalando el candidato actual para que esté entre cero y uno según el rango del objeto de parámetro, por lo que los valores de radio entre 0,05 y 0,15 parecen razonables. Para estos valores, lo más rápido que puede ir la búsqueda de un lado al otro del espacio de parámetros es de aproximadamente 10 iteraciones. El tamaño del radio controla la rapidez con la que la búsqueda explora el espacio de parámetros. En nuestra implementación, se especifica un rango de radios para que diferentes magnitudes de “local” definan los nuevos valores candidatos.\nPara ilustrar, usaremos los dos parámetros de ajuste principales glmnet:\n\nEl importe de la regularización total (penalty). El rango predeterminado para este parámetro es \\(10^{-10}\\) a \\(10^{0}\\). Es típico utilizar una transformación logarítmica (base-10) para este parámetro.\nLa proporción de la pena de lazo (mixture). Esto está acotado en cero y uno sin transformación.\n\nEl proceso comienza con valores iniciales de penalty = 0,025 y mixture = 0,050. Utilizando un radio que fluctúa aleatoriamente entre 0,050 y 0,015, los datos se escalan adecuadamente, se generan valores aleatorios en los radios alrededor del punto inicial y luego se elige uno al azar como candidato. A modo de ilustración, asumiremos que todos los valores candidatos son mejoras. Utilizando el nuevo valor, se genera un conjunto de nuevos vecinos aleatorios, se elige uno, y así sucesivamente. Figura 14.8 muestra six iteraciones a medida que la búsqueda avanza hacia la esquina superior izquierda.\n\n\n\n\n\n\n\n\nFigura 14.8: Una ilustración de cómo el recocido simulado determina cuál es la vecindad local para dos parámetros de ajuste numéricos. Las nubes de puntos muestran posibles siguientes valores donde se seleccionaría uno al azar.\n\n\n\n\n\nTenga en cuenta que, durante algunas iteraciones, los conjuntos candidatos a lo largo del radio excluyen puntos fuera de los límites del parámetro. Además, nuestra implementación desvía la elección de las siguientes configuraciones de parámetros de ajuste lejos de nuevos valores que son muy similares a las configuraciones anteriores.\nPara parámetros no numéricos, asignamos una probabilidad de con qué frecuencia cambia el valor del parámetro.\n\n\n14.3.2 La función tune_sim_anneal()\nPara implementar la búsqueda iterativa mediante recocido simulado, utilice la función tune_sim_anneal(). La sintaxis de esta función es casi idéntica a tune_bayes(). No hay opciones para funciones de adquisición o muestreo de incertidumbre. La función control_sim_anneal() tiene algunos detalles que definen la vecindad local y el programa de enfriamiento:\n\nno_improve, para recocido simulado, es un número entero que detendrá la búsqueda si no se descubren resultados globales mejores o mejorados dentro de las iteraciones no_improve. Los parámetros subóptimos aceptados o descartados cuentan como “sin mejora”.\nrestart es el número de iteraciones sin nuevos mejores resultados antes de comenzar con los mejores resultados anteriores.\nradius es un vector numérico en (0, 1) que define el radio mínimo y máximo de la vecindad local alrededor del punto inicial.\nflip es un valor de probabilidad que define las posibilidades de alterar el valor de parámetros categóricos o enteros.\ncooling_coef es el coeficiente \\(c\\) en \\(\\exp(c\\times D_i \\times i)\\) que modula la rapidez con la que la probabilidad de aceptación disminuye a lo largo de las iteraciones. Los valores más grandes de cooling_coef disminuyen la probabilidad de aceptar una configuración de parámetro subóptima.\n\nPara los datos de segmentación de celdas, la sintaxis es muy consistente con las funciones utilizadas anteriormente:\n\nctrl_sa &lt;- control_sim_anneal(verbose = TRUE, no_improve = 10L)\n\nset.seed(1404)\nsvm_sa &lt;-\n  svm_wflow %&gt;%\n  tune_sim_anneal(\n    resamples = cell_folds,\n    metrics = roc_res,\n    initial = svm_initial,\n    param_info = svm_param,\n    iter = 50,\n    control = ctrl_sa\n  )\n\nEl proceso de recocido simulado descubrió nuevos óptimos globales en 0 diferentes iteraciones. Había 4 se reinicia en iteraciones 13, 21, 35, and 43.\nLa opción verbose imprime detalles del proceso de búsqueda. El resultado de las primeras cinco iteraciones fue:\nEl resultado de las últimas diez iteraciones fue:\n\n## 40 ◯ accept suboptimal  roc_auc=0.89606 (+/-0.008203)\n## \n## 41 ─ discard suboptimal roc_auc=0.87556 (+/-0.009272)\n## \n## 42 ─ discard suboptimal roc_auc=0.87198 (+/-0.009301)\n## \n## 43 ✖ restart from best  roc_auc=0.89801 (+/-0.008224)\n## \n## 44 ◯ accept suboptimal  roc_auc=0.89006 (+/-0.008789)\n## \n## 45 + better suboptimal  roc_auc=0.89781 (+/-0.008104)\n## \n## 46 ◯ accept suboptimal  roc_auc=0.89563 (+/-0.008601)\n## \n## 47 ─ discard suboptimal roc_auc=0.88527 (+/-0.008766)\n## \n## 48 ◯ accept suboptimal  roc_auc=0.8922 (+/-0.008891)\n## \n## 49 ─ discard suboptimal roc_auc=0.87691 (+/-0.008352)\n## \n## 50 ◯ accept suboptimal  roc_auc=0.88803 (+/-0.008728)\n\nAl igual que con las otras funciones tune_*(), la función autoplot() correspondiente produce evaluaciones visuales de los resultados. El uso de autoplot(svm_sa, type = \"performance\") muestra el rendimiento en iteraciones (Figura 14.9) mientras que autoplot(svm_sa, type = \"parameters\") traza el rendimiento versus valores de parámetros de ajuste específicos (Figura 14.10).\n\n\n\n\n\n\n\n\nFigura 14.9: El progreso del proceso de recocido simulado se muestra cuando se usa el método autoplot() con type = \"performance\"\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigura 14.10: Rendimiento versus valores de parámetros de ajuste cuando el método autoplot() se usa con type = \"parameters\".\n\n\n\n\n\nUna visualización de la ruta de búsqueda ayuda a comprender dónde funcionó bien el proceso de búsqueda y dónde se extravió:\n\n\n\nAl igual que tune_bayes(), detener manualmente la ejecución devolverá las iteraciones completadas.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Búsqueda Iterativa</span>"
    ]
  },
  {
    "objectID": "14-iterative-search.html#sec-iterative-summary",
    "href": "14-iterative-search.html#sec-iterative-summary",
    "title": "14  Búsqueda Iterativa",
    "section": "14.4 Resumen Del Capítulo",
    "text": "14.4 Resumen Del Capítulo\nEste capítulo describió dos métodos de búsqueda iterativos para optimizar los parámetros de ajuste. La optimización de Bayes utiliza un modelo predictivo entrenado en los resultados de remuestreo existentes para sugerir valores de parámetros de ajuste, mientras que el recocido simulado recorre el espacio de hiperparámetros para encontrar buenos valores. Ambos pueden ser eficaces para encontrar buenos valores por sí solos o como método de seguimiento utilizado después de una búsqueda inicial en la cuadrícula para mejorar el rendimiento de finetune.\n\n\n\n\nBohachevsky, I, M Johnson, y M Stein. 1986. «Generalized Simulated Annealing for Function Optimization». Technometrics 28 (3): 209-17.\n\n\nFrazier, R. 2018. «A Tutorial on Bayesian Optimization». https://arxiv.org/abs/1807.02811.\n\n\nKirkpatrick, S, D Gelatt, y M Vecchi. 1983. «Optimization by Simulated Annealing». Science 220 (4598): 671-80.\n\n\nKuhn, M, y K Johnson. 2013. Applied Predictive Modeling. Springer.\n\n\nRasmussen, C, y C Williams. 2006. Gaussian Processes for Machine Learning. Gaussian Processes for Machine Learning. MIT Press.\n\n\nSchulz, E, M Speekenbrink, y A Krause. 2018. «A tutorial on Gaussian process regression: Modelling, exploring, and exploiting functions». Journal of Mathematical Psychology 85: 1-16.\n\n\nShahriari, B., K. Swersky, Z. Wang, R. P. Adams, y N. de Freitas. 2016. «Taking the Human Out of the Loop: A Review of Bayesian Optimization». Proceedings of the IEEE 104 (1): 148-75.\n\n\nVan Laarhoven, P, y E Aarts. 1987. «Simulated Annealing». En Simulated Annealing: Theory and applications, 7-15. Springer.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Búsqueda Iterativa</span>"
    ]
  },
  {
    "objectID": "14-iterative-search.html#footnotes",
    "href": "14-iterative-search.html#footnotes",
    "title": "14  Búsqueda Iterativa",
    "section": "",
    "text": "Esta ecuación también es la misma que la función de base radial utilizada en los métodos del núcleo, como el modelo SVM que se utiliza actualmente. Esto es una coincidencia; esta función de covarianza no está relacionada con el parámetro de ajuste SVM que estamos usando.↩︎",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Búsqueda Iterativa</span>"
    ]
  },
  {
    "objectID": "15-workflow-sets.html",
    "href": "15-workflow-sets.html",
    "title": "15  Probando Muchos Modelos",
    "section": "",
    "text": "15.1 Modelado De La Resistencia De Una Mezcla De Hormigón\nPara demostrar cómo filtrar múltiples flujos de trabajo de modelos, usaremos los datos de mezcla de concreto de Applied Predictive Modeling (Kuhn y Johnson 2013) como ejemplo. El capítulo 10 de ese libro demostró modelos para predecir la resistencia a la compresión de mezclas de concreto utilizando los ingredientes como predictores. Se evaluó una amplia variedad de modelos con diferentes conjuntos de predictores y necesidades de preprocesamiento. ¿Cómo pueden los conjuntos de flujos de trabajo facilitar este proceso de pruebas de modelos a gran escala?\nPrimero, definamos los esquemas de división y remuestreo de datos.\nlibrary(tidymodels)\ntidymodels_prefer()\ndata(concrete, package = \"modeldata\")\nglimpse(concrete)\n## Rows: 1,030\n## Columns: 9\n## $ cement               &lt;dbl&gt; 540.0, 540.0, 332.5, 332.5, 198.6, 266.0, 380.0, 380.…\n## $ blast_furnace_slag   &lt;dbl&gt; 0.0, 0.0, 142.5, 142.5, 132.4, 114.0, 95.0, 95.0, 114…\n## $ fly_ash              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n## $ water                &lt;dbl&gt; 162, 162, 228, 228, 192, 228, 228, 228, 228, 228, 192…\n## $ superplasticizer     &lt;dbl&gt; 2.5, 2.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0…\n## $ coarse_aggregate     &lt;dbl&gt; 1040.0, 1055.0, 932.0, 932.0, 978.4, 932.0, 932.0, 93…\n## $ fine_aggregate       &lt;dbl&gt; 676.0, 676.0, 594.0, 594.0, 825.5, 670.0, 594.0, 594.…\n## $ age                  &lt;int&gt; 28, 28, 270, 365, 360, 90, 365, 28, 28, 28, 90, 28, 2…\n## $ compressive_strength &lt;dbl&gt; 79.99, 61.89, 40.27, 41.05, 44.30, 47.03, 43.70, 36.4…\nLa columna compression_strength es el resultado. El predictor de “edad” nos dice la edad de la muestra de concreto en la prueba en días (el concreto se fortalece con el tiempo) y el resto de los predictores como “cemento” y “agua” son componentes de concreto en unidades de kilogramos por metro cúbico.\nPara abordar esto, utilizaremos la resistencia a la compresión media por mezcla de concreto para modelar:\nconcrete &lt;- \n   concrete %&gt;% \n   group_by(across(-compressive_strength)) %&gt;% \n   summarize(compressive_strength = mean(compressive_strength),\n             .groups = \"drop\")\nnrow(concrete)\n## [1] 992\nDividamos los datos usando la proporción predeterminada de 3:1 de entrenamiento a prueba y volvamos a muestrear el conjunto de entrenamiento usando cinco repeticiones de validación cruzada de 10 veces:\nset.seed(1501)\nconcrete_split &lt;- initial_split(concrete, strata = compressive_strength)\nconcrete_train &lt;- training(concrete_split)\nconcrete_test  &lt;- testing(concrete_split)\n\nset.seed(1502)\nconcrete_folds &lt;- \n   vfold_cv(concrete_train, strata = compressive_strength, repeats = 5)\nAlgunos modelos (en particular, redes neuronales, KNN y máquinas de vectores de soporte) requieren predictores centrados y escalados, por lo que algunos flujos de trabajo de modelos requerirán recetas con estos pasos de preprocesamiento. Para otros modelos, una expansión del modelo de diseño de superficie de respuesta tradicional (es decir, interacciones cuadráticas y bidireccionales) es una buena idea. Para estos fines, creamos dos recetas:\nnormalized_rec &lt;- \n   recipe(compressive_strength ~ ., data = concrete_train) %&gt;% \n   step_normalize(all_predictors()) \n\npoly_recipe &lt;- \n   normalized_rec %&gt;% \n   step_poly(all_predictors()) %&gt;% \n   step_interact(~ all_predictors():all_predictors())\nPara los modelos, utilizamos el complemento parsnip para crear un conjunto de especificaciones de modelo:\nlibrary(rules)\nlibrary(baguette)\n\nlinear_reg_spec &lt;- \n   linear_reg(penalty = tune(), mixture = tune()) %&gt;% \n   set_engine(\"glmnet\")\n\nnnet_spec &lt;- \n   mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %&gt;% \n   set_engine(\"nnet\", MaxNWts = 2600) %&gt;% \n   set_mode(\"regression\")\n\nmars_spec &lt;- \n   mars(prod_degree = tune()) %&gt;%  #&lt;- use GCV to choose terms\n   set_engine(\"earth\") %&gt;% \n   set_mode(\"regression\")\n\nsvm_r_spec &lt;- \n   svm_rbf(cost = tune(), rbf_sigma = tune()) %&gt;% \n   set_engine(\"kernlab\") %&gt;% \n   set_mode(\"regression\")\n\nsvm_p_spec &lt;- \n   svm_poly(cost = tune(), degree = tune()) %&gt;% \n   set_engine(\"kernlab\") %&gt;% \n   set_mode(\"regression\")\n\nknn_spec &lt;- \n   nearest_neighbor(neighbors = tune(), dist_power = tune(), weight_func = tune()) %&gt;% \n   set_engine(\"kknn\") %&gt;% \n   set_mode(\"regression\")\n\ncart_spec &lt;- \n   decision_tree(cost_complexity = tune(), min_n = tune()) %&gt;% \n   set_engine(\"rpart\") %&gt;% \n   set_mode(\"regression\")\n\nbag_cart_spec &lt;- \n   bag_tree() %&gt;% \n   set_engine(\"rpart\", times = 50L) %&gt;% \n   set_mode(\"regression\")\n\nrf_spec &lt;- \n   rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %&gt;% \n   set_engine(\"ranger\") %&gt;% \n   set_mode(\"regression\")\n\nxgb_spec &lt;- \n   boost_tree(tree_depth = tune(), learn_rate = tune(), loss_reduction = tune(), \n              min_n = tune(), sample_size = tune(), trees = tune()) %&gt;% \n   set_engine(\"xgboost\") %&gt;% \n   set_mode(\"regression\")\n\ncubist_spec &lt;- \n   cubist_rules(committees = tune(), neighbors = tune()) %&gt;% \n   set_engine(\"Cubist\")\nEl análisis en Kuhn y Johnson (2013) especifica que la red neuronal debe tener hasta 27 unidades ocultas en la capa. La función extract_parameter_set_dials() extrae el conjunto de parámetros, que modificamos para tener el rango de parámetros correcto:\nnnet_param &lt;- \n   nnet_spec %&gt;% \n   extract_parameter_set_dials() %&gt;% \n   update(hidden_units = hidden_units(c(1, 27)))\n¿Cómo podemos hacer coincidir estos modelos con sus recetas, ajustarlos y luego evaluar su desempeño de manera eficiente? Un conjunto de flujo de trabajo ofrece una solución.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Probando Muchos Modelos</span>"
    ]
  },
  {
    "objectID": "15-workflow-sets.html#modelado-de-la-resistencia-de-una-mezcla-de-hormigón",
    "href": "15-workflow-sets.html#modelado-de-la-resistencia-de-una-mezcla-de-hormigón",
    "title": "15  Probando Muchos Modelos",
    "section": "",
    "text": "En algunos casos de este conjunto de datos, la misma fórmula concreta se probó varias veces. Preferimos no incluir estas mezclas replicadas como puntos de datos individuales, ya que podrían distribuirse tanto en el conjunto de entrenamiento como en el de prueba. Hacerlo podría inflar artificialmente nuestras estimaciones de desempeño.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Probando Muchos Modelos</span>"
    ]
  },
  {
    "objectID": "15-workflow-sets.html#crear-el-conjunto-de-flujo-de-trabajo",
    "href": "15-workflow-sets.html#crear-el-conjunto-de-flujo-de-trabajo",
    "title": "15  Probando Muchos Modelos",
    "section": "15.2 Crear El Conjunto De Flujo De Trabajo",
    "text": "15.2 Crear El Conjunto De Flujo De Trabajo\nLos conjuntos de flujos de trabajo toman listas con nombres de preprocesadores y especificaciones de modelos y las combinan en un objeto que contiene múltiples flujos de trabajo. Hay tres tipos posibles de preprocesadores:\n\nUna fórmula R estándar\nUn objeto de receta (antes de la estimación/preparación)\nUn selector estilo dplyr para elegir el resultado y los predictores.\n\nComo primer ejemplo de conjunto de flujo de trabajo, combinemos la receta que solo estandariza los predictores con los modelos no lineales que requieren que los predictores estén en las mismas unidades:\n\nnormalized &lt;- \n   workflow_set(\n      preproc = list(normalized = normalized_rec), \n      models = list(SVM_radial = svm_r_spec, SVM_poly = svm_p_spec, \n                    KNN = knn_spec, neural_network = nnet_spec)\n   )\nnormalized\n## # A workflow set/tibble: 4 × 4\n##   wflow_id                  info             option    result    \n##   &lt;chr&gt;                     &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n## 1 normalized_SVM_radial     &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n## 2 normalized_SVM_poly       &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n## 3 normalized_KNN            &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n## 4 normalized_neural_network &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\nDado que solo hay un preprocesador, esta función crea un conjunto de flujos de trabajo con este valor. Si el preprocesador contuviera más de una entrada, la función crearía todas las combinaciones de preprocesadores y modelos.\nLa columna wflow_id se crea automáticamente pero se puede modificar usando una llamada a mutate(). La columna info contiene un tibble con algunos identificadores y el objeto de flujo de trabajo. El flujo de trabajo se puede extraer:\n\nnormalized %&gt;% extract_workflow(id = \"normalized_KNN\")\n## ══ Workflow ═════════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: nearest_neighbor()\n## \n## ── Preprocessor ─────────────────────────────────────────────────────────────────────\n## 1 Recipe Step\n## \n## • step_normalize()\n## \n## ── Model ────────────────────────────────────────────────────────────────────────────\n## K-Nearest Neighbor Model Specification (regression)\n## \n## Main Arguments:\n##   neighbors = tune()\n##   weight_func = tune()\n##   dist_power = tune()\n## \n## Computational engine: kknn\n\nLa columna option es un marcador de posición para cualquier argumento que se utilice cuando evaluamos el flujo de trabajo. Por ejemplo, para agregar el objeto de parámetro de red neuronal:\n\nnormalized &lt;- \n   normalized %&gt;% \n   option_add(param_info = nnet_param, id = \"normalized_neural_network\")\nnormalized\n## # A workflow set/tibble: 4 × 4\n##   wflow_id                  info             option    result    \n##   &lt;chr&gt;                     &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n## 1 normalized_SVM_radial     &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n## 2 normalized_SVM_poly       &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n## 3 normalized_KNN            &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n## 4 normalized_neural_network &lt;tibble [1 × 4]&gt; &lt;opts[1]&gt; &lt;list [0]&gt;\n\nCuando se usa una función del paquete tune o finetune para ajustar (o volver a muestrear) el flujo de trabajo, se usará este argumento.\nLa columna result es un marcador de posición para la salida de las funciones de ajuste o remuestreo.\nPara los otros modelos no lineales, creemos otro conjunto de flujo de trabajo que use selectores dplyr para el resultado y los predictores:\n\nmodel_vars &lt;- \n   workflow_variables(outcomes = compressive_strength, \n                      predictors = everything())\n\nno_pre_proc &lt;- \n   workflow_set(\n      preproc = list(simple = model_vars), \n      models = list(MARS = mars_spec, CART = cart_spec, CART_bagged = bag_cart_spec,\n                    RF = rf_spec, boosting = xgb_spec, Cubist = cubist_spec)\n   )\nno_pre_proc\n## # A workflow set/tibble: 6 × 4\n##   wflow_id           info             option    result    \n##   &lt;chr&gt;              &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n## 1 simple_MARS        &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n## 2 simple_CART        &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n## 3 simple_CART_bagged &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n## 4 simple_RF          &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n## 5 simple_boosting    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n## 6 simple_Cubist      &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\nFinalmente, ensamblamos el conjunto que utiliza términos no lineales e interacciones con los modelos apropiados:\n\nwith_features &lt;- \n   workflow_set(\n      preproc = list(full_quad = poly_recipe), \n      models = list(linear_reg = linear_reg_spec, KNN = knn_spec)\n   )\n\nEstos objetos son tibbles con la clase adicional workflow_set. La vinculación de filas no afecta el estado de los conjuntos y el resultado es en sí mismo un conjunto de flujo de trabajo:\n\nall_workflows &lt;- \n   bind_rows(no_pre_proc, normalized, with_features) %&gt;% \n   # Haga que los ID del flujo de trabajo sean un poco más simples:\n   mutate(wflow_id = gsub(\"(simple_)|(normalized_)\", \"\", wflow_id))\nall_workflows\n## # A workflow set/tibble: 12 × 4\n##   wflow_id    info             option    result    \n##   &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n## 1 MARS        &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n## 2 CART        &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n## 3 CART_bagged &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n## 4 RF          &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n## 5 boosting    &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n## 6 Cubist      &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n## # ℹ 6 more rows",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Probando Muchos Modelos</span>"
    ]
  },
  {
    "objectID": "15-workflow-sets.html#ajuste-y-evaluación-de-los-modelos",
    "href": "15-workflow-sets.html#ajuste-y-evaluación-de-los-modelos",
    "title": "15  Probando Muchos Modelos",
    "section": "15.3 Ajuste Y Evaluación De Los Modelos",
    "text": "15.3 Ajuste Y Evaluación De Los Modelos\nCasi todos los miembros de all_workflows contienen parámetros de ajuste. Para evaluar su rendimiento, podemos utilizar las funciones estándar de ajuste o remuestreo (por ejemplo, tune_grid(), etc.). La función workflow_map() aplicará la misma función a todos los flujos de trabajo del conjunto; el valor predeterminado es tune_grid().\nPara este ejemplo, la búsqueda de cuadrícula se aplica a cada flujo de trabajo utilizando hasta 25 candidatos de parámetros diferentes. Hay un conjunto de opciones comunes para usar con cada ejecución de tune_grid(). Por ejemplo, en el siguiente código usaremos los mismos objetos de remuestreo y control para cada flujo de trabajo, junto con un tamaño de cuadrícula de 25. La función workflow_map() tiene un argumento adicional llamado seed, que se usa para garantizar que cada ejecución de tune_grid() consume los mismos números aleatorios.\n\ngrid_ctrl &lt;-\n   control_grid(\n      save_pred = TRUE,\n      parallel_over = \"everything\",\n      save_workflow = TRUE\n   )\n\ngrid_results &lt;-\n   all_workflows %&gt;%\n   workflow_map(\n      seed = 1503,\n      resamples = concrete_folds,\n      grid = 25,\n      control = grid_ctrl\n   )\n\nLos resultados muestran que las columnas option y result se han actualizado:\n\ngrid_results\n## # A workflow set/tibble: 12 × 4\n##   wflow_id    info             option    result   \n##   &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n## 1 MARS        &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n## 2 CART        &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n## 3 CART_bagged &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;rsmp[+]&gt;\n## 4 RF          &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n## 5 boosting    &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n## 6 Cubist      &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;tune[+]&gt;\n## # ℹ 6 more rows\n\nLa columna option ahora contiene todas las opciones que usamos en la llamada workflow_map(). Esto hace que nuestros resultados sean reproducibles. En las columnas result, las notaciones “tune[+]” y “rsmp[+]” significan que el objeto no tuvo problemas. Un valor como “tune[x]” ocurre si todos los modelos fallaron por algún motivo.\nHay algunas funciones convenientes para examinar resultados, como grid_results. La función rank_results() ordenará los modelos según alguna métrica de rendimiento. De forma predeterminada, utiliza la primera métrica del conjunto de métricas (RMSE en este caso). Vamos a filter() para mirar solo RMSE:\n\ngrid_results %&gt;% \n   rank_results() %&gt;% \n   filter(.metric == \"rmse\") %&gt;% \n   select(model, .config, rmse = mean, rank)\n## # A tibble: 252 × 4\n##   model      .config                rmse  rank\n##   &lt;chr&gt;      &lt;chr&gt;                 &lt;dbl&gt; &lt;int&gt;\n## 1 boost_tree Preprocessor1_Model04  4.24     1\n## 2 boost_tree Preprocessor1_Model13  4.30     2\n## 3 boost_tree Preprocessor1_Model14  4.33     3\n## 4 boost_tree Preprocessor1_Model06  4.34     4\n## 5 boost_tree Preprocessor1_Model16  4.46     5\n## 6 boost_tree Preprocessor1_Model15  4.46     6\n## # ℹ 246 more rows\n\nAdemás, de forma predeterminada, la función clasifica todos los conjuntos de candidatos; es por eso que el mismo modelo puede aparecer varias veces en el resultado. Se puede utilizar una opción, llamada select_best, para clasificar los modelos utilizando su mejor combinación de parámetros de ajuste.\nEl método autoplot() traza las clasificaciones; también tiene un argumento select_best. El gráfico en Figura 15.1 visualiza los mejores resultados para cada modelo y se genera con:\n\nautoplot(\n   grid_results,\n   rank_metric = \"rmse\",  # &lt;- cómo pedir modelos\n   metric = \"rmse\",       # &lt;- qué métrica visualizar\n   select_best = TRUE     # &lt;- un punto por flujo de trabajo\n) +\n   geom_text(aes(y = mean - 1/2, label = wflow_id), angle = 90, hjust = 1) +\n   lims(y = c(3.5, 9.5)) +\n   theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nFigura 15.1: RMSE estimado (e intervalos de confianza aproximados) para la mejor configuración del modelo en cada flujo de trabajo.\n\n\n\n\n\nEn caso de que desee ver los resultados de los parámetros de ajuste para un modelo específico, como Figura 15.2, el argumento id puede tomar un único valor de la columna wflow_id para qué modelo trazar:\n\nautoplot(grid_results, id = \"Cubist\", metric = \"rmse\")\n\n\n\n\n\n\n\n\n\nFigura 15.2: Los resultados autoplot() para el modelo cubista contenido en el conjunto de flujo de trabajo.\n\n\n\n\n\nTambién hay métodos para collect_predictions() y collect_metrics().\nLa selección del modelo de ejemplo con nuestros datos de mezcla de concreto se ajusta a un total de modelos 12,600. Utilizando trabajadores de 4 en paralelo, el proceso de estimación tardó 1 horas en completarse.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Probando Muchos Modelos</span>"
    ]
  },
  {
    "objectID": "15-workflow-sets.html#sec-racing-example",
    "href": "15-workflow-sets.html#sec-racing-example",
    "title": "15  Probando Muchos Modelos",
    "section": "15.4 Modelos De Detección Eficiente",
    "text": "15.4 Modelos De Detección Eficiente\nUn método eficaz para seleccionar un gran conjunto de modelos de manera eficiente es utilizar el enfoque de carreras descrito en Sección 13.5.5. Con un flujo de trabajo configurado, podemos usar la función workflow_map() para este enfoque de carrera. Recuerde que después de canalizar nuestro conjunto de flujos de trabajo, el argumento que usamos es la función que se aplicará a los flujos de trabajo; en este caso, podemos usar un valor de \"tune_race_anova\". También pasamos un objeto de control apropiado; de lo contrario, las opciones serían las mismas que el código de la sección anterior.\n\nlibrary(finetune)\n\nrace_ctrl &lt;-\n   control_race(\n      save_pred = TRUE,\n      parallel_over = \"everything\",\n      save_workflow = TRUE\n   )\n\nrace_results &lt;-\n   all_workflows %&gt;%\n   workflow_map(\n      \"tune_race_anova\",\n      seed = 1503,\n      resamples = concrete_folds,\n      grid = 25,\n      control = race_ctrl\n   )\n\nEl nuevo objeto se ve muy similar, aunque los elementos de la columna resultado muestran un valor de \"race[+]\", lo que indica un tipo diferente de objeto:\n\nrace_results\n## # A workflow set/tibble: 12 × 4\n##   wflow_id    info             option    result   \n##   &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n## 1 MARS        &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n## 2 CART        &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n## 3 CART_bagged &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;rsmp[+]&gt;\n## 4 RF          &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n## 5 boosting    &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n## 6 Cubist      &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n## # ℹ 6 more rows\n\nLas mismas funciones útiles están disponibles para este objeto para interrogar los resultados y, de hecho, el método básico autoplot() que se muestra en Figura 15.31 produce tendencias. similar a Figura 15.1. Esto es producido por:\n\nautoplot(\n   race_results,\n   rank_metric = \"rmse\",  \n   metric = \"rmse\",       \n   select_best = TRUE    \n) +\n   geom_text(aes(y = mean - 1/2, label = wflow_id), angle = 90, hjust = 1) +\n   lims(y = c(3.0, 9.5)) +\n   theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nFigura 15.3: RMSE estimado (e intervalos de confianza aproximados) para la mejor configuración del modelo en cada flujo de trabajo en los resultados de las carreras.\n\n\n\n\n\nEn general, el enfoque de carreras estimó un total de 1,100 modelos, 8.73% del conjunto completo de 12,600 modelos en la cuadrícula completa. Como resultado, el enfoque de carrera fue 5-veces más rápido.\n¿Obtuvimos resultados similares? Para ambos objetos, clasificamos los resultados, los fusionamos y los comparamos entre sí en Figura 15.4.\n\nmatched_results &lt;- \n   rank_results(race_results, select_best = TRUE) %&gt;% \n   select(wflow_id, .metric, race = mean, config_race = .config) %&gt;% \n   inner_join(\n      rank_results(grid_results, select_best = TRUE) %&gt;% \n         select(wflow_id, .metric, complete = mean, \n                config_complete = .config, model),\n      by = c(\"wflow_id\", \".metric\"),\n   ) %&gt;%  \n   filter(.metric == \"rmse\")\n\nlibrary(ggrepel)\n\nmatched_results %&gt;% \n   ggplot(aes(x = complete, y = race)) + \n   geom_abline(lty = 3) + \n   geom_point() + \n   geom_text_repel(aes(label = model)) +\n   coord_obs_pred() + \n   labs(x = \"Cuadrícula completa RMSE\", y = \"Carreras RMSE\") \n\n\n\n\n\n\n\n\n\nFigura 15.4: RMSE estimado para la cuadrícula completa y los resultados de las carreras.\n\n\n\n\n\nSi bien el enfoque de carreras seleccionó los mismos parámetros candidatos que la cuadrícula completa solo para 41.67% de los modelos, las métricas de rendimiento de los modelos seleccionados por las carreras eran casi iguales. La correlación de los valores RMSE fue 0.967 y la correlación de rango fue 0.951. Esto indica que, dentro de un modelo, había múltiples combinaciones de parámetros de ajuste que tenían resultados casi idénticos.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Probando Muchos Modelos</span>"
    ]
  },
  {
    "objectID": "15-workflow-sets.html#finalizando-un-modelo",
    "href": "15-workflow-sets.html#finalizando-un-modelo",
    "title": "15  Probando Muchos Modelos",
    "section": "15.5 Finalizando Un Modelo",
    "text": "15.5 Finalizando Un Modelo\nDe manera similar a lo que hemos mostrado en capítulos anteriores, el proceso de elegir el modelo final y ajustarlo al conjunto de entrenamiento es sencillo. El primer paso es elegir un flujo de trabajo para finalizar. Dado que el modelo de árbol mejorado funcionó bien, lo extraeremos del conjunto, actualizaremos los parámetros con la mejor configuración numérica y lo ajustaremos al conjunto de entrenamiento:\n\nbest_results &lt;- \n   race_results %&gt;% \n   extract_workflow_set_result(\"boosting\") %&gt;% \n   select_best(metric = \"rmse\")\nbest_results\n## # A tibble: 1 × 7\n##   trees min_n tree_depth learn_rate loss_reduction sample_size .config              \n##   &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;                \n## 1  1957     8          7     0.0756    0.000000145       0.679 Preprocessor1_Model04\n\nboosting_test_results &lt;- \n   race_results %&gt;% \n   extract_workflow(\"boosting\") %&gt;% \n   finalize_workflow(best_results) %&gt;% \n   last_fit(split = concrete_split)\n\nPodemos ver los resultados de las métricas del conjunto de pruebas y visualizar las predicciones en Figura 15.5.\n\ncollect_metrics(boosting_test_results)\n## # A tibble: 2 × 4\n##   .metric .estimator .estimate .config             \n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n## 1 rmse    standard       3.41  Preprocessor1_Model1\n## 2 rsq     standard       0.954 Preprocessor1_Model1\n\n\nboosting_test_results %&gt;% \n   collect_predictions() %&gt;% \n   ggplot(aes(x = compressive_strength, y = .pred)) + \n   geom_abline(color = \"gray50\", lty = 2) + \n   geom_point(alpha = 0.5) + \n   coord_obs_pred() + \n   labs(x = \"observados\", y = \"predichos\")\n\n\n\n\n\n\n\n\n\nFigura 15.5: Valores observados versus valores predichos para el conjunto de prueba.\n\n\n\n\n\nAquí vemos qué tan bien se alinean la resistencia a la compresión observada y prevista para estas mezclas de concreto.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Probando Muchos Modelos</span>"
    ]
  },
  {
    "objectID": "15-workflow-sets.html#sec-workflow-sets-summary",
    "href": "15-workflow-sets.html#sec-workflow-sets-summary",
    "title": "15  Probando Muchos Modelos",
    "section": "15.6 Resumen Del Capítulo",
    "text": "15.6 Resumen Del Capítulo\nA menudo, un profesional de datos necesita considerar una gran cantidad de posibles enfoques de modelado para una tarea en cuestión, especialmente para nuevos conjuntos de datos y/o cuando hay poco conocimiento sobre qué estrategia de modelado funcionará mejor. Este capítulo ilustró cómo utilizar conjuntos de flujos de trabajo para investigar múltiples modelos o estrategias de ingeniería de características en tal situación. Los métodos de carrera pueden clasificar los modelos de manera más eficiente que ajustar cada modelo candidato que se esté considerando.\n\n\n\n\nKuhn, M, y K Johnson. 2013. Applied Predictive Modeling. Springer.",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Probando Muchos Modelos</span>"
    ]
  },
  {
    "objectID": "15-workflow-sets.html#footnotes",
    "href": "15-workflow-sets.html#footnotes",
    "title": "15  Probando Muchos Modelos",
    "section": "",
    "text": "A partir de febrero de 2022, vemos métricas de rendimiento ligeramente diferentes para la red neuronal cuando se entrena con macOS en arquitectura ARM (chip Apple M1) en comparación con la arquitectura Intel.↩︎",
    "crumbs": [
      "HERRAMIENTAS PARA CREAR MODELOS EFECTIVOS",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Probando Muchos Modelos</span>"
    ]
  },
  {
    "objectID": "16-dimensionality-reduction.html",
    "href": "16-dimensionality-reduction.html",
    "title": "16  Reducción De Dimensionalidad",
    "section": "",
    "text": "16.1 ¿Qué Problemas Puede Resolver La Reducción De Dimensionalidad?\nLa reducción de dimensionalidad se puede utilizar en ingeniería de características o en análisis de datos exploratorios. Por ejemplo, en experimentos de biología de alta dimensión, una de las primeras tareas, antes de cualquier modelado, es determinar si hay tendencias no deseadas en los datos (por ejemplo, efectos no relacionados con la cuestión de interés, como la transferencia de laboratorio a laboratorio). diferencias de laboratorio). Depurar los datos es difícil cuando hay cientos de miles de dimensiones y la reducción de la dimensionalidad puede ser una ayuda para el análisis exploratorio de datos.\nOtra posible consecuencia de tener una multitud de predictores es el posible daño a un modelo. El ejemplo más simple es un método como la regresión lineal ordinaria donde la cantidad de predictores debe ser menor que la cantidad de puntos de datos utilizados para ajustar el modelo. Otro problema es la multicolinealidad, donde las correlaciones entre predictores pueden afectar negativamente las operaciones matemáticas utilizadas para estimar un modelo. Si hay un número extremadamente grande de predictores, es bastante improbable que haya un número igual de efectos subyacentes reales. Los predictores pueden estar midiendo los mismos efectos latentes y, por lo tanto, dichos predictores estarán altamente correlacionados. Muchas técnicas de reducción de dimensionalidad prosperan en esta situación. De hecho, la mayoría puede ser eficaz sólo cuando existen relaciones entre predictores que puedan explotarse.\nEl análisis de componentes principales (PCA) es uno de los métodos más sencillos para reducir el número de columnas en el conjunto de datos porque se basa en métodos lineales y no está supervisado (es decir, no considera los datos de resultados). Para un problema de clasificación de alta dimensión, un gráfico inicial de los componentes principales del PCA podría mostrar una separación clara entre las clases. Si este es el caso, entonces es bastante seguro asumir que un clasificador lineal podría hacer un buen trabajo. Sin embargo, lo contrario no es cierto; la falta de separación no significa que el problema sea insuperable.\nLos métodos de reducción de dimensionalidad discutidos en este capítulo generalmente no son métodos de selección de características. Los métodos como PCA representan los predictores originales utilizando un subconjunto más pequeño de características nuevas. Se requieren todos los predictores originales para calcular estas nuevas características. La excepción a esto son los métodos escasos que tienen la capacidad de eliminar por completo el impacto de los predictores al crear nuevas funciones.\nEsto último resulta útil a la hora de probar o depurar una receta. Sin embargo, como se describe en Sección 8.2, la mejor manera de utilizar una receta para modelar es desde un objeto de flujo de trabajo.\nAdemás del paquete tidymodels, este capítulo utiliza los siguientes paquetes: baguette, beans, bestNormalize, corrplot, discrim, embed, ggforce, klaR, learntidymodels,1 mixOmics,2 y uwot.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reducción De Dimensionalidad</span>"
    ]
  },
  {
    "objectID": "16-dimensionality-reduction.html#qué-problemas-puede-resolver-la-reducción-de-dimensionalidad",
    "href": "16-dimensionality-reduction.html#qué-problemas-puede-resolver-la-reducción-de-dimensionalidad",
    "title": "16  Reducción De Dimensionalidad",
    "section": "",
    "text": "Al iniciar un nuevo proyecto de modelado, reducir las dimensiones de los datos puede proporcionar cierta intuición sobre cuán difícil puede ser el problema de modelado.\n\n\n\n\nEste capítulo tiene dos objetivos:\n\nDemuestre cómo utilizar recetas para crear un pequeño conjunto de funciones que capturen los aspectos principales del conjunto de predictores original.\nDescribir cómo se pueden usar las recetas por sí solas (en lugar de usarse en un objeto de flujo de trabajo, como en Sección 8.2).",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reducción De Dimensionalidad</span>"
    ]
  },
  {
    "objectID": "16-dimensionality-reduction.html#sec-beans",
    "href": "16-dimensionality-reduction.html#sec-beans",
    "title": "16  Reducción De Dimensionalidad",
    "section": "16.2 Una Imagen Vale Más Que Mil… Frijoles",
    "text": "16.2 Una Imagen Vale Más Que Mil… Frijoles\nVeamos cómo usar la reducción de dimensionalidad con recipes para ver un conjunto de datos de ejemplo. Koklu y Ozkan (2020) publicó un conjunto de datos de características visuales de los frijoles secos y describió métodos para determinar las variedades de frijoles secos en una imagen. Si bien la dimensionalidad de estos datos no es muy grande en comparación con muchos problemas de modelado del mundo real, proporciona un buen ejemplo práctico para demostrar cómo reducir la cantidad de funciones. De su manuscrito:\n\nEl objetivo principal de este estudio es proporcionar un método para la obtención de variedades de semillas uniformes a partir de la producción de cultivos, la cual es en forma de población, por lo que las semillas no están certificadas como una única variedad. Así, se desarrolló un sistema de visión por computadora para distinguir siete variedades diferentes registradas de frijol seco con características similares con el fin de obtener una clasificación uniforme de las semillas. Para el modelo de clasificación se tomaron imágenes de 13.611 granos de 7 diferentes frijoles secos registrados con una cámara de alta resolución.\n\nCada imagen contiene varios beans. El proceso de determinar qué píxeles corresponden a un frijol en particular se llama segmentación de imágenes. Estos píxeles se pueden analizar para producir características para cada frijol, como el color y la morfología (es decir, la forma). Estas características se utilizan luego para modelar el resultado (variedad de frijol) porque las diferentes variedades de frijol se ven diferentes. Los datos de entrenamiento provienen de un conjunto de imágenes etiquetadas manualmente, y este conjunto de datos se utiliza para crear un modelo predictivo que puede distinguir entre siete variedades de frijol: Cali, Horoz, Dermason, Seker, Bombay, Barbunya y Sira. Producir un modelo eficaz puede ayudar a los fabricantes a cuantificar la homogeneidad de un lote de granos.\nExisten numerosos métodos para cuantificar las formas de los objetos (Mingqiang, Kidiyo, y Joseph 2008). Muchos están relacionados con los límites o regiones del objeto de interés. Ejemplos de características incluyen:\n\nEl área (o tamaño) se puede estimar utilizando el número de píxeles del objeto o el tamaño del casco convexo alrededor del objeto.\nPodemos medir el perímetro usando el número de píxeles en el límite así como el área del cuadro delimitador (el rectángulo más pequeño que encierra un objeto).\nEl eje mayor cuantifica la línea más larga que conecta las partes más extremas del objeto. El eje menor es perpendicular al eje mayor.\nPodemos medir la compacidad de un objeto usando la relación entre el área del objeto y el área de un círculo con el mismo perímetro. Por ejemplo, los símbolos “•” y “×” tienen compacidades muy diferentes.\nTambién existen diferentes medidas de qué tan alargado u oblongo es un objeto. Por ejemplo, la estadística de excentricidad es la relación entre los ejes mayor y menor. También existen estimaciones relacionadas para la redondez y la convexidad.\n\nObserve la excentricidad de las diferentes formas en Figura 16.1.\n\n\n\n\n\n\n\n\nFigura 16.1: Algunos ejemplos de formas y sus estadísticas de excentricidad.\n\n\n\n\n\nLas formas como círculos y cuadrados tienen una excentricidad baja, mientras que las formas oblongas tienen valores altos. Además, la métrica no se ve afectada por la rotación del objeto.\nMuchas de estas características de la imagen tienen altas correlaciones; es más probable que los objetos con áreas grandes tengan perímetros grandes. A menudo existen varios métodos para cuantificar las mismas características subyacentes (por ejemplo, tamaño).\nEn los datos de los frijoles, se calcularon las características morfológicas de 16: area, perimeter, major axis length, minor axis length, aspect ratio, eccentricity, convex area, equiv diameter, extent, solidity, roundness, compactness, shape factor 1, shape factor 2, shape factor 3, and shape factor 4. Los últimos cuatro se describen en Symons y Fulcher (1988).\nPodemos comenzar cargando los datos:\n\nlibrary(tidymodels)\ntidymodels_prefer()\nlibrary(beans)\n\n\nEs importante mantener una buena disciplina de datos al evaluar técnicas de reducción de dimensionalidad, especialmente si las utilizará dentro de un modelo.\n\nPara nuestros análisis, comenzamos reteniendo un conjunto de pruebas con initial_split(). Los datos restantes se dividen en conjuntos de entrenamiento y validación:\n\nset.seed(1601)\nbean_split &lt;- initial_validation_split(beans, strata = class, prop = c(0.75, 0.125))\n## Warning: Too little data to stratify.\n## • Resampling will be unstratified.\nbean_split\n## &lt;Training/Validation/Testing/Total&gt;\n## &lt;10206/1702/1703/13611&gt;\n\n# Devolver marcos de datos:\nbean_train &lt;- training(bean_split)\nbean_test &lt;- testing(bean_split)\nbean_validation &lt;- validation(bean_split)\n\nset.seed(1602)\n# Devuelve un objeto 'rset' para usarlo con las funciones de sintonización:\nbean_val &lt;- validation_set(bean_split)\nbean_val$splits[[1]]\n## &lt;Training/Validation/Total&gt;\n## &lt;10206/1702/11908&gt;\n\nPara evaluar visualmente qué tan bien funcionan los diferentes métodos, podemos estimar los métodos en el conjunto de entrenamiento (n = 10,206 beans) y mostrar los resultados usando el conjunto de validación ( n = formato r(nrow(bean_validation), big.mark = \",\")).\nAntes de comenzar cualquier reducción de dimensionalidad, podemos dedicar algún tiempo a investigar nuestros datos. Como sabemos que muchas de estas características de forma probablemente miden conceptos similares, echemos un vistazo a la estructura de correlación de los datos en Figura 16.2 usando este código.\n\nlibrary(corrplot)\ntmwr_cols &lt;- colorRampPalette(c(\"#91CBD765\", \"#CA225E\"))\nbean_train %&gt;% \n  select(-class) %&gt;% \n  cor() %&gt;% \n  corrplot(col = tmwr_cols(200), tl.col = \"black\", method = \"ellipse\")\n\n\n\n\n\n\n\n\n\nFigura 16.2: Matriz de correlación de los predictores con variables ordenadas mediante clustering\n\n\n\n\n\nMuchos de estos predictores están altamente correlacionados, como el área y el perímetro o los factores de forma 2 y 3. Si bien no nos tomamos el tiempo para hacerlo aquí, también es importante ver si esta estructura de correlación cambia significativamente entre las categorías de resultados. Esto puede ayudar a crear mejores modelos.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reducción De Dimensionalidad</span>"
    ]
  },
  {
    "objectID": "16-dimensionality-reduction.html#una-receta-inicial",
    "href": "16-dimensionality-reduction.html#una-receta-inicial",
    "title": "16  Reducción De Dimensionalidad",
    "section": "16.3 Una Receta Inicial",
    "text": "16.3 Una Receta Inicial\nEs hora de mirar los datos de los beans en un espacio más pequeño. Podemos comenzar con una receta básica para preprocesar los datos antes de cualquier paso de reducción de dimensionalidad. Varios predictores son razones y, por lo tanto, es probable que tengan distribuciones sesgadas. Estas distribuciones pueden causar estragos en los cálculos de varianza (como los que se utilizan en PCA). El paquete bestNormalize tiene un paso que puede imponer una distribución simétrica para los predictores. Usaremos esto para mitigar el problema de las distribuciones sesgadas:\n\nlibrary(bestNormalize)\nbean_rec &lt;-\n  # Utilice los datos de entrenamiento del objeto dividido bean_val\n  recipe(class ~ ., data = bean_train) %&gt;%\n  step_zv(all_numeric_predictors()) %&gt;%\n  step_orderNorm(all_numeric_predictors()) %&gt;% \n  step_normalize(all_numeric_predictors())\n\n\nRecuerde que al invocar la función recipe() los pasos no se estiman ni ejecutan de ninguna manera.\n\nEsta receta se ampliará con pasos adicionales para los análisis de reducción de dimensionalidad. Antes de hacerlo, repasemos cómo se puede utilizar una receta fuera de un flujo de trabajo.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reducción De Dimensionalidad</span>"
    ]
  },
  {
    "objectID": "16-dimensionality-reduction.html#sec-recipe-functions",
    "href": "16-dimensionality-reduction.html#sec-recipe-functions",
    "title": "16  Reducción De Dimensionalidad",
    "section": "16.4 Recetas En La Naturaleza",
    "text": "16.4 Recetas En La Naturaleza\nComo se menciona en Sección 8.2, un flujo de trabajo que contiene una receta usa fit() para estimar la receta y el modelo, luego predict() para procesar los datos y hacer predicciones del modelo. Hay funciones análogas en el paquete recipes que se pueden usar para el mismo propósito:\n\nprep(recipe, training) ajusta la receta al conjunto de entrenamiento.\nbake(recipe, new_data) aplica las operaciones de la receta a new_data.\n\nFigura 16.3 resume esto. Veamos cada una de estas funciones con más detalle.\n\n\n\n\n\n\n\n\nFigura 16.3: Resumen de funciones relacionadas con recetas\n\n\n\n\n\n\n16.4.1 Preparando una receta\nEstimemos bean_rec usando los datos del conjunto de entrenamiento, con prep(bean_rec):\n\nbean_rec_trained &lt;- prep(bean_rec)\nbean_rec_trained\n## \n## ── Recipe ───────────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:    1\n## predictor: 16\n## \n## ── Training information\n## Training data contained 10206 data points and no incomplete rows.\n## \n## ── Operations\n## • Zero variance filter removed: &lt;none&gt; | Trained\n## • orderNorm transformation on: area, perimeter, major_axis_length, ... | Trained\n## • Centering and scaling for: area, perimeter, major_axis_length, ... | Trained\n\n\nRecuerde que prep() para una receta es como fit() para un modelo.\n\nTenga en cuenta en el resultado que los pasos han sido entrenados y que los selectores ya no son generales (es decir, all_numeric_predictors()); ahora muestran las columnas reales que se seleccionaron. Además, prep(bean_rec) no requiere el argumento training. Puede pasar cualquier dato a ese argumento, pero omitirlo significa que se utilizarán los “datos”, data, originales de la llamada a recipe(). En nuestro caso, estos fueron los datos del conjunto de entrenamiento.\nUn argumento importante para prep() es retain. Cuando retain = TRUE (el valor predeterminado), la versión estimada del conjunto de entrenamiento se mantiene dentro de la receta. Este conjunto de datos ha sido preprocesado siguiendo todos los pasos enumerados en la receta. Dado que prep() tiene que ejecutar la receta a medida que avanza, puede ser ventajoso mantener esta versión del conjunto de entrenamiento para que, si ese conjunto de datos se va a utilizar más adelante, se puedan evitar cálculos redundantes. Sin embargo, si el conjunto de entrenamiento es grande, puede resultar problemático mantener una cantidad tan grande de datos en la memoria. Utilice retain = FALSE para evitar esto.\nUna vez que se agregan nuevos pasos a esta receta estimada, volver a aplicar prep() estimará solo los pasos no entrenados. Esto será útil cuando probemos diferentes métodos de extracción de características.\n\nSi encuentra errores al trabajar con una receta, puede usar prep() con su opción verbose para solucionar problemas:\n\n\nbean_rec_trained %&gt;% \n  step_dummy(cornbread) %&gt;%  # &lt;- no es un predictor real\n  prep(verbose = TRUE)\n## oper 1 step zv [pre-trained]\n## oper 2 step orderNorm [pre-trained]\n## oper 3 step normalize [pre-trained]\n## oper 4 step dummy [training]\n## Error in `step_dummy()`:\n## Caused by error in `prep()`:\n## ! Can't select columns that don't exist.\n## ✖ Column `cornbread` doesn't exist.\n\nOtra opción que puede ayudarte a entender lo que sucede en el análisis es log_changes:\n\nshow_variables &lt;- \n  bean_rec %&gt;% \n  prep(log_changes = TRUE)\n## step_zv (zv_RLYwH): same number of columns\n## \n## step_orderNorm (orderNorm_Jx8oD): same number of columns\n## \n## step_normalize (normalize_GU75D): same number of columns\n\n\n\n16.4.2 Hornear la receta\n\nUsar bake() con una receta es muy parecido a usar predict() con un modelo; las operaciones estimadas a partir del conjunto de entrenamiento se aplican a cualquier dato, como datos de prueba o datos nuevos en el momento de la predicción.\n\nPor ejemplo, las muestras del conjunto de validación se pueden procesar:\n\nbean_val_processed &lt;- bake(bean_rec_trained, new_data = bean_validation)\n\nFigura 16.4 muestra histogramas del predictor de area antes y después de que se preparara la receta.\n\nlibrary(patchwork)\np1 &lt;- \n  bean_validation %&gt;% \n  ggplot(aes(x = area)) + \n  geom_histogram(bins = 30, color = \"white\", fill = \"blue\", alpha = 1/3) + \n  ggtitle(\"Datos del conjunto de validación original\")\n\np2 &lt;- \n  bean_val_processed %&gt;% \n  ggplot(aes(x = area)) + \n  geom_histogram(bins = 30, color = \"white\", fill = \"red\", alpha = 1/3) + \n  ggtitle(\"Datos del conjunto de validación procesados\")\n\np1 + p2\n\n\n\n\n\n\n\n\n\nFigura 16.4: El predictor de area antes y después del preprocesamiento\n\n\n\n\n\nAquí vale la pena señalar dos aspectos importantes de bake().\nPrimero, como se mencionó anteriormente, el uso de prep(recipe, retener = TRUE) mantiene la versión procesada existente del conjunto de entrenamiento en la receta. Esto permite al usuario utilizar bake(recipe, new_data = NULL), que devuelve ese conjunto de datos sin más cálculos. Por ejemplo:\n\nbake(bean_rec_trained, new_data = NULL) %&gt;% nrow()\n## [1] 10206\nbean_train %&gt;% nrow()\n## [1] 10206\n\nSi el conjunto de entrenamiento no es patológicamente grande, usar este valor de retain puede ahorrar mucho tiempo de cálculo.\nEn segundo lugar, se pueden utilizar selectores adicionales en la llamada para especificar qué columnas devolver. El selector predeterminado es everything(), pero se pueden usar directivas más específicas.\nUsaremos prep() y bake() en la siguiente sección para ilustrar algunas de estas opciones.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reducción De Dimensionalidad</span>"
    ]
  },
  {
    "objectID": "16-dimensionality-reduction.html#técnicas-de-extracción-de-características",
    "href": "16-dimensionality-reduction.html#técnicas-de-extracción-de-características",
    "title": "16  Reducción De Dimensionalidad",
    "section": "16.5 Técnicas De Extracción De Características",
    "text": "16.5 Técnicas De Extracción De Características\nDado que las recetas son la opción principal en tidymodels para la reducción de dimensionalidad, escribamos una función que estime la transformación y represente los datos resultantes en una matriz de diagrama de dispersión a través del paquete ggforce:\n\nlibrary(ggforce)\nplot_validation_results &lt;- function(recipe, dat = bean_validation) {\n  recipe %&gt;%\n    # Calcule los pasos adicionales\n    prep() %&gt;%\n    # Procesar los datos (la validación establecida por defecto)\n    bake(new_data = dat) %&gt;%\n    # Crear la matriz del diagrama de dispersión\n    ggplot(aes(x = .panel_x, y = .panel_y, color = class, fill = class)) +\n    geom_point(alpha = 0.4, size = 0.5) +\n    geom_autodensity(alpha = .3) +\n    facet_matrix(vars(-class), layer.diag = 2) + \n    scale_color_brewer(palette = \"Dark2\") + \n    scale_fill_brewer(palette = \"Dark2\")\n}\n\nReutilizaremos esta función varias veces en este capítulo.\nAquí se exploran una serie de varias metodologías de extracción de características. Se puede encontrar una descripción general de la mayoría en la Sección 6.3.1 de Kuhn y Johnson (2020) y las referencias allí contenidas. . El método UMAP se describe en McInnes, Healy, y Melville (2020).\n\n16.5.1 Análisis de componentes principales\nYa hemos mencionado la PCA varias veces en este libro y es hora de entrar en más detalles. PCA es un método no supervisado que utiliza combinaciones lineales de predictores para definir nuevas características. Estas características intentan dar cuenta de la mayor variación posible en los datos originales. Agregamos step_pca() a la receta original y usamos nuestra función para visualizar los resultados en el conjunto de validación en Figura 16.5 usando:\n\nbean_rec_trained %&gt;%\n  step_pca(all_numeric_predictors(), num_comp = 4) %&gt;%\n  plot_validation_results() + \n  ggtitle(\"Análisis de componentes principales\")\n\n\n\n\n\n\n\n\n\nFigura 16.5: Puntuaciones de los componentes principales para el conjunto de validación de beans, coloreadas por clase\n\n\n\n\n\nVemos que los dos primeros componentes PC1 y PC2, especialmente cuando se usan juntos, hacen un trabajo eficaz al distinguir o separar las clases. Esto puede llevarnos a esperar que el problema general de clasificar estos granos no sea especialmente difícil.\nRecuerde que PCA no está supervisada. Para estos datos, resulta que los componentes del PCA que explican la mayor variación en los predictores también predicen las clases. ¿Qué características son el rendimiento de conducción? El paquete learntidymodels tiene funciones que pueden ayudar a visualizar las características principales de cada componente. Necesitaremos la receta preparada; el paso PCA se agrega en el siguiente código junto con una llamada a prep():\n\nlibrary(learntidymodels)\nbean_rec_trained %&gt;%\n  step_pca(all_numeric_predictors(), num_comp = 4) %&gt;% \n  prep() %&gt;% \n  plot_top_loadings(component_number &lt;= 4, n = 5) + \n  scale_fill_brewer(palette = \"Paired\") +\n  ggtitle(\"Análisis de Componentes Principales\")\n\nEsto produce Figura 16.6.\n\n\n\n\n\n\n\n\nFigura 16.6: Cargas de predictores para la transformación PCA.\n\n\n\n\n\nLas cargas superiores están relacionadas principalmente con el grupo de predictores correlacionados que se muestran en la parte superior izquierda del gráfico de correlación anterior: perímetro, área, longitud del eje principal y área convexa. Todos estos están relacionados con el tamaño del frijol. El factor de forma 2, de Symons y Fulcher (1988), es el área sobre el cubo de la longitud del eje mayor y, por lo tanto, también está relacionado con el tamaño del frijol. Las medidas de alargamiento parecen dominar el segundo componente de PCA.\n\n\n16.5.2 Mínimos cuadrados parciales\nPLS, que presentamos en Sección 13.5.1, es una versión supervisada de PCA. Intenta encontrar componentes que maximicen simultáneamente la variación en los predictores y al mismo tiempo maximicen la relación entre esos componentes y el resultado. Figura 16.7 muestra los resultados de esta versión ligeramente modificada del código PCA:\n\nbean_rec_trained %&gt;%\n  step_pls(all_numeric_predictors(), outcome = \"class\", num_comp = 4) %&gt;%\n  plot_validation_results() + \n  ggtitle(\"Mínimos cuadrados parciales\")\n\n\n\n\n\n\n\n\n\nFigura 16.7: Puntuaciones de componentes PLS para el conjunto de validación de beans, coloreadas por clase\n\n\n\n\n\n¡Los dos primeros componentes PLS trazados en Figura 16.7 son casi idénticos a los dos primeros componentes PCA! Encontramos este resultado porque esos componentes de PCA son muy efectivos para separar las variedades de frijoles. Los componentes restantes son diferentes. Figura 16.8 visualiza las cargas, las características principales de cada componente.\n\nbean_rec_trained %&gt;%\n  step_pls(all_numeric_predictors(), outcome = \"class\", num_comp = 4) %&gt;%\n  prep() %&gt;% \n  plot_top_loadings(component_number &lt;= 4, n = 5, type = \"pls\") + \n  scale_fill_brewer(palette = \"Paired\") +\n  ggtitle(\"Mínimos cuadrados parciales\")\n\n\n\n\n\n\n\n\n\nFigura 16.8: Predictor loadings for the PLS transformation\n\n\n\n\n\nLa solidez (es decir, la densidad del grano) impulsa el tercer componente del PLS, junto con la redondez. La solidez puede estar capturando características del frijol relacionadas con las “baches” de la superficie del frijol, ya que puede medir la irregularidad de los límites del frijol.\n\n\n16.5.3 Análisis de componentes independientes\nICA (Análisis de componentes independientes) es ligeramente diferente a PCA en que encuentra componentes que son lo más independientes estadísticamente posible entre sí (en lugar de no estar correlacionados). Se puede considerar que maximiza la “no gaussianidad” de los componentes de ICA, o separa información en lugar de comprimir información como PCA. Usemos step_ica() para producir Figura 16.9:\n\nbean_rec_trained %&gt;%\n  step_ica(all_numeric_predictors(), num_comp = 4) %&gt;%\n  plot_validation_results() + \n  ggtitle(\"Análisis de componentes independientes\")\n\n\n\n\n\n\n\n\n\nFigura 16.9: Puntuaciones de componentes ICA para el conjunto de validación de beans, coloreadas por clase\n\n\n\n\n\nAl inspeccionar este gráfico, no parece haber mucha separación entre las clases en los primeros componentes cuando se usa ICA. Estos componentes independientes (o lo más independientes posible) no separan los tipos de frijoles.\n\n\n16.5.4 Aproximación y proyección de variedades uniformes.\nUMAP es similar al popular método t-SNE para la reducción de dimensiones no lineales. En el espacio original de alta dimensión, UMAP utiliza un método de vecino más cercano basado en la distancia para encontrar áreas locales de los datos donde es más probable que los puntos de datos estén relacionados. La relación entre puntos de datos se guarda como un modelo de gráfico dirigido donde la mayoría de los puntos no están conectados.\nA partir de ahí, UMAP traduce los puntos del gráfico al espacio dimensional reducido. Para hacer esto, el algoritmo tiene un proceso de optimización que utiliza entropía cruzada para asignar puntos de datos al conjunto más pequeño de características para que el gráfico sea bien aproximado.\nPara crear el mapeo, el paquete embed contiene una función de paso para este método, visualizada en Figura 16.10.\n\nlibrary(embed)\nbean_rec_trained %&gt;%\n  step_umap(all_numeric_predictors(), num_comp = 4) %&gt;%\n  plot_validation_results() +\n  ggtitle(\"UMAP\")\n\n\n\n\n\n\n\n\n\nFigura 16.10: Puntuaciones de componentes UMAP para el conjunto de validación de beans, coloreadas por clase\n\n\n\n\n\nSi bien el espacio entre grupos es pronunciado, los grupos pueden contener una mezcla heterogénea de clases.\nTambién existe una versión supervisada de UMAP:\n\nbean_rec_trained %&gt;%\n  step_umap(all_numeric_predictors(), outcome = \"class\", num_comp = 4) %&gt;%\n  plot_validation_results() +\n  ggtitle(\"UMAP (supervisado)\")\n\n\n\n\n\n\n\n\n\nFigura 16.11: Puntajes de componentes UMAP supervisados para el conjunto de validación de beans, coloreados por clase\n\n\n\n\n\nEl método supervisado que se muestra en Figura 16.11 parece prometedor para modelar los datos.\nUMAP es un método poderoso para reducir el espacio de funciones. Sin embargo, puede ser muy sensible a los parámetros de ajuste (por ejemplo, el número de vecinos, etc.). Por esta razón, sería útil experimentar con algunos de los parámetros para evaluar qué tan sólidos son los resultados de estos datos.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reducción De Dimensionalidad</span>"
    ]
  },
  {
    "objectID": "16-dimensionality-reduction.html#sec-bean-models",
    "href": "16-dimensionality-reduction.html#sec-bean-models",
    "title": "16  Reducción De Dimensionalidad",
    "section": "16.6 Modelado",
    "text": "16.6 Modelado\nVale la pena investigar tanto el método PLS como el UMAP junto con diferentes modelos. Exploremos una variedad de modelos diferentes con estas técnicas de reducción de dimensionalidad (sin ninguna transformación): una red neuronal de una sola capa, árboles en bolsas, análisis discriminante flexible (FDA), Bayes ingenuo y análisis discriminante regularizado (RDA).\nAhora que volvemos al “modo de modelado”, crearemos una serie de especificaciones de modelo y luego usaremos un conjunto de flujo de trabajo para ajustar los modelos en el siguiente código. Tenga en cuenta que los parámetros del modelo se ajustan junto con los parámetros de la receta (por ejemplo, tamaño de la dimensión reducida, parámetros UMAP).\n\nlibrary(baguette)\nlibrary(discrim)\n\nmlp_spec &lt;-\n  mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %&gt;%\n  set_engine('nnet') %&gt;%\n  set_mode('classification')\n\nbagging_spec &lt;-\n  bag_tree() %&gt;%\n  set_engine('rpart') %&gt;%\n  set_mode('classification')\n\nfda_spec &lt;-\n  discrim_flexible(\n    prod_degree = tune()\n  ) %&gt;%\n  set_engine('earth')\n\nrda_spec &lt;-\n  discrim_regularized(frac_common_cov = tune(), frac_identity = tune()) %&gt;%\n  set_engine('klaR')\n\nbayes_spec &lt;-\n  naive_Bayes() %&gt;%\n  set_engine('klaR')\n\nTambién necesitamos recetas para los métodos de reducción de dimensionalidad que probaremos. Comencemos con una receta base bean_rec y luego ampliémosla con diferentes pasos de reducción de dimensionalidad:\n\nbean_rec &lt;-\n  recipe(class ~ ., data = bean_train) %&gt;%\n  step_zv(all_numeric_predictors()) %&gt;%\n  step_orderNorm(all_numeric_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\npls_rec &lt;- \n  bean_rec %&gt;% \n  step_pls(all_numeric_predictors(), outcome = \"class\", num_comp = tune())\n\numap_rec &lt;-\n  bean_rec %&gt;%\n  step_umap(\n    all_numeric_predictors(),\n    outcome = \"class\",\n    num_comp = tune(),\n    neighbors = tune(),\n    min_dist = tune()\n  )\n\nUna vez más, el paquete workflowsets toma los preprocesadores y modelos y los cruza. La opción control parallel_over está configurada para que el procesamiento paralelo pueda funcionar simultáneamente en todas las combinaciones de parámetros de ajuste. La función workflow_map() aplica la búsqueda de cuadrícula para optimizar los parámetros del modelo/preprocesamiento (si los hay) en 10 combinaciones de parámetros. El área multiclase bajo la curva ROC se estima en el conjunto de validación.\n\nctrl &lt;- control_grid(parallel_over = \"everything\")\nbean_res &lt;- \n  workflow_set(\n    preproc = list(basic = class ~., pls = pls_rec, umap = umap_rec), \n    models = list(bayes = bayes_spec, fda = fda_spec,\n                  rda = rda_spec, bag = bagging_spec,\n                  mlp = mlp_spec)\n  ) %&gt;% \n  workflow_map(\n    verbose = TRUE,\n    seed = 1603,\n    resamples = bean_val,\n    grid = 10,\n    metrics = metric_set(roc_auc),\n    control = ctrl\n  )\n\nPodemos clasificar los modelos según sus estimaciones del conjunto de validación del área bajo la curva ROC:\n\nrankings &lt;- \n  rank_results(bean_res, select_best = TRUE) %&gt;% \n  mutate(method = map_chr(wflow_id, ~ str_split(.x, \"_\", simplify = TRUE)[1])) \n\ntidymodels_prefer()\nfilter(rankings, rank &lt;= 5) %&gt;% dplyr::select(rank, mean, model, method)\n## # A tibble: 5 × 4\n##    rank  mean model               method\n##   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt; \n## 1     1 0.996 mlp                 pls   \n## 2     2 0.996 discrim_regularized pls   \n## 3     3 0.995 discrim_flexible    basic \n## 4     4 0.995 naive_Bayes         pls   \n## 5     5 0.994 naive_Bayes         basic\n\nFigura 16.12 ilustra esta clasificación.\n\n## Warning: Removed 1 row containing missing values or values outside the scale range\n## (`geom_point()`).\n## Warning: Removed 1 row containing missing values or values outside the scale range\n## (`geom_text()`).\n\n\n\n\n\n\n\nFigura 16.12: Área bajo la curva ROC del conjunto de validación\n\n\n\n\n\nDe estos resultados se desprende claramente que la mayoría de los modelos ofrecen muy buen rendimiento; hay pocas malas decisiones aquí. Para la demostración, usaremos el modelo RDA con características PLS como modelo final. Finalizaremos el flujo de trabajo con los mejores parámetros numéricamente, lo ajustaremos al conjunto de entrenamiento y luego lo evaluaremos con el conjunto de prueba:\n\nrda_res &lt;- \n  bean_res %&gt;% \n  extract_workflow(\"pls_rda\") %&gt;% \n  finalize_workflow(\n    bean_res %&gt;% \n      extract_workflow_set_result(\"pls_rda\") %&gt;% \n      select_best(metric = \"roc_auc\")\n  ) %&gt;% \n  last_fit(split = bean_split, metrics = metric_set(roc_auc))\n\nrda_wflow_fit &lt;- extract_workflow(rda_res)\n\n¿Cuáles son los resultados de nuestra métrica (ROC AUC multiclase) en el conjunto de pruebas?\n\ncollect_metrics(rda_res)\n## # A tibble: 1 × 4\n##   .metric .estimator .estimate .config             \n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n## 1 roc_auc hand_till      0.995 Preprocessor1_Model1\n\n¡Bastante bien! Usaremos este modelo en el próximo capítulo para demostrar métodos de importancia variable.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reducción De Dimensionalidad</span>"
    ]
  },
  {
    "objectID": "16-dimensionality-reduction.html#sec-dimensionality-summary",
    "href": "16-dimensionality-reduction.html#sec-dimensionality-summary",
    "title": "16  Reducción De Dimensionalidad",
    "section": "16.7 Resumen Del Capítulo",
    "text": "16.7 Resumen Del Capítulo\nLa reducción de dimensionalidad puede ser un método útil para el análisis exploratorio de datos y el modelado. Los paquetes recipes y embed contienen pasos para una variedad de métodos diferentes y workflowsets facilita la elección de un método apropiado para un conjunto de datos. Este capítulo también analizó cómo se pueden usar recetas por sí solas, ya sea para depurar problemas con una receta o directamente para análisis exploratorio de datos y visualización de datos.\n\n\n\n\nKoklu, M, y IA Ozkan. 2020. «Multiclass classification of dry beans using computer vision and machine learning techniques». Computers and Electronics in Agriculture 174: 105507.\n\n\nKuhn, M, y K Johnson. 2020. Feature engineering and selection: A practical approach for predictive models. CRC Press.\n\n\nMcInnes, L, J Healy, y J Melville. 2020. «UMAP: Uniform manifold approximation and projection for dimension reduction».\n\n\nMingqiang, Y, K Kidiyo, y R Joseph. 2008. «A survey of shape feature extraction techniques». En Pattern Recognition, editado por PY Yin. Rijeka: IntechOpen. https://doi.org/10.5772/6237.\n\n\nSymons, S, y RG Fulcher. 1988. «Determination of wheat kernel morphological variation by digital image analysis: I. Variation in Eastern Canadian milling quality wheats». Journal of Cereal Science 8 (3): 211-18.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reducción De Dimensionalidad</span>"
    ]
  },
  {
    "objectID": "16-dimensionality-reduction.html#footnotes",
    "href": "16-dimensionality-reduction.html#footnotes",
    "title": "16  Reducción De Dimensionalidad",
    "section": "",
    "text": "El paquete learntidymodels se puede encontrar en su sitio de GitHub: https://github.com/tidymodels/learntidymodels↩︎\nEl paquete mixOmics no está disponible en CRAN, sino en Bioconductor: https://doi.org/doi:10.18129/B9.bioc.mixOmics↩︎",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Reducción De Dimensionalidad</span>"
    ]
  },
  {
    "objectID": "17-encoding-categorical-data.html",
    "href": "17-encoding-categorical-data.html",
    "title": "17  Codificación De Datos Categóricos",
    "section": "",
    "text": "17.1 ¿Es Necesaria Una Codificación?\nUna minoría de modelos, como los basados en árboles o reglas, pueden manejar datos categóricos de forma nativa y no requieren codificación ni transformación de este tipo de características. Un modelo basado en árbol puede dividir de forma nativa una variable como Bldg_Type en grupos de niveles de factores, tal vez OneFam solo en un grupo y Duplex y Twnhs juntos en otro grupo. Los modelos Naive Bayes son otro ejemplo en el que la estructura del modelo puede tratar variables categóricas de forma nativa; las distribuciones se calculan dentro de cada nivel, por ejemplo, para todos los diferentes tipos de Bldg_Type en el conjunto de datos.\nEstos modelos que pueden manejar características categóricas de forma nativa también pueden manejar características numéricas continuas, lo que hace que la transformación o codificación de dichas variables sea opcional. ¿Esto ayuda de alguna manera, quizás con el rendimiento del modelo o con el tiempo para entrenar modelos? Normalmente no, como muestra la Sección 5.7 de Kuhn y Johnson (2020) utilizando conjuntos de datos de referencia con variables factoriales no transformadas en comparación con variables ficticias transformadas para esas mismas características. En resumen, el uso de codificaciones ficticias normalmente no daba como resultado un mejor rendimiento del modelo, pero a menudo requería más tiempo para entrenar los modelos.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Codificación De Datos Categóricos</span>"
    ]
  },
  {
    "objectID": "17-encoding-categorical-data.html#es-necesaria-una-codificación",
    "href": "17-encoding-categorical-data.html#es-necesaria-una-codificación",
    "title": "17  Codificación De Datos Categóricos",
    "section": "",
    "text": "Recomendamos comenzar con variables categóricas no transformadas cuando un modelo lo permita; tenga en cuenta que las codificaciones más complejas a menudo no dan como resultado un mejor rendimiento para dichos modelos.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Codificación De Datos Categóricos</span>"
    ]
  },
  {
    "objectID": "17-encoding-categorical-data.html#codificación-de-predictores-ordinales",
    "href": "17-encoding-categorical-data.html#codificación-de-predictores-ordinales",
    "title": "17  Codificación De Datos Categóricos",
    "section": "17.2 Codificación De Predictores Ordinales",
    "text": "17.2 Codificación De Predictores Ordinales\nA veces, las columnas cualitativas se pueden ordenar, como “baja”, “media” y “alta”. En base R, la estrategia de codificación predeterminada es crear nuevas columnas numéricas que sean expansiones polinómicas de los datos. Para las columnas que tienen cinco valores ordinales, como el ejemplo que se muestra en Tabla 17.2, la columna de factores se reemplaza con columnas para términos lineales, cuadráticos, cúbicos y cuárticos:\n\n\n\nTabla 17.2: Polynominal expansions for encoding an ordered variable.\n\n\n\n\n\n\nRaw Data\nLinear\nQuadratic\nCubic\nQuartic\n\n\n\n\nnone\n-0.63\n0.53\n-0.32\n0.12\n\n\na little\n-0.32\n-0.27\n0.63\n-0.48\n\n\nsome\n0.00\n-0.53\n0.00\n0.72\n\n\na bunch\n0.32\n-0.27\n-0.63\n-0.48\n\n\ncopious amounts\n0.63\n0.53\n0.32\n0.12\n\n\n\n\n\n\n\n\n\nSi bien esto no es descabellado, no es un enfoque que la gente tienda a encontrar útil. Por ejemplo, un polinomio de 11 grados probablemente no sea la forma más eficaz de codificar un factor ordinal para los meses del año. En su lugar, considere probar pasos de recetas relacionados con factores ordenados, como step_unorder(), para convertir a factores regulares, y step_ordinalscore(), que asigna valores numéricos específicos a cada nivel de factor.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Codificación De Datos Categóricos</span>"
    ]
  },
  {
    "objectID": "17-encoding-categorical-data.html#uso-del-resultado-para-codificar-predictores",
    "href": "17-encoding-categorical-data.html#uso-del-resultado-para-codificar-predictores",
    "title": "17  Codificación De Datos Categóricos",
    "section": "17.3 Uso Del Resultado Para Codificar Predictores",
    "text": "17.3 Uso Del Resultado Para Codificar Predictores\nExisten múltiples opciones para codificaciones más complejas que las variables ficticias o indicadoras. Un método llamado efecto o codificaciones de probabilidad reemplaza las variables categóricas originales con una sola columna numérica que mide el efecto de esos datos (Micci-Barreca 2001; Zumel y Mount 2019). Por ejemplo, para el predictor de vecindario en los datos de vivienda de Ames, podemos calcular el precio de venta medio o mediano para cada vecindario (como se muestra en Figura 17.1) y sustituir estos medios por los valores de los datos originales:\n\names_train %&gt;%\n  group_by(Neighborhood) %&gt;%\n  summarize(mean = mean(Sale_Price),\n            std_err = sd(Sale_Price) / sqrt(length(Sale_Price))) %&gt;% \n  ggplot(aes(y = reorder(Neighborhood, mean), x = mean)) + \n  geom_point() +\n  geom_errorbar(aes(xmin = mean - 1.64 * std_err, xmax = mean + 1.64 * std_err)) +\n  labs(y = NULL, x = \"Price (mean, log scale)\")\n\n\n\n\n\n\n\nFigura 17.1: Precio medio de la vivienda para vecindarios en el conjunto de entrenamiento de Ames, que puede usarse como codificación de efecto para esta variable categórica\n\n\n\n\n\nEste tipo de codificación de efectos funciona bien cuando su variable categórica tiene muchos niveles. En tidymodels, el paquete embed incluye varias funciones de pasos de recetas para diferentes tipos de codificaciones de efectos, como step_lencode_glm(), step_lencode_mixed() y step_lencode_bayes(). Estos pasos utilizan un modelo lineal generalizado para estimar el efecto de cada nivel en un predictor categórico sobre el resultado. Cuando utilice un paso de receta como step_lencode_glm(), especifique primero la variable que se codifica y luego el resultado usando vars():\n\nlibrary(embed)\n\names_glm &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_lencode_glm(Neighborhood, outcome = vars(Sale_Price)) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;% \n  step_ns(Latitude, Longitude, deg_free = 20)\n\names_glm\n## \n## ── Recipe ───────────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:   1\n## predictor: 6\n## \n## ── Operations\n## • Log transformation on: Gr_Liv_Area\n## • Linear embedding for factors via GLM for: Neighborhood\n## • Dummy variables from: all_nominal_predictors()\n## • Interactions with: Gr_Liv_Area:starts_with(\"Bldg_Type_\")\n## • Natural splines on: Latitude and Longitude\n\nComo se detalla en Sección 16.4, podemos preparar, prep(), nuestra receta para ajustar o estimar parámetros para las transformaciones de preprocesamiento utilizando datos de entrenamiento. Luego podemos tidy() esta receta preparada para ver los resultados:\n\nglm_estimates &lt;-\n  prep(ames_glm) %&gt;%\n  tidy(number = 2)\n\nglm_estimates\n## # A tibble: 29 × 4\n##   level              value terms        id               \n##   &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;            \n## 1 North_Ames          5.15 Neighborhood lencode_glm_ZsXdy\n## 2 College_Creek       5.29 Neighborhood lencode_glm_ZsXdy\n## 3 Old_Town            5.07 Neighborhood lencode_glm_ZsXdy\n## 4 Edwards             5.09 Neighborhood lencode_glm_ZsXdy\n## 5 Somerset            5.35 Neighborhood lencode_glm_ZsXdy\n## 6 Northridge_Heights  5.49 Neighborhood lencode_glm_ZsXdy\n## # ℹ 23 more rows\n\nCuando utilizamos la variable numérica Neighborhood recién codificada creada mediante este método, sustituimos el nivel original (como \"North_Ames\") con la estimación de Sale_Price del GLM.\nLos métodos de codificación de efectos como este también pueden manejar sin problemas situaciones en las que se encuentra un nivel de factor novedoso en los datos. Este “valor” es el precio previsto por el GLM cuando no tenemos ninguna información específica del vecindario:\n\nglm_estimates %&gt;%\n  filter(level == \"..new\")\n## # A tibble: 1 × 4\n##   level value terms        id               \n##   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;            \n## 1 ..new  5.23 Neighborhood lencode_glm_ZsXdy\n\n\nLas codificaciones de efectos pueden ser poderosas, pero deben usarse con cuidado. Los efectos deben calcularse a partir del conjunto de entrenamiento, después de dividir los datos. Este tipo de preprocesamiento supervisado debe remuestrearse rigurosamente para evitar el sobreajuste (consulte el Capítulo 10).\n\nCuando crea una codificación de efecto para su variable categórica, efectivamente está superponiendo un minimodelo dentro de su modelo real. La posibilidad de sobreajustar con codificaciones de efectos es un ejemplo representativo de por qué la ingeniería de características debe considerarse parte del proceso del modelo, como se describe en el Capítulo 7, y por qué la ingeniería de características debe estimarse junto con los parámetros del modelo dentro del remuestreo. .\n\n17.3.1 Codificaciones de efectos con agrupación parcial\nLa creación de una codificación de efecto con step_lencode_glm() estima el efecto por separado para cada nivel de factor (en este ejemplo, vecindad). Sin embargo, algunos de estos vecindarios tienen muchas casas y otros tienen solo unas pocas. Hay mucha más incertidumbre en nuestra medición del precio para el conjunto de entrenamiento único que se encuentra en el vecindario Landmark que en el 354 casas de entrenamiento en North Ames. Podemos utilizar agrupación parcial para ajustar estas estimaciones de modo que los niveles con tamaños de muestra pequeños se reduzcan hacia la media general. Los efectos para cada nivel se modelan todos a la vez utilizando un modelo lineal generalizado mixto o jerárquico:\n\names_mixed &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_lencode_mixed(Neighborhood, outcome = vars(Sale_Price)) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;% \n  step_ns(Latitude, Longitude, deg_free = 20)\n\names_mixed\n## \n## ── Recipe ───────────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:   1\n## predictor: 6\n## \n## ── Operations\n## • Log transformation on: Gr_Liv_Area\n## • Linear embedding for factors via mixed effects for: Neighborhood\n## • Dummy variables from: all_nominal_predictors()\n## • Interactions with: Gr_Liv_Area:starts_with(\"Bldg_Type_\")\n## • Natural splines on: Latitude and Longitude\n\nVamos a prep() y tidy() esta receta para ver los resultados:\n\nmixed_estimates &lt;-\n  prep(ames_mixed) %&gt;%\n  tidy(number = 2)\n\nmixed_estimates\n## # A tibble: 29 × 4\n##   level              value terms        id                 \n##   &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;              \n## 1 North_Ames          5.15 Neighborhood lencode_mixed_SC9hi\n## 2 College_Creek       5.29 Neighborhood lencode_mixed_SC9hi\n## 3 Old_Town            5.07 Neighborhood lencode_mixed_SC9hi\n## 4 Edwards             5.10 Neighborhood lencode_mixed_SC9hi\n## 5 Somerset            5.35 Neighborhood lencode_mixed_SC9hi\n## 6 Northridge_Heights  5.49 Neighborhood lencode_mixed_SC9hi\n## # ℹ 23 more rows\n\nLuego, los nuevos niveles se codifican casi con el mismo valor que con el GLM:\n\nmixed_estimates %&gt;%\n  filter(level == \"..new\")\n## # A tibble: 1 × 4\n##   level value terms        id                 \n##   &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;              \n## 1 ..new  5.23 Neighborhood lencode_mixed_SC9hi\n\n\nPuedes usar un modelo jerárquico completamente bayesiano para los efectos de la misma manera con step_lencode_bayes().\n\nComparemos visualmente los efectos usando agrupación parcial versus sin agrupación en Figura 17.2:\n\nglm_estimates %&gt;%\n  rename(`no pooling` = value) %&gt;%\n  left_join(\n    mixed_estimates %&gt;%\n      rename(`partial pooling` = value), by = \"level\"\n  ) %&gt;%\n  left_join(\n    ames_train %&gt;% \n      count(Neighborhood) %&gt;% \n      mutate(level = as.character(Neighborhood))\n  ) %&gt;%\n  ggplot(aes(`no pooling`, `partial pooling`, size = sqrt(n))) +\n  geom_abline(color = \"gray50\", lty = 2) +\n  geom_point(alpha = 0.7) +\n  coord_fixed()\n## Warning: Removed 1 row containing missing values or values outside the scale range\n## (`geom_point()`).\n\n\n\n\n\n\n\nFigura 17.2: Comparación de las codificaciones de efectos para el vecindario estimado sin agrupación con aquellas con agrupación parcial\n\n\n\n\n\nObserve en Figura 17.2 que la mayoría de las estimaciones de los efectos de vecindad son aproximadamente las mismas cuando comparamos la agrupación con la no agrupación. Sin embargo, los barrios con menos viviendas se han visto arrastrados (ya sea hacia arriba o hacia abajo) hacia el efecto medio. Cuando utilizamos la agrupación, reducimos las estimaciones del efecto hacia la media porque no tenemos tanta evidencia sobre el precio en esos vecindarios.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Codificación De Datos Categóricos</span>"
    ]
  },
  {
    "objectID": "17-encoding-categorical-data.html#hashing-de-características",
    "href": "17-encoding-categorical-data.html#hashing-de-características",
    "title": "17  Codificación De Datos Categóricos",
    "section": "17.4 Hashing De Características",
    "text": "17.4 Hashing De Características\nLas variables ficticias tradicionales, como se describe en Sección 8.4.1, requieren que se conozcan todas las categorías posibles para crear un conjunto completo de características numéricas. Los métodos de hash de funciones (Weinberger et al. 2009) también crean variables ficticias, pero solo consideran el valor de la categoría para asignarla a un grupo predefinido de variables ficticias. Miremos nuevamente los valores de Neighborhood en Ames y usemos la función rlang::hash() para entender más:\n\nlibrary(rlang)\n\names_hashed &lt;-\n  ames_train %&gt;%\n  mutate(Hash = map_chr(Neighborhood, hash))\n\names_hashed %&gt;%\n  select(Neighborhood, Hash)\n## # A tibble: 2,342 × 2\n##   Neighborhood    Hash                            \n##   &lt;fct&gt;           &lt;chr&gt;                           \n## 1 North_Ames      076543f71313e522efe157944169d919\n## 2 North_Ames      076543f71313e522efe157944169d919\n## 3 Briardale       b598bec306983e3e68a3118952df8cf0\n## 4 Briardale       b598bec306983e3e68a3118952df8cf0\n## 5 Northpark_Villa 6af95b5db968bf393e78188a81e0e1e4\n## 6 Northpark_Villa 6af95b5db968bf393e78188a81e0e1e4\n## # ℹ 2,336 more rows\n\nSi ingresamos Briardale a esta función hash, siempre obtendremos el mismo resultado. En este caso, las vecindades se denominan “claves”, mientras que las salidas son “hashes”.\n\nUna función hash toma una entrada de tamaño variable y la asigna a una salida de tamaño fijo. Las funciones hash se utilizan comúnmente en criptografía y bases de datos.\n\nLa función rlang::hash() genera un hash de 128 bits, lo que significa que hay 2^128 valores hash posibles. Esto es excelente para algunas aplicaciones, pero no ayuda con el hash de funciones de variables de alta cardinalidad (variables con muchos niveles). En el hash de características, el número de hashes posibles es un hiperparámetro y lo establece el desarrollador del modelo calculando el módulo de los hashes enteros. Podemos obtener dieciséis valores hash posibles usando Hash %% 16:\n\names_hashed %&gt;%\n  ## primero haga un hash más pequeño para números enteros que R pueda manejar\n  mutate(Hash = strtoi(substr(Hash, 26, 32), base = 16L),  \n         ## ahora toma el módulo\n         Hash = Hash %% 16) %&gt;%\n  select(Neighborhood, Hash)\n## # A tibble: 2,342 × 2\n##   Neighborhood     Hash\n##   &lt;fct&gt;           &lt;dbl&gt;\n## 1 North_Ames          9\n## 2 North_Ames          9\n## 3 Briardale           0\n## 4 Briardale           0\n## 5 Northpark_Villa     4\n## 6 Northpark_Villa     4\n## # ℹ 2,336 more rows\n\nAhora, en lugar de los vecindarios 28 en nuestros datos originales o una cantidad increíblemente grande de hashes originales, tenemos dieciséis valores hash. Este método es muy rápido y eficiente en cuanto a memoria, y puede ser una buena estrategia cuando hay una gran cantidad de categorías posibles.\n\nEl hash de características es útil para datos de texto, así como para datos categóricos de alta cardinalidad. Consulte la Sección 6.7 de Hvitfeldt y Silge (2021) para ver una demostración de un estudio de caso con predictores de texto.\n\nPodemos implementar hash de características usando un paso de receta tidymodels del paquete textrecipes:\n\nlibrary(textrecipes)\names_hash &lt;- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %&gt;%\n  step_log(Gr_Liv_Area, base = 10) %&gt;% \n  step_dummy_hash(Neighborhood, signed = FALSE, num_terms = 16L) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %&gt;% \n  step_ns(Latitude, Longitude, deg_free = 20)\n\names_hash\n## \n## ── Recipe ───────────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:   1\n## predictor: 6\n## \n## ── Operations\n## • Log transformation on: Gr_Liv_Area\n## • Feature hashing with: Neighborhood\n## • Dummy variables from: all_nominal_predictors()\n## • Interactions with: Gr_Liv_Area:starts_with(\"Bldg_Type_\")\n## • Natural splines on: Latitude and Longitude\n\nEl hash de funciones es rápido y eficiente, pero tiene algunas desventajas. Por ejemplo, diferentes valores de categorías a menudo se asignan al mismo valor hash. Esto se llama colisión o aliasing. ¿Con qué frecuencia sucedió esto en nuestros vecindarios de Ames? Tabla 17.3 presenta la distribución del número de vecindarios por valor hash.\n\n\n\n\nTabla 17.3: La cantidad de características hash en cada número de vecindarios.\n\n\n\n\n\n\n\nNúmero de vecindarios dentro de una característica hash\nNumero de incidentes\n\n\n\n\n0\n1\n\n\n1\n7\n\n\n2\n4\n\n\n3\n3\n\n\n4\n1\n\n\n\n\n\n\n\n\n\n\n\nEl número de vecindades asignadas a cada valor hash varía entre zero y four. Todos los valores hash mayores que uno son ejemplos de colisiones hash.\n¿Cuáles son algunas cosas a considerar al utilizar hash de funciones?\n\nEl hash de características no se puede interpretar directamente porque las funciones hash no se pueden revertir. No podemos determinar cuáles eran los niveles de categoría de entrada a partir del valor hash o si ocurrió una colisión.\nEl número de valores hash es un parámetro de ajuste de esta técnica de preprocesamiento, y debes probar varios valores para determinar cuál es mejor para tu enfoque de modelado particular. Una cantidad menor de valores hash da como resultado más colisiones, pero una cantidad alta puede no ser una mejora con respecto a la variable de cardinalidad alta original.\nEl hash de características puede manejar nuevos niveles de categorías en el momento de la predicción, ya que no depende de variables ficticias predeterminadas.\nPuedes reducir las colisiones de hash con un hash firmado usando signed = TRUE. Esto expande los valores de solo 1 a +1 o -1, según el signo del hash.\n\n\nEs probable que algunas columnas hash contengan solo ceros, como vemos en este ejemplo. Recomendamos un filtro de variación cero a través de step_zv() para filtrar dichas columnas.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Codificación De Datos Categóricos</span>"
    ]
  },
  {
    "objectID": "17-encoding-categorical-data.html#más-opciones-de-codificación",
    "href": "17-encoding-categorical-data.html#más-opciones-de-codificación",
    "title": "17  Codificación De Datos Categóricos",
    "section": "17.5 Más Opciones De Codificación",
    "text": "17.5 Más Opciones De Codificación\nHay aún más opciones disponibles para transformar factores a una representación numérica.\nPodemos construir un conjunto completo de incrustaciones de entidades (Guo y Berkhahn 2016) para transformar una variable categórica con muchos niveles en un conjunto de vectores de dimensiones inferiores. Este enfoque se adapta mejor a una variable nominal con muchos niveles de categoría, muchos más que el ejemplo que hemos usado con los barrios de Ames.\n\nLa idea de incrustaciones de entidades proviene de los métodos utilizados para crear incrustaciones de palabras a partir de datos de texto. Consulte el Capítulo 5 de Hvitfeldt y Silge (2021) para obtener más información sobre la incrustación de palabras.\n\nLas incrustaciones de una variable categórica se pueden aprender a través de una red neuronal TensorFlow con la función step_embed() en embed. Podemos usar el resultado solo o, opcionalmente, el resultado más un conjunto de predictores adicionales. Al igual que en el hashing de funciones, la cantidad de nuevas columnas de codificación que se crearán es un hiperparámetro de la ingeniería de funciones. También debemos tomar decisiones sobre la estructura de la red neuronal (la cantidad de unidades ocultas) y cómo ajustar la red neuronal (cuántas épocas entrenar, cuántos datos usar para la validación en la medición de métricas).\nUna opción más disponible para abordar un resultado binario es transformar un conjunto de niveles de categorías en función de su asociación con el resultado binario. Esta transformación de peso de la evidencia (WoE) (Good 1985) utiliza el logaritmo del “factor Bayes” (la relación entre las probabilidades posteriores y las probabilidades anteriores) y crea un diccionario que asigna cada nivel de categoría a un valor WoE. Las codificaciones WoE se pueden determinar con la función step_woe() en embed.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Codificación De Datos Categóricos</span>"
    ]
  },
  {
    "objectID": "17-encoding-categorical-data.html#sec-categorical-summary",
    "href": "17-encoding-categorical-data.html#sec-categorical-summary",
    "title": "17  Codificación De Datos Categóricos",
    "section": "17.6 Resumen Del capítulo",
    "text": "17.6 Resumen Del capítulo\nEn este capítulo, aprendió a utilizar recetas de preprocesamiento para codificar predictores categóricos. La opción más sencilla para transformar una variable categórica en una representación numérica es crear variables ficticias a partir de los niveles, pero esta opción no funciona bien cuando tienes una variable con alta cardinalidad (demasiados niveles) o cuando puedes ver valores novedosos en tiempo de predicción (nuevos niveles). Una opción en tal situación es crear codificaciones de efectos, un método de codificación supervisado que utiliza el resultado. Las codificaciones de efectos se pueden aprender agrupando o sin las categorías. Otra opción utiliza una función hashing para asignar niveles de categoría a un conjunto nuevo y más pequeño de variables ficticias. El hash de funciones es rápido y ocupa poca memoria. Otras opciones incluyen incorporaciones de entidades (aprendidas a través de una red neuronal) y transformación del peso de la evidencia.\nLa mayoría de los algoritmos modelo requieren algún tipo de transformación o codificación de este tipo para variables categóricas. Una minoría de modelos, incluidos los basados ​​en árboles y reglas, pueden manejar variables categóricas de forma nativa y no requieren dichas codificaciones.\n\n\n\n\nGood, I. J. 1985. «Weight of evidence: A brief survey». Bayesian Statistics 2: 249-70.\n\n\nGuo, Cheng, y Felix Berkhahn. 2016. «Entity Embeddings of Categorical Variables». http://arxiv.org/abs/1604.06737.\n\n\nHvitfeldt, E., y J. Silge. 2021. Supervised Machine Learning for Text Analysis in R. A Chapman & Hall libro. CRC Press. https://smltar.com/.\n\n\nKuhn, M, y K Johnson. 2020. Feature engineering and selection: A practical approach for predictive models. CRC Press.\n\n\nMicci-Barreca, Daniele. 2001. «A Preprocessing Scheme for High-Cardinality Categorical Attributes in Classification and Prediction Problems». SIGKDD Explor. Newsl. 3 (1): 27-32. https://doi.org/10.1145/507533.507538.\n\n\nWeinberger, K, A Dasgupta, J Langford, A Smola, y J Attenberg. 2009. «Feature hashing for large scale multitask learning». En Proceedings of the 26th Annual International Conference on Machine Learning, 1113-20. ACM.\n\n\nZumel, Nina, y John Mount. 2019. «vtreat: a data.frame Processor for Predictive Modeling». http://arxiv.org/abs/1611.09477.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Codificación De Datos Categóricos</span>"
    ]
  },
  {
    "objectID": "17-encoding-categorical-data.html#footnotes",
    "href": "17-encoding-categorical-data.html#footnotes",
    "title": "17  Codificación De Datos Categóricos",
    "section": "",
    "text": "Esto contrasta con el modelado estadístico en Python, donde las variables categóricas a menudo se representan directamente solo con números enteros, como “0, 1, 2” que representa rojo, azul y verde.↩︎",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Codificación De Datos Categóricos</span>"
    ]
  },
  {
    "objectID": "18-explaining-models-and-predictions.html",
    "href": "18-explaining-models-and-predictions.html",
    "title": "18  Explicando Modelos Y Predicciones",
    "section": "",
    "text": "18.1 Software Para Explicaciones De Modelos\nEl marco tidymodels no contiene software para explicaciones de modelos. En cambio, los modelos entrenados y evaluados con tidymodels se pueden explicar con otro software complementario en paquetes R como lime, vip y DALEX. A menudo elegimos:\nEn los capítulos 10 y 11, entrenamos y comparamos varios modelos para predecir el precio de las viviendas en Ames, IA, incluido un modelo lineal con interacciones y un modelo forestal aleatorio, y los resultados se muestran en Figura 18.1.\nFigura 18.1: Comparación de precios previstos para un modelo lineal con interacciones y un modelo forestal aleatorio\nConstruyamos explicadores independientes del modelo para ambos modelos para descubrir por qué hacen estas predicciones. Podemos usar el paquete complementario DALEXtra para DALEX, que proporciona soporte para tidymodels. Biecek y Burzykowski (2021) proporciona una exploración exhaustiva de cómo utilizar DALEX para explicaciones de modelos; Este capítulo sólo resume algunos enfoques importantes, específicos de tidymodels. Para calcular cualquier tipo de explicación de modelo, global o local, usando DALEX, primero preparamos los datos apropiados y luego creamos un explicador para cada modelo:\nlibrary(DALEXtra)\nvip_features &lt;- c(\"Neighborhood\", \"Gr_Liv_Area\", \"Year_Built\", \n                  \"Bldg_Type\", \"Latitude\", \"Longitude\")\n\nvip_train &lt;- \n  ames_train %&gt;% \n  select(all_of(vip_features))\n\nexplainer_lm &lt;- \n  explain_tidymodels(\n    lm_fit, \n    data = vip_train, \n    y = ames_train$Sale_Price,\n    label = \"lm + interactions\",\n    verbose = FALSE\n  )\n\nexplainer_rf &lt;- \n  explain_tidymodels(\n    rf_fit, \n    data = vip_train, \n    y = ames_train$Sale_Price,\n    label = \"random forest\",\n    verbose = FALSE\n  )\nTratar con importantes transformaciones de ingeniería de características durante la explicabilidad del modelo resalta algunas opciones que tenemos (o, a veces, la ambigüedad en dichos análisis). Podemos cuantificar las explicaciones del modelo global o local en términos de:",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Explicando Modelos Y Predicciones</span>"
    ]
  },
  {
    "objectID": "18-explaining-models-and-predictions.html#software-para-explicaciones-de-modelos",
    "href": "18-explaining-models-and-predictions.html#software-para-explicaciones-de-modelos",
    "title": "18  Explicando Modelos Y Predicciones",
    "section": "",
    "text": "Las funciones de vip cuando queremos usar métodos basados en modelos que aprovechan la estructura del modelo (y a menudo son más rápidos)\nLas funciones de DALEX cuando queremos usar métodos independientes del modelo que se pueden aplicar a cualquier modelo\n\n\n\n\n\n\nUn modelo lineal suele ser sencillo de interpretar y explicar; Es posible que no se encuentre frecuentemente utilizando algoritmos de explicación de modelos separados para un modelo lineal. Sin embargo, a veces puede resultar difícil comprender o explicar las predicciones incluso de un modelo lineal una vez que tiene splines y términos de interacción.\n\n\n\npredictores básicos originales tal como existían sin transformaciones significativas de ingeniería de características, o\ncaracterísticas derivadas, como las creadas mediante reducción de dimensionalidad (Capítulo 16) o interacciones y términos spline, como en este ejemplo.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Explicando Modelos Y Predicciones</span>"
    ]
  },
  {
    "objectID": "18-explaining-models-and-predictions.html#explicaciones-locales",
    "href": "18-explaining-models-and-predictions.html#explicaciones-locales",
    "title": "18  Explicando Modelos Y Predicciones",
    "section": "18.2 Explicaciones Locales",
    "text": "18.2 Explicaciones Locales\nLas explicaciones de los modelos locales proporcionan información sobre una predicción para una sola observación. Por ejemplo, consideremos un dúplex antiguo en el vecindario de North Ames (Sección 4.1):\n\nduplex &lt;- vip_train[120,]\nduplex\n## # A tibble: 1 × 6\n##   Neighborhood Gr_Liv_Area Year_Built Bldg_Type Latitude Longitude\n##   &lt;fct&gt;              &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n## 1 North_Ames          1040       1949 Duplex        42.0     -93.6\n\nExisten múltiples enfoques posibles para comprender por qué un modelo predice un precio determinado para este dúplex. Una es una explicación detallada, implementada con la función DALEX predict_parts(); calcula cómo las contribuciones atribuidas a características individuales cambian la predicción del modelo medio para una observación particular, como nuestro dúplex. Para el modelo lineal, el estado dúplex (Bldg_Type = 3),1 el tamaño, la longitud y la antigüedad son los que más contribuyen a que el precio baje desde la intersección:\n\nlm_breakdown &lt;- predict_parts(explainer = explainer_lm, new_observation = duplex)\nlm_breakdown\n##                                              contribution\n## lm + interactions: intercept                        5.221\n## lm + interactions: Gr_Liv_Area = 1040              -0.082\n## lm + interactions: Bldg_Type = Duplex              -0.049\n## lm + interactions: Longitude = -93.608903          -0.043\n## lm + interactions: Year_Built = 1949               -0.039\n## lm + interactions: Latitude = 42.035841            -0.007\n## lm + interactions: Neighborhood = North_Ames        0.001\n## lm + interactions: prediction                       5.002\n\nDado que este modelo lineal se entrenó utilizando términos spline para latitud y longitud, la contribución al precio de “Longitud”, Longitude que se muestra aquí combina los efectos de todos sus términos spline individuales. La contribución se realiza en términos de la característica “Longitud” original, no de las características spline derivadas.\nLas características más importantes son ligeramente diferentes para el modelo de bosque aleatorio, siendo el tamaño, la edad y el estado dúplex los más importantes:\n\nrf_breakdown &lt;- predict_parts(explainer = explainer_rf, new_observation = duplex)\nrf_breakdown\n##                                          contribution\n## random forest: intercept                        5.221\n## random forest: Year_Built = 1949               -0.076\n## random forest: Gr_Liv_Area = 1040              -0.075\n## random forest: Bldg_Type = Duplex              -0.027\n## random forest: Longitude = -93.608903          -0.043\n## random forest: Latitude = 42.035841            -0.028\n## random forest: Neighborhood = North_Ames       -0.003\n## random forest: prediction                       4.969\n\n\nLas explicaciones de desglose del modelo como estas dependen del orden de las características.\n\nSi elegimos que el orden, order, para la explicación del modelo de bosque aleatorio sea el mismo que el predeterminado para el modelo lineal (elegido mediante una heurística), podemos cambiar la importancia relativa de las características:\n\npredict_parts(\n  explainer = explainer_rf, \n  new_observation = duplex,\n  order = lm_breakdown$variable_name\n)\n##                                          contribution\n## random forest: intercept                        5.221\n## random forest: Gr_Liv_Area = 1040              -0.075\n## random forest: Bldg_Type = Duplex              -0.019\n## random forest: Longitude = -93.608903          -0.023\n## random forest: Year_Built = 1949               -0.104\n## random forest: Latitude = 42.035841            -0.028\n## random forest: Neighborhood = North_Ames       -0.003\n## random forest: prediction                       4.969\n\nPodemos utilizar el hecho de que estas explicaciones desglosadas cambian según el orden para calcular las características más importantes en todos (o muchos) ordenamientos posibles. Esta es la idea detrás de Shapley Additive Explanations (Lundberg y Lee 2017), donde las contribuciones promedio de las características se calculan bajo diferentes combinaciones o “coaliciones” de ordenamiento de características. Calculemos las atribuciones SHAP para nuestro dúplex, usando ordenamientos aleatorios B = 20:\n\nset.seed(1801)\nshap_duplex &lt;- \n  predict_parts(\n    explainer = explainer_rf, \n    new_observation = duplex, \n    type = \"shap\",\n    B = 20\n  )\n\nPodríamos usar el método de trazado predeterminado de DALEX llamando a plot(shap_duplex), o podemos acceder a los datos subyacentes y crear un gráfico personalizado. Los diagramas de caja en Figura 18.2 muestran la distribución de las contribuciones en todos los ordenamientos que probamos, y las barras muestran la atribución promedio de cada característica:\n\nlibrary(forcats)\nshap_duplex %&gt;%\n  group_by(variable) %&gt;%\n  mutate(mean_val = mean(contribution)) %&gt;%\n  ungroup() %&gt;%\n  mutate(variable = fct_reorder(variable, abs(mean_val))) %&gt;%\n  ggplot(aes(contribution, variable, fill = mean_val &gt; 0)) +\n  geom_col(data = ~distinct(., variable, mean_val), \n           aes(mean_val, variable), \n           alpha = 0.5) +\n  geom_boxplot(width = 0.5) +\n  theme(legend.position = \"none\") +\n  scale_fill_viridis_d() +\n  labs(y = NULL)\n\n\n\n\n\n\n\n\n\nFigura 18.2: Explicaciones aditivas de Shapley del modelo de bosque aleatorio para una propiedad dúplex\n\n\n\n\n\n¿Qué pasa con una observación diferente en nuestro conjunto de datos? Veamos una casa unifamiliar más grande y nueva en el vecindario de Gilbert:\n\nbig_house &lt;- vip_train[1269,]\nbig_house\n## # A tibble: 1 × 6\n##   Neighborhood Gr_Liv_Area Year_Built Bldg_Type Latitude Longitude\n##   &lt;fct&gt;              &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n## 1 Gilbert             2267       2002 OneFam        42.1     -93.6\n\nPodemos calcular las atribuciones promedio SHAP para esta casa de la misma manera:\n\nset.seed(1802)\nshap_house &lt;- \n  predict_parts(\n    explainer = explainer_rf, \n    new_observation = big_house, \n    type = \"shap\",\n    B = 20\n  )\n\nLos resultados se muestran en Figura 18.3; a diferencia del dúplex, el tamaño y antigüedad de esta vivienda contribuyen a que su precio sea más elevado.\n\n\n\n\n\n\n\n\nFigura 18.3: Explicaciones aditivas de Shapley a partir del modelo de bosque aleatorio para una vivienda unifamiliar en Gilbert",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Explicando Modelos Y Predicciones</span>"
    ]
  },
  {
    "objectID": "18-explaining-models-and-predictions.html#explicaciones-globales",
    "href": "18-explaining-models-and-predictions.html#explicaciones-globales",
    "title": "18  Explicando Modelos Y Predicciones",
    "section": "18.3 Explicaciones Globales",
    "text": "18.3 Explicaciones Globales\nLas explicaciones del modelo global, también llamadas importancia de característica global o importancia variable, nos ayudan a comprender qué características son más importantes para impulsar las predicciones de los modelos forestales lineales y aleatorios en general, agregados en todo el conjunto de entrenamiento. Si bien la sección anterior abordó qué variables o características son más importantes para predecir el precio de venta de una vivienda individual, la importancia de las características globales aborda las variables más importantes para un modelo en conjunto.\n\nUna forma de calcular la importancia de una variable es permutar las características (Breiman 2001). Podemos permutar o mezclar los valores de una característica, predecir a partir del modelo y luego medir cuánto peor se ajusta el modelo a los datos en comparación con antes de la mezcla.\n\nSi barajar una columna provoca una gran degradación en el rendimiento del modelo, es importante; Si mezclar los valores de una columna no supone mucha diferencia en el rendimiento del modelo, no debe ser una variable importante. Este enfoque se puede aplicar a cualquier tipo de modelo (es independiente del modelo) y los resultados son fáciles de entender.\nUsando DALEX, calculamos este tipo de importancia variable mediante la función model_parts().\n\nset.seed(1803)\nvip_lm &lt;- model_parts(explainer_lm, loss_function = loss_root_mean_square)\nset.seed(1804)\nvip_rf &lt;- model_parts(explainer_rf, loss_function = loss_root_mean_square)\n\nNuevamente, podríamos usar el método de trazado predeterminado de DALEX llamando a plot(vip_lm, vip_rf) pero los datos subyacentes están disponibles para exploración, análisis y trazado. Creemos una función para trazar:\n\nggplot_imp &lt;- function(...) {\n  obj &lt;- list(...)\n  metric_name &lt;- attr(obj[[1]], \"loss_name\")\n  metric_lab &lt;- paste(metric_name, \n                      \"después de permutaciones\\n(más alto indica más importante)\")\n  \n  full_vip &lt;- bind_rows(obj) %&gt;%\n    filter(variable != \"_baseline_\")\n  \n  perm_vals &lt;- full_vip %&gt;% \n    filter(variable == \"_full_model_\") %&gt;% \n    group_by(label) %&gt;% \n    summarise(dropout_loss = mean(dropout_loss))\n  \n  p &lt;- full_vip %&gt;%\n    filter(variable != \"_full_model_\") %&gt;% \n    mutate(variable = fct_reorder(variable, dropout_loss)) %&gt;%\n    ggplot(aes(dropout_loss, variable)) \n  if(length(obj) &gt; 1) {\n    p &lt;- p + \n      facet_wrap(vars(label)) +\n      geom_vline(data = perm_vals, aes(xintercept = dropout_loss, color = label),\n                 linewidth = 1.4, lty = 2, alpha = 0.7) +\n      geom_boxplot(aes(color = label, fill = label), alpha = 0.2)\n  } else {\n    p &lt;- p + \n      geom_vline(data = perm_vals, aes(xintercept = dropout_loss),\n                 linewidth = 1.4, lty = 2, alpha = 0.7) +\n      geom_boxplot(fill = \"#91CBD765\", alpha = 0.4)\n    \n  }\n  p +\n    theme(legend.position = \"none\") +\n    labs(x = metric_lab, \n         y = NULL,  fill = NULL,  color = NULL)\n}\n\nEl uso de ggplot_imp(vip_lm, vip_rf) produce Figura 18.4.\n\n\n\n\n\n\n\n\nFigura 18.4: Explicador global para los modelos de regresión lineal y de bosque aleatorio\n\n\n\n\n\nLa línea discontinua en cada panel de Figura 18.4 muestra el RMSE para el modelo completo, ya sea el modelo lineal o el modelo de bosque aleatorio. Las características más a la derecha son más importantes porque permutarlas da como resultado un RMSE más alto. Hay bastante información interesante que aprender de esta trama; por ejemplo, la vecindad es bastante importante en el modelo lineal con interacciones/splines, pero es la segunda característica menos importante para el modelo de bosque aleatorio.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Explicando Modelos Y Predicciones</span>"
    ]
  },
  {
    "objectID": "18-explaining-models-and-predictions.html#construyendo-explicaciones-globales-a-partir-de-explicaciones-locales",
    "href": "18-explaining-models-and-predictions.html#construyendo-explicaciones-globales-a-partir-de-explicaciones-locales",
    "title": "18  Explicando Modelos Y Predicciones",
    "section": "18.4 Construyendo Explicaciones Globales A Partir De Explicaciones Locales",
    "text": "18.4 Construyendo Explicaciones Globales A Partir De Explicaciones Locales\nHasta ahora en este capítulo, nos hemos centrado en explicaciones de modelos locales para una sola observación (a través de explicaciones aditivas de Shapley) y explicaciones de modelos globales para un conjunto de datos en su conjunto (a través de características de permutación). También es posible construir explicaciones de modelos globales agregando explicaciones de modelos locales, como con los perfiles de dependencia parcial.\n\nLos perfiles de dependencia parcial muestran cómo el valor esperado de la predicción de un modelo, como el precio previsto de una casa en Ames, cambia en función de una característica, como la edad o la superficie habitable bruta.\n\nUna forma de crear dicho perfil es agregando o promediando perfiles para observaciones individuales. Un perfil que muestra cómo cambia la predicción de una observación individual en función de una característica determinada se denomina perfil ICE (expectativa condicional individual) o perfil CP (ceteris paribus). Podemos calcular dichos perfiles individuales (para 500 de las observaciones en nuestro conjunto de entrenamiento) y luego agregarlos usando la función DALEX model_profile():\n\nset.seed(1805)\npdp_age &lt;- model_profile(explainer_rf, N = 500, variables = \"Year_Built\")\n\nCreemos otra función para trazar los datos subyacentes en este objeto:\n\nggplot_pdp &lt;- function(obj, x) {\n  \n  p &lt;- \n    as_tibble(obj$agr_profiles) %&gt;%\n    mutate(`_label_` = stringr::str_remove(`_label_`, \"^[^_]*_\")) %&gt;%\n    ggplot(aes(`_x_`, `_yhat_`)) +\n    geom_line(data = as_tibble(obj$cp_profiles),\n              aes(x = {{ x }}, group = `_ids_`),\n              linewidth = 0.5, alpha = 0.05, color = \"gray50\")\n  \n  num_colors &lt;- n_distinct(obj$agr_profiles$`_label_`)\n  \n  if (num_colors &gt; 1) {\n    p &lt;- p + geom_line(aes(color = `_label_`), linewidth = 1.2, alpha = 0.8)\n  } else {\n    p &lt;- p + geom_line(color = \"midnightblue\", linewidth = 1.2, alpha = 0.8)\n  }\n  \n  p\n}\n\nEl uso de esta función genera Figura 18.5, donde podemos ver el comportamiento no lineal del modelo de bosque aleatorio.\n\nggplot_pdp(pdp_age, Year_Built)  +\n  labs(x = \"Year built\", \n       y = \"Sale Price (log)\", \n       color = NULL)\n\n\n\n\n\n\n\n\n\nFigura 18.5: Perfiles de dependencia parcial para el modelo de bosque aleatorio centrados en el predictor del año de construcción\n\n\n\n\n\nEl precio de venta de las casas construidas en diferentes años es en su mayor parte estable, con un modesto aumento después de 1960 aproximadamente. Se pueden calcular perfiles de dependencia parcial para cualquier otra característica del modelo, y también para grupos en los datos, como Bldg_Type. Utilicemos 1000 observaciones para estos perfiles.\n\nset.seed(1806)\npdp_liv &lt;- model_profile(explainer_rf, N = 1000, \n                         variables = \"Gr_Liv_Area\", \n                         groups = \"Bldg_Type\")\n\nggplot_pdp(pdp_liv, Gr_Liv_Area) +\n  scale_x_log10() +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(x = \"Superficie habitable bruta\", \n       y = \"Precio de venta (registro)\", \n       color = NULL)\n\nEste código produce Figura 18.6, donde vemos que el precio de venta aumenta más entre aproximadamente 1,000 y 3,000 pies cuadrados de área habitable, y que los diferentes tipos de viviendas (como casas unifamiliares o diferentes tipos de casas adosadas) exhiben en su mayoría tendencias crecientes similares en el precio con más espacio habitable.\n\n\n\n\n\n\n\n\nFigura 18.6: Perfiles de dependencia parcial para el modelo de bosque aleatorio centrado en los tipos de construcción y la superficie habitable bruta\n\n\n\n\n\nTenemos la opción de usar plot(pdp_liv) para los gráficos predeterminados DALEX, pero dado que aquí estamos haciendo gráficos con los datos subyacentes, incluso podemos facetar una de las características para visualizar si las predicciones cambian. de manera diferente y resaltando el desequilibrio en estos subgrupos (como se muestra en Figura 18.7).\n\nas_tibble(pdp_liv$agr_profiles) %&gt;%\n  mutate(Bldg_Type = stringr::str_remove(`_label_`, \"random forest_\")) %&gt;%\n  ggplot(aes(`_x_`, `_yhat_`, color = Bldg_Type)) +\n  geom_line(data = as_tibble(pdp_liv$cp_profiles),\n            aes(x = Gr_Liv_Area, group = `_ids_`),\n            linewidth = 0.5, alpha = 0.1, color = \"gray50\") +\n  geom_line(linewidth = 1.2, alpha = 0.8, show.legend = FALSE) +\n  scale_x_log10() +\n  facet_wrap(~Bldg_Type) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(x = \"Superficie habitable bruta\", \n       y = \"Precio de venta (registro)\", \n       color = NULL)\n\n\n\n\n\n\n\n\n\nFigura 18.7: Perfiles de dependencia parcial para el modelo de bosque aleatorio que se centran en los tipos de construcción y la superficie habitable bruta mediante facetas\n\n\n\n\n\nNo existe un único enfoque correcto para crear explicaciones de modelos y las opciones descritas en este capítulo no son exhaustivas. Hemos destacado buenas opciones para explicaciones tanto a nivel individual como global, así como también cómo tender un puente entre uno y otro, y le remitimos a Biecek y Burzykowski (2021) y Molnar (2020) para obtener más información.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Explicando Modelos Y Predicciones</span>"
    ]
  },
  {
    "objectID": "18-explaining-models-and-predictions.html#de-vuelta-a-los-frijoles",
    "href": "18-explaining-models-and-predictions.html#de-vuelta-a-los-frijoles",
    "title": "18  Explicando Modelos Y Predicciones",
    "section": "18.5 ¡De Vuelta A Los Frijoles!",
    "text": "18.5 ¡De Vuelta A Los Frijoles!\nEn el Capítulo 16, analizamos cómo utilizar la reducción de dimensionalidad como un paso de preprocesamiento o ingeniería de características al modelar datos de alta dimensión. Para nuestro conjunto de datos de ejemplo de medidas de morfología de frijoles secos que predicen el tipo de frijol, vimos excelentes resultados de la reducción de la dimensionalidad de mínimos cuadrados parciales (PLS) combinada con un modelo de análisis discriminante regularizado. ¿Cuáles de esas características morfológicas fueron más importantes en las predicciones del tipo de frijol? Podemos usar el mismo enfoque descrito a lo largo de este capítulo para crear un explicador independiente del modelo y calcular, por ejemplo, explicaciones del modelo global a través de model_parts():\n\nset.seed(1807)\nvip_beans &lt;- \n  explain_tidymodels(\n    rda_wflow_fit, \n    data = bean_train %&gt;% select(-class), \n    y = bean_train$class,\n    label = \"RDA\",\n    verbose = FALSE\n  ) %&gt;% \n  model_parts() \n\nUsando nuestra función de trazado de importancia previamente definida, ggplot_imp(vip_beans) produce Figura 18.8.\n\n\n\n\n\n\n\n\nFigura 18.8: Explicador global del modelo de análisis discriminante regularizado de los datos de frijoles\n\n\n\n\n\n\nLas medidas de importancia de características globales que vemos en Figura 18.8 incorporan los efectos de todos los componentes de PLS, pero en términos de las variables originales.\n\nFigura 18.8 nos muestra que los factores de forma se encuentran entre las características más importantes para predecir el tipo de frijol, especialmente el factor de forma 4, una medida de solidez que toma en cuenta el área \\(A\\), el eje mayor \\(L\\) y el eje menor \\(l\\):\n\\[\\text{SF4} = \\frac{A}{\\pi(L/2)(l/2)}\\]\nPodemos ver en Figura 18.8 que el factor de forma 1 (la relación entre el eje mayor y el área), la longitud del eje menor y la redondez son las siguientes características más importantes del frijol para predecir la variedad de frijol.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Explicando Modelos Y Predicciones</span>"
    ]
  },
  {
    "objectID": "18-explaining-models-and-predictions.html#sec-explain-summary",
    "href": "18-explaining-models-and-predictions.html#sec-explain-summary",
    "title": "18  Explicando Modelos Y Predicciones",
    "section": "18.6 Resumen Del Capítulo",
    "text": "18.6 Resumen Del Capítulo\nPara algunos tipos de modelos, la respuesta a por qué un modelo hizo una determinada predicción es sencilla, pero para otros tipos de modelos, debemos usar algoritmos explicativos separados para comprender qué características son relativamente más importantes para las predicciones. Puede generar dos tipos principales de explicaciones de modelo a partir de un modelo entrenado. Las explicaciones globales brindan información agregada sobre un conjunto de datos completo, mientras que las explicaciones locales brindan comprensión sobre las predicciones de un modelo para una sola observación.\nPaquetes como DALEX y su paquete de soporte DALEXtra, vip y lime se pueden integrar en un análisis de tidymodels para proporcionar explicaciones de estos modelos. . Las explicaciones del modelo son sólo una parte de la comprensión de si su modelo es apropiado y eficaz, junto con las estimaciones del rendimiento del modelo; El Capítulo 19 explora más a fondo la calidad y confiabilidad de las predicciones.\n\n\n\n\nBiecek, Przemyslaw, y Tomasz Burzykowski. 2021. Explanatory Model Analysis. Chapman; Hall/CRC, New York. https://ema.drwhy.ai/.\n\n\nBreiman, L. 2001. «Random forests». Machine learning 45 (1): 5-32.\n\n\nLundberg, Scott M., y Su-In Lee. 2017. «A Unified Approach to Interpreting Model Predictions». En Proceedings of the 31st International Conference on Neural Information Processing Systems, 4768-77. NIPS’17. Red Hook, NY, USA: Curran Associates Inc.\n\n\nMolnar, Christopher. 2020. Interpretable Machine Learning. lulu.com. https://christophm.github.io/interpretable-ml-book/.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Explicando Modelos Y Predicciones</span>"
    ]
  },
  {
    "objectID": "18-explaining-models-and-predictions.html#footnotes",
    "href": "18-explaining-models-and-predictions.html#footnotes",
    "title": "18  Explicando Modelos Y Predicciones",
    "section": "",
    "text": "Tenga en cuenta que este paquete de explicaciones de modelos se centra en el nivel de predictores categóricos en este tipo de salida, como Bldg_Type = 3 para dúplex y Neighborhood = 1 para North Ames.↩︎",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Explicando Modelos Y Predicciones</span>"
    ]
  },
  {
    "objectID": "19-when-should-you-trust-predictions.html",
    "href": "19-when-should-you-trust-predictions.html",
    "title": "19  ¿Cuándo Debería Confiar En Sus Predicciones?",
    "section": "",
    "text": "19.1 Resultados Equívocos\nSi el resultado de un modelo indicara que usted tenía un 51% de posibilidades de haber contraído COVID-19, sería natural ver el diagnóstico con cierto escepticismo. De hecho, los organismos reguladores suelen exigir que muchos diagnósticos médicos tengan una zona equívoca. Esta zona es un rango de resultados en los que la predicción no debe informarse a los pacientes, por ejemplo, algún rango de resultados de pruebas de COVID-19 que son demasiado inciertos para informarse a un paciente. Consulte Danowski et al. (1970) y Kerleguer et al. (2003) para ver ejemplos. La misma noción se puede aplicar a modelos creados fuera del diagnóstico médico.\nUsemos una función que pueda simular datos de clasificación con dos clases y dos predictores (x e y). El verdadero modelo es un modelo de regresión logística con la ecuación:\n\\[\n\\mathrm{logit}(p) = -1 - 2x - \\frac{x^2}{5} + 2y^2\n\\]\nLos dos predictores siguen una distribución normal bivariada con una correlación de 0,70. Crearemos un conjunto de entrenamiento de 200 muestras y un conjunto de prueba de 50:\nlibrary(tidymodels)\ntidymodels_prefer()\n\nsimulate_two_classes &lt;- \n  function (n, error = 0.1, eqn = quote(-1 - 2 * x - 0.2 * x^2 + 2 * y^2))  {\n    # Predictores ligeramente correlacionados\n    sigma &lt;- matrix(c(1, 0.7, 0.7, 1), nrow = 2, ncol = 2)\n    dat &lt;- MASS::mvrnorm(n = n, mu = c(0, 0), Sigma = sigma)\n    colnames(dat) &lt;- c(\"x\", \"y\")\n    cls &lt;- paste0(\"class_\", 1:2)\n    dat &lt;- \n      as_tibble(dat) %&gt;% \n      mutate(\n        linear_pred = !!eqn,\n        # Agregue algo de ruido de clasificación errónea\n        linear_pred = linear_pred + rnorm(n, sd = error),\n        prob = binomial()$linkinv(linear_pred),\n        class = ifelse(prob &gt; runif(n), cls[1], cls[2]),\n        class = factor(class, levels = cls)\n      )\n    dplyr::select(dat, x, y, class)\n  }\n\nset.seed(1901)\ntraining_set &lt;- simulate_two_classes(200)\ntesting_set  &lt;- simulate_two_classes(50)\nEstimamos un modelo de regresión logística utilizando métodos bayesianos (utilizando las distribuciones previas gaussianas predeterminadas para los parámetros):\ntwo_class_mod &lt;- \n  logistic_reg() %&gt;% \n  set_engine(\"stan\", seed = 1902) %&gt;% \n  fit(class ~ . + I(x^2)+ I(y^2), data = training_set)\nprint(two_class_mod, digits = 3)\n## parsnip model object\n## \n## stan_glm\n##  family:       binomial [logit]\n##  formula:      class ~ . + I(x^2) + I(y^2)\n##  observations: 200\n##  predictors:   5\n## ------\n##             Median MAD_SD\n## (Intercept)  1.092  0.287\n## x            2.290  0.423\n## y            0.314  0.354\n## I(x^2)       0.077  0.307\n## I(y^2)      -2.465  0.424\n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\nEl límite de clase ajustado se superpone al conjunto de prueba en Figura 19.1. Los puntos de datos más cercanos al límite de clase son los más inciertos. Si sus valores cambiaran ligeramente, su clase prevista podría cambiar. Un método simple para descalificar algunos resultados es llamarlos “equívocos” si los valores están dentro de algún rango alrededor del 50% (o el límite de probabilidad apropiado para una situación determinada). Dependiendo del problema al que se aplique el modelo, esto podría indicar que debemos recopilar otra medición o que necesitamos más información antes de que sea posible una predicción confiable.\nFigura 19.1: Conjunto de datos simulado de dos clases con ajuste de regresión logística y límite de decisión.\nPodríamos basar el ancho de la banda alrededor del límite en cómo mejora el rendimiento cuando se eliminan los resultados inciertos. Sin embargo, también debemos estimar la tasa reportable (la proporción esperada de resultados utilizables). Por ejemplo, no sería útil en situaciones del mundo real tener un rendimiento perfecto pero publicar predicciones en solo el 2% de las muestras pasadas al modelo.\nUtilicemos el conjunto de pruebas para determinar el equilibrio entre mejorar el rendimiento y tener suficientes resultados reportables. Las predicciones se crean usando:\ntest_pred &lt;- augment(two_class_mod, testing_set)\ntest_pred %&gt;% head()\n## # A tibble: 6 × 6\n##   .pred_class .pred_class_1 .pred_class_2      x      y class  \n##   &lt;fct&gt;               &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;  \n## 1 class_2           0.0256          0.974  1.12  -0.176 class_2\n## 2 class_1           0.555           0.445 -0.126 -0.582 class_2\n## 3 class_2           0.00620         0.994  1.92   0.615 class_2\n## 4 class_2           0.472           0.528 -0.400  0.252 class_2\n## 5 class_2           0.163           0.837  1.30   1.09  class_1\n## 6 class_2           0.0317          0.968  2.59   1.36  class_2\nCon tidymodels, el paquete probably contiene funciones para zonas equívocas. Para casos con dos clases, la función make_two_class_pred() crea una columna similar a un factor que tiene las clases predichas con una zona equívoca:\nlibrary(probably)\n\nlvls &lt;- levels(training_set$class)\n\ntest_pred &lt;- \n  test_pred %&gt;% \n  mutate(.pred_with_eqz = make_two_class_pred(.pred_class_1, lvls, buffer = 0.15))\n\ntest_pred %&gt;% count(.pred_with_eqz)\n## # A tibble: 3 × 2\n##   .pred_with_eqz     n\n##       &lt;clss_prd&gt; &lt;int&gt;\n## 1           [EQ]     9\n## 2        class_1    20\n## 3        class_2    21\nRows that are within \\(0.50\\pm0.15\\) are given a value of [EQ].\nDado que los niveles de los factores son los mismos que los de los datos originales, las matrices de confusión y otras estadísticas se pueden calcular sin errores. Cuando se utilizan funciones estándar del paquete yardstick, los resultados equívocos se convierten a NA y no se utilizan en los cálculos que utilizan predicciones de clases estrictas. Observe las diferencias en estas matrices de confusión:\n# Toda la información\ntest_pred %&gt;% conf_mat(class, .pred_class)\n##           Truth\n## Prediction class_1 class_2\n##    class_1      20       6\n##    class_2       5      19\n\n# Sólo resultados reportables:\ntest_pred %&gt;% conf_mat(class, .pred_with_eqz)\n##           Truth\n## Prediction class_1 class_2\n##    class_1      17       3\n##    class_2       5      16\nTambién está disponible una función is_equivocal() para filtrar estas filas de los datos.\n¿La zona equívoca ayuda a mejorar la precisión? Veamos diferentes tamaños de búfer, como se muestra en Figura 19.2:\n# Una función para cambiar el búfer y luego calcular el rendimiento.\neq_zone_results &lt;- function(buffer) {\n  test_pred &lt;- \n    test_pred %&gt;% \n    mutate(.pred_with_eqz = make_two_class_pred(.pred_class_1, lvls, buffer = buffer))\n  acc &lt;- test_pred %&gt;% accuracy(class, .pred_with_eqz)\n  rep_rate &lt;- reportable_rate(test_pred$.pred_with_eqz)\n  tibble(accuracy = acc$.estimate, reportable = rep_rate, buffer = buffer)\n}\n\n# Evaluar una secuencia de buffers y trazar los resultados.\nmap(seq(0, .1, length.out = 40), eq_zone_results) %&gt;% \n  list_rbind() %&gt;% \n  pivot_longer(c(-buffer), names_to = \"statistic\", values_to = \"value\") %&gt;% \n  ggplot(aes(x = buffer, y = value, lty = statistic)) + \n  geom_step(linewidth = 1.2, alpha = 0.8) + \n  labs(y = NULL, lty = NULL)\nFigura 19.2: El efecto de las zonas equívocas en el rendimiento del modelo.\nFigura 19.2 nos muestra que la precisión mejora en unos pocos puntos porcentuales, ¡pero a costa de que casi el 10% de las predicciones sean inutilizables! El valor de tal compromiso depende de cómo se utilizarán las predicciones del modelo.\nEste análisis se centró en utilizar la probabilidad de clase prevista para descalificar puntos, ya que esta es una medida fundamental de incertidumbre en los modelos de clasificación. Un enfoque ligeramente mejor sería utilizar el error estándar de la probabilidad de clase. Dado que utilizamos un modelo bayesiano, las estimaciones de probabilidad que encontramos son en realidad la media de la distribución predictiva posterior. En otras palabras, el modelo bayesiano nos da una distribución para la probabilidad de clase. Medir la desviación estándar de esta distribución nos da un error estándar de predicción de la probabilidad. En la mayoría de los casos, este valor está directamente relacionado con la probabilidad de clase media. Quizás recuerdes que, para una variable aleatoria de Bernoulli con probabilidad \\(p\\), la varianza es \\(p(1-p)\\). Debido a esta relación, el error estándar es mayor cuando la probabilidad es del 50%. En lugar de asignar un resultado equívoco utilizando la probabilidad de clase, podríamos utilizar un límite en el error estándar de predicción.\nUn aspecto importante del error estándar de predicción es que tiene en cuenta algo más que la probabilidad de clase. En los casos en los que hay una extrapolación significativa o valores predictivos aberrantes, el error estándar podría aumentar. El beneficio de utilizar el error estándar de predicción es que también podría señalar predicciones que son problemáticas (en lugar de simplemente inciertas). Una razón por la que utilizamos el modelo bayesiano es que estima naturalmente el error estándar de predicción; No muchos modelos pueden calcular esto. Para nuestro conjunto de prueba, usar type = \"pred_int\" producirá límites superior e inferior y std_error agrega una columna para esa cantidad. Para intervalos del 80%:\ntest_pred &lt;- \n  test_pred %&gt;% \n  bind_cols(\n    predict(two_class_mod, testing_set, type = \"pred_int\", std_error = TRUE)\n  )\nPara nuestro ejemplo donde el modelo y los datos se comportan bien, Figura 19.3 muestra el error estándar de predicción en todo el espacio:\nFigura 19.3: El efecto del error estándar de predicción superpuesto con los datos del conjunto de pruebas\nEl uso del error estándar como medida para evitar que se predigan muestras también se puede aplicar a modelos con resultados numéricos. Sin embargo, como se muestra en la siguiente sección, es posible que esto no siempre funcione.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>¿Cuándo Debería Confiar En Sus Predicciones?</span>"
    ]
  },
  {
    "objectID": "19-when-should-you-trust-predictions.html#sec-equivocal-zones",
    "href": "19-when-should-you-trust-predictions.html#sec-equivocal-zones",
    "title": "19  ¿Cuándo Debería Confiar En Sus Predicciones?",
    "section": "",
    "text": "En algunos casos, la cantidad de incertidumbre asociada con una predicción es demasiado alta para confiar en ella.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa notación [EQ] en este ejemplo no es un nivel de factor sino un atributo de esa columna.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>¿Cuándo Debería Confiar En Sus Predicciones?</span>"
    ]
  },
  {
    "objectID": "19-when-should-you-trust-predictions.html#sec-applicability-domains",
    "href": "19-when-should-you-trust-predictions.html#sec-applicability-domains",
    "title": "19  ¿Cuándo Debería Confiar En Sus Predicciones?",
    "section": "19.2 Determinación De La Aplicabilidad Del Modelo",
    "text": "19.2 Determinación De La Aplicabilidad Del Modelo\nLas zonas equívocas intentan medir la confiabilidad de una predicción basada en los resultados del modelo. Puede ser que las estadísticas del modelo, como el error estándar de predicción, no puedan medir el impacto de la extrapolación, por lo que necesitamos otra forma de evaluar si debemos confiar en una predicción y responder: “¿Es nuestro modelo aplicable para predecir un punto de datos específico?” Tomemos los datos del tren de Chicago utilizados ampliamente en Kuhn y Johnson (2019) y mostrados por primera vez en Sección 2.2 . El objetivo es predecir la cantidad de clientes que ingresan a la estación de tren de Clark y Lake cada día.\nEl conjunto de datos en el paquete modeldata (un paquete tidymodels con conjuntos de datos de ejemplo) tiene valores diarios entre January 22, 2001 y formato r(max(Chicago$fecha), \"%B %d, %Y\"). Creemos un pequeño conjunto de pruebas utilizando las últimas dos semanas de datos:\n\n## loads tanto el conjunto de datos de \"Chicago\" como las \"estaciones\"\ndata(Chicago)\n\nChicago &lt;- Chicago %&gt;% select(ridership, date, one_of(stations))\n\nn &lt;- nrow(Chicago)\n\nChicago_train &lt;- Chicago %&gt;% slice(1:(n - 14))\nChicago_test  &lt;- Chicago %&gt;% slice((n - 13):n)\n\nLos principales predictores son los datos retrasados ​​sobre el número de pasajeros en diferentes estaciones de tren, incluidas Clark y Lake, así como la fecha. Los predictores de número de pasajeros están altamente correlacionados entre sí. En la siguiente receta, la columna de fecha se amplía con varias características nuevas y los predictores de número de pasajeros se representan mediante componentes de mínimos cuadrados parciales (PLS). PLS (Geladi y Kowalski 1986), como comentamos en Sección 16.5.2, es una versión supervisada del análisis de componentes principales donde las nuevas características han sido descorrelacionadas pero predicen los datos de resultado.\nUtilizando los datos preprocesados, ajustamos un modelo lineal estándar:\n\nbase_recipe &lt;-\n  recipe(ridership ~ ., data = Chicago_train) %&gt;%\n  # Crear funciones de fecha\n  step_date(date) %&gt;%\n  step_holiday(date, keep_original_cols = FALSE) %&gt;%\n  # Crear variables ficticias a partir de columnas de factores\n  step_dummy(all_nominal()) %&gt;%\n  # Elimine cualquier columna con un único valor único\n  step_zv(all_predictors()) %&gt;%\n  step_normalize(!!!stations)%&gt;%\n  step_pls(!!!stations, num_comp = 10, outcome = vars(ridership))\n\nlm_spec &lt;-\n  linear_reg() %&gt;%\n  set_engine(\"lm\") \n\nlm_wflow &lt;-\n  workflow() %&gt;%\n  add_recipe(base_recipe) %&gt;%\n  add_model(lm_spec)\n\nset.seed(1902)\nlm_fit &lt;- fit(lm_wflow, data = Chicago_train)\n\n¿Qué tan bien encajan los datos en el conjunto de prueba? Podemos predecit() para que el conjunto de pruebas encuentre tanto predicciones como intervalos de predicción:\n\nres_test &lt;-\n  predict(lm_fit, Chicago_test) %&gt;%\n  bind_cols(\n    predict(lm_fit, Chicago_test, type = \"pred_int\"),\n    Chicago_test\n  )\n\nres_test %&gt;% select(date, ridership, starts_with(\".pred\"))\n## # A tibble: 14 × 5\n##   date       ridership .pred .pred_lower .pred_upper\n##   &lt;date&gt;         &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n## 1 2016-08-15     20.6  20.3        16.2         24.5\n## 2 2016-08-16     21.0  21.3        17.1         25.4\n## 3 2016-08-17     21.0  21.4        17.3         25.6\n## 4 2016-08-18     21.3  21.4        17.3         25.5\n## 5 2016-08-19     20.4  20.9        16.7         25.0\n## 6 2016-08-20      6.22  7.52        3.34        11.7\n## # ℹ 8 more rows\nres_test %&gt;% rmse(ridership, .pred)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 rmse    standard       0.865\n\nEstos son resultados bastante buenos. Figura 19.4 visualiza las predicciones junto con intervalos de predicción del 95%.\n\n\n\n\n\n\n\n\nFigura 19.4: Dos semanas de predicciones de 2016 para los datos de Chicago junto con intervalos de predicción del 95%\n\n\n\n\n\nDada la escala del número de usuarios, estos resultados parecen particularmente buenos para un modelo tan simple. Si se hubiera implementado este modelo, ¿qué tan bien habría funcionado unos años más tarde, en junio de 2020? El modelo realiza con éxito una predicción, como casi siempre lo hace un modelo predictivo cuando se le dan datos de entrada:\n\nres_2020 &lt;-\n  predict(lm_fit, Chicago_2020) %&gt;%\n  bind_cols(\n    predict(lm_fit, Chicago_2020, type = \"pred_int\"),\n    Chicago_2020\n  ) \n\nres_2020 %&gt;% select(date, contains(\".pred\"))\n## # A tibble: 14 × 4\n##   date       .pred .pred_lower .pred_upper\n##   &lt;date&gt;     &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n## 1 2020-06-01 20.1        15.9         24.3\n## 2 2020-06-02 21.4        17.2         25.6\n## 3 2020-06-03 21.5        17.3         25.6\n## 4 2020-06-04 21.3        17.1         25.4\n## 5 2020-06-05 20.7        16.6         24.9\n## 6 2020-06-06  9.04        4.88        13.2\n## # ℹ 8 more rows\n\nLos intervalos de predicción tienen aproximadamente el mismo ancho, aunque estos datos están mucho más allá del período de tiempo del conjunto de entrenamiento original. Sin embargo, dada la pandemia mundial en 2020, el desempeño según estos datos es pésimo:\n\nres_2020 %&gt;% select(date, ridership, starts_with(\".pred\"))\n## # A tibble: 14 × 5\n##   date       ridership .pred .pred_lower .pred_upper\n##   &lt;date&gt;         &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n## 1 2020-06-01     0.002 20.1        15.9         24.3\n## 2 2020-06-02     0.005 21.4        17.2         25.6\n## 3 2020-06-03     0.566 21.5        17.3         25.6\n## 4 2020-06-04     1.66  21.3        17.1         25.4\n## 5 2020-06-05     1.95  20.7        16.6         24.9\n## 6 2020-06-06     1.08   9.04        4.88        13.2\n## # ℹ 8 more rows\nres_2020 %&gt;% rmse(ridership, .pred)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 rmse    standard        17.2\n\nPuedes ver visualmente el terrible rendimiento de este modelo en Figura 19.5.\n\n\n\n\n\n\n\n\nFigura 19.5: Dos semanas de predicciones de 2020 para los datos de Chicago junto con intervalos de predicción del 95%\n\n\n\n\n\nLos intervalos de confianza y predicción para la regresión lineal se expanden a medida que los datos se alejan cada vez más del centro del conjunto de entrenamiento. Sin embargo, ese efecto no es lo suficientemente dramático como para señalar que estas predicciones son deficientes.\n\nA veces las estadísticas producidas por los modelos no miden muy bien la calidad de las predicciones.\n\nEsta situación se puede evitar teniendo una metodología secundaria que pueda cuantificar qué tan aplicable es el modelo para cualquier nueva predicción (es decir, el dominio de aplicabilidad del modelo). Existe una variedad de métodos para calcular un modelo de dominio de aplicabilidad, como Jaworska, Nikolova-Jeliazkova, y Aldenberg (2005) o Netzeva et al. (2005). El enfoque utilizado en este capítulo es un método no supervisado bastante simple que intenta medir cuánto (si lo hay) un nuevo punto de datos está más allá de los datos de entrenamiento.1\n\nLa idea es acompañar una predicción con una puntuación que mida qué tan similar es el nuevo punto al conjunto de entrenamiento.\n\nUn método que funciona bien utiliza el análisis de componentes principales (PCA) en los valores predictivos numéricos. Ilustraremos el proceso utilizando sólo dos de los predictores que corresponden al número de pasajeros en diferentes estaciones (estaciones de California y Austin). El conjunto de entrenamiento se muestra en el panel (a) en Figura 19.6. Los datos sobre el número de pasajeros de estas estaciones están altamente correlacionados y las dos distribuciones que se muestran en el diagrama de dispersión corresponden al número de pasajeros los fines de semana y días laborables.\nEl primer paso es realizar PCA con los datos de entrenamiento. Las puntuaciones de PCA para el conjunto de entrenamiento se muestran en el panel (b) en Figura 19.6. A continuación, utilizando estos resultados, medimos la distancia de cada punto de ajuste de entrenamiento al centro de los datos de PCA (panel (c) de Figura 19.6). Luego podemos usar esta distribución de referencia (panel (d) de Figura 19.6) para estimar qué tan lejos está un punto de datos de la corriente principal de los datos de entrenamiento.\n\n\n\n\n\n\n\n\nFigura 19.6: La distribución de referencia de PCA basada en el conjunto de entrenamiento.\n\n\n\n\n\nPara una nueva muestra, las puntuaciones de PCA se calculan junto con la distancia al centro del conjunto de entrenamiento.\nSin embargo, ¿qué significa que una nueva muestra tenga una distancia de X? Dado que los componentes de PCA pueden tener diferentes rangos de un conjunto de datos a otro, no existe un límite obvio para decir que una distancia es demasiado grande.\nUn enfoque es tratar las distancias de los datos del conjunto de entrenamiento como “normales”. Para muestras nuevas, podemos determinar cómo se compara la nueva distancia con el rango en la distribución de referencia (del conjunto de entrenamiento). Se puede calcular un percentil para nuevas muestras que refleje qué parte del conjunto de entrenamiento es menos extremo que las nuevas muestras.\n\nUn percentil del 90% significa que la mayoría de los datos del conjunto de entrenamiento están más cerca del centro de datos que la nueva muestra.\n\nEl gráfico en Figura 19.7 superpone una muestra del conjunto de prueba (triángulo y línea discontinua) y una muestra de 2020 (círculo y línea continua) con las distancias PCA del conjunto de entrenamiento.\n\n\n\n\n\n\n\n\nFigura 19.7: La distribución de referencia con dos nuevos puntos: uno usando el conjunto de prueba y otro a partir de los datos de 2020\n\n\n\n\n\nEl punto de ajuste de prueba tiene una distancia de 1.28. Está en el percentil 51.8% de la distribución del conjunto de entrenamiento, lo que indica que está cómodamente dentro de la corriente principal del conjunto de entrenamiento.\nLa muestra de 2020 está más alejada del centro que cualquiera de las muestras del conjunto de entrenamiento (con un percentil de 100%). Esto indica que la muestra es muy extrema y que su predicción correspondiente sería una extrapolación severa (y probablemente no debería informarse).\nEl paquete applicable puede desarrollar un modelo de dominio de aplicabilidad utilizando PCA. Usaremos los predictores de número de pasajeros de 20 estaciones retrasadas como datos de entrada para el análisis de PCA. Hay un argumento adicional llamado umbral, threshold, que determina cuántos componentes se utilizan en el cálculo de la distancia. Para nuestro ejemplo, usaremos un valor grande que indica que debemos usar suficientes componentes para representar el 99 % de la variación en los predictores de número de pasajeros:\n\nlibrary(applicable)\npca_stat &lt;- apd_pca(~ ., data = Chicago_train %&gt;% select(one_of(stations)), \n                    threshold = 0.99)\npca_stat\n## # Predictors:\n##    20\n## # Principal Components:\n##    9 components were needed\n##    to capture at least 99% of the\n##    total variation in the predictors.\n\nEl método autoplot() traza la distribución de referencia. Tiene un argumento opcional para qué datos trazar. Agregaremos un valor de distancia, distance para trazar solo la distribución de distancia del conjunto de entrenamiento. Este código genera la trama en Figura 19.8:\n\nautoplot(pca_stat, distance) + labs(x = \"distance\")\n\n\n\n\n\n\n\n\n\nFigura 19.8: Los resultados de usar el método autoplot() en un objeto aplicable\n\n\n\n\n\nEl eje x muestra los valores de la distancia y el eje y muestra los percentiles de la distribución. Por ejemplo, la mitad de las muestras del conjunto de entrenamiento tenían distancias menores que 3.7.\nPara calcular los percentiles de datos nuevos, la función score() funciona de la misma manera que predict():\n\nscore(pca_stat, Chicago_test) %&gt;% select(starts_with(\"distance\"))\n## # A tibble: 14 × 2\n##   distance distance_pctl\n##      &lt;dbl&gt;         &lt;dbl&gt;\n## 1     4.88          66.7\n## 2     5.21          71.4\n## 3     5.19          71.1\n## 4     5.00          68.5\n## 5     4.36          59.3\n## 6     4.10          55.2\n## # ℹ 8 more rows\n\nEstos parecen bastante razonables. Para los datos de 2020:\n\nscore(pca_stat, Chicago_2020) %&gt;% select(starts_with(\"distance\"))\n## # A tibble: 14 × 2\n##   distance distance_pctl\n##      &lt;dbl&gt;         &lt;dbl&gt;\n## 1     9.39          99.8\n## 2     9.40          99.8\n## 3     9.30          99.7\n## 4     9.30          99.7\n## 5     9.29          99.7\n## 6    10.1            1  \n## # ℹ 8 more rows\n\nLos valores de distancia de 2020 indican que estos valores predictivos están fuera de la gran mayoría de los datos vistos por el modelo en el momento del entrenamiento. Estos deben señalarse para que las predicciones no se informen en absoluto o se vean con escepticismo.\n\nUn aspecto importante de este análisis se refiere a qué predictores se utilizan para desarrollar el modelo de dominio de aplicabilidad. En nuestro análisis, utilizamos las columnas predictoras sin procesar. Sin embargo, al construir el modelo, se utilizaron en su lugar características de puntuación PLS. ¿Cuál de estos debería usar apd_pca()? La función apd_pca() también puede tomar una receta como entrada (en lugar de una fórmula) para que las distancias reflejen las puntuaciones PLS en lugar de las columnas predictivas individuales. Puede evaluar ambos métodos para comprender cuál ofrece resultados más relevantes.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>¿Cuándo Debería Confiar En Sus Predicciones?</span>"
    ]
  },
  {
    "objectID": "19-when-should-you-trust-predictions.html#sec-trust-summary",
    "href": "19-when-should-you-trust-predictions.html#sec-trust-summary",
    "title": "19  ¿Cuándo Debería Confiar En Sus Predicciones?",
    "section": "19.3 Resumen Del Capítulo",
    "text": "19.3 Resumen Del Capítulo\nEste capítulo mostró dos métodos para evaluar si las predicciones deben informarse a los consumidores de modelos. Las zonas equívocas se ocupan de resultados/predicciones y pueden resultar útiles cuando la cantidad de incertidumbre en una predicción es demasiado grande.\nLos modelos de dominio de aplicabilidad tratan con características/predictores y cuantifican la cantidad de extrapolación (si la hay) que ocurre al hacer una predicción. Este capítulo mostró un método básico que utiliza el análisis de componentes principales, aunque hay muchas otras formas de medir la aplicabilidad. El paquete applicable también contiene métodos especializados para conjuntos de datos donde todos los predictores son binarios. Este método calcula puntuaciones de similitud entre los puntos de datos del conjunto de entrenamiento para definir la distribución de referencia.\n\n\n\n\nBartley, E AND Schliep, M . AND Hanks. 2019. «Identifying and characterizing extrapolation in multivariate response data». PLOS ONE 14 (diciembre): 1-20.\n\n\nDanowski, T, J Aarons, J Hydovitz, y J Wingert. 1970. «Utility of equivocal glucose tolerances». Diabetes 19 (7): 524-26.\n\n\nGeladi, P., y B Kowalski. 1986. «Partial Least-Squares Regression: A Tutorial». Analytica Chimica Acta 185: 1-17.\n\n\nJaworska, J, N Nikolova-Jeliazkova, y T Aldenberg. 2005. «QSAR Applicability Domain Estimation by Projection of the Training Set in Descriptor Space: A Review». Alternatives to Laboratory Animals 33 (5): 445-59.\n\n\nKerleguer, A., J.-L. Koeck, M. Fabre, P. Gérôme, R. Teyssou, y V. Hervé. 2003. «Use of equivocal zone in interpretation of results of the amplified Mycobacterium Tuberculosis direct test for diagnosis of tuberculosis». Journal of Clinical Microbiology 41 (4): 1783-84.\n\n\nNetzeva, T, A Worth, T Aldenberg, R Benigni, M Cronin, P Gramatica, J Jaworska, et al. 2005. «Current Status of Methods for Defining the Applicability Domain of (Quantitative) Structure-Activity Relationships: The Report and Recommendations of ECVAM Workshop 52». Alternatives to Laboratory Animals 33 (2): 155-73.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>¿Cuándo Debería Confiar En Sus Predicciones?</span>"
    ]
  },
  {
    "objectID": "19-when-should-you-trust-predictions.html#footnotes",
    "href": "19-when-should-you-trust-predictions.html#footnotes",
    "title": "19  ¿Cuándo Debería Confiar En Sus Predicciones?",
    "section": "",
    "text": "Bartley (2019) muestra otro método más y lo aplica a estudios ecológicos.↩︎",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>¿Cuándo Debería Confiar En Sus Predicciones?</span>"
    ]
  },
  {
    "objectID": "20-ensemble-models.html",
    "href": "20-ensemble-models.html",
    "title": "20  Conjuntos De Modelos",
    "section": "",
    "text": "20.1 Crear El Conjunto De Entrenamiento Para Apilar\nEl primer paso para construir un conjunto apilado se basa en las predicciones del conjunto de evaluación a partir de un esquema de remuestreo con múltiples divisiones. Para cada punto de datos en el conjunto de entrenamiento, el apilamiento requiere algún tipo de predicción fuera de la muestra. Para los modelos de regresión, este es el resultado previsto. Para los modelos de clasificación, las clases o probabilidades predichas están disponibles para su uso, aunque estas últimas contienen más información que las predicciones de clases estrictas. Para un conjunto de modelos, se ensambla un conjunto de datos donde las filas son las muestras del conjunto de entrenamiento y las columnas son las predicciones fuera de la muestra del conjunto de múltiples modelos.\nEn el Capítulo 15, utilizamos cinco repeticiones de validación cruzada 10 veces para volver a muestrear los datos. Este esquema de remuestreo genera cinco predicciones de conjuntos de evaluación para cada muestra de conjunto de entrenamiento. Pueden ocurrir múltiples predicciones fuera de la muestra en varias otras técnicas de remuestreo (por ejemplo, arranque). A los efectos del apilamiento, cualquier predicción replicada para un punto de datos en el conjunto de entrenamiento se promedia de modo que haya una única predicción por muestra del conjunto de entrenamiento por miembro candidato.\nPara el ejemplo concreto, el conjunto de entrenamiento utilizado para el apilamiento de modelos tiene columnas para todos los resultados de los parámetros de ajuste candidatos. Tabla 20.1 presenta las primeras seis filas y columnas seleccionadas.\nTabla 20.1: Predicciones a partir de configuraciones de parámetros de ajuste candidatos.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicciones Candidatas Apiladas\n\n\n\nMuestra #\nBagged Tree\nMARS 1\nMARS 2\nCubist 1\n...\nCubist 25\n...\n\n\n\n\n1\n25.18\n17.92\n17.21\n17.79\n\n17.82\n\n\n\n2\n5.18\n-1.77\n-0.74\n2.83\n\n3.87\n\n\n\n3\n9.71\n7.26\n5.91\n6.31\n\n8.60\n\n\n\n4\n25.21\n20.93\n21.52\n23.72\n\n21.61\n\n\n\n5\n6.33\n1.53\n0.15\n3.60\n\n4.57\n\n\n\n6\n7.88\n4.88\n1.74\n7.69\n\n7.55\nHay una sola columna para el modelo de árbol en bolsas ya que no tiene parámetros de ajuste. Además, recuerde que MARS se sintonizó en función de un único parámetro (el grado del producto) con dos configuraciones posibles, por lo que este modelo está representado por dos columnas. La mayoría de los otros modelos tienen 25 columnas correspondientes, como se muestra para Cubist en este ejemplo.\nPara resumir dónde nos encontramos hasta ahora, el primer paso para el apilamiento es ensamblar las predicciones del conjunto de evaluación para el conjunto de entrenamiento de cada modelo candidato. Podemos utilizar estas predicciones de conjuntos de evaluación para avanzar y construir un conjunto apilado.\nPara comenzar a ensamblar con el paquete stacks, cree una pila de datos vacía usando la función stacks() y luego agregue modelos candidatos. Recuerde que utilizamos conjuntos de flujos de trabajo para ajustar una amplia variedad de modelos a estos datos. Usaremos los resultados de las carreras:\nrace_results\n## # A workflow set/tibble: 12 × 4\n##   wflow_id    info             option    result   \n##   &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n## 1 MARS        &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n## 2 CART        &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n## 3 CART_bagged &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;rsmp[+]&gt;\n## 4 RF          &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n## 5 boosting    &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n## 6 Cubist      &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n## # ℹ 6 more rows\nEn este caso, nuestra sintaxis es:\nlibrary(tidymodels)\nlibrary(stacks)\ntidymodels_prefer()\n\nconcrete_stack &lt;- \n  stacks() %&gt;% \n  add_candidates(race_results)\n\nconcrete_stack\n## # A data stack with 12 model definitions and 22 candidate members:\n## #   MARS: 1 model configuration\n## #   CART: 1 model configuration\n## #   CART_bagged: 1 model configuration\n## #   RF: 1 model configuration\n## #   boosting: 1 model configuration\n## #   Cubist: 1 model configuration\n## #   SVM_radial: 1 model configuration\n## #   SVM_poly: 1 model configuration\n## #   KNN: 3 model configurations\n## #   neural_network: 5 model configurations\n## #   full_quad_linear_reg: 5 model configurations\n## #   full_quad_KNN: 1 model configuration\n## # Outcome: compressive_strength (numeric)\nRecuerde que los métodos de carrera (Sección 13.5.5) son más eficientes ya que es posible que no evalúen todas las configuraciones en todos los remuestreos. El apilamiento requiere que todos los miembros candidatos tengan el conjunto completo de remuestras. add_candidates() incluye solo las configuraciones del modelo que tienen resultados completos.\nSi no hubiéramos usado el paquete workflowsets, los objetos de tune y finetune también podrían pasarse a add_candidates(). Esto puede incluir objetos de búsqueda iterativos y de cuadrícula.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conjuntos De Modelos</span>"
    ]
  },
  {
    "objectID": "20-ensemble-models.html#sec-data-stack",
    "href": "20-ensemble-models.html#sec-data-stack",
    "title": "20  Conjuntos De Modelos",
    "section": "",
    "text": "También se pueden utilizar conjuntos de validación simples con el apilamiento, ya que tidymodels considera que se trata de un remuestreo único.\n\n\n\n\n\nPara los modelos de clasificación, las columnas de predicción candidatas serían probabilidades de clase predichas. Dado que estas columnas suman una para cada modelo, las probabilidades de una de las clases pueden omitirse.\n\n\n\n\n\n\n\n\n¿Por qué utilizar los resultados de las carreras en lugar del conjunto completo de modelos candidatos contenidos en grid_results? Se puede utilizar cualquiera de los dos. Encontramos un mejor rendimiento para estos datos utilizando los resultados de las carreras. Esto podría deberse a que el método de carrera preselecciona los mejores modelos de la parrilla más grande.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conjuntos De Modelos</span>"
    ]
  },
  {
    "objectID": "20-ensemble-models.html#sec-blend-predictions",
    "href": "20-ensemble-models.html#sec-blend-predictions",
    "title": "20  Conjuntos De Modelos",
    "section": "20.2 Combina Las Predicciones",
    "text": "20.2 Combina Las Predicciones\nLas predicciones del conjunto de entrenamiento y los datos de resultados observados correspondientes se utilizan para crear un modelo de metaaprendizaje donde las predicciones del conjunto de evaluación son los predictores de los datos de resultados observados. El metaaprendizaje se puede lograr utilizando cualquier modelo. El modelo más utilizado es un modelo lineal generalizado regularizado, que abarca modelos lineales, logísticos y multinomiales. Específicamente, la regularización mediante la penalización de lazo (Tibshirani 1996), que utiliza la contracción para atraer puntos hacia un valor central, tiene varias ventajas:\n\nEl uso de la penalización de lazo puede eliminar candidatos (y, a veces, tipos de modelos completos) del conjunto.\nLa correlación entre los candidatos a conjuntos tiende a ser muy alta y la regularización ayuda a aliviar este problema.\n\nBreiman (1996b) también sugirió que, cuando se utiliza un modelo lineal para combinar las predicciones, podría ser útil restringir los coeficientes de combinación para que no sean negativos. En general, hemos encontrado que este es un buen consejo y es el valor predeterminado para el paquete stacks (pero se puede cambiar mediante un argumento opcional).\nDado que nuestro resultado es numérico, se utiliza la regresión lineal para el metamodelo. Ajustar el metamodelo es tan sencillo como usar:\n\nset.seed(2001)\nens &lt;- blend_predictions(concrete_stack)\n\nEsto evalúa el modelo de metaaprendizaje sobre una cuadrícula predefinida de valores de penalización de lazo y utiliza un método de remuestreo interno para determinar el mejor valor. El método autoplot(), que se muestra en Figura 20.1, nos ayuda a comprender si el método de penalización predeterminado fue suficiente:\n\nautoplot(ens)\n\n\n\n\n\n\n\n\n\nFigura 20.1: Resultados del uso del método autoplot() en el objeto de pilas combinadas\n\n\n\n\n\nEl panel superior de Figura 20.1 muestra el número promedio de miembros candidatos del conjunto retenidos por el modelo de metaaprendizaje. Podemos ver que el número de miembros es bastante constante y, a medida que aumenta, el RMSE también aumenta.\nEs posible que el rango predeterminado no nos haya servido bien aquí. Para evaluar el modelo de metaaprendizaje con penalizaciones mayores, pasemos una opción adicional:\n\nset.seed(2002)\nens &lt;- blend_predictions(concrete_stack, penalty = 10^seq(-2, -0.5, length = 20))\n\nAhora, en Figura 20.2, vemos un rango en el que el modelo de conjunto se vuelve peor que con nuestra primera combinación (pero no mucho). Los valores de \\(R^2\\) aumentan con más miembros y sanciones mayores.\n\nautoplot(ens)\n\n\n\n\n\n\n\n\n\nFigura 20.2: Los resultados del uso del método autoplot() en el objeto de pilas combinadas actualizado\n\n\n\n\n\nAl combinar predicciones utilizando un modelo de regresión, es común restringir los parámetros de combinación para que no sean negativos. Para estos datos, esta restricción tiene el efecto de eliminar muchos de los miembros potenciales del conjunto; Incluso con sanciones bastante bajas, el conjunto se limita a una fracción de los dieciocho originales.\nEl valor de penalización asociado con el RMSE más pequeño fue 0.074. Imprimir el objeto muestra los detalles del modelo de metaaprendizaje:\n\nens\n## ── A stacked ensemble model ─────────────────────────────────────\n## \n## \n## Out of 22 possible candidate members, the ensemble retained 7.\n## \n## Penalty: 0.0738619982207936.\n## \n## Mixture: 1.\n## \n## \n## The 7 highest weighted members are:\n## # A tibble: 7 × 3\n##   member                    type          weight\n##   &lt;chr&gt;                     &lt;chr&gt;          &lt;dbl&gt;\n## 1 boosting_1_04             boost_tree   0.760  \n## 2 neural_network_1_12       mlp          0.119  \n## 3 Cubist_1_25               cubist_rules 0.0693 \n## 4 neural_network_1_04       mlp          0.0471 \n## 5 full_quad_linear_reg_1_16 linear_reg   0.0149 \n## 6 neural_network_1_17       mlp          0.00708\n## # ℹ 1 more row\n## \n## Members have not yet been fitted with `fit_members()`.\n\nEl modelo de metaaprendizaje de regresión lineal regularizado contenía coeficientes de combinación seven entre tipos de modelos four. El método autoplot() se puede utilizar nuevamente para mostrar las contribuciones de cada tipo de modelo, para producir Figura 20.3.\n\nautoplot(ens, \"weights\") +\n  geom_text(aes(x = weight + 0.01, label = model), hjust = 0) + \n  theme(legend.position = \"none\") +\n  lims(x = c(-0.01, 0.8))\n\n\n\n\n\n\n\n\n\nFigura 20.3: Coeficientes de mezcla para el conjunto de apilamiento.\n\n\n\n\n\nLos modelos boosted tree and neural network tienen las mayores contribuciones al conjunto. Para este conjunto, el resultado se predice con la ecuación:\n\n\\[\\begin{align}\n\\text{ensemble prediction} &=-0.69 \\\\\n    +&0.76 \\times \\text{boost tree prediction} \\notag \\\\\n    +&0.1192 \\times \\text{mlp prediction (config 2)} \\notag \\\\\n    +&0.069 \\times \\text{cubist rules prediction} \\notag \\\\\n    +&0.0471 \\times \\text{mlp prediction (config 1)} \\notag \\\\\n    +&0.0149 \\times \\text{linear reg prediction (config 1)} \\notag \\\\\n    +&0.0071 \\times \\text{mlp prediction (config 3)} \\notag \\\\\n    +&0.0024 \\times \\text{linear reg prediction (config 2)} \\notag\n\\end{align}\\]\n\ndonde los predictores en la ecuación son los valores de resistencia a la compresión pronosticados a partir de esos modelos.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conjuntos De Modelos</span>"
    ]
  },
  {
    "objectID": "20-ensemble-models.html#sec-fit-members",
    "href": "20-ensemble-models.html#sec-fit-members",
    "title": "20  Conjuntos De Modelos",
    "section": "20.3 Ajustar Los Modelos Miembros",
    "text": "20.3 Ajustar Los Modelos Miembros\nEl conjunto contiene miembros candidatos seven y ahora sabemos cómo se pueden combinar sus predicciones en una predicción final para el conjunto. Sin embargo, estos ajustes de modelos individuales aún no se han creado. Para poder utilizar el modelo de apilamiento, se requieren ajustes de modelo adicionales seven. Estos utilizan todo el conjunto de entrenamiento con los predictores originales.\nLos modelos seven a ajustar son:\n\n\nboosting: number of trees = 1957, minimal node size = 8, tree depth = 7, learning rate = 0.0756, minimum loss reduction = 1.45e-07, and proportion of observations sampled = 0.679\nCubist: number of committees = 98 and number of nearest neighbors = 2\nlinear regression (quadratic features): amount of regularization = 6.28e-09 and proportion of lasso penalty = 0.636 (config 1)\nlinear regression (quadratic features): amount of regularization = 2e-09 and proportion of lasso penalty = 0.668 (config 2)\nneural network: number of hidden units = 14, amount of regularization = 0.0345, and number of epochs = 979 (config 1)\nneural network: number of hidden units = 22, amount of regularization = 2.08e-10, and number of epochs = 92 (config 2)\nneural network: number of hidden units = 26, amount of regularization = 0.0149, and number of epochs = 203 (config 3)\n\n\nEl paquete stacks tiene una función, fit_members(), que entrena y devuelve estos modelos:\n\nens &lt;- fit_members(ens)\n\nEsto actualiza el objeto de apilamiento con los objetos de flujo de trabajo ajustados para cada miembro. En este punto, el modelo de apilamiento se puede utilizar para la predicción.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conjuntos De Modelos</span>"
    ]
  },
  {
    "objectID": "20-ensemble-models.html#resultados-del-conjunto-de-pruebas",
    "href": "20-ensemble-models.html#resultados-del-conjunto-de-pruebas",
    "title": "20  Conjuntos De Modelos",
    "section": "20.4 Resultados Del Conjunto De Pruebas",
    "text": "20.4 Resultados Del Conjunto De Pruebas\nDado que el proceso de combinación utilizó remuestreo, podemos estimar que el conjunto con miembros seven tenía un RMSE estimado de 4.11. Recuerde del Capítulo 15 que el árbol mejor impulsado tenía un conjunto de prueba RMSE de 3.41. ¿Cómo se comparará el modelo de conjunto en el conjunto de prueba? Podemos usar predecit() para averiguarlo:\n\nreg_metrics &lt;- metric_set(rmse, rsq)\nens_test_pred &lt;- \n  predict(ens, concrete_test) %&gt;% \n  bind_cols(concrete_test)\n\nens_test_pred %&gt;% \n  reg_metrics(compressive_strength, .pred)\n## # A tibble: 2 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 rmse    standard       3.40 \n## 2 rsq     standard       0.955\n\nEsto es moderadamente mejor que nuestro mejor modelo individual. Es bastante común que el apilamiento produzca beneficios incrementales en comparación con el mejor modelo individual.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conjuntos De Modelos</span>"
    ]
  },
  {
    "objectID": "20-ensemble-models.html#sec-ensembles-summary",
    "href": "20-ensemble-models.html#sec-ensembles-summary",
    "title": "20  Conjuntos De Modelos",
    "section": "20.5 Resumen Del Capítulo",
    "text": "20.5 Resumen Del Capítulo\nEste capítulo demostró cómo combinar diferentes modelos en un conjunto para obtener un mejor rendimiento predictivo. El proceso de creación del conjunto puede eliminar automáticamente los modelos candidatos para encontrar un pequeño subconjunto que mejore el rendimiento. El paquete stacks tiene una interfaz fluida para combinar resultados de remuestreo y ajuste en un metamodelo.\n\n\n\n\nBreiman, L. 1996a. «Bagging predictors». Machine learning 24 (2): 123-40.\n\n\n———. 1996b. «Stacked regressions». Machine Learning 24 (1): 49-64.\n\n\n———. 2001. «Random forests». Machine learning 45 (1): 5-32.\n\n\nFreund, Y, y R Schapire. 1997. «A decision-theoretic generalization of on-line learning and an application to boosting». Journal of Computer and System Sciences 55 (1): 119-39.\n\n\nHo, T. 1995. «Random decision forests». En Proceedings of 3rd International Conference on Document Analysis and Recognition, 1:278-82. IEEE.\n\n\nTibshirani, Robert. 1996. «Regression Shrinkage and Selection via the Lasso». Journal of the Royal Statistical Society. Series B (Methodological) 58 (1): 267-88. http://www.jstor.org/stable/2346178.\n\n\nWolpert, D. 1992. «Stacked generalization». Neural Networks 5 (2): 241-59.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Conjuntos De Modelos</span>"
    ]
  },
  {
    "objectID": "21-inferential-analysis.html",
    "href": "21-inferential-analysis.html",
    "title": "21  Análisis Inferencial",
    "section": "",
    "text": "21.1 Inferencia Para Datos De Recuento\nPara comprender cómo se pueden utilizar los paquetes tidymodels para el modelado inferencial, centrémonos en un ejemplo con datos de recuento. Usaremos datos de publicaciones de bioquímica del paquete pscl. Estos datos consisten en información sobre 915 Ph.D. se gradúan en bioquímica e intenta explicar los factores que impactan su productividad académica (medida a través del número o recuento de artículos publicados en tres años). Los predictores incluyen el género del graduado, su estado civil, el número de hijos del graduado que tengan al menos cinco años, el prestigio de su departamento y el número de artículos producidos por su mentor en el mismo período de tiempo. Los datos reflejan doctorados en bioquímica que terminaron su educación entre 1956 y 1963. Los datos son una muestra algo sesgada de todos los doctorados en bioquímica otorgados durante este período (basado en la integridad de la información).\nUn gráfico de los datos mostrados en Figura 21.1 indica que muchos graduados no publicaron ningún artículo durante este tiempo y que el resultado sigue una distribución sesgada a la derecha:\nlibrary(tidymodels)\ntidymodels_prefer()\n\ndata(\"bioChemists\", package = \"pscl\")\n\nggplot(bioChemists, aes(x = art)) + \n  geom_histogram(binwidth = 1, color = \"white\") + \n  labs(x = \"Número de artículos dentro de los 3 años posteriores a la graduación\")\nFigura 21.1: Distribución del número de artículos escritos dentro de los 3 años posteriores a la graduación.\nDado que los datos de los resultados son recuentos, la suposición de distribución más común es que el resultado tiene una distribución de Poisson. En este capítulo se utilizarán estos datos para varios tipos de análisis.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Análisis Inferencial</span>"
    ]
  },
  {
    "objectID": "21-inferential-analysis.html#inferencia-para-datos-de-recuento",
    "href": "21-inferential-analysis.html#inferencia-para-datos-de-recuento",
    "title": "21  Análisis Inferencial",
    "section": "",
    "text": "Recuerde que en el Capítulo 19 hicimos la pregunta “¿Es nuestro modelo aplicable para predecir un punto de datos específico?” Es muy importante definir a qué poblaciones se aplica un análisis inferencial. Para estos datos, los resultados probablemente se aplicarían a los doctorados en bioquímica dados aproximadamente en el período en que se recopilaron los datos. ¿Se aplica también a otros tipos de doctorado en química (por ejemplo, química medicinal, etc.)? Éstas son preguntas importantes que se deben abordar (y documentar) al realizar análisis inferenciales.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Análisis Inferencial</span>"
    ]
  },
  {
    "objectID": "21-inferential-analysis.html#comparaciones-con-pruebas-de-dos-muestras",
    "href": "21-inferential-analysis.html#comparaciones-con-pruebas-de-dos-muestras",
    "title": "21  Análisis Inferencial",
    "section": "21.2 Comparaciones Con Pruebas De Dos Muestras",
    "text": "21.2 Comparaciones Con Pruebas De Dos Muestras\nPodemos comenzar con la prueba de hipótesis. El objetivo del autor original con este conjunto de datos sobre publicaciones de bioquímica era determinar si existe una diferencia en las publicaciones entre hombres y mujeres (Long 1992). Los datos del estudio muestran:\n\nbioChemists %&gt;% \n  group_by(fem) %&gt;% \n  summarize(counts = sum(art), n = length(art))\n## # A tibble: 2 × 3\n##   fem   counts     n\n##   &lt;fct&gt;  &lt;int&gt; &lt;int&gt;\n## 1 Men      930   494\n## 2 Women    619   421\n\nHabía muchas más publicaciones de hombres, aunque también había más hombres en los datos. El enfoque más simple para analizar estos datos sería hacer una comparación de dos muestras usando la función poisson.test() en el paquete stats. Requiere los conteos para uno o dos grupos.\nPara nuestra aplicación, las hipótesis para comparar los dos sexos son:\n\\[\\begin{align}\nH_0&: \\lambda_m = \\lambda_f \\notag \\\\\nH_a&: \\lambda_m \\ne \\lambda_f \\notag\n\\end{align}\\]\ndonde los valores \\(\\lambda\\) son las tasas de publicaciones (durante el mismo período de tiempo).\nUna aplicación básica de la prueba es:Una aplicación básica de la prueba es:1\n\npoisson.test(c(930, 619), T = 3)\n## \n##  Comparison of Poisson rates\n## \n## data:  c(930, 619) time base: 3\n## count1 = 930, expected count1 = 774, p-value = 3e-15\n## alternative hypothesis: true rate ratio is not equal to 1\n## 95 percent confidence interval:\n##  1.356 1.666\n## sample estimates:\n## rate ratio \n##      1.502\n\nLa función informa un valor p así como un intervalo de confianza para la relación de las tasas de publicación. Los resultados indican que la diferencia observada es mayor que el ruido experiencial y favorece a \\(H_a\\).\nUn problema con el uso de esta función es que los resultados regresan como un objeto “htest”. Si bien este tipo de objeto tiene una estructura bien definida, puede resultar difícil consumirlo para operaciones posteriores, como informes o visualizaciones. La herramienta más impactante que ofrece tidymodels para modelos inferenciales son las funciones tidy() en el paquete broom. Como se vio anteriormente, esta función crea un tibble bien formado y con un nombre predecible a partir del objeto. Podemos tidy() los resultados de nuestra prueba de comparación de dos muestras:\n\npoisson.test(c(930, 619)) %&gt;% \n  tidy()\n## # A tibble: 1 × 8\n##   estimate statistic  p.value parameter conf.low conf.high method        alternative\n##      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;      \n## 1     1.50       930 2.73e-15      774.     1.36      1.67 Comparison o… two.sided\n\n\nEntre broom y broom.mixed, existen métodos tidy() para más de 150 modelos.\n\nSi bien la distribución de Poisson es razonable, es posible que también deseemos evaluarla utilizando menos supuestos distributivos. Dos métodos que podrían resultar útiles son las pruebas de arranque y de permutación (Davison y Hinkley 1997).\nEl paquete infer, parte del marco tidymodels, es una herramienta poderosa e intuitiva para probar hipótesis (Ismay y Kim 2021). Su sintaxis es concisa y está diseñada para no estadísticos.\nPrimero, specify() que usaremos la diferencia en el número medio de artículos entre los sexos y luego calculate() la estadística a partir de los datos. Recuerde que el estimador de máxima verosimilitud para la media de Poisson es la media muestral. Las hipótesis probadas aquí son las mismas que las de la prueba anterior (pero se llevan a cabo mediante un procedimiento de prueba diferente).\nCon infer, especificamos el resultado y la covariable, luego indicamos la estadística de interés:\n\nlibrary(infer)\n\nobserved &lt;- \n  bioChemists %&gt;%\n  specify(art ~ fem) %&gt;%\n  calculate(stat = \"diff in means\", order = c(\"Men\", \"Women\"))\nobserved\n## Response: art (numeric)\n## Explanatory: fem (factor)\n## # A tibble: 1 × 1\n##    stat\n##   &lt;dbl&gt;\n## 1 0.412\n\nA partir de aquí, calculamos un intervalo de confianza para esta media creando la distribución de arranque mediante generate(); se calcula la misma estadística para cada versión remuestreada de los datos:\n\nset.seed(2101)\nbootstrapped &lt;- \n  bioChemists %&gt;%\n  specify(art ~ fem)  %&gt;%\n  generate(reps = 2000, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"diff in means\", order = c(\"Men\", \"Women\"))\nbootstrapped\n## Response: art (numeric)\n## Explanatory: fem (factor)\n## # A tibble: 2,000 × 2\n##   replicate  stat\n##       &lt;int&gt; &lt;dbl&gt;\n## 1         1 0.467\n## 2         2 0.107\n## 3         3 0.467\n## 4         4 0.308\n## 5         5 0.369\n## 6         6 0.428\n## # ℹ 1,994 more rows\n\nUn intervalo percentil se calcula usando:\n\npercentile_ci &lt;- get_ci(bootstrapped)\npercentile_ci\n## # A tibble: 1 × 2\n##   lower_ci upper_ci\n##      &lt;dbl&gt;    &lt;dbl&gt;\n## 1    0.158    0.653\n\nEl paquete infer tiene una API de alto nivel para mostrar los resultados del análisis, como se muestra en Figura 21.2.\n\nvisualize(bootstrapped) +\n    shade_confidence_interval(endpoints = percentile_ci)\n\n\n\n\n\n\n\n\n\nFigura 21.2: La distribución bootstrap de la diferencia de medias. La región resaltada es el intervalo de confianza.\n\n\n\n\n\nDado que el intervalo visualizado en Figura 21.2 no incluye cero, estos resultados indican que los hombres han publicado más artículos que las mujeres.\nSi requerimos un valor p, el paquete infer puede calcular el valor mediante una prueba de permutación, que se muestra en el siguiente código. La sintaxis es muy similar al código de arranque que usamos anteriormente. Agregamos un verbo hypothesize() para indicar el tipo de suposición a probar y la llamada generate() contiene una opción para mezclar los datos.\n\nset.seed(2102)\npermuted &lt;- \n  bioChemists %&gt;%\n  specify(art ~ fem)  %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 2000, type = \"permute\") %&gt;%\n  calculate(stat = \"diff in means\", order = c(\"Men\", \"Women\"))\npermuted\n## Response: art (numeric)\n## Explanatory: fem (factor)\n## Null Hypothesis: independence\n## # A tibble: 2,000 × 2\n##   replicate     stat\n##       &lt;int&gt;    &lt;dbl&gt;\n## 1         1  0.201  \n## 2         2 -0.133  \n## 3         3  0.109  \n## 4         4 -0.195  \n## 5         5 -0.00128\n## 6         6 -0.102  \n## # ℹ 1,994 more rows\n\nEl siguiente código de visualización también es muy similar al enfoque de arranque. Este código genera Figura 21.3 donde la línea vertical indica el valor observado:\n\nvisualize(permuted) +\n    shade_p_value(obs_stat = observed, direction = \"two-sided\")\n\n\n\n\n\n\n\n\n\nFigura 21.3: Distribución empírica del estadístico de prueba bajo la hipótesis nula. La línea vertical indica la estadística de prueba observada.\n\n\n\n\n\nThe actual p-value is:\n\npermuted %&gt;%\n  get_p_value(obs_stat = observed, direction = \"two-sided\")\n## # A tibble: 1 × 1\n##   p_value\n##     &lt;dbl&gt;\n## 1   0.002\n\nLa línea vertical que representa la hipótesis nula en Figura 21.3 está muy lejos de la distribución de permutación. Esto significa que, si de hecho la hipótesis nula fuera cierta, la probabilidad de observar datos al menos tan extremos como los que tenemos a mano es extremadamente pequeña.\nLas pruebas de dos muestras que se muestran en esta sección probablemente no sean óptimas porque no tienen en cuenta otros factores que podrían explicar la relación observada entre la tasa de publicación y el sexo. Pasemos a un modelo más complejo que pueda considerar covariables adicionales.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Análisis Inferencial</span>"
    ]
  },
  {
    "objectID": "21-inferential-analysis.html#modelos-log-lineales",
    "href": "21-inferential-analysis.html#modelos-log-lineales",
    "title": "21  Análisis Inferencial",
    "section": "21.3 Modelos log-lineales",
    "text": "21.3 Modelos log-lineales\nEl resto de este capítulo se centrará en un modelo lineal generalizado (Dobson 1999) donde asumimos que los recuentos siguen una distribución de Poisson. Para este modelo, las covariables/predictores ingresan al modelo de forma log-lineal:\n\\[\n\\log(\\lambda) = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_px_p\n\\]\ndonde \\(\\lambda\\) es el valor esperado de los recuentos.\nAjustemos un modelo simple que contenga todas las columnas predictoras. El paquete poissonreg, un paquete de extensión parsnip en tidymodels, se ajustará a esta especificación de modelo:\n\nlibrary(poissonreg)\n\n# El motor predeterminado es 'glm'\nlog_lin_spec &lt;- poisson_reg()\n\nlog_lin_fit &lt;- \n  log_lin_spec %&gt;% \n  fit(art ~ ., data = bioChemists)\nlog_lin_fit\n## parsnip model object\n## \n## \n## Call:  stats::glm(formula = art ~ ., family = stats::poisson, data = data)\n## \n## Coefficients:\n## (Intercept)     femWomen   marMarried         kid5          phd         ment  \n##      0.3046      -0.2246       0.1552      -0.1849       0.0128       0.0255  \n## \n## Degrees of Freedom: 914 Total (i.e. Null);  909 Residual\n## Null Deviance:       1820 \n## Residual Deviance: 1630  AIC: 3310\n\nEl método tidy() resume sucintamente los coeficientes del modelo (junto con intervalos de confianza del 90%):\n\ntidy(log_lin_fit, conf.int = TRUE, conf.level = 0.90)\n## # A tibble: 6 × 7\n##   term        estimate std.error statistic  p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)   0.305    0.103       2.96  3.10e- 3   0.134     0.473 \n## 2 femWomen     -0.225    0.0546     -4.11  3.92e- 5  -0.315    -0.135 \n## 3 marMarried    0.155    0.0614      2.53  1.14e- 2   0.0545    0.256 \n## 4 kid5         -0.185    0.0401     -4.61  4.08e- 6  -0.251    -0.119 \n## 5 phd           0.0128   0.0264      0.486 6.27e- 1  -0.0305    0.0563\n## 6 ment          0.0255   0.00201    12.7   3.89e-37   0.0222    0.0288\n\nEn este resultado, los valores p corresponden a pruebas de hipótesis separadas para cada parámetro:\n\\[\\begin{align}\nH_0&: \\beta_j = 0 \\notag \\\\\nH_a&: \\beta_j \\ne 0 \\notag\n\\end{align}\\]\npara cada uno de los parámetros del modelo. Al observar estos resultados, es posible que el phd (el prestigio de su departamento) no tenga ninguna relación con el resultado.\nSi bien la distribución de Poisson es el supuesto habitual para datos como estos, puede resultar beneficioso realizar una verificación aproximada de los supuestos del modelo ajustando los modelos sin utilizar la probabilidad de Poisson para calcular los intervalos de confianza. El paquete rsample tiene una función conveniente para calcular intervalos de confianza de arranque para los modelos lm() y glm(). Podemos usar esta función, mientras declaramos explícitamente family = poisson, para calcular una gran cantidad de ajustes del modelo. De forma predeterminada, calculamos un intervalo bootstrap-t de confianza del 90% (los intervalos percentiles también están disponibles):\n\nset.seed(2103)\nglm_boot &lt;- \n  reg_intervals(art ~ ., data = bioChemists, model_fn = \"glm\", family = poisson)\nglm_boot\n## # A tibble: 5 × 6\n##   term          .lower .estimate  .upper .alpha .method  \n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    \n## 1 femWomen   -0.358      -0.226  -0.0856   0.05 student-t\n## 2 kid5       -0.298      -0.184  -0.0789   0.05 student-t\n## 3 marMarried  0.000264    0.155   0.317    0.05 student-t\n## 4 ment        0.0182      0.0256  0.0322   0.05 student-t\n## 5 phd        -0.0707      0.0130  0.102    0.05 student-t\n\n\nCuando comparamos estos resultados (en Figura 21.4) con los resultados puramente paramétricos de glm(), los intervalos de arranque son algo más amplios. Si los datos fueran verdaderamente de Poisson, estos intervalos tendrían anchos más similares.\n\n\n\n\n\n\n\n\n\nFigura 21.4: Dos tipos de intervalos de confianza para el modelo de regresión de Poisson\n\n\n\n\n\nDeterminar qué predictores incluir en el modelo es un problema difícil. Un enfoque consiste en realizar pruebas de índice de verosimilitud (LRT) (McCullagh y Nelder 1989) entre modelos anidados. Según los intervalos de confianza, tenemos evidencia de que un modelo más simple sin phd puede ser suficiente. Ajustemos un modelo más pequeño y luego realicemos una prueba estadística:\n\\[\\begin{align}\nH_0&: \\beta_{phd} = 0 \\notag \\\\\nH_a&: \\beta_{phd} \\ne 0 \\notag\n\\end{align}\\]\nEsta hipótesis se probó previamente cuando mostramos los resultados ordenados de log_lin_fit. Ese enfoque particular utilizó resultados de un ajuste de modelo único mediante una estadística de Wald (es decir, el parámetro dividido por su error estándar). Para ese enfoque, el valor p era 0.63. Podemos ordenar los resultados del LRT para obtener el valor p:\n\nlog_lin_reduced &lt;- \n  log_lin_spec %&gt;% \n  fit(art ~ ment + kid5 + fem + mar, data = bioChemists)\n\nanova(\n  extract_fit_engine(log_lin_reduced),\n  extract_fit_engine(log_lin_fit),\n  test = \"LRT\"\n) %&gt;%\n  tidy()\n## # A tibble: 2 × 6\n##   term                          df.residual residual.deviance    df deviance p.value\n##   &lt;chr&gt;                               &lt;dbl&gt;             &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n## 1 art ~ ment + kid5 + fem + mar         910             1635.    NA   NA      NA    \n## 2 art ~ fem + mar + kid5 + phd…         909             1634.     1    0.236   0.627\n\nLos resultados son los mismos y, en base a estos y al intervalo de confianza para este parámetro, excluiremos phd de análisis adicionales ya que no parece estar asociado con el resultado.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Análisis Inferencial</span>"
    ]
  },
  {
    "objectID": "21-inferential-analysis.html#un-modelo-más-complejo",
    "href": "21-inferential-analysis.html#un-modelo-más-complejo",
    "title": "21  Análisis Inferencial",
    "section": "21.4 Un Modelo Más Complejo",
    "text": "21.4 Un Modelo Más Complejo\nPodemos pasar a modelos aún más complejos dentro de nuestro enfoque de tidymodels. Para los datos de recuento, hay ocasiones en las que el número de recuentos de ceros es mayor de lo que prescribiría una distribución de Poisson simple. Un modelo más complejo apropiado para esta situación es el modelo de Poisson (ZIP) con inflación cero; consulte Mullahy (1986), Lambert (1992) y Zeileis, Kleiber, y Jackman (2008). Aquí, hay dos conjuntos de covariables: uno para los datos de recuento y otros que afectan la probabilidad (indicada como \\(\\pi\\)) de ceros. La ecuación para la media \\(\\lambda\\) es:\n\\[\\lambda = 0 \\pi + (1 - \\pi) \\lambda_{nz}\\]\ndonde\n\\[\\begin{align}\n\\log(\\lambda_{nz}) &= \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_px_p \\notag \\\\\n\\log\\left(\\frac{\\pi}{1-\\pi}\\right) &= \\gamma_0 + \\gamma_1z_1 + \\ldots + \\gamma_qz_q \\notag\n\\end{align}\\]\ny las covariables \\(x\\) afectan los valores de recuento, mientras que las covariables \\(z\\) influyen en la probabilidad de un cero. No es necesario que los dos conjuntos de predictores sean mutuamente excluyentes.\nAjustaremos un modelo con un conjunto completo de covariables \\(z\\):\n\nzero_inflated_spec &lt;- poisson_reg() %&gt;% set_engine(\"zeroinfl\")\n\nzero_inflated_fit &lt;- \n  zero_inflated_spec %&gt;% \n  fit(art ~ fem + mar + kid5 + ment | fem + mar + kid5 + phd + ment,\n      data = bioChemists)\n\nzero_inflated_fit\n## parsnip model object\n## \n## \n## Call:\n## pscl::zeroinfl(formula = art ~ fem + mar + kid5 + ment | fem + mar + kid5 + \n##     phd + ment, data = data)\n## \n## Count model coefficients (poisson with log link):\n## (Intercept)     femWomen   marMarried         kid5         ment  \n##       0.621       -0.209        0.105       -0.143        0.018  \n## \n## Zero-inflation model coefficients (binomial with logit link):\n## (Intercept)     femWomen   marMarried         kid5          phd         ment  \n##     -0.6086       0.1093      -0.3529       0.2195       0.0124      -0.1351\n\nDado que los coeficientes de este modelo también se estiman utilizando la máxima verosimilitud, intentemos utilizar otra prueba de razón de verosimilitud para comprender si los términos del nuevo modelo son útiles. Simultáneamente probaremos que:\n\\[\\begin{align}\nH_0&: \\gamma_1 = 0, \\gamma_2 = 0, \\cdots, \\gamma_5 = 0 \\notag \\\\\nH_a&: \\text{at least one } \\gamma \\ne 0  \\notag\n\\end{align}\\]\nProbemos ANOVA nuevamente:\n\nanova(\n  extract_fit_engine(zero_inflated_fit),\n  extract_fit_engine(log_lin_reduced),\n  test = \"LRT\"\n) %&gt;%\n  tidy()\n## Error in UseMethod(\"anova\"): no applicable method for 'anova' applied to an object of class \"zeroinfl\"\n\n¡No se implementa un método anova() para objetos zeroinfl!\nUna alternativa es utilizar una estadística de criterio de información, como el criterio de información de Akaike (AIC) (Claeskens 2016). Esto calcula la probabilidad logarítmica (del conjunto de entrenamiento) y penaliza ese valor según el tamaño del conjunto de entrenamiento y la cantidad de parámetros del modelo. En la parametrización de R, los valores AIC más pequeños son mejores. En este caso, no estamos realizando una prueba estadística formal sino estimando la capacidad de los datos para ajustarse a los datos.\nLos resultados indican que el modelo ZIP es preferible:\n\nzero_inflated_fit %&gt;% extract_fit_engine() %&gt;% AIC()\n## [1] 3232\nlog_lin_reduced   %&gt;% extract_fit_engine() %&gt;% AIC()\n## [1] 3312\n\nSin embargo, es difícil contextualizar este par de valores individuales y evaluar cuán diferentes son en realidad. Para resolver este problema, volveremos a muestrear una gran cantidad de cada uno de estos dos modelos. A partir de estos, podemos calcular los valores AIC para cada uno y determinar con qué frecuencia los resultados favorecen el modelo ZIP. Básicamente, caracterizaremos la incertidumbre de las estadísticas del AIC para medir su diferencia en relación con el ruido en los datos.\nTambién calcularemos más intervalos de confianza de arranque para los parámetros en un momento, por lo que especificamos la opción apparent = TRUE al crear las muestras de arranque. Esto es necesario para algunos tipos de intervalos.\nPrimero, creamos los 4000 ajustes del modelo:\n\nzip_form &lt;- art ~ fem + mar + kid5 + ment | fem + mar + kid5 + phd + ment\nglm_form &lt;- art ~ fem + mar + kid5 + ment\n\nset.seed(2104)\nbootstrap_models &lt;-\n  bootstraps(bioChemists, times = 2000, apparent = TRUE) %&gt;%\n  mutate(\n    glm = map(splits, ~ fit(log_lin_spec,       glm_form, data = analysis(.x))),\n    zip = map(splits, ~ fit(zero_inflated_spec, zip_form, data = analysis(.x)))\n  )\nbootstrap_models\n## # Bootstrap sampling with apparent sample \n## # A tibble: 2,001 × 4\n##   splits            id            glm      zip     \n##   &lt;list&gt;            &lt;chr&gt;         &lt;list&gt;   &lt;list&gt;  \n## 1 &lt;split [915/355]&gt; Bootstrap0001 &lt;fit[+]&gt; &lt;fit[+]&gt;\n## 2 &lt;split [915/333]&gt; Bootstrap0002 &lt;fit[+]&gt; &lt;fit[+]&gt;\n## 3 &lt;split [915/337]&gt; Bootstrap0003 &lt;fit[+]&gt; &lt;fit[+]&gt;\n## 4 &lt;split [915/344]&gt; Bootstrap0004 &lt;fit[+]&gt; &lt;fit[+]&gt;\n## 5 &lt;split [915/351]&gt; Bootstrap0005 &lt;fit[+]&gt; &lt;fit[+]&gt;\n## 6 &lt;split [915/354]&gt; Bootstrap0006 &lt;fit[+]&gt; &lt;fit[+]&gt;\n## # ℹ 1,995 more rows\n\nAhora podemos extraer los ajustes del modelo y sus correspondientes valores AIC:\n\nbootstrap_models &lt;-\n  bootstrap_models %&gt;%\n  mutate(\n    glm_aic = map_dbl(glm, ~ extract_fit_engine(.x) %&gt;% AIC()),\n    zip_aic = map_dbl(zip, ~ extract_fit_engine(.x) %&gt;% AIC())\n  )\nmean(bootstrap_models$zip_aic &lt; bootstrap_models$glm_aic)\n## [1] 1\n\nA partir de estos resultados, parece definitivo que tener en cuenta el número excesivo de conteos de cero es una buena idea.\n\nPodríamos haber usado fit_resamples() o un conjunto de flujo de trabajo para realizar estos cálculos. En esta sección, usamos mutate() y map() para calcular los modelos y demostrar cómo se pueden usar las herramientas tidymodels para modelos que no son compatibles con uno de los paquetes parsnip.\n\nDado que hemos calculado los ajustes del modelo remuestreado, creemos intervalos de arranque para los coeficientes del modelo de probabilidad cero (es decir, \\(\\gamma_j\\)). Podemos extraerlos con el método tidy() y usar la opción type = \"zero\" para obtener estas estimaciones:\n\nbootstrap_models &lt;-\n  bootstrap_models %&gt;%\n  mutate(zero_coefs  = map(zip, ~ tidy(.x, type = \"zero\")))\n\n# One example:\nbootstrap_models$zero_coefs[[1]]\n## # A tibble: 6 × 6\n##   term        type  estimate std.error statistic   p.value\n##   &lt;chr&gt;       &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept) zero   -0.128     0.497     -0.257 0.797    \n## 2 femWomen    zero   -0.0763    0.319     -0.240 0.811    \n## 3 marMarried  zero   -0.112     0.365     -0.307 0.759    \n## 4 kid5        zero    0.270     0.186      1.45  0.147    \n## 5 phd         zero   -0.178     0.132     -1.35  0.177    \n## 6 ment        zero   -0.123     0.0315    -3.91  0.0000936\n\nEs una buena idea visualizar las distribuciones de arranque de los coeficientes, como en Figura 21.5.\n\nbootstrap_models %&gt;% \n  unnest(zero_coefs) %&gt;% \n  ggplot(aes(x = estimate)) +\n  geom_histogram(bins = 25, color = \"white\") + \n  facet_wrap(~ term, scales = \"free_x\") + \n  geom_vline(xintercept = 0, lty = 2, color = \"gray70\")\n\n\n\n\n\n\n\n\n\nFigura 21.5: Distribuciones bootstrap de los coeficientes del modelo ZIP. Las líneas verticales indican las estimaciones observadas.\n\n\n\n\n\nUna de las covariables (ment) que parece ser importante tiene una distribución muy sesgada. El espacio extra en algunas de las facetas indica que hay algunos valores atípicos en las estimaciones. Esto podría ocurrir cuando los modelos no convergieran; esos resultados probablemente deberían excluirse de las nuevas muestras. Para los resultados visualizados en Figura 21.5, los valores atípicos se deben únicamente a estimaciones extremas de parámetros; todos los modelos convergieron.\nEl paquete rsample contiene un conjunto de funciones denominadas int_*() que calculan diferentes tipos de intervalos de arranque. Dado que el método tidy() contiene estimaciones de error estándar, se pueden calcular los intervalos bootstrap-t. También calcularemos los intervalos percentiles estándar. De forma predeterminada, se calculan intervalos de confianza del 90%.\n\nbootstrap_models %&gt;% int_pctl(zero_coefs)\n## # A tibble: 6 × 6\n##   term        .lower .estimate  .upper .alpha .method   \n##   &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n## 1 (Intercept) -1.76    -0.624   0.420    0.05 percentile\n## 2 femWomen    -0.523    0.116   0.818    0.05 percentile\n## 3 kid5        -0.327    0.217   0.677    0.05 percentile\n## 4 marMarried  -1.20    -0.381   0.362    0.05 percentile\n## 5 ment        -0.401   -0.162  -0.0515   0.05 percentile\n## 6 phd         -0.274    0.0229  0.333    0.05 percentile\nbootstrap_models %&gt;% int_t(zero_coefs)\n## # A tibble: 6 × 6\n##   term        .lower .estimate  .upper .alpha .method  \n##   &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    \n## 1 (Intercept) -1.61    -0.624   0.332    0.05 student-t\n## 2 femWomen    -0.486    0.116   0.671    0.05 student-t\n## 3 kid5        -0.211    0.217   0.599    0.05 student-t\n## 4 marMarried  -0.988   -0.381   0.293    0.05 student-t\n## 5 ment        -0.322   -0.162  -0.0275   0.05 student-t\n## 6 phd         -0.276    0.0229  0.291    0.05 student-t\n\nA partir de estos resultados, podemos tener una buena idea de qué predictores incluir en el modelo de probabilidad de conteo cero. Puede ser sensato reajustar un modelo más pequeño para evaluar si la distribución de arranque para “ment” todavía está sesgada.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Análisis Inferencial</span>"
    ]
  },
  {
    "objectID": "21-inferential-analysis.html#sec-inference-options",
    "href": "21-inferential-analysis.html#sec-inference-options",
    "title": "21  Análisis Inferencial",
    "section": "21.5 Más Análisis Inferencial",
    "text": "21.5 Más Análisis Inferencial\nEste capítulo demostró solo un pequeño subconjunto de lo que está disponible para el análisis inferencial en modelos tidy y se ha centrado en los métodos frecuentistas y de remuestreo. Podría decirse que el análisis bayesiano es un enfoque de inferencia muy eficaz y, a menudo, superior. Hay una variedad de modelos bayesianos disponibles a través de parsnip. Además, el paquete multilevelmod permite a los usuarios ajustarse a modelos jerárquicos bayesianos y no bayesianos (por ejemplo, modelos mixtos). Los paquetes broom.mixed y tidybayes son excelentes herramientas para extraer datos para gráficos y resúmenes. Finalmente, para conjuntos de datos con una única jerarquía, como datos de medidas longitudinales o repetidas simples, la función group_vfold_cv() de rsample facilita caracterizaciones directas fuera de la muestra del rendimiento del modelo.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Análisis Inferencial</span>"
    ]
  },
  {
    "objectID": "21-inferential-analysis.html#sec-inference-summary",
    "href": "21-inferential-analysis.html#sec-inference-summary",
    "title": "21  Análisis Inferencial",
    "section": "21.6 Resumen Del Capítulo",
    "text": "21.6 Resumen Del Capítulo\nEl marco tidymodels sirve para algo más que el modelado predictivo. Los paquetes y funciones de tidymodels se pueden utilizar para probar hipótesis, así como para ajustar y evaluar modelos inferenciales. El marco tidymodels brinda soporte para trabajar con modelos R que no son tidymodels y puede ayudar a evaluar las cualidades estadísticas de sus modelos.\n\n\n\n\nClaeskens, G. 2016. «Statistical model choice». Annual Review of Statistics and its Application 3: 233-56.\n\n\nDavison, A, y D Hinkley. 1997. Bootstrap methods and their application. Vol. 1. Cambridge university press.\n\n\nDobson, A. 1999. An introduction to generalized linear models. Chapman; Hall: Boca Raton.\n\n\nIsmay, C, y A Kim. 2021. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. Chapman; Hall/CRC. https://moderndive.com/.\n\n\nLambert, D. 1992. «Zero-Inflated Poisson Regression, with an Application to Defects in Manufacturing». Technometrics 34 (1): 1-14.\n\n\nLong, J. 1992. «Measures of Sex Differences in Scientific Productivity*». Social Forces 71 (1): 159-78.\n\n\nMcCullagh, P, y J Nelder. 1989. Generalized Linear Models. London: Chapman; Hall.\n\n\nMullahy, J. 1986. «Specification and testing of some modified count data models». Journal of Econometrics 33 (3): 341-65.\n\n\nWasserstein, R, y N Lazar. 2016. «The ASA statement on p-values: Context, process, and purpose». The American Statistician 70 (2): 129-33.\n\n\nZeileis, A, C Kleiber, y S Jackman. 2008. «Regression models for count data in R». Journal of Statistical Software 27 (8): 1-25. https://www.jstatsoft.org/v027/i08.",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Análisis Inferencial</span>"
    ]
  },
  {
    "objectID": "21-inferential-analysis.html#footnotes",
    "href": "21-inferential-analysis.html#footnotes",
    "title": "21  Análisis Inferencial",
    "section": "",
    "text": "El argumento T nos permite dar cuenta del tiempo en que se contaron los eventos (publicaciones), que fue de tres años tanto para hombres como para mujeres. Hay más hombres que mujeres en estos datos, pero poisson.test() tiene una funcionalidad limitada, por lo que se pueden utilizar análisis más sofisticados para explicar esta diferencia.↩︎",
    "crumbs": [
      "MAS ALLÁ DE LO BÁSICO",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Análisis Inferencial</span>"
    ]
  },
  {
    "objectID": "pre-proc-table.html",
    "href": "pre-proc-table.html",
    "title": "Apéndice A — Preprocesamiento Recomendado",
    "section": "",
    "text": "El tipo de preprocesamiento necesario depende del tipo de modelo que se ajuste. Por ejemplo, los modelos que utilizan funciones de distancia o productos escalares deben tener todos sus predictores en la misma escala para que la distancia se mida adecuadamente.\nPara obtener más información sobre cada uno de estos modelos y otros que podrían estar disponibles, consulte https://www.tidymodels.org/find/parsnip/.\nEste Apéndice proporciona recomendaciones para los niveles básicos de preprocesamiento que se necesitan para diversas funciones del modelo. En Tabla A.1, los métodos de preprocesamiento se clasifican como:\n\ndummy: ¿Los predictores cualitativos requieren una codificación numérica (por ejemplo, mediante variables ficticias u otros métodos)?\nzv: ¿Deberían eliminarse las columnas con un único valor único?\nimpute: Si faltan algunos predictores, ¿deberían estimarse mediante imputación?\ndecorrelate: Si existen predictores correlacionados, ¿debería mitigarse esta correlación? Esto podría significar filtrar los predictores, utilizar el análisis de componentes principales o una técnica basada en modelos (por ejemplo, regularización).\nnormalize: ¿Deben centrarse y escalarse los predictores?\ntransform: ¿Es útil transformar los predictores para que sean más simétricos?\n\nLa información en Tabla A.1 no es exhaustiva y depende en cierta medida de la implementación. Por ejemplo, como se indica debajo de la tabla, es posible que algunos modelos no requieran una operación de preprocesamiento particular, pero la implementación puede requerirla. En la tabla, ✔ indica que el método es necesario para el modelo y × indica que no. El símbolo ◌ significa que la técnica puede ayudar al modelo, pero no es necesario.\n\n\n\nTabla A.1: Métodos de preprocesamiento para diferentes modelos.\n\n\n\n\n\n\nmodel\ndummy\nzv\nimpute\ndecorrelate\nnormalize\ntransform\n\n\n\n\n&lt;tt&gt;C5_rules()&lt;/tt&gt;\n×\n×\n×\n×\n×\n×\n\n\n&lt;tt&gt;bag_mars()&lt;/tt&gt;\n✔\n×\n✔\n◌\n×\n◌\n\n\n&lt;tt&gt;bag_tree()&lt;/tt&gt;\n×\n×\n×\n◌¹\n×\n×\n\n\n&lt;tt&gt;bart()&lt;/tt&gt;\n×\n×\n×\n◌¹\n×\n×\n\n\n&lt;tt&gt;boost_tree()&lt;/tt&gt;\n×²\n◌\n✔²\n◌¹\n×\n×\n\n\n&lt;tt&gt;cubist_rules()&lt;/tt&gt;\n×\n×\n×\n×\n×\n×\n\n\n&lt;tt&gt;decision_tree()&lt;/tt&gt;\n×\n×\n×\n◌¹\n×\n×\n\n\n&lt;tt&gt;discrim_flexible()&lt;/tt&gt;\n✔\n×\n✔\n✔\n×\n◌\n\n\n&lt;tt&gt;discrim_linear()&lt;/tt&gt;\n✔\n✔\n✔\n✔\n×\n◌\n\n\n&lt;tt&gt;discrim_regularized()&lt;/tt&gt;\n✔\n✔\n✔\n✔\n×\n◌\n\n\n&lt;tt&gt;gen_additive_mod()&lt;/tt&gt;\n✔\n✔\n✔\n✔\n×\n◌\n\n\n&lt;tt&gt;linear_reg()&lt;/tt&gt;\n✔\n✔\n✔\n✔\n×\n◌\n\n\n&lt;tt&gt;logistic_reg()&lt;/tt&gt;\n✔\n✔\n✔\n✔\n×\n◌\n\n\n&lt;tt&gt;mars()&lt;/tt&gt;\n✔\n×\n✔\n◌\n×\n◌\n\n\n&lt;tt&gt;mlp()&lt;/tt&gt;\n✔\n✔\n✔\n✔\n✔\n✔\n\n\n&lt;tt&gt;multinom_reg()&lt;/tt&gt;\n✔\n✔\n✔\n✔\n×²\n◌\n\n\n&lt;tt&gt;naive_Bayes()&lt;/tt&gt;\n×\n✔\n✔\n◌¹\n×\n×\n\n\n&lt;tt&gt;nearest_neighbor()&lt;/tt&gt;\n✔\n✔\n✔\n◌\n✔\n✔\n\n\n&lt;tt&gt;pls()&lt;/tt&gt;\n✔\n✔\n✔\n×\n✔\n✔\n\n\n&lt;tt&gt;poisson_reg()&lt;/tt&gt;\n✔\n✔\n✔\n✔\n×\n◌\n\n\n&lt;tt&gt;rand_forest()&lt;/tt&gt;\n×\n◌\n✔²\n◌¹\n×\n×\n\n\n&lt;tt&gt;rule_fit()&lt;/tt&gt;\n✔\n×\n✔\n◌¹\n✔\n×\n\n\n&lt;tt&gt;svm_*()&lt;/tt&gt;\n✔\n✔\n✔\n✔\n✔\n✔\n\n\n\n\n\n\n\n\n\nNotas a pie de página:\n\nEs posible que la descorrelación de predictores no ayude a mejorar el rendimiento. Sin embargo, menos predictores correlacionados pueden mejorar la estimación de las puntuaciones de importancia de la varianza (ver Fig. 11.4 de Kuhn y Johnson (2020)). Básicamente, la selección de predictores altamente correlacionados es casi aleatoria.\nEl preprocesamiento necesario para estos modelos depende de la implementación. Específicamente:\n\n\nTeóricamente, cualquier modelo basado en árboles no requiere imputación. Sin embargo, muchas implementaciones de conjuntos de árboles requieren imputación.\nSi bien los métodos de impulso basados en árboles generalmente no requieren la creación de variables ficticias, los modelos que utilizan el motor “xgboost” sí la requieren.\n\n\n\n\n\nKuhn, M, y K Johnson. 2020. Feature engineering and selection: A practical approach for predictive models. CRC Press.",
    "crumbs": [
      "Apéndices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Preprocesamiento Recomendado</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referencias",
    "section": "",
    "text": "Abrams, B. 2003. “The Pit of Success.” https://blogs.msdn.microsoft.com/brada/2003/10/02/the-pit-of-success/.\n\n\nBaggerly, K, and K Coombes. 2009. “Deriving Chemosensitivity from\nCell Lines: Forensic Bioinformatics and Reproducible\nResearch in High-Throughput Biology.” The Annals of Applied\nStatistics 3 (4): 1309–34.\n\n\nBartley, E AND Schliep, M . AND Hanks. 2019. “Identifying and\nCharacterizing Extrapolation in Multivariate Response Data.”\nPLOS ONE 14 (December): 1–20.\n\n\nBiecek, Przemyslaw, and Tomasz Burzykowski. 2021. Explanatory\nModel Analysis. Chapman; Hall/CRC, New York. https://ema.drwhy.ai/.\n\n\nBohachevsky, I, M Johnson, and M Stein. 1986. “Generalized\nSimulated Annealing for Function Optimization.”\nTechnometrics 28 (3): 209–17.\n\n\nBolstad, B. 2004. Low-Level Analysis of High-Density Oligonucleotide\nArray Data: Background, Normalization and Summarization. University\nof California, Berkeley.\n\n\nBox, GEP, W Hunter, and J Hunter. 2005. Statistics for\nExperimenters: An Introduction to Design, Data Analysis, and Model\nBuilding. Wiley.\n\n\nBradley, R, and M Terry. 1952. “Rank Analysis of Incomplete Block\nDesigns: I. The Method of Paired Comparisons.”\nBiometrika 39 (3/4): 324–45.\n\n\nBreiman, L. 1996a. “Bagging Predictors.” Machine\nLearning 24 (2): 123–40.\n\n\n———. 1996b. “Stacked Regressions.” Machine\nLearning 24 (1): 49–64.\n\n\n———. 2001a. “Random Forests.” Machine Learning 45\n(1): 5–32.\n\n\n———. 2001b. “Statistical Modeling: The Two Cultures.”\nStatistical Science 16 (3): 199–231.\n\n\nCarlson, B. 2012. “Putting Oncology Patients at Risk.”\nBiotechnology Healthcare 9 (3): 17–21.\n\n\nChambers, J. 1998. Programming with Data: A Guide to the\nS Language. Berlin, Heidelberg: Springer-Verlag.\n\n\nChambers, J, and T Hastie, eds. 1992. Statistical Models in\nS. Boca Raton, FL: CRC Press, Inc.\n\n\nClaeskens, G. 2016. “Statistical Model Choice.” Annual\nReview of Statistics and Its Application 3: 233–56.\n\n\nCleveland, W. 1979. “Robust Locally Weighted Regression and\nSmoothing Scatterplots.” Journal of the American Statistical\nAssociation 74 (368): 829–36.\n\n\nCraig–Schapiro, R, M Kuhn, C Xiong, E Pickering, J Liu, T Misko, R\nPerrin, et al. 2011. “Multiplexed Immunoassay Panel Identifies\nNovel CSF Biomarkers for Alzheimer’s Disease Diagnosis and\nPrognosis.” PLoS ONE 6 (4): e18850.\n\n\nCybenko, G. 1989. “Approximation by Superpositions of a Sigmoidal\nFunction.” Mathematics of Control, Signals and Systems 2\n(4): 303–14.\n\n\nDanowski, T, J Aarons, J Hydovitz, and J Wingert. 1970. “Utility\nof Equivocal Glucose Tolerances.” Diabetes 19 (7):\n524–26.\n\n\nDavison, A, and D Hinkley. 1997. Bootstrap Methods and Their\nApplication. Vol. 1. Cambridge university press.\n\n\nDe Cock, D. 2011. “Ames, Iowa: Alternative to the\nBoston Housing Data as an End of Semester Regression\nProject.” Journal of Statistics Education 19 (3).\n\n\nDobson, A. 1999. An Introduction to Generalized Linear Models.\nChapman; Hall: Boca Raton.\n\n\nDurrleman, S, and R Simon. 1989. “Flexible Regression Models with\nCubic Splines.” Statistics in Medicine 8 (5): 551–61.\n\n\nFaraway, J. 2016. Extending the Linear Model with R:\nGeneralized Linear, Mixed Effects and Nonparametric Regression\nModels. CRC press.\n\n\nFox, J. 2008. Applied Regression Analysis and Generalized Linear\nModels. Second. Thousand Oaks, CA: Sage.\n\n\nFrazier, R. 2018. “A Tutorial on Bayesian Optimization.” https://arxiv.org/abs/1807.02811.\n\n\nFreund, Y, and R Schapire. 1997. “A Decision-Theoretic\nGeneralization of on-Line Learning and an Application to\nBoosting.” Journal of Computer and System Sciences 55\n(1): 119–39.\n\n\nFriedman, J. 1991. “Multivariate Adaptive Regression\nSplines.” The Annals of Statistics 19 (1): 1–141.\n\n\n———. 2001. “Greedy Function Approximation: A Gradient Boosting\nMachine.” Annals of Statistics 29 (5): 1189–1232.\n\n\nFriedman, J, T Hastie, and R Tibshirani. 2010. “Regularization\nPaths for Generalized Linear Models via Coordinate Descent.”\nJournal of Statistical Software 33 (1): 1.\n\n\nGeladi, P., and B Kowalski. 1986. “Partial Least-Squares\nRegression: A Tutorial.” Analytica Chimica Acta 185:\n1–17.\n\n\nGentleman, R, V Carey, W Huber, R Irizarry, and S Dudoit. 2005.\nBioinformatics and Computational Biology Solutions Using\nR and Bioconductor. Berlin, Heidelberg:\nSpringer-Verlag.\n\n\nGood, I. J. 1985. “Weight of Evidence: A Brief Survey.”\nBayesian Statistics 2: 249–70.\n\n\nGoodfellow, I, Y Bengio, and A Courville. 2016. Deep Learning.\nMIT Press.\n\n\nGuo, Cheng, and Felix Berkhahn. 2016. “Entity Embeddings of\nCategorical Variables.” http://arxiv.org/abs/1604.06737.\n\n\nHand, D, and R Till. 2001. “A Simple Generalisation of the Area\nUnder the ROC Curve for Multiple Class Classification\nProblems.” Machine Learning 45 (August): 171–86.\n\n\nHill, A, P LaPan, Y Li, and S Haney. 2007. “Impact of Image\nSegmentation on High-Content Screening Data Quality for\nSK-BR-3 Cells.” BMC\nBioinformatics 8 (1): 340.\n\n\nHo, T. 1995. “Random Decision Forests.” In Proceedings\nof 3rd International Conference on Document Analysis and\nRecognition, 1:278–82. IEEE.\n\n\nHosmer, D, and Sy Lemeshow. 2000. Applied Logistic Regression.\nNew York: John Wiley; Sons.\n\n\nHvitfeldt, E., and J. Silge. 2021. Supervised Machine Learning for\nText Analysis in r. A Chapman & Hall Book. CRC Press. https://smltar.com/.\n\n\nHyndman, R, and G Athanasopoulos. 2018. Forecasting: Principles and\nPractice. OTexts.\n\n\nIsmay, C, and A Kim. 2021. Statistical Inference via Data Science: A\nModernDive into r and the Tidyverse. Chapman; Hall/CRC. https://moderndive.com/.\n\n\nJaworska, J, N Nikolova-Jeliazkova, and T Aldenberg. 2005. “QSAR\nApplicability Domain Estimation by Projection of the Training Set in\nDescriptor Space: A Review.” Alternatives to Laboratory\nAnimals 33 (5): 445–59.\n\n\nJohnson, D, P Eckart, N Alsamadisi, H Noble, C Martin, and R Spicer.\n2018. “Polar Auxin Transport Is Implicated in Vessel\nDifferentiation and Spatial Patterning During Secondary Growth in\nPopulus.” American Journal of Botany 105 (2): 186–96.\n\n\nJoseph, V, E Gul, and S Ba. 2015. “Maximum Projection Designs for\nComputer Experiments.” Biometrika 102 (2): 371–80.\n\n\nJungsu, K, D Basak, and D Holtzman. 2009. “The Role of\nApolipoprotein E in Alzheimer’s\nDisease.” Neuron 63 (3): 287–303.\n\n\nKerleguer, A., J.-L. Koeck, M. Fabre, P. Gérôme, R. Teyssou, and V.\nHervé. 2003. “Use of Equivocal Zone in Interpretation of Results\nof the Amplified Mycobacterium Tuberculosis Direct Test for\nDiagnosis of Tuberculosis.” Journal of Clinical\nMicrobiology 41 (4): 1783–84.\n\n\nKirkpatrick, S, D Gelatt, and M Vecchi. 1983. “Optimization by\nSimulated Annealing.” Science 220 (4598): 671–80.\n\n\nKoklu, M, and IA Ozkan. 2020. “Multiclass Classification of Dry\nBeans Using Computer Vision and Machine Learning Techniques.”\nComputers and Electronics in Agriculture 174: 105507.\n\n\nKrueger, T, D Panknin, and M Braun. 2015. “Fast Cross-Validation\nvia Sequential Testing.” Journal of Machine Learning\nResearch 16 (33): 1103–55.\n\n\nKruschke, J, and T Liddell. 2018. “The Bayesian New\nStatistics: Hypothesis Testing, Estimation, Meta-Analysis, and Power\nAnalysis from a Bayesian Perspective.”\nPsychonomic Bulletin and Review 25 (1): 178–206.\n\n\nKuhn, Max. 2014. “Futility Analysis in the Cross-Validation of\nMachine Learning Models.” https://arxiv.org/abs/1405.6974.\n\n\nKuhn, M, and K Johnson. 2013. Applied Predictive Modeling.\nSpringer.\n\n\n———. 2020. Feature Engineering and Selection: A Practical Approach\nfor Predictive Models. CRC Press.\n\n\nLambert, D. 1992. “Zero-Inflated Poisson Regression, with an\nApplication to Defects in Manufacturing.” Technometrics\n34 (1): 1–14.\n\n\nLittell, R, J Pendergast, and R Natarajan. 2000. “Modelling\nCovariance Structure in the Analysis of Repeated Measures Data.”\nStatistics in Medicine 19 (13): 1793–1819.\n\n\nLong, J. 1992. “Measures of Sex Differences\nin Scientific Productivity*.” Social Forces 71\n(1): 159–78.\n\n\nLundberg, Scott M., and Su-In Lee. 2017. “A Unified Approach to\nInterpreting Model Predictions.” In Proceedings of the 31st\nInternational Conference on Neural Information Processing Systems,\n4768–77. NIPS’17. Red Hook, NY, USA: Curran Associates Inc.\n\n\nMangiafico, S. 2015. “An R Companion for the Handbook\nof Biological Statistics.” https://rcompanion.org/handbook/.\n\n\nMaron, O, and A Moore. 1994. “Hoeffding Races: Accelerating Model\nSelection Search for Classification and Function Approximation.”\nIn Advances in Neural Information Processing Systems, 59–66.\n\n\nMcCullagh, P, and J Nelder. 1989. Generalized Linear Models.\nLondon: Chapman; Hall.\n\n\nMcDonald, J. 2009. Handbook of Biological Statistics. Sparky\nHouse Publishing.\n\n\nMcElreath, R. 2020. Statistical Rethinking: A Bayesian\nCourse with Examples in R and Stan. CRC\npress.\n\n\nMcInnes, L, J Healy, and J Melville. 2020. “UMAP: Uniform Manifold\nApproximation and Projection for Dimension Reduction.”\n\n\nMcKay, M, R Beckman, and W Conover. 1979. “A Comparison of Three\nMethods for Selecting Values of Input Variables in the Analysis of\nOutput from a Computer Code.” Technometrics 21 (2):\n239–45.\n\n\nMicci-Barreca, Daniele. 2001. “A Preprocessing Scheme for\nHigh-Cardinality Categorical Attributes in Classification and Prediction\nProblems.” SIGKDD Explor. Newsl. 3 (1): 27–32. https://doi.org/10.1145/507533.507538.\n\n\nMingqiang, Y, K Kidiyo, and R Joseph. 2008. “A Survey of Shape\nFeature Extraction Techniques.” In Pattern Recognition,\nedited by PY Yin. Rijeka: IntechOpen. https://doi.org/10.5772/6237.\n\n\nMolnar, Christopher. 2020. Interpretable Machine\nLearning. lulu.com. https://christophm.github.io/interpretable-ml-book/.\n\n\nMullahy, J. 1986. “Specification and Testing of Some Modified\nCount Data Models.” Journal of Econometrics 33 (3):\n341–65.\n\n\nNetzeva, T, A Worth, T Aldenberg, R Benigni, M Cronin, P Gramatica, J\nJaworska, et al. 2005. “Current Status of Methods for Defining the\nApplicability Domain of (Quantitative) Structure-Activity Relationships:\nThe Report and Recommendations of ECVAM Workshop 52.”\nAlternatives to Laboratory Animals 33 (2): 155–73.\n\n\nOlsson, D, and L Nelson. 1975. “The\nNelder-Mead Simplex Procedure for Function\nMinimization.” Technometrics 17 (1): 45–51.\n\n\nOpitz, J, and S Burst. 2019. “Macro F1 and Macro F1.” https://arxiv.org/abs/1911.03347.\n\n\nR Core Team. 2014. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttp://www.R-project.org/.\n\n\nRasmussen, C, and C Williams. 2006. Gaussian Processes for Machine\nLearning. Gaussian Processes for Machine Learning. MIT\nPress.\n\n\nSantner, T, B Williams, W Notz, and B Williams. 2003. The Design and\nAnalysis of Computer Experiments. Springer.\n\n\nSchmidberger, M, M Morgan, D Eddelbuettel, H Yu, L Tierney, and U\nMansmann. 2009. “State of the Art in Parallel Computing with\nR.” Journal of Statistical Software 31 (1):\n1–27. https://www.jstatsoft.org/v031/i01.\n\n\nSchulz, E, M Speekenbrink, and A Krause. 2018. “A Tutorial on\nGaussian Process Regression: Modelling, Exploring, and Exploiting\nFunctions.” Journal of Mathematical Psychology 85: 1–16.\n\n\nShahriari, B., K. Swersky, Z. Wang, R. P. Adams, and N. de Freitas.\n2016. “Taking the Human Out of the Loop: A Review of Bayesian\nOptimization.” Proceedings of the IEEE 104 (1): 148–75.\n\n\nShewry, M, and H Wynn. 1987. “Maximum Entropy Sampling.”\nJournal of Applied Statistics 14 (2): 165–70.\n\n\nShmueli, G. 2010. “To Explain or to Predict?”\nStatistical Science 25 (3): 289–310.\n\n\nSymons, S, and RG Fulcher. 1988. “Determination of Wheat Kernel\nMorphological Variation by Digital Image Analysis: I.\nVariation in Eastern Canadian Milling Quality\nWheats.” Journal of Cereal Science 8 (3): 211–18.\n\n\nThomas, R, and D Uminsky. 2020. “The Problem with Metrics Is a\nFundamental Problem for AI.” https://arxiv.org/abs/2002.08512.\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via\nthe Lasso.” Journal of the Royal Statistical Society. Series\nB (Methodological) 58 (1): 267–88. http://www.jstor.org/stable/2346178.\n\n\nVan Laarhoven, P, and E Aarts. 1987. “Simulated Annealing.”\nIn Simulated Annealing: Theory and Applications, 7–15.\nSpringer.\n\n\nWasserstein, R, and N Lazar. 2016. “The ASA Statement\non p-Values: Context, Process, and Purpose.” The American\nStatistician 70 (2): 129–33.\n\n\nWeinberger, K, A Dasgupta, J Langford, A Smola, and J Attenberg. 2009.\n“Feature Hashing for Large Scale Multitask Learning.” In\nProceedings of the 26th Annual International Conference on Machine\nLearning, 1113–20. ACM.\n\n\nWickham, H. 2019. Advanced r. 2nd ed. Chapman & Hall/CRC\nthe r Series. Taylor & Francis. https://doi.org/10.1201/9781351201315.\n\n\nWickham, H, M Averick, J Bryan, W Chang, L McGowan, R François, G\nGrolemund, et al. 2019. “Welcome to the\nTidyverse.” Journal of Open Source Software\n4 (43).\n\n\nWickham, H, and G Grolemund. 2016. R\nfor Data Science: Import, Tidy, Transform, Visualize, and\nModel Data. O’Reilly Media, Inc.\n\n\nWolpert, D. 1992. “Stacked Generalization.” Neural\nNetworks 5 (2): 241–59.\n\n\nWu, X, and Z Zhou. 2017. “A Unified View of Multi-Label\nPerformance Measures.” In International Conference on Machine\nLearning, 3780–88.\n\n\nWundervald, B, A Parnell, and K Domijan. 2020. “Generalizing Gain\nPenalization for Feature Selection in Tree-Based Models.” https://arxiv.org/abs/2006.07515.\n\n\nXu, Q, and Y Liang. 2001. “Monte Carlo Cross\nValidation.” Chemometrics and Intelligent Laboratory\nSystems 56 (1): 1–11.\n\n\nYeo, I-K, and R Johnson. 2000. “A New Family of Power\nTransformations to Improve Normality or Symmetry.”\nBiometrika 87 (4): 954–59.\n\n\nZeileis, A, C Kleiber, and S Jackman. 2008. “Regression Models for\nCount Data in R.” Journal of Statistical\nSoftware 27 (8): 1–25. https://www.jstatsoft.org/v027/i08.\n\n\nZumel, Nina, and John Mount. 2019. “Vtreat: A Data.frame Processor\nfor Predictive Modeling.” http://arxiv.org/abs/1611.09477.",
    "crumbs": [
      "Apéndices",
      "Referencias"
    ]
  }
]