{
  "hash": "6e2ad89743ed1cdc31715807e1350c12",
  "result": {
    "markdown": "\n\n\n# Resampling for Evaluating Performance {#sec-resampling}\n\nWe have already covered several pieces that must be put together to evaluate the performance of a model. [Chapter @sec-performance] described statistics for measuring model performance. [Chapter @sec-splitting] introduced the idea of data spending, and we recommended the test set for obtaining an unbiased estimate of performance. However, we usually need to understand the performance of a model or even multiple models *before using the test set*.\n\n::: rmdwarning\nTypically we can't decide on which final model to use with the test set before first assessing model performance. There is a gap between our need to measure performance reliably and the data splits (training and testing) we have available.\n:::\n\nIn this chapter, we describe an approach called resampling that can fill this gap. Resampling estimates of performance can generalize to new data in a similar way as estimates from a test set. The next chapter complements this one by demonstrating statistical methods that compare resampling results.\n\nIn order to fully appreciate the value of resampling, let's first take a look the resubstitution approach, which can often fail.\n\n## The Resubstitution Approach {#sec-resampling-resubstition}\n\nWhen we measure performance on the same data that we used for training (as opposed to new data or testing data), we say we have *resubstituted* the data. Let's again use the Ames housing data to demonstrate these concepts. @sec-recipes-summary summarizes the current state of our Ames analysis. It includes a recipe object named `ames_rec`, a linear model, and a workflow using that recipe and model called `lm_wflow`. This workflow was fit on the training set, resulting in `lm_fit`.\n\nFor a comparison to this linear model, we can also fit a different type of model. *Random forests* are a tree ensemble method that operates by creating a large number of decision trees from slightly different versions of the training set [@breiman2001random]. This collection of trees makes up the ensemble. When predicting a new sample, each ensemble member makes a separate prediction. These are averaged to create the final ensemble prediction for the new data point.\n\nRandom forest models are very powerful, and they can emulate the underlying data patterns very closely. While this model can be computationally intensive, it is very low maintenance; very little preprocessing is required (as documented in [Appendix @sec-pre-proc-table]).\n\nUsing the same predictor set as the linear model (without the extra preprocessing steps), we can fit a random forest model to the training set via the `\"ranger\"` engine (which uses the <span class=\"pkg\">ranger</span> R package for computation). This model requires no preprocessing, so a simple formula can be used:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrf_model <- \n  rand_forest(trees = 1000) %>% \n  set_engine(\"ranger\") %>% \n  set_mode(\"regression\")\n\nrf_wflow <- \n  workflow() %>% \n  add_formula(\n    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n      Latitude + Longitude) %>% \n  add_model(rf_model) \n\nrf_fit <- rf_wflow %>% fit(data = ames_train)\n```\n:::\n\n\nHow should we compare the linear and random forest models? For demonstration, we will predict the training set to produce what is known as an *apparent metric* or *resubstitution metric*. This function creates predictions and formats the results:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nestimate_perf <- function(model, dat) {\n  # Capture the names of the `model` and `dat` objects\n  cl <- match.call()\n  obj_name <- as.character(cl$model)\n  data_name <- as.character(cl$dat)\n  data_name <- gsub(\"ames_\", \"\", data_name)\n  \n  # Estimate these metrics:\n  reg_metrics <- metric_set(rmse, rsq)\n  \n  model %>%\n    predict(dat) %>%\n    bind_cols(dat %>% select(Sale_Price)) %>%\n    reg_metrics(Sale_Price, .pred) %>%\n    select(-.estimator) %>%\n    mutate(object = obj_name, data = data_name)\n}\n```\n:::\n\n\nBoth RMSE and $R^2$ are computed. The resubstitution statistics are:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nestimate_perf(rf_fit, ames_train)\n## # A tibble: 2 × 4\n##   .metric .estimate object data \n##   <chr>       <dbl> <chr>  <chr>\n## 1 rmse       0.0366 rf_fit train\n## 2 rsq        0.960  rf_fit train\nestimate_perf(lm_fit, ames_train)\n## # A tibble: 2 × 4\n##   .metric .estimate object data \n##   <chr>       <dbl> <chr>  <chr>\n## 1 rmse       0.0754 lm_fit train\n## 2 rsq        0.816  lm_fit train\n```\n:::\n\n\n\n\nBased on these results, the random forest is much more capable of predicting the sale prices; the RMSE estimate is two-fold better than linear regression. If we needed to choose between these two models for this price prediction problem, we would probably chose the random forest because, on the log scale we are using, its RMSE is about half as large. The next step applies the random forest model to the test set for final verification:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nestimate_perf(rf_fit, ames_test)\n## # A tibble: 2 × 4\n##   .metric .estimate object data \n##   <chr>       <dbl> <chr>  <chr>\n## 1 rmse       0.0704 rf_fit test \n## 2 rsq        0.852  rf_fit test\n```\n:::\n\n\nThe test set RMSE estimate, 0.0704, is *much worse than the training set* value of 0.0366! Why did this happen?\n\nMany predictive models are capable of learning complex trends from the data. In statistics, these are commonly referred to as *low bias models*.\n\n::: rmdnote\nIn this context, *bias* is the difference between the true pattern or relationships in data and the types of patterns that the model can emulate. Many black-box machine learning models have low bias, meaning they can reproduce complex relationships. Other models (such as linear/logistic regression, discriminant analysis, and others) are not as adaptable and are considered *high bias* models.[^10-resampling-1]\n:::\n\n[^10-resampling-1]: See Section 1.2.5 of @fes for a discussion: <https://bookdown.org/max/FES/important-concepts.html#model-bias-and-variance>\n\nFor a low bias model, the high degree of predictive capacity can sometimes result in the model nearly memorizing the training set data. As an obvious example, consider a 1-nearest neighbor model. It will always provide perfect predictions for the training set no matter how well it truly works for other data sets. Random forest models are similar; repredicting the training set will always result in an artificially optimistic estimate of performance.\n\nFor both models, @tbl-rmse-results summarizes the RMSE estimate for the training and test sets:\n\n\n::: {#tbl-rmse-results .cell layout-align=\"center\" tbl-cap='Performance statistics for training and test sets.'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n<tr>\n<th style=\"empty-cells: hide;border-bottom:hidden;\" colspan=\"1\"></th>\n<th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"2\"><div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \">RMSE Estimates</div></th>\n</tr>\n  <tr>\n   <th style=\"text-align:left;\"> object </th>\n   <th style=\"text-align:right;\"> train </th>\n   <th style=\"text-align:right;\"> test </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> <tt>lm_fit</tt> </td>\n   <td style=\"text-align:right;\"> 0.0754 </td>\n   <td style=\"text-align:right;\"> 0.0736 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> <tt>rf_fit</tt> </td>\n   <td style=\"text-align:right;\"> 0.0366 </td>\n   <td style=\"text-align:right;\"> 0.0704 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nNotice that the linear regression model is consistent between training and testing, because of its limited complexity.[^10-resampling-2]\n\n[^10-resampling-2]: It is possible for a linear model to nearly memorize the training set, like the random forest model did. In the `ames_rec` object, change the number of spline terms for `longitude` and `latitude` to a large number (say 1,000). This would produce a model fit with a very small resubstitution RMSE and a test set RMSE that is much larger.\n\n::: rmdwarning\nThe main takeaway from this example is that repredicting the training set will result in an artificially optimistic estimate of performance. It is a bad idea for most models.\n:::\n\nIf the test set should not be used immediately, and repredicting the training set is a bad idea, what should be done? Resampling methods, such as cross-validation or validation sets, are the solution.\n\n## Resampling Methods\n\nResampling methods are empirical simulation systems that emulate the process of using some data for modeling and different data for evaluation. Most resampling methods are iterative, meaning that this process is repeated multiple times. The diagram in @fig-resampling-scheme illustrates how resampling methods generally operate.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Data splitting scheme from the initial data split to resampling](premade/resampling.svg){#fig-resampling-scheme fig-align='center' fig-alt='A diagram of the data splitting scheme from the initial data split to resampling. The first level is the training/testing set partition. The second level of splitting takes the training set and splits it into multiple \\'analysis\\' and \\'assessment\\' sets (which are analogous to training and test).' width=85%}\n:::\n:::\n\n\nResampling is conducted only on the training set, as you see in @fig-resampling-scheme. The test set is not involved. For each iteration of resampling, the data are partitioned into two subsamples:\n\n-   The model is fit with the *analysis set*.\n\n-   The model is evaluated with the *assessment set*.\n\nThese two subsamples are somewhat analogous to training and test sets. Our language of *analysis* and *assessment* avoids confusion with the initial split of the data. These data sets are mutually exclusive. The partitioning scheme used to create the analysis and assessment sets is usually the defining characteristic of the method.\n\nSuppose 20 iterations of resampling are conducted. This means that 20 separate models are fit on the analysis sets, and the corresponding assessment sets produce 20 sets of performance statistics. The final estimate of performance for a model is the average of the 20 replicates of the statistics. This average has very good generalization properties and is far better than the resubstitution estimates.\n\nThe next section defines several commonly used resampling methods and discusses their pros and cons.\n\n### Cross-validation {#sec-cv}\n\nCross-validation is a well established resampling method. While there are a number of variations, the most common cross-validation method is *V*-fold cross-validation. The data are randomly partitioned into *V* sets of roughly equal size (called the folds). For illustration, *V* = 3 is shown in @fig-cross-validation-allocation for a data set of 30 training set points with random fold allocations. The number inside the symbols is the sample number.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![V-fold cross-validation randomly assigns data to folds](premade/three-CV.svg){#fig-cross-validation-allocation fig-align='center' fig-alt='A diagram of how V-fold cross-validation randomly assigns data to folds (where V equals three). A set of thirty data points are assigned to three groups of roughly the same size.' width=50%}\n:::\n:::\n\n\nThe color of the symbols in @fig-cross-validation-allocation represents their randomly assigned folds. Stratified sampling is also an option for assigning folds (previously discussed in @sec-splitting-methods).\n\nFor three-fold cross-validation, the three iterations of resampling are illustrated in @fig-cross-validation. For each iteration, one fold is held out for assessment statistics and the remaining folds are substrate for the model. This process continues for each fold so that three models produce three sets of performance statistics.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![V-fold cross-validation data usage](premade/three-CV-iter.svg){#fig-cross-validation fig-align='center' fig-alt='A diagram of V-fold cross-validation data usage (where V equals three). For each of the three groups, the data for the fold are held out for performance while the other two are used for modeling.' width=70%}\n:::\n:::\n\n\nWhen *V* = 3, the analysis sets are 2/3 of the training set and each assessment set is a distinct 1/3. The final resampling estimate of performance averages each of the *V* replicates.\n\nUsing *V* = 3 is a good choice to illustrate cross-validation, but it is a poor choice in practice because it is too low to generate reliable estimates. In practice, values of *V* are most often 5 or 10; we generally prefer 10-fold cross-validation as a default because it is large enough for good results in most situations.\n\n::: rmdnote\nWhat are the effects of changing *V*? Larger values result in resampling estimates with small bias but substantial variance. Smaller values of *V* have large bias but low variance. We prefer 10-fold since noise is reduced by replication, but bias is not.[^10-resampling-3]\n:::\n\n[^10-resampling-3]: See Section 3.4 of @fes for a longer description of the results of changing *V*: <https://bookdown.org/max/FES/resampling.html>\n\nThe primary input is the training set data frame as well as the number of folds (defaulting to 10):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(1001)\names_folds <- vfold_cv(ames_train, v = 10)\names_folds\n## #  10-fold cross-validation \n## # A tibble: 10 × 2\n##   splits             id    \n##   <list>             <chr> \n## 1 <split [2107/235]> Fold01\n## 2 <split [2107/235]> Fold02\n## 3 <split [2108/234]> Fold03\n## 4 <split [2108/234]> Fold04\n## 5 <split [2108/234]> Fold05\n## 6 <split [2108/234]> Fold06\n## # ℹ 4 more rows\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\nThe column named `splits` contains the information on how to split the data (similar to the object used to create the initial training/test partition). While each row of `splits` has an embedded copy of the entire training set, R is smart enough not to make copies of the data in memory.[^10-resampling-4] The print method inside of the tibble shows the frequency of each: `[2107/235]` indicates that about two thousand samples are in the analysis set and 235 are in that particular assessment set.\n\n[^10-resampling-4]: To see this for yourself, try executing `lobstr::obj_size(ames_folds)` and `lobstr::obj_size(ames_train)`. The size of the resample object is much less than ten times the size of the original data.\n\nThese objects also always contain a character column called `id` that labels the partition.[^10-resampling-5]\n\n[^10-resampling-5]: Some resampling methods require multiple `id` fields.\n\nTo manually retrieve the partitioned data, the `analysis()` and `assessment()` functions return the corresponding data frames:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# For the first fold:\names_folds$splits[[1]] %>% analysis() %>% dim()\n## [1] 2107   74\n```\n:::\n\n\nThe <span class=\"pkg\">tidymodels</span> packages, such as [<span class=\"pkg\">tune</span>](https://tune.tidymodels.org/), contain high-level user interfaces so that functions like `analysis()` are not generally needed for day-to-day work. @sec-resampling-performance demonstrates a function to fit a model over these resamples.\n\nThere are a variety of cross-validation variations; we'll go through the most important ones.\n\n### Repeated cross-validation {.unnumbered}\n\nThe most important variation on cross-validation is repeated *V*-fold cross-validation. Depending on data size or other characteristics, the resampling estimate produced by *V*-fold cross-validation may be excessively noisy.[^10-resampling-6] As with many statistical problems, one way to reduce noise is to gather more data. For cross-validation, this means averaging more than *V* statistics.\n\n[^10-resampling-6]: For more details, see Section 3.4.6 of @fes: <https://bookdown.org/max/FES/resampling.html#resample-var-bias>.\n\nTo create *R* repeats of *V*-fold cross-validation, the same fold generation process is done *R* times to generate *R* collections of *V* partitions. Now, instead of averaging *V* statistics, $V \\times R$ statistics produce the final resampling estimate. Due to the Central Limit Theorem, the summary statistics from each model tend toward a normal distribution, as long as we have a lot of data relative to $V \\times R$.\n\nConsider the Ames data. On average, 10-fold cross-validation uses assessment sets that contain roughly 234 properties. If RMSE is the statistic of choice, we can denote that estimate's standard deviation as $\\sigma$. With simple 10-fold cross-validation, the standard error of the mean RMSE is $\\sigma/\\sqrt{10}$. If this is too noisy, repeats reduce the standard error to $\\sigma/\\sqrt{10R}$. For 10-fold cross-validation with $R$ replicates, the plot in @fig-variance-reduction shows how quickly the standard error[^10-resampling-7] decreases with replicates.\n\n[^10-resampling-7]: These are *approximate* standard errors. As will be discussed in the next chapter, there is a within-replicate correlation that is typical of resampled results. By ignoring this extra component of variation, the simple calculations shown in this plot are overestimates of the reduction in noise in the standard errors.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Relationship between the relative variance in performance estimates versus the number of cross-validation repeats](figures/fig-variance-reduction-1.png){#fig-variance-reduction fig-align='center' fig-alt='The relationship between the relative variance in performance estimates versus the number of cross-validation repeats. As the repeats increase, the variance is reduced in a harmonically decreasing pattern with diminishing returns for large number of replicates.' width=672}\n:::\n:::\n\n\nLarger numbers of replicates tend to have less impact on the standard error. However, if the baseline value of $\\sigma$ is impractically large, the diminishing returns on replication may still be worth the extra computational costs.\n\nTo create repeats, invoke `vfold_cv()` with an additional argument `repeats`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvfold_cv(ames_train, v = 10, repeats = 5)\n## #  10-fold cross-validation repeated 5 times \n## # A tibble: 50 × 3\n##   splits             id      id2   \n##   <list>             <chr>   <chr> \n## 1 <split [2107/235]> Repeat1 Fold01\n## 2 <split [2107/235]> Repeat1 Fold02\n## 3 <split [2108/234]> Repeat1 Fold03\n## 4 <split [2108/234]> Repeat1 Fold04\n## 5 <split [2108/234]> Repeat1 Fold05\n## 6 <split [2108/234]> Repeat1 Fold06\n## # ℹ 44 more rows\n```\n:::\n\n\n### Leave-one-out cross-validation {.unnumbered}\n\nOne variation of cross-validation is leave-one-out (LOO) cross-validation. If there are $n$ training set samples, $n$ models are fit using $n-1$ rows of the training set. Each model predicts the single excluded data point. At the end of resampling, the $n$ predictions are pooled to produce a single performance statistic.\n\nLeave-one-out methods are deficient compared to almost any other method. For anything but pathologically small samples, LOO is computationally excessive, and it may not have good statistical properties. Although the <span class=\"pkg\">rsample</span> package contains a `loo_cv()` function, these objects are not generally integrated into the broader tidymodels frameworks.\n\n### Monte Carlo cross-validation {.unnumbered}\n\nAnother variant of *V*-fold cross-validation is Monte Carlo cross-validation (MCCV, @xu2001monte). Like *V*-fold cross-validation, it allocates a fixed proportion of data to the assessment sets. The difference between MCCV and regular cross-validation is that, for MCCV, this proportion of the data is randomly selected each time. This results in assessment sets that are not mutually exclusive. To create these resampling objects:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmc_cv(ames_train, prop = 9/10, times = 20)\n## # Monte Carlo cross-validation (0.9/0.1) with 20 resamples  \n## # A tibble: 20 × 2\n##   splits             id        \n##   <list>             <chr>     \n## 1 <split [2107/235]> Resample01\n## 2 <split [2107/235]> Resample02\n## 3 <split [2107/235]> Resample03\n## 4 <split [2107/235]> Resample04\n## 5 <split [2107/235]> Resample05\n## 6 <split [2107/235]> Resample06\n## # ℹ 14 more rows\n```\n:::\n\n\n### Validation sets {#sec-validation}\n\nIn @sec-what-about-a-validation-set, we briefly discussed the use of a validation set, a single partition that is set aside to estimate performance separate from the test set. When using a validation set, the initial available data set is split into a training set, a validation set, and a test set (see @fig-three-way-split).\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![A three-way initial split into training, testing, and validation sets](premade/validation.svg){#fig-three-way-split fig-align='center' fig-alt='A three-way initial split into training, testing, and validation sets.' width=50%}\n:::\n:::\n\n\nValidation sets are often used when the original pool of data is very large. In this case, a single large partition may be adequate to characterize model performance without having to do multiple resampling iterations.\n\nWith the <span class=\"pkg\">rsample</span> package, a validation set is like any other resampling object; this type is different only in that it has a single iteration.[^10-resampling-8] @fig-validation-split shows this scheme.\n\n[^10-resampling-8]: In essence, a validation set can be considered a single iteration of Monte Carlo cross-validation.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![A two-way initial split into training and testing with an additional validation set split on the training set](premade/validation-alt.svg){#fig-validation-split fig-align='center' fig-alt='A two-way initial split into training and testing with an additional validation set split on the training set.' width=45%}\n:::\n:::\n\n\nTo build on the code from @sec-what-about-a-validation-set, the function `validation_set()` can take the results of `initial_validation_split()` and convert it to an rset object that is similar to the ones produced by functions such as `vfold_cv()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Previously:\n\nset.seed(52)\n# To put 60% into training, 20% in validation, and 20% in testing:\names_val_split <- initial_validation_split(ames, prop = c(0.6, 0.2))\names_val_split\n## <Training/Validation/Testing/Total>\n## <1758/586/586/2930>\n\n# Object used for resampling: \nval_set <- validation_set(ames_val_split)\nval_set\n## # A tibble: 1 × 2\n##   splits             id        \n##   <list>             <chr>     \n## 1 <split [1758/586]> validation\n```\n:::\n\n\nAs you'll see in @sec-resampling-performance, the `fit_resamples()` function will be used to compute correct estimates of performance using resampling. The `val_set` object can be used in in this and other functions even though it is a single \"resample\" of the data.\n\n### Bootstrapping {#sec-bootstrap}\n\nBootstrap resampling was originally invented as a method for approximating the sampling distribution of statistics whose theoretical properties are intractable [@davison1997bootstrap]. Using it to estimate model performance is a secondary application of the method.\n\nA bootstrap sample of the training set is a sample that is the same size as the training set but is drawn *with replacement*. This means that some training set data points are selected multiple times for the analysis set. Each data point has a 63.2% chance of inclusion in the training set at least once. The assessment set contains all of the training set samples that were not selected for the analysis set (on average, with 36.8% of the training set). When bootstrapping, the assessment set is often called the *out-of-bag* sample.\n\nFor a training set of 30 samples, a schematic of three bootstrap samples is shown in @fig-bootstrapping.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Bootstrapping data usage](premade/bootstraps.svg){#fig-bootstrapping fig-align='center' fig-alt='A diagram of bootstrapping data usage. For each bootstrap resample, the analysis set is the same size as the training set (due to sampling with replacement) and the assessment set consists of samples not in the analysis set.' width=80%}\n:::\n:::\n\n\nNote that the sizes of the assessment sets vary.\n\nUsing the <span class=\"pkg\">rsample</span> package, we can create such bootstrap resamples:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbootstraps(ames_train, times = 5)\n## # Bootstrap sampling \n## # A tibble: 5 × 2\n##   splits             id        \n##   <list>             <chr>     \n## 1 <split [2342/867]> Bootstrap1\n## 2 <split [2342/869]> Bootstrap2\n## 3 <split [2342/859]> Bootstrap3\n## 4 <split [2342/858]> Bootstrap4\n## 5 <split [2342/873]> Bootstrap5\n```\n:::\n\n\nBootstrap samples produce performance estimates that have very low variance (unlike cross-validation) but have significant pessimistic bias. This means that, if the true accuracy of a model is 90%, the bootstrap would tend to estimate the value to be less than 90%. The amount of bias cannot be empirically determined with sufficient accuracy. Additionally, the amount of bias changes over the scale of the performance metric. For example, the bias is likely to be different when the accuracy is 90% versus when it is 70%.\n\nThe bootstrap is also used inside of many models. For example, the random forest model mentioned earlier contained 1,000 individual decision trees. Each tree was the product of a different bootstrap sample of the training set.\n\n### Rolling forecasting origin resampling {#sec-rolling}\n\nWhen the data have a strong time component, a resampling method should support modeling to estimate seasonal and other temporal trends within the data. A technique that randomly samples values from the training set can disrupt the model's ability to estimate these patterns.\n\nRolling forecast origin resampling [@hyndman2018forecasting] provides a method that emulates how time series data is often partitioned in practice, estimating the model with historical data and evaluating it with the most recent data. For this type of resampling, the size of the initial analysis and assessment sets are specified. The first iteration of resampling uses these sizes, starting from the beginning of the series. The second iteration uses the same data sizes but shifts over by a set number of samples.\n\nTo illustrate, a training set of fifteen samples was resampled with an analysis size of eight samples and an assessment set size of three. The second iteration discards the first training set sample and both data sets shift forward by one. This configuration results in five resamples, as shown in @fig-rolling.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Data usage for rolling forecasting origin resampling](premade/rolling.svg){#fig-rolling fig-align='center' fig-alt='The data usage for rolling forecasting origin resampling. For each split, earlier data are used for modeling and a few subsequent instances are used to measure performance.' width=65%}\n:::\n:::\n\n\nHere are two different configurations of this method:\n\n-   The analysis set can cumulatively grow (as opposed to remaining the same size). After the first initial analysis set, new samples can accrue without discarding the earlier data.\n\n-   The resamples need not increment by one. For example, for large data sets, the incremental block could be a week or month instead of a day.\n\nFor a year's worth of data, suppose that six sets of 30-day blocks define the analysis set. For assessment sets of 30 days with a 29-day skip, we can use the <span class=\"pkg\">rsample</span> package to specify:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntime_slices <- \n  tibble(x = 1:365) %>% \n  rolling_origin(initial = 6 * 30, assess = 30, skip = 29, cumulative = FALSE)\n\ndata_range <- function(x) {\n  summarize(x, first = min(x), last = max(x))\n}\n\nmap_dfr(time_slices$splits, ~   analysis(.x) %>% data_range())\n## # A tibble: 6 × 2\n##   first  last\n##   <int> <int>\n## 1     1   180\n## 2    31   210\n## 3    61   240\n## 4    91   270\n## 5   121   300\n## 6   151   330\nmap_dfr(time_slices$splits, ~ assessment(.x) %>% data_range())\n## # A tibble: 6 × 2\n##   first  last\n##   <int> <int>\n## 1   181   210\n## 2   211   240\n## 3   241   270\n## 4   271   300\n## 5   301   330\n## 6   331   360\n```\n:::\n\n\n## Estimating Performance {#sec-resampling-performance}\n\nAny of the resampling methods discussed in this chapter can be used to evaluate the modeling process (including preprocessing, model fitting, etc). These methods are effective because different groups of data are used to train the model and assess the model. To reiterate, the process to use resampling is:\n\n1.  During resampling, the analysis set is used to preprocess the data, apply the preprocessing to itself, and use these processed data to fit the model.\n\n2.  The preprocessing statistics produced by the analysis set are applied to the assessment set. The predictions from the assessment set estimate performance on new data.\n\nThis sequence repeats for every resample. If there are *B* resamples, there are *B* replicates of each of the performance metrics. The final resampling estimate is the average of these *B* statistics. If *B* = 1, as with a validation set, the individual statistics represent overall performance.\n\nLet's reconsider the previous random forest model contained in the `rf_wflow` object. The `fit_resamples()` function is analogous to `fit()`, but instead of having a `data` argument, `fit_resamples()` has `resamples`, which expects an `rset` object like the ones shown in this chapter. The possible interfaces to the function are:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_spec %>% fit_resamples(formula,  resamples, ...)\nmodel_spec %>% fit_resamples(recipe,   resamples, ...)\nworkflow   %>% fit_resamples(          resamples, ...)\n```\n:::\n\n\nThere are a number of other optional arguments, such as:\n\n-   `metrics`: A metric set of performance statistics to compute. By default, regression models use RMSE and $R^2$ while classification models compute the area under the ROC curve and overall accuracy. Note that this choice also defines what predictions are produced during the evaluation of the model. For classification, if only accuracy is requested, class probability estimates are not generated for the assessment set (since they are not needed).\n\n-   `control`: A list created by `control_resamples()` with various options.\n\nThe control arguments include:\n\n-   `verbose`: A logical for printing logging.\n\n-   `extract`: A function for retaining objects from each model iteration (discussed later in this chapter).\n\n-   `save_pred`: A logical for saving the assessment set predictions.\n\nFor our example, let's save the predictions in order to visualize the model fit and residuals:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkeep_pred <- control_resamples(save_pred = TRUE, save_workflow = TRUE)\n\nset.seed(1003)\nrf_res <- \n  rf_wflow %>% \n  fit_resamples(resamples = ames_folds, control = keep_pred)\nrf_res\n## # Resampling results\n## # 10-fold cross-validation \n## # A tibble: 10 × 5\n##   splits             id     .metrics         .notes           .predictions      \n##   <list>             <chr>  <list>           <list>           <list>            \n## 1 <split [2107/235]> Fold01 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [235 × 4]>\n## 2 <split [2107/235]> Fold02 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [235 × 4]>\n## 3 <split [2108/234]> Fold03 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [234 × 4]>\n## 4 <split [2108/234]> Fold04 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [234 × 4]>\n## 5 <split [2108/234]> Fold05 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [234 × 4]>\n## 6 <split [2108/234]> Fold06 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [234 × 4]>\n## # ℹ 4 more rows\n```\n:::\n\n\n\n\nThe return value is a tibble similar to the input resamples, along with some extra columns:\n\n-   `.metrics` is a list column of tibbles containing the assessment set performance statistics.\n\n-   `.notes` is another list column of tibbles cataloging any warnings or errors generated during resampling. Note that errors will not stop subsequent execution of resampling.\n\n-   `.predictions` is present when `save_pred = TRUE`. This list column contains tibbles with the out-of-sample predictions.\n\nWhile these list columns may look daunting, they can be easily reconfigured using <span class=\"pkg\">tidyr</span> or with convenience functions that tidymodels provides. For example, to return the performance metrics in a more usable format:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncollect_metrics(rf_res)\n## # A tibble: 2 × 6\n##   .metric .estimator   mean     n std_err .config             \n##   <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n## 1 rmse    standard   0.0721    10 0.00305 Preprocessor1_Model1\n## 2 rsq     standard   0.831     10 0.0108  Preprocessor1_Model1\n```\n:::\n\n\nThese are the resampling estimates averaged over the individual replicates. To get the metrics for each resample, use the option `summarize = FALSE`.\n\nNotice how much more realistic the performance estimates are than the resubstitution estimates from @sec-resampling-resubstition!\n\nTo obtain the assessment set predictions:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nassess_res <- collect_predictions(rf_res)\nassess_res\n## # A tibble: 2,342 × 5\n##   id     .pred  .row Sale_Price .config             \n##   <chr>  <dbl> <int>      <dbl> <chr>               \n## 1 Fold01  5.10    10       5.09 Preprocessor1_Model1\n## 2 Fold01  4.92    27       4.90 Preprocessor1_Model1\n## 3 Fold01  5.21    47       5.08 Preprocessor1_Model1\n## 4 Fold01  5.13    52       5.10 Preprocessor1_Model1\n## 5 Fold01  5.13    59       5.10 Preprocessor1_Model1\n## 6 Fold01  5.13    63       5.11 Preprocessor1_Model1\n## # ℹ 2,336 more rows\n```\n:::\n\n\nThe prediction column names follow the conventions discussed for <span class=\"pkg\">parsnip</span> models in [Chapter @sec-models], for consistency and ease of use. The observed outcome column always uses the original column name from the source data. The `.row` column is an integer that matches the row of the original training set so that these results can be properly arranged and joined with the original data.\n\n::: rmdnote\nFor some resampling methods, such as the bootstrap or repeated cross-validation, there will be multiple predictions per row of the original training set. To obtain summarized values (averages of the replicate predictions) use `collect_predictions(object, summarize = TRUE)`.\n:::\n\nSince this analysis used 10-fold cross-validation, there is one unique prediction for each training set sample. These data can generate helpful plots of the model to understand where it potentially failed. For example, @fig-ames-resampled-performance compares the observed and held-out predicted values (analogous to @fig-ames-performance-plot):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nassess_res %>% \n  ggplot(aes(x = Sale_Price, y = .pred)) + \n  geom_point(alpha = .15) +\n  geom_abline(color = \"red\") + \n  coord_obs_pred() + \n  ylab(\"Predicted\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Out-of-sample observed versus predicted values for an Ames regression model, using log-10 units on both axes](figures/fig-ames-resampled-performance-1.png){#fig-ames-resampled-performance fig-align='center' fig-alt='Scatter plots of out-of-sample observed versus predicted values for an Ames regression model. Both axes using log-10 units. The model shows good concordance with two outlying data points that are significantly over-predicted.' width=480}\n:::\n:::\n\n\nThere are two houses in the training set with a low observed sale price that are significantly overpredicted by the model. Which houses are these? Let's find out from the `assess_res` result:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nover_predicted <- \n  assess_res %>% \n  mutate(residual = Sale_Price - .pred) %>% \n  arrange(desc(abs(residual))) %>% \n  slice(1:2)\nover_predicted\n## # A tibble: 2 × 6\n##   id     .pred  .row Sale_Price .config              residual\n##   <chr>  <dbl> <int>      <dbl> <chr>                   <dbl>\n## 1 Fold09  4.96    32       4.11 Preprocessor1_Model1   -0.858\n## 2 Fold08  4.93   317       4.12 Preprocessor1_Model1   -0.816\n\names_train %>% \n  slice(over_predicted$.row) %>% \n  select(Gr_Liv_Area, Neighborhood, Year_Built, Bedroom_AbvGr, Full_Bath)\n## # A tibble: 2 × 5\n##   Gr_Liv_Area Neighborhood           Year_Built Bedroom_AbvGr Full_Bath\n##         <int> <fct>                       <int>         <int>     <int>\n## 1         832 Old_Town                     1923             2         1\n## 2         733 Iowa_DOT_and_Rail_Road       1952             2         1\n```\n:::\n\n\nIdentifying examples like these with especially poor performance can help us follow up and investigate why these specific predictions are so poor.\n\nLet's move back to the homes overall. How can we use a validation set instead of cross-validation? From our previous <span class=\"pkg\">rsample</span> object:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nval_res <- rf_wflow %>% fit_resamples(resamples = val_set)\n## Warning in `[.tbl_df`(x, is.finite(x <- as.numeric(x))): NAs introducidos por\n## coerción\nval_res\n## # Resampling results\n## #  \n## # A tibble: 1 × 4\n##   splits             id         .metrics         .notes          \n##   <list>             <chr>      <list>           <list>          \n## 1 <split [1758/586]> validation <tibble [2 × 4]> <tibble [0 × 3]>\n\ncollect_metrics(val_res)\n## # A tibble: 2 × 6\n##   .metric .estimator   mean     n std_err .config             \n##   <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n## 1 rmse    standard   0.0727     1      NA Preprocessor1_Model1\n## 2 rsq     standard   0.823      1      NA Preprocessor1_Model1\n```\n:::\n\n\nThese results are also much closer to the test set results than the resubstitution estimates of performance.\n\n::: rmdnote\nIn these analyses, the resampling results are very close to the test set results. The two types of estimates tend to be well correlated. However, this could be from random chance. A seed value of `55` fixed the random numbers before creating the resamples. Try changing this value and re-running the analyses to investigate whether the resampled estimates match the test set results as well.\n:::\n\n## Parallel Processing {#sec-parallel}\n\nThe models created during resampling are independent of one another. Computations of this kind are sometimes called *embarrassingly parallel*; each model could be fit simultaneously without issues.[^10-resampling-9] The <span class=\"pkg\">tune</span> package uses the [<span class=\"pkg\">foreach</span>](https://CRAN.R-project.org/package=foreach) package to facilitate parallel computations. These computations could be split across processors on the same computer or across different computers, depending on the chosen technology.\n\n[^10-resampling-9]: @parallel gives a technical overview of these technologies.\n\nFor computations conducted on a single computer, the number of possible worker processes is determined by the <span class=\"pkg\">parallel</span> package:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# The number of physical cores in the hardware:\nparallel::detectCores(logical = FALSE)\n## [1] 2\n\n# The number of possible independent processes that can \n# be simultaneously used:  \nparallel::detectCores(logical = TRUE)\n## [1] 4\n```\n:::\n\n\nThe difference between these two values is related to the computer's processor. For example, most Intel processors use hyperthreading, which creates two virtual cores for each physical core. While these extra resources can improve performance, most of the speed-ups produced by parallel processing occur when processing uses fewer than the number of physical cores.\n\nFor `fit_resamples()` and other functions in <span class=\"pkg\">tune</span>, parallel processing occurs when the user registers a parallel backend package. These R packages define how to execute parallel processing. On Unix and macOS operating systems, one method of splitting computations is by forking threads. To enable this, load the <span class=\"pkg\">doMC</span> package and register the number of parallel cores with <span class=\"pkg\">foreach</span>:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Unix and macOS only\nlibrary(doMC)\nregisterDoMC(cores = 2)\n\n# Now run fit_resamples()...\n```\n:::\n\n\nThis instructs `fit_resamples()` to run half of the computations on each of two cores. To reset the computations to sequential processing:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nregisterDoSEQ()\n```\n:::\n\n\nAlternatively, a different approach to parallelizing computations uses network sockets. The <span class=\"pkg\">doParallel</span> package enables this method (usable by all operating systems):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# All operating systems\nlibrary(doParallel)\n\n# Create a cluster object and then register: \ncl <- makePSOCKcluster(2)\nregisterDoParallel(cl)\n\n# Now run fit_resamples()`...\n\nstopCluster(cl)\n```\n:::\n\n\nAnother R package that facilitates parallel processing is the [<span class=\"pkg\">future</span>](https://future.futureverse.org/) package. Like <span class=\"pkg\">foreach</span>, it provides a framework for parallelism. This package is used in conjunction with <span class=\"pkg\">foreach</span> via the <span class=\"pkg\">doFuture</span> package.\n\n::: rmdnote\nThe R packages with parallel backends for <span class=\"pkg\">foreach</span> start with the prefix `\"do\"`.\n:::\n\nParallel processing with the <span class=\"pkg\">tune</span> package tends to provide linear speed-ups for the first few cores. This means that, with two cores, the computations are twice as fast. Depending on the data and model type, the linear speed-up deteriorates after four to five cores. Using more cores will still reduce the time it takes to complete the task; there are just diminishing returns for the additional cores.\n\nLet's wrap up with one final note about parallelism. For each of these technologies, the memory requirements multiply for each additional core used. For example, if the current data set is 2 GB in memory and three cores are used, the total memory requirement is 8 GB (2 for each worker process plus the original). Using too many cores might cause the computations (and the computer) to slow considerably.\n\n## Saving the Resampled Objects {#sec-extract}\n\nThe models created during resampling are not retained. These models are trained for the purpose of evaluating performance, and we typically do not need them after we have computed performance statistics. If a particular modeling approach does turn out to be the best option for our data set, then the best choice is to fit again to the whole training set so the model parameters can be estimated with more data.\n\nWhile these models created during resampling are not preserved, there is a method for keeping them or some of their components. The `extract` option of `control_resamples()` specifies a function that takes a single argument; we'll use `x`. When executed, `x` results in a fitted workflow object, regardless of whether you provided `fit_resamples()` with a workflow. Recall that the <span class=\"pkg\">workflows</span> package has functions that can pull the different components of the objects (e.g., the model, recipe, etc.).\n\nLet's fit a linear regression model using the recipe we developed in [Chapter @sec-recipes]:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\names_rec <- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %>%\n  step_other(Neighborhood, threshold = 0.01) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %>% \n  step_ns(Latitude, Longitude, deg_free = 20)\n\nlm_wflow <-  \n  workflow() %>% \n  add_recipe(ames_rec) %>% \n  add_model(linear_reg() %>% set_engine(\"lm\")) \n\nlm_fit <- lm_wflow %>% fit(data = ames_train)\n\n# Select the recipe: \nextract_recipe(lm_fit, estimated = TRUE)\n## \n## ── Recipe ───────────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:   1\n## predictor: 6\n## \n## ── Training information\n## Training data contained 2342 data points and no incomplete rows.\n## \n## ── Operations\n## • Collapsing factor levels for: Neighborhood | Trained\n## • Dummy variables from: Neighborhood, Bldg_Type | Trained\n## • Interactions with: Gr_Liv_Area:(Bldg_Type_TwoFmCon + Bldg_Type_Duplex +\n##   Bldg_Type_Twnhs + Bldg_Type_TwnhsE) | Trained\n## • Natural splines on: Latitude, Longitude | Trained\n```\n:::\n\n\nWe can save the linear model coefficients for a fitted model object from a workflow:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nget_model <- function(x) {\n  extract_fit_parsnip(x) %>% tidy()\n}\n\n# Test it using: \n# get_model(lm_fit)\n```\n:::\n\n\nNow let's apply this function to the ten resampled fits. The results of the extraction function is wrapped in a list object and returned in a tibble:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nctrl <- control_resamples(extract = get_model)\n\nlm_res <- lm_wflow %>%  fit_resamples(resamples = ames_folds, control = ctrl)\nlm_res\n## # Resampling results\n## # 10-fold cross-validation \n## # A tibble: 10 × 5\n##   splits             id     .metrics         .notes           .extracts       \n##   <list>             <chr>  <list>           <list>           <list>          \n## 1 <split [2107/235]> Fold01 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [1 × 2]>\n## 2 <split [2107/235]> Fold02 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [1 × 2]>\n## 3 <split [2108/234]> Fold03 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [1 × 2]>\n## 4 <split [2108/234]> Fold04 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [1 × 2]>\n## 5 <split [2108/234]> Fold05 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [1 × 2]>\n## 6 <split [2108/234]> Fold06 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [1 × 2]>\n## # ℹ 4 more rows\n```\n:::\n\n\nNow there is a `.extracts` column with nested tibbles. What do these contain? Let's find out by subsetting.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm_res$.extracts[[1]]\n## # A tibble: 1 × 2\n##   .extracts         .config             \n##   <list>            <chr>               \n## 1 <tibble [73 × 5]> Preprocessor1_Model1\n\n# To get the results\nlm_res$.extracts[[1]][[1]]\n## [[1]]\n## # A tibble: 73 × 5\n##   term                        estimate  std.error statistic   p.value\n##   <chr>                          <dbl>      <dbl>     <dbl>     <dbl>\n## 1 (Intercept)                 1.48     0.320         4.62   4.11e-  6\n## 2 Gr_Liv_Area                 0.000158 0.00000476   33.2    9.72e-194\n## 3 Year_Built                  0.00180  0.000149     12.1    1.57e- 32\n## 4 Neighborhood_College_Creek -0.00163  0.0373       -0.0438 9.65e-  1\n## 5 Neighborhood_Old_Town      -0.0757   0.0138       -5.47   4.92e-  8\n## 6 Neighborhood_Edwards       -0.109    0.0310       -3.53   4.21e-  4\n## # ℹ 67 more rows\n```\n:::\n\n\nThis might appear to be a convoluted method for saving the model results. However, `extract` is flexible and does not assume that the user will only save a single tibble per resample. For example, the `tidy()` method might be run on the recipe as well as the model. In this case, a list of two tibbles will be returned.\n\nFor our more simple example, all of the results can be flattened and collected using:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nall_coef <- map_dfr(lm_res$.extracts, ~ .x[[1]][[1]])\n# Show the replicates for a single predictor:\nfilter(all_coef, term == \"Year_Built\")\n## # A tibble: 10 × 5\n##   term       estimate std.error statistic  p.value\n##   <chr>         <dbl>     <dbl>     <dbl>    <dbl>\n## 1 Year_Built  0.00180  0.000149      12.1 1.57e-32\n## 2 Year_Built  0.00180  0.000151      12.0 6.45e-32\n## 3 Year_Built  0.00185  0.000150      12.3 1.00e-33\n## 4 Year_Built  0.00183  0.000147      12.5 1.90e-34\n## 5 Year_Built  0.00184  0.000150      12.2 2.47e-33\n## 6 Year_Built  0.00180  0.000150      12.0 3.35e-32\n## # ℹ 4 more rows\n```\n:::\n\n\nChapters [-@sec-grid-search] and [-@sec-iterative-search] discuss a suite of functions for tuning models. Their interfaces are similar to `fit_resamples()` and many of the features described here apply to those functions.\n\n## Chapter Summary {#sec-resampling-summary}\n\nThis chapter describes one of the fundamental tools of data analysis, the ability to measure the performance and variation in model results. Resampling enables us to determine how well the model works without using the test set.\n\nAn important function from the <span class=\"pkg\">tune</span> package, called `fit_resamples()`, was introduced. The interface for this function is also used in future chapters that describe model tuning tools.\n\nThe data analysis code, so far, for the Ames data is:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels)\ndata(ames)\names <- mutate(ames, Sale_Price = log10(Sale_Price))\n\nset.seed(502)\names_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train <- training(ames_split)\names_test  <-  testing(ames_split)\n\names_rec <- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %>%\n  step_log(Gr_Liv_Area, base = 10) %>% \n  step_other(Neighborhood, threshold = 0.01) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %>% \n  step_ns(Latitude, Longitude, deg_free = 20)\n\nlm_model <- linear_reg() %>% set_engine(\"lm\")\n\nlm_wflow <- \n  workflow() %>% \n  add_model(lm_model) %>% \n  add_recipe(ames_rec)\n\nlm_fit <- fit(lm_wflow, ames_train)\n\nrf_model <- \n  rand_forest(trees = 1000) %>% \n  set_engine(\"ranger\") %>% \n  set_mode(\"regression\")\n\nrf_wflow <- \n  workflow() %>% \n  add_formula(\n    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n      Latitude + Longitude) %>% \n  add_model(rf_model) \n\nset.seed(1001)\names_folds <- vfold_cv(ames_train, v = 10)\n\nkeep_pred <- control_resamples(save_pred = TRUE, save_workflow = TRUE)\n\nset.seed(1003)\nrf_res <- rf_wflow %>% fit_resamples(resamples = ames_folds, control = keep_pred)\n```\n:::\n",
    "supporting": [
      "10-resampling_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}