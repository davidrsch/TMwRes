{
  "hash": "8f8ce952b4b6ddaa3f001be22e083f5c",
  "result": {
    "markdown": "\n\n\n# Remuestrear para Evaluar el Rendimiento {#sec-resampling}\n\nYa hemos cubierto varias piezas que se deben juntar para evaluar el desempeño de un modelo. El [Capítulo @sec-performance] describió estadísticas para medir el rendimiento del modelo. El [Capítulo @sec-splitting] introdujo la idea del gasto de datos y recomendamos el conjunto de pruebas para obtener una estimación imparcial del rendimiento. Sin embargo, normalmente necesitamos comprender el rendimiento de un modelo o incluso de varios modelos *antes de utilizar el conjunto de prueba*.\n\n::: rmdwarning\nNormalmente no podemos decidir qué modelo final usar con el conjunto de pruebas antes de evaluar primero el rendimiento del modelo. Existe una brecha entre nuestra necesidad de medir el desempeño de manera confiable y las divisiones de datos (capacitación y pruebas) que tenemos disponibles.\n:::\n\nEn este capítulo, describimos un enfoque llamado remuestreo que puede llenar este vacío. Las estimaciones de rendimiento remuestreadas pueden generalizarse a nuevos datos de manera similar a las estimaciones de un conjunto de pruebas. El próximo capítulo complementa éste demostrando métodos estadísticos que comparan los resultados del remuestreo.\n\nPara apreciar plenamente el valor del remuestreo, primero echemos un vistazo al enfoque de resustitución, que a menudo puede fallar.\n\n## El Enfoque de la Resustitución {#sec-resampling-resubstition}\n\nCuando medimos el rendimiento con los mismos datos que utilizamos para el entrenamiento (a diferencia de datos nuevos o datos de prueba), decimos que hemos *resustituido* los datos. Utilicemos nuevamente los datos de vivienda de Ames para demostrar estos conceptos. @sec-recipes-summary resume el estado actual de nuestro análisis de Ames. Incluye un objeto de receta llamado `ames_rec`, un modelo lineal y un flujo de trabajo que usa esa receta y modelo llamado `lm_wflow`. Este flujo de trabajo se ajustó al conjunto de entrenamiento, lo que dio como resultado `lm_fit`.\n\nPara comparar con este modelo lineal, también podemos ajustar un tipo diferente de modelo. Los *random forest* son un método de conjunto de árboles que opera creando una gran cantidad de árboles de decisión a partir de versiones ligeramente diferentes del conjunto de entrenamiento [@breiman2001random]. Esta colección de árboles conforma el conjunto. Al predecir una nueva muestra, cada miembro del conjunto realiza una predicción por separado. Estos se promedian para crear la predicción conjunta final para el nuevo punto de datos.\n\nLos modelos de random forest son muy potentes y pueden emular muy fielmente los patrones de datos subyacentes. Si bien este modelo puede requerir un uso intensivo de computación, requiere muy poco mantenimiento; se requiere muy poco preprocesamiento (como se documenta en [Apéndice @sec-pre-proc-table]).\n\nUsando el mismo conjunto de predictores que el modelo lineal (sin los pasos de preprocesamiento adicionales), podemos ajustar un modelo de random forest al conjunto de entrenamiento a través del motor `\"ranger\"` (que usa el paquete R <span class=\"pkg\">ranger</span> para cálculo). Este modelo no requiere procesamiento previo, por lo que se puede utilizar una fórmula simple:\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-rand-forest-spec_8a5429c0f01b93ef96984bd72b1f3f9e'}\n\n```{.r .cell-code}\nrf_model <- \n  rand_forest(trees = 1000) %>% \n  set_engine(\"ranger\") %>% \n  set_mode(\"regression\")\n\nrf_wflow <- \n  workflow() %>% \n  add_formula(\n    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n      Latitude + Longitude) %>% \n  add_model(rf_model) \n\nrf_fit <- rf_wflow %>% fit(data = ames_train)\n```\n:::\n\n\n¿Cómo deberíamos comparar los modelos forestales lineales y aleatorios? A modo de demostración, predeciremos que el conjunto de entrenamiento producirá lo que se conoce como *métrica aparente* o *métrica de resustitución*. Esta función crea predicciones y formatea los resultados:\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-eval-func_8ed58fe0bd413e031fc5cd33951c36b8'}\n\n```{.r .cell-code}\nestimate_perf <- function(model, dat) {\n  # Captura los nombres de los objetos `model` y `dat`\n  cl <- match.call()\n  obj_name <- as.character(cl$model)\n  data_name <- as.character(cl$dat)\n  data_name <- gsub(\"ames_\", \"\", data_name)\n  \n  # Estima estas metricas\n  reg_metrics <- metric_set(rmse, rsq)\n  \n  model %>%\n    predict(dat) %>%\n    bind_cols(dat %>% select(Sale_Price)) %>%\n    reg_metrics(Sale_Price, .pred) %>%\n    select(-.estimator) %>%\n    mutate(object = obj_name, data = data_name)\n}\n```\n:::\n\n\nSe calculan tanto RMSE como $R^2$. Las estadísticas de resustitución son:\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-eval-train_fceb338bd7eee9acba4c41bc7d8da1f5'}\n\n```{.r .cell-code}\nestimate_perf(rf_fit, ames_train)\n## # A tibble: 2 × 4\n##   .metric .estimate object data \n##   <chr>       <dbl> <chr>  <chr>\n## 1 rmse       0.0366 rf_fit train\n## 2 rsq        0.960  rf_fit train\nestimate_perf(lm_fit, ames_train)\n## # A tibble: 2 × 4\n##   .metric .estimate object data \n##   <chr>       <dbl> <chr>  <chr>\n## 1 rmse       0.0754 lm_fit train\n## 2 rsq        0.816  lm_fit train\n```\n:::\n\n\n\n\nEn base a estos resultados, el random forest es mucho más capaz de predecir los precios de venta; la estimación de RMSE es two, veces mejor que la regresión lineal. Si tuviéramos que elegir entre estos dos modelos para este problema de predicción de precios, probablemente elegiríamos el random fores porque, en la escala logarítmica que estamos usando, su RMSE es aproximadamente la mitad. El siguiente paso aplica el modelo de random forest al conjunto de prueba para la verificación final:\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-eval-test-rf_ba74cdf4edab422c5033e306106aa4b2'}\n\n```{.r .cell-code}\nestimate_perf(rf_fit, ames_test)\n## # A tibble: 2 × 4\n##   .metric .estimate object data \n##   <chr>       <dbl> <chr>  <chr>\n## 1 rmse       0.0704 rf_fit test \n## 2 rsq        0.852  rf_fit test\n```\n:::\n\n\n¡La estimación RMSE del conjunto de prueba, 0.0704, es *much worse than the training set* valor de 0.0366! ¿Por qué pasó esto?\n\nMuchos modelos predictivos son capaces de aprender tendencias complejas a partir de los datos. En estadística, estos se conocen comúnmente como *modelos de bajo sesgo*.\n\n::: rmdnote\nEn este contexto, *sesgo* es la diferencia entre el patrón o las relaciones verdaderas en los datos y los tipos de patrones que el modelo puede emular. Muchos modelos de aprendizaje automático de caja negra tienen un sesgo bajo, lo que significa que pueden reproducir relaciones complejas. Otros modelos (como la regresión lineal/logística, el análisis discriminante y otros) no son tan adaptables y se consideran modelos de *alto sesgo*.[^10-resampling-1]\n:::\n\n[^10-resampling-1]: Consulte la Sección 1.2.5 de @fes para una discusión: <https://bookdown.org/max/FES/important-concepts.html#model-bias-and-variance>\n\nPara un modelo de bajo sesgo, el alto grado de capacidad predictiva a veces puede hacer que el modelo casi memorice los datos del conjunto de entrenamiento. Como ejemplo obvio, considere un modelo de 1 vecino más cercano. Siempre proporcionará predicciones perfectas para el conjunto de entrenamiento, sin importar qué tan bien funcione para otros conjuntos de datos. Los modelos de random forest son similares; Repredecir el conjunto de entrenamiento siempre dará como resultado una estimación artificialmente optimista del rendimiento.\n\nPara ambos modelos, @tbl-rmse-results resume la estimación de RMSE para los conjuntos de entrenamiento y prueba:\n\n\n::: {#tbl-rmse-results .cell layout-align=\"center\" tbl-cap='Estadísticas de rendimiento para conjuntos de entrenamiento y prueba.' hash='10-resampling_cache/html/tbl-rmse-results_81e981163f1ed6867d4eb5587567ddbd'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n<tr>\n<th style=\"empty-cells: hide;border-bottom:hidden;\" colspan=\"1\"></th>\n<th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"2\"><div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \">Estimaciones de RMSE</div></th>\n</tr>\n  <tr>\n   <th style=\"text-align:left;\"> object </th>\n   <th style=\"text-align:right;\"> train </th>\n   <th style=\"text-align:right;\"> test </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> <tt>lm_fit</tt> </td>\n   <td style=\"text-align:right;\"> 0.0754 </td>\n   <td style=\"text-align:right;\"> 0.0736 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> <tt>rf_fit</tt> </td>\n   <td style=\"text-align:right;\"> 0.0366 </td>\n   <td style=\"text-align:right;\"> 0.0704 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nObserve que el modelo de regresión lineal es consistente entre el entrenamiento y las pruebas, debido a su complejidad limitada.[^10-resampling-2]\n\n[^10-resampling-2]: Es posible que un modelo lineal casi memorice el conjunto de entrenamiento, como lo hizo el modelo de bosque aleatorio. En el objeto `ames_rec`, cambie el número de términos spline para `longitud` y `latitud` a un número grande (digamos 1000). Esto produciría un modelo ajustado con un RMSE de resustitución muy pequeño y un RMSE del conjunto de prueba que es mucho más grande.\n\n::: rmdwarning\nLa principal conclusión de este ejemplo es que repredecir el conjunto de entrenamiento dará como resultado una estimación del rendimiento artificialmente optimista. Es una mala idea para la mayoría de los modelos.\n:::\n\nSi el conjunto de prueba no se debe utilizar de inmediato y repredecir el conjunto de entrenamiento es una mala idea, ¿qué se debe hacer? Los métodos de remuestreo, como la validación cruzada o los conjuntos de validación, son la solución.\n\n## Métodos de Remuestreo\n\nLos métodos de remuestreo son sistemas de simulación empíricos que emulan el proceso de utilizar algunos datos para modelar y diferentes datos para evaluación. La mayoría de los métodos de remuestreo son iterativos, lo que significa que este proceso se repite varias veces. El diagrama en @fig-resampling-scheme ilustra cómo funcionan generalmente los métodos de remuestreo.\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/fig-resampling-scheme_a2a97b4c7ab51f7bb1060502d0c0c074'}\n::: {.cell-output-display}\n![Esquema de división de datos desde la división de datos inicial hasta el remuestreo](premade/resampling.svg){#fig-resampling-scheme fig-align='center' fig-alt='Un diagrama del esquema de división de datos desde la división de datos inicial hasta el remuestreo. El primer nivel es la partición del conjunto de entrenamiento/prueba. El segundo nivel de división toma el conjunto de entrenamiento y lo divide en múltiples conjuntos de `análisis` y `evaluación` (que son análogos al entrenamiento y la prueba).' width=85%}\n:::\n:::\n\n\nEl remuestreo se realiza solo en el conjunto de entrenamiento, como se ve en @fig-resampling-scheme. El equipo de prueba no está involucrado. Para cada iteración de remuestreo, los datos se dividen en dos submuestras:\n\n-   El modelo se ajusta a la *conjunto de análisis*.\n\n-   El modelo se evalúa con el *conjunto de evaluación*.\n\nEstas dos submuestras son algo análogas a los conjuntos de entrenamiento y prueba. Nuestro lenguaje de *análisis* y *evaluación* evita confusiones con la división inicial de los datos. Estos conjuntos de datos son mutuamente excluyentes. El esquema de partición utilizado para crear los conjuntos de análisis y evaluación suele ser la característica definitoria del método.\n\nSupongamos que se realizan 20 iteraciones de remuestreo. Esto significa que se ajustan 20 modelos separados a los conjuntos de análisis y los conjuntos de evaluación correspondientes producen 20 conjuntos de estadísticas de desempeño. La estimación final del rendimiento de un modelo es el promedio de las 20 réplicas de las estadísticas. Este promedio tiene muy buenas propiedades de generalización y es mucho mejor que las estimaciones de resustitución.\n\nLa siguiente sección define varios métodos de remuestreo comúnmente utilizados y analiza sus ventajas y desventajas.\n\n### Validación cruzada {#sec-cv}\n\nLa validación cruzada es un método de remuestreo bien establecido. Si bien existen varias variaciones, el método de validación cruzada más común es la validación cruzada *V*. Los datos se dividen aleatoriamente en *V* conjuntos de tamaño aproximadamente igual (llamados pliegues). A modo de ilustración, *V* = 3 se muestra en @fig-cross-validation-allocation para un conjunto de datos de 30 puntos de configuración de entrenamiento con asignaciones de pliegues aleatorios. El número dentro de los símbolos es el número de muestra.\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/fig-cross-validation-allocation_df33e303fd3ce7e84a6b936c28014639'}\n::: {.cell-output-display}\n![La validación cruzada de pliegues en V asigna aleatoriamente datos a los pliegues](premade/three-CV.svg){#fig-cross-validation-allocation fig-align='center' fig-alt='Un diagrama de cómo la validación cruzada de pliegues en V asigna aleatoriamente datos a los pliegues (donde V es igual a tres). Se asigna un conjunto de treinta puntos de datos a tres grupos de aproximadamente el mismo tamaño.' width=50%}\n:::\n:::\n\n\nEl color de los símbolos en @fig-cross-validation-allocation representa sus pliegues asignados aleatoriamente. El muestreo estratificado también es una opción para asignar pliegues (analizado anteriormente en @sec-splitting-methods).\n\nPara una validación cruzada triple, las tres iteraciones de remuestreo se ilustran en @fig-cross-validation. Para cada iteración, se reserva un pliegue para las estadísticas de evaluación y los pliegues restantes son sustrato para el modelo. Este proceso continúa en cada pliegue, de modo que tres modelos producen tres conjuntos de estadísticas de rendimiento.\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/fig-cross-validation_1b2fdd45b81a5f561da078161d285dbe'}\n::: {.cell-output-display}\n![Uso de datos de validación cruzada V-fold](premade/three-CV-iter.svg){#fig-cross-validation fig-align='center' fig-alt='Un diagrama del uso de datos de validación cruzada multiplicado por V (donde V es igual a tres). Para cada uno de los tres grupos, los datos del pliegue se reservan para el rendimiento, mientras que los otros dos se utilizan para modelar.' width=70%}\n:::\n:::\n\n\nCuando *V* = 3, los conjuntos de análisis son 2/3 del conjunto de entrenamiento y cada conjunto de evaluación es un 1/3 distinto. La estimación final del remuestreo del rendimiento promedia cada una de las *V* réplicas.\n\nUsar *V* = 3 es una buena opción para ilustrar la validación cruzada, pero en la práctica es una mala opción porque es demasiado baja para generar estimaciones confiables. En la práctica, los valores de *V* suelen ser 5 o 10; Generalmente preferimos la validación cruzada 10 veces como valor predeterminado porque es lo suficientemente grande para obtener buenos resultados en la mayoría de las situaciones.\n\n::: rmdnote\n¿Cuáles son los efectos de cambiar *V*? Los valores más grandes dan como resultado estimaciones de remuestreo con un sesgo pequeño pero una varianza sustancial. Los valores más pequeños de *V* tienen un sesgo grande pero una varianza baja. Preferimos 10 veces, ya que la replicación reduce el ruido, pero no el sesgo.[^10-resampling-3]\n:::\n\n[^10-resampling-3]: Consulte la Sección 3.4 de @fes para obtener una descripción más detallada de los resultados del cambio de *V*: <https://bookdown.org/max/FES/resampling.html>\n\nLa entrada principal es el marco de datos del conjunto de entrenamiento, así como el número de pliegues (por defecto, 10):\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-ames-cv_169b2180df79c0ed11f139b655c27743'}\n\n```{.r .cell-code}\nset.seed(1001)\names_folds <- vfold_cv(ames_train, v = 10)\names_folds\n## #  10-fold cross-validation \n## # A tibble: 10 × 2\n##   splits             id    \n##   <list>             <chr> \n## 1 <split [2107/235]> Fold01\n## 2 <split [2107/235]> Fold02\n## 3 <split [2108/234]> Fold03\n## 4 <split [2108/234]> Fold04\n## 5 <split [2108/234]> Fold05\n## 6 <split [2108/234]> Fold06\n## # ℹ 4 more rows\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-cv-printing_c627f94f45216323ba1b4aa5c93581ec'}\n\n:::\n\n\nLa columna denominada `splits` contiene información sobre cómo dividir los datos (similar al objeto utilizado para crear la partición de entrenamiento/prueba inicial). Si bien cada fila de \"divisiones\" tiene una copia incrustada de todo el conjunto de entrenamiento, R es lo suficientemente inteligente como para no hacer copias de los datos en la memoria.[^10-resampling-4] El método de impresión dentro del tibble muestra la frecuencia de cada: `[2107/235]` indica que alrededor de dos mil muestras están en el conjunto de análisis y 235 están en ese conjunto de evaluación en particular.\n\n[^10-resampling-4]: Para comprobarlo usted mismo, intente ejecutar `lobstr::obj_size(ames_folds)` y `lobstr::obj_size(ames_train)`. El tamaño del objeto de remuestreo es mucho menor que diez veces el tamaño de los datos originales.\n\nEstos objetos también contienen siempre una columna de caracteres llamada `id` que etiqueta la partición.[^10-resampling-5]\n\n[^10-resampling-5]: Algunos métodos de remuestreo requieren múltiples campos `id`.\n\nPara recuperar manualmente los datos particionados, las funciones `analysis()` y `assessment()` devuelven los marcos de datos correspondientes:\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-analysis_695826739a10cc0b49c3d54764bfcc5e'}\n\n```{.r .cell-code}\n# Para el primer pliegue:\names_folds$splits[[1]] %>% analysis() %>% dim()\n## [1] 2107   74\n```\n:::\n\n\nLos paquetes <span class=\"pkg\">tidymodels</span>, como [<span class=\"pkg\">tune</span>](https://tune.tidymodels.org/), contienen interfaces de usuario de alto nivel para que funciones como `analysis()` no sean generalmente necesarias para el trabajo diario. @sec-resampling-performance demuestra una función para ajustar un modelo sobre estos remuestreos.\n\nExiste una variedad de variaciones de validación cruzada; repasaremos los más importantes.\n\n### Validación cruzada repetida {.unnumbered}\n\nLa variación más importante de la validación cruzada es la validación cruzada repetida *V* veces. Dependiendo del tamaño de los datos u otras características, la estimación de remuestreo producida por la validación cruzada *V* veces puede ser excesivamente ruidosa.[^10-resampling-6] Como ocurre con muchos problemas estadísticos, una forma de reducir el ruido es recopilar más datos . Para la validación cruzada, esto significa promediar más de *V* estadísticas.\n\n[^10-resampling-6]: Para obtener más detalles, consulte la Sección 3.4.6 de @fes: <https://bookdown.org/max/FES/resampling.html#resample-var-bias>.\n\nPara crear repeticiones *R* de validación cruzada de pliegues *V*, se realiza el mismo proceso de generación de pliegues *R* veces para generar colecciones *R* de particiones *V*. Ahora, en lugar de promediar las estadísticas *V*, las estadísticas $V \\times R$ producen la estimación final del remuestreo. Debido al teorema del límite central, las estadísticas resumidas de cada modelo tienden a una distribución normal, siempre que tengamos muchos datos relativos a $V \\times R$.\n\nConsideremos los datos de Ames. En promedio, la validación cruzada 10 veces utiliza conjuntos de evaluación que contienen aproximadamente propiedades 234. Si RMSE es la estadística elegida, podemos denotar la desviación estándar de esa estimación como $\\sigma$. Con una validación cruzada simple de 10 veces, el error estándar del RMSE medio es $\\sigma/\\sqrt{10}$. Si esto es demasiado ruidoso, las repeticiones reducen el error estándar a $\\sigma/\\sqrt{10R}$. Para una validación cruzada de 10 veces con réplicas de $R$, el gráfico en @fig-variance-reduction muestra qué tan rápido disminuye el error estándar[^10-resampling-7] con las réplicas.\n\n[^10-resampling-7]: Estos son errores estándar *aproximados*. Como se analizará en el próximo capítulo, existe una correlación dentro de las réplicas que es típica de los resultados remuestreados. Al ignorar este componente adicional de variación, los cálculos simples que se muestran en este gráfico son sobreestimaciones de la reducción del ruido en los errores estándar.\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/fig-variance-reduction_f43451005df8f3a68b2ee6a409367f6b'}\n::: {.cell-output-display}\n![Relación entre la varianza relativa en las estimaciones de desempeño versus el número de repeticiones de validación cruzada](figures/fig-variance-reduction-1.png){#fig-variance-reduction fig-align='center' fig-alt='La relación entre la varianza relativa en las estimaciones de rendimiento versus el número de repeticiones de validación cruzada. A medida que aumentan las repeticiones, la varianza se reduce en un patrón armónicamente decreciente con rendimientos decrecientes para un gran número de repeticiones.' width=672}\n:::\n:::\n\n\nUn mayor número de réplicas tiende a tener menos impacto en el error estándar. Sin embargo, si el valor de referencia de $\\sigma$ es imprácticamente grande, los rendimientos decrecientes de la replicación aún pueden valer los costos computacionales adicionales.\n\nPara crear repeticiones, invoque `vfold_cv()` con un argumento adicional `repeats`:\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-repeated_a95e594bd50e50a8f123327049f7ad66'}\n\n```{.r .cell-code}\nvfold_cv(ames_train, v = 10, repeats = 5)\n## #  10-fold cross-validation repeated 5 times \n## # A tibble: 50 × 3\n##   splits             id      id2   \n##   <list>             <chr>   <chr> \n## 1 <split [2107/235]> Repeat1 Fold01\n## 2 <split [2107/235]> Repeat1 Fold02\n## 3 <split [2108/234]> Repeat1 Fold03\n## 4 <split [2108/234]> Repeat1 Fold04\n## 5 <split [2108/234]> Repeat1 Fold05\n## 6 <split [2108/234]> Repeat1 Fold06\n## # ℹ 44 more rows\n```\n:::\n\n\n### Validación cruzada de dejar uno fuera {.unnumbered}\n\nUna variación de la validación cruzada es la validación cruzada de dejar uno fuera (LOO). Si hay $n$ muestras de conjuntos de entrenamiento, $n$ modelos se ajustan usando $n-1$ filas del conjunto de entrenamiento. Cada modelo predice el único punto de datos excluido. Al final del remuestreo, las predicciones $n$ se combinan para producir una única estadística de rendimiento.\n\nLos métodos de dejar uno fuera son deficientes en comparación con casi cualquier otro método. Para cualquier cosa que no sea una muestra patológicamente pequeña, LOO es computacionalmente excesivo y puede que no tenga buenas propiedades estadísticas. Aunque el paquete <span class=\"pkg\">rsample</span> contiene una función `loo_cv()`, estos objetos generalmente no están integrados en los marcos más amplios de tidymodels.\n\n### Validación cruzada de Monte Carlo {.unnumbered}\n\nOtra variante de la validación cruzada *V* es la validación cruzada de Monte Carlo (MCCV, @xu2001monte). Al igual que la validación cruzada *V* veces, asigna una proporción fija de datos a los conjuntos de evaluación. La diferencia entre MCCV y la validación cruzada regular es que, para MCCV, esta proporción de datos se selecciona aleatoriamente cada vez. Esto da como resultado conjuntos de evaluación que no son mutuamente excluyentes. Para crear estos objetos de remuestreo:\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-mccv_bb0b554570ad7a8fdc4822b4637214c4'}\n\n```{.r .cell-code}\nmc_cv(ames_train, prop = 9/10, times = 20)\n## # Monte Carlo cross-validation (0.9/0.1) with 20 resamples  \n## # A tibble: 20 × 2\n##   splits             id        \n##   <list>             <chr>     \n## 1 <split [2107/235]> Resample01\n## 2 <split [2107/235]> Resample02\n## 3 <split [2107/235]> Resample03\n## 4 <split [2107/235]> Resample04\n## 5 <split [2107/235]> Resample05\n## 6 <split [2107/235]> Resample06\n## # ℹ 14 more rows\n```\n:::\n\n\n### Conjuntos de validación {#sec-validation}\n\nEn @sec-what-about-a-validation-set, analizamos brevemente el uso de un conjunto de validación, una única partición que se reserva para estimar el rendimiento separada del conjunto de prueba. Cuando se utiliza un conjunto de validación, el conjunto de datos inicial disponible se divide en un conjunto de entrenamiento, un conjunto de validación y un conjunto de prueba (consulte @fig-three-way-split).\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/fig-three-way-split_a42b513c227ee9a8f5e36fef2fa1f755'}\n::: {.cell-output-display}\n![Una división inicial de tres vías en conjuntos de entrenamiento, pruebas y validación.](premade/validation.svg){#fig-three-way-split fig-align='center' fig-alt='Una división inicial de tres vías en conjuntos de entrenamiento, pruebas y validación.' width=50%}\n:::\n:::\n\n\nLos conjuntos de validación se utilizan a menudo cuando el conjunto de datos original es muy grande. En este caso, una única partición grande puede ser adecuada para caracterizar el rendimiento del modelo sin tener que realizar múltiples iteraciones de remuestreo.\n\nCon el paquete <span class=\"pkg\">rsample</span>, un conjunto de validación es como cualquier otro objeto de remuestreo; este tipo se diferencia únicamente en que tiene una única iteración.[^10-resampling-8] @fig-validation-split muestra este esquema.\n\n[^10-resampling-8]: En esencia, un conjunto de validación puede considerarse una iteración única de la validación cruzada de Monte Carlo.\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/fig-validation-split_6a8100e619777629259c294590882674'}\n::: {.cell-output-display}\n![Una división inicial bidireccional en entrenamiento y pruebas con un conjunto de validación adicional dividido en el conjunto de entrenamiento](premade/validation-alt.svg){#fig-validation-split fig-align='center' fig-alt='Una división inicial bidireccional en entrenamiento y prueba con un conjunto de validación adicional dividido en el conjunto de entrenamiento.' width=45%}\n:::\n:::\n\n\nPara construir sobre el código de @sec-what-about-a-validation-set, la función `validation_set()` puede tomar los resultados de `initial_validation_split()` y convertirlos en un objeto rset similar a los producidos. mediante funciones como `vfold_cv()`:\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-validation-split_8e741e2fac2b589e3237334db7108df7'}\n\n```{.r .cell-code}\n# Previamente:\n\nset.seed(52)\n# Para dedicar el 60 % al entrenamiento, el 20 % a la validación y el 20 % a las pruebas:\names_val_split <- initial_validation_split(ames, prop = c(0.6, 0.2))\names_val_split\n## <Training/Validation/Testing/Total>\n## <1758/586/586/2930>\n\n# Objeto utilizado para remuestreo: \nval_set <- validation_set(ames_val_split)\nval_set\n## # A tibble: 1 × 2\n##   splits             id        \n##   <list>             <chr>     \n## 1 <split [1758/586]> validation\n```\n:::\n\n\nComo verá en @sec-resampling-rendimiento, la función `fit_resamples()` se utilizará para calcular estimaciones correctas de rendimiento mediante el remuestreo. El objeto `val_set` se puede utilizar en esta y otras funciones aunque sea una única \"muestra\" de los datos.\n\n### Bootstrapping {#sec-bootstrap}\n\nEl remuestreo Bootstrap se inventó originalmente como un método para aproximar la distribución muestral de estadísticas cuyas propiedades teóricas son intratables [@davison1997bootstrap]. Usarlo para estimar el desempeño del modelo es una aplicación secundaria del método.\n\nUna muestra bootstrap del conjunto de entrenamiento es una muestra que tiene el mismo tamaño que el conjunto de entrenamiento pero que se extrae *con reemplazo*. Esto significa que algunos puntos de datos del conjunto de entrenamiento se seleccionan varias veces para el conjunto de análisis. Cada punto de datos tiene una probabilidad 63.2% de ser incluido en el conjunto de entrenamiento al menos una vez. El conjunto de evaluación contiene todas las muestras del conjunto de entrenamiento que no fueron seleccionadas para el conjunto de análisis (en promedio, con 36.8% del conjunto de entrenamiento). Al realizar el arranque, el conjunto de evaluación a menudo se denomina muestra *fuera de la bolsa*.\n\nPara un conjunto de entrenamiento de 30 muestras, en la siguiente figura se muestra un esquema de tres muestras bootstrap @fig-bootstrapping.\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/fig-bootstrapping_68e8bedbb26b662d47b8253272dab0be'}\n::: {.cell-output-display}\n![Uso de datos Bootstraping](premade/bootstraps.svg){#fig-bootstrapping fig-align='center' fig-alt='Un diagrama del uso de datos en bootstrapping. Para cada remuestreo de bootstrap, el conjunto de análisis es del mismo tamaño que el conjunto de entrenamiento (debido al muestreo con reemplazo) y el conjunto de evaluación consta de muestras que no están en el conjunto de análisis.' width=80%}\n:::\n:::\n\n\nTenga en cuenta que los tamaños de los conjuntos de evaluación varían.\n\nUsando el paquete <span class=\"pkg\">rsample</span>, podemos crear tales remuestreos de bootstrap:\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-boot-set_116a86d7e03a54262f1450506af2bfaa'}\n\n```{.r .cell-code}\nbootstraps(ames_train, times = 5)\n## # Bootstrap sampling \n## # A tibble: 5 × 2\n##   splits             id        \n##   <list>             <chr>     \n## 1 <split [2342/867]> Bootstrap1\n## 2 <split [2342/869]> Bootstrap2\n## 3 <split [2342/859]> Bootstrap3\n## 4 <split [2342/858]> Bootstrap4\n## 5 <split [2342/873]> Bootstrap5\n```\n:::\n\n\nLas muestras Bootstrap producen estimaciones de rendimiento que tienen una varianza muy baja (a diferencia de la validación cruzada) pero tienen un sesgo pesimista significativo. Esto significa que, si la precisión real de un modelo es del 90%, el bootstrap tendería a estimar que el valor es inferior al 90%. La cantidad de sesgo no se puede determinar empíricamente con suficiente precisión. Además, la cantidad de sesgo cambia a lo largo de la escala de la métrica de desempeño. Por ejemplo, es probable que el sesgo sea diferente cuando la precisión es del 90 % y cuando es del 70 %.\n\nEl bootstrap también se utiliza en muchos modelos. Por ejemplo, el modelo de random forest mencionado anteriormente contenía 1000 árboles de decisión individuales. Cada árbol fue producto de una muestra de bootstrap diferente del conjunto de entrenamiento.\n\n### Remuestreo continuo del origen de la previsión {#sec-rolling}\n\nCuando los datos tienen un fuerte componente temporal, un método de remuestreo debería respaldar el modelado para estimar tendencias estacionales y otras tendencias temporales dentro de los datos. Una técnica que muestrea aleatoriamente valores del conjunto de entrenamiento puede alterar la capacidad del modelo para estimar estos patrones.\n\nEl remuestreo continuo del origen del pronóstico [@hyndman2018forecasting] proporciona un método que emula cómo los datos de series de tiempo a menudo se dividen en la práctica, estimando el modelo con datos históricos y evaluándolo con los datos más recientes. Para este tipo de remuestreo, se especifica el tamaño de los conjuntos de análisis y evaluación iniciales. La primera iteración de remuestreo utiliza estos tamaños, comenzando desde el principio de la serie. La segunda iteración utiliza los mismos tamaños de datos pero cambia en un número determinado de muestras.\n\nA modo de ilustración, se volvió a muestrear un conjunto de entrenamiento de quince muestras con un tamaño de análisis de ocho muestras y un tamaño de conjunto de evaluación de tres. La segunda iteración descarta la primera muestra del conjunto de entrenamiento y ambos conjuntos de datos avanzan uno. Esta configuración da como resultado cinco remuestreos, como se muestra en @fig-rolling.\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/fig-rolling_6283d771aa2134ab577f772666c766e0'}\n::: {.cell-output-display}\n![Uso de datos para el remuestreo continuo del origen de la previsión](premade/rolling.svg){#fig-rolling fig-align='center' fig-alt='El uso de datos para el remuestreo continuo del origen de la previsión. Para cada división, se utilizan datos anteriores para modelar y algunas instancias posteriores se utilizan para medir el desempeño.' width=65%}\n:::\n:::\n\n\nAquí hay dos configuraciones diferentes de este método:\n\n-   El conjunto de análisis puede crecer acumulativamente (en lugar de permanecer del mismo tamaño). Después del primer conjunto de análisis inicial, se pueden acumular nuevas muestras sin descartar los datos anteriores.\n\n-   Los remuestreos no necesitan incrementarse en uno. Por ejemplo, para conjuntos de datos grandes, el bloque incremental podría ser una semana o un mes en lugar de un día.\n\nPara los datos de un año, supongamos que seis conjuntos de bloques de 30 días definen el conjunto de análisis. Para conjuntos de evaluación de 30 días con un salto de 29 días, podemos usar el paquete <span class=\"pkg\">rsample</span> para especificar:\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-rolling-forcast_469eb59366d9a1c4d6dc01c5f00f8c7f'}\n\n```{.r .cell-code}\ntime_slices <- \n  tibble(x = 1:365) %>% \n  rolling_origin(initial = 6 * 30, assess = 30, skip = 29, cumulative = FALSE)\n\ndata_range <- function(x) {\n  summarize(x, first = min(x), last = max(x))\n}\n\nmap_dfr(time_slices$splits, ~   analysis(.x) %>% data_range())\n## # A tibble: 6 × 2\n##   first  last\n##   <int> <int>\n## 1     1   180\n## 2    31   210\n## 3    61   240\n## 4    91   270\n## 5   121   300\n## 6   151   330\nmap_dfr(time_slices$splits, ~ assessment(.x) %>% data_range())\n## # A tibble: 6 × 2\n##   first  last\n##   <int> <int>\n## 1   181   210\n## 2   211   240\n## 3   241   270\n## 4   271   300\n## 5   301   330\n## 6   331   360\n```\n:::\n\n\n## Estimación del rendimiento {#sec-resampling-performance}\n\nCualquiera de los métodos de remuestreo discutidos en este capítulo se puede utilizar para evaluar el proceso de modelado (incluido el preprocesamiento, el ajuste del modelo, etc.). Estos métodos son eficaces porque se utilizan diferentes grupos de datos para entrenar el modelo y evaluarlo. Para reiterar, el proceso para utilizar el remuestreo es:\n\n1.  Durante el remuestreo, el conjunto de análisis se utiliza para preprocesar los datos, aplicar el preprocesamiento a sí mismo y utilizar estos datos procesados para ajustarse al modelo.\n\n2. Las estadísticas de preprocesamiento producidas por el conjunto de análisis se aplican al conjunto de evaluación. Las predicciones del conjunto de evaluación estiman el rendimiento con datos nuevos.\n\nEsta secuencia se repite para cada nuevo muestreo. Si hay *B* remuestreos, hay *B* réplicas de cada una de las métricas de rendimiento. La estimación final del remuestreo es el promedio de estas estadísticas *B*. Si *B* = 1, como ocurre con un conjunto de validación, las estadísticas individuales representan el rendimiento general.\n\nReconsideremos el modelo de random forest anterior contenido en el objeto `rf_wflow`. La función `fit_resamples()` es análoga a `fit()`, pero en lugar de tener un argumento `data`, `fit_resamples()` tiene `resamples`, que espera un objeto `rset` como los que se muestran en este capítulo Las posibles interfaces para la función son:\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-usage_96e86ae06511969d32dedfba99620c14'}\n\n```{.r .cell-code}\nmodel_spec %>% fit_resamples(formula,  resamples, ...)\nmodel_spec %>% fit_resamples(recipe,   resamples, ...)\nworkflow   %>% fit_resamples(          resamples, ...)\n```\n:::\n\n\nHay una serie de otros argumentos opcionales, como por ejemplo:\n\n-   `metrics`: Un conjunto de métricas de estadísticas de rendimiento para calcular. De forma predeterminada, los modelos de regresión utilizan RMSE y $R^2$, mientras que los modelos de clasificación calculan el área bajo la curva ROC y la precisión general. Tenga en cuenta que esta elección también define qué predicciones se producen durante la evaluación del modelo. Para la clasificación, si solo se solicita precisión, no se generan estimaciones de probabilidad de clase para el conjunto de evaluación (ya que no son necesarias).\n\n-   `control`: Una lista creada por `control_resamples()` con varias opciones.\n\nLos argumentos de control incluyen:\n\n-   `verbose`: Una lógica para imprimir el registro.\n\n-   `extract`: Una función para retener objetos de cada iteración del modelo (que se analiza más adelante en este capítulo).\n\n-   `save_pred`: Una lógica para guardar las predicciones del conjunto de evaluación.\n\nPara nuestro ejemplo, guardemos las predicciones para visualizar el ajuste y los residuos del modelo:\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-cv-ames_ba4d88928c2ec9948759de0c670b7a52'}\n\n```{.r .cell-code}\nkeep_pred <- control_resamples(save_pred = TRUE, save_workflow = TRUE)\n\nset.seed(1003)\nrf_res <- \n  rf_wflow %>% \n  fit_resamples(resamples = ames_folds, control = keep_pred)\nrf_res\n## # Resampling results\n## # 10-fold cross-validation \n## # A tibble: 10 × 5\n##   splits             id     .metrics         .notes           .predictions      \n##   <list>             <chr>  <list>           <list>           <list>            \n## 1 <split [2107/235]> Fold01 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [235 × 4]>\n## 2 <split [2107/235]> Fold02 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [235 × 4]>\n## 3 <split [2108/234]> Fold03 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [234 × 4]>\n## 4 <split [2108/234]> Fold04 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [234 × 4]>\n## 5 <split [2108/234]> Fold05 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [234 × 4]>\n## 6 <split [2108/234]> Fold06 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [234 × 4]>\n## # ℹ 4 more rows\n```\n:::\n\n\n\n\nEl valor de retorno es un tibble similar a las muestras de entrada, junto con algunas columnas adicionales:\n\n-   `.metrics` es una columna de lista de tibbles que contiene las estadísticas de rendimiento del conjunto de evaluación.\n\n-   `.notes` Hay otra columna de lista de tibbles que cataloga cualquier advertencia o error generado durante el remuestreo. Tenga en cuenta que los errores no detendrán la ejecución posterior del remuestreo.\n\n-   `.predictions` está presente cuando `save_pred = TRUE`. Esta columna de lista contiene tibbles con predicciones fuera de muestra.\n\nSi bien estas columnas de lista pueden parecer desalentadoras, se pueden reconfigurar fácilmente usando <span class=\"pkg\">tidyr</span> o con las funciones convenientes que proporciona tidymodels. Por ejemplo, para devolver las métricas de rendimiento en un formato más utilizable:\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-cv-stats_7cf2c6d3c6588a14312cbb0c534b2d5a'}\n\n```{.r .cell-code}\ncollect_metrics(rf_res)\n## # A tibble: 2 × 6\n##   .metric .estimator   mean     n std_err .config             \n##   <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n## 1 rmse    standard   0.0721    10 0.00305 Preprocessor1_Model1\n## 2 rsq     standard   0.831     10 0.0108  Preprocessor1_Model1\n```\n:::\n\n\nEstas son las estimaciones de remuestreo promediadas sobre las réplicas individuales. Para obtener las métricas para cada remuestreo, use la opción `summarize = FALSE`.\n\n¡Observe cuánto más realistas son las estimaciones de rendimiento que las estimaciones de resustitución de @sec-resampling-resubstition!\n\nPara obtener las predicciones del conjunto de evaluación:\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-cv-pred_0b6f4a9dbbe527d061e35289ae9b8b71'}\n\n```{.r .cell-code}\nassess_res <- collect_predictions(rf_res)\nassess_res\n## # A tibble: 2,342 × 5\n##   id     .pred  .row Sale_Price .config             \n##   <chr>  <dbl> <int>      <dbl> <chr>               \n## 1 Fold01  5.10    10       5.09 Preprocessor1_Model1\n## 2 Fold01  4.92    27       4.90 Preprocessor1_Model1\n## 3 Fold01  5.21    47       5.08 Preprocessor1_Model1\n## 4 Fold01  5.13    52       5.10 Preprocessor1_Model1\n## 5 Fold01  5.13    59       5.10 Preprocessor1_Model1\n## 6 Fold01  5.13    63       5.11 Preprocessor1_Model1\n## # ℹ 2,336 more rows\n```\n:::\n\n\nLos nombres de las columnas de predicción siguen las convenciones analizadas para los modelos <span class=\"pkg\">parsnip</span> en el [Capítulo @sec-models], para mayor coherencia y facilidad de uso. La columna de resultados observados siempre utiliza el nombre de la columna original de los datos de origen. La columna `.row` es un número entero que coincide con la fila del conjunto de entrenamiento original para que estos resultados puedan organizarse y unirse adecuadamente con los datos originales.\n\n::: rmdnote\nPara algunos métodos de remuestreo, como el bootstrap o la validación cruzada repetida, habrá múltiples predicciones por fila del conjunto de entrenamiento original. Para obtener valores resumidos (promedios de las predicciones replicadas), utilice `collect_predictions(object, summarize = TRUE)`.\n:::\n\nDado que este análisis utilizó una validación cruzada de 10 veces, existe una predicción única para cada muestra del conjunto de entrenamiento. Estos datos pueden generar gráficos útiles del modelo para comprender dónde potencialmente falló. Por ejemplo, @fig-ames-resampled-dance compara los valores observados y predichos retenidos (análogo a @fig-ames-rendimiento-plot):\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-cv-pred-plot_abe76fe8dac6daac82ed80eda240b377'}\n\n```{.r .cell-code}\nassess_res %>% \n  ggplot(aes(x = Sale_Price, y = .pred)) + \n  geom_point(alpha = .15) +\n  geom_abline(color = \"red\") + \n  coord_obs_pred() + \n  ylab(\"Predichos\") +\n  xlab(\"Precio de Venta\")\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/fig-ames-resampled-performance_e4df6d0aeefb5be1e14a6314e3111699'}\n::: {.cell-output-display}\n![Valores observados versus predichos fuera de la muestra para un modelo de regresión de Ames, utilizando unidades log-10 en ambos ejes](figures/fig-ames-resampled-performance-1.png){#fig-ames-resampled-performance fig-align='center' fig-alt='Gráficos de dispersión de valores observados versus valores predichos fuera de la muestra para un modelo de regresión de Ames. Ambos ejes utilizan unidades log-10. El modelo muestra una buena concordancia con dos puntos de datos periféricos que están significativamente sobreestimados.' width=480}\n:::\n:::\n\n\nHay dos casas en el conjunto de entrenamiento con un precio de venta bajo observado que el modelo sobreestima significativamente. ¿Qué casas son estas? Averigüemos por el resultado de `assess_res`:\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-investigate_6f102c4f79f1001fcac29c79f7082fec'}\n\n```{.r .cell-code}\nover_predicted <- \n  assess_res %>% \n  mutate(residual = Sale_Price - .pred) %>% \n  arrange(desc(abs(residual))) %>% \n  slice(1:2)\nover_predicted\n## # A tibble: 2 × 6\n##   id     .pred  .row Sale_Price .config              residual\n##   <chr>  <dbl> <int>      <dbl> <chr>                   <dbl>\n## 1 Fold09  4.96    32       4.11 Preprocessor1_Model1   -0.858\n## 2 Fold08  4.93   317       4.12 Preprocessor1_Model1   -0.816\n\names_train %>% \n  slice(over_predicted$.row) %>% \n  select(Gr_Liv_Area, Neighborhood, Year_Built, Bedroom_AbvGr, Full_Bath)\n## # A tibble: 2 × 5\n##   Gr_Liv_Area Neighborhood           Year_Built Bedroom_AbvGr Full_Bath\n##         <int> <fct>                       <int>         <int>     <int>\n## 1         832 Old_Town                     1923             2         1\n## 2         733 Iowa_DOT_and_Rail_Road       1952             2         1\n```\n:::\n\n\nIdentificar ejemplos como estos con un desempeño especialmente pobre puede ayudarnos a realizar un seguimiento e investigar por qué estas predicciones específicas son tan pobres.\n\nVolvamos a las casas en general. ¿Cómo podemos utilizar un conjunto de validación en lugar de validación cruzada? De nuestro objeto anterior <span class=\"pkg\">rsample</span>:\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-val-ames_4ac371cdc61f378c993acb2f4ce29c46'}\n\n```{.r .cell-code}\nval_res <- rf_wflow %>% fit_resamples(resamples = val_set)\n## Warning in `[.tbl_df`(x, is.finite(x <- as.numeric(x))): NAs introducidos por\n## coerción\nval_res\n## # Resampling results\n## #  \n## # A tibble: 1 × 4\n##   splits             id         .metrics         .notes          \n##   <list>             <chr>      <list>           <list>          \n## 1 <split [1758/586]> validation <tibble [2 × 4]> <tibble [0 × 3]>\n\ncollect_metrics(val_res)\n## # A tibble: 2 × 6\n##   .metric .estimator   mean     n std_err .config             \n##   <chr>   <chr>       <dbl> <int>   <dbl> <chr>               \n## 1 rmse    standard   0.0727     1      NA Preprocessor1_Model1\n## 2 rsq     standard   0.823      1      NA Preprocessor1_Model1\n```\n:::\n\n\nEstos resultados también están mucho más cerca de los resultados del conjunto de pruebas que las estimaciones de rendimiento de resustitución.\n\n::: rmdnote\nEn estos análisis, los resultados del remuestreo son muy cercanos a los resultados del conjunto de pruebas. Los dos tipos de estimaciones tienden a estar bien correlacionados. Sin embargo, esto podría deberse al azar. Un valor inicial de \"55\" fijó los números aleatorios antes de crear las nuevas muestras. Intente cambiar este valor y vuelva a ejecutar los análisis para investigar si las estimaciones remuestreadas también coinciden con los resultados del conjunto de pruebas.\n:::\n\n## Procesamiento en paralelo {#sec-parallel}\n\nLos modelos creados durante el remuestreo son independientes entre sí. Los cálculos de este tipo a veces se denominan *vergonzosamente paralelos*; cada modelo podría adaptarse simultáneamente sin problemas.[^10-resampling-9] El paquete <span class=\"pkg\">tune</span> usa [<span class=\"pkg\">foreach</span>](https://CRAN.R-project.org/ package=foreach) paquete para facilitar los cálculos paralelos. Estos cálculos podrían dividirse entre procesadores de la misma computadora o entre computadoras diferentes, según la tecnología elegida.\n\n[^10-resampling-9]: @parallel ofrece una descripción técnica de estas tecnologías.\n\nPara cálculos realizados en una sola computadora, el número de procesos de trabajo posibles está determinado por el paquete <span class=\"pkg\">parallel</span>:\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-find-cores_a05d49f1b11e2182c027efe8ea0f4af1'}\n\n```{.r .cell-code}\n# La cantidad de núcleos físicos en el hardware:\nparallel::detectCores(logical = FALSE)\n## [1] 2\n\n# El número de posibles procesadors independientes que pueden\n# ser utilizados simultáneamente:  \nparallel::detectCores(logical = TRUE)\n## [1] 4\n```\n:::\n\n\nLa diferencia entre estos dos valores está relacionada con el procesador de la computadora. Por ejemplo, la mayoría de los procesadores Intel utilizan hyperthreading, que crea dos núcleos virtuales para cada núcleo físico. Si bien estos recursos adicionales pueden mejorar el rendimiento, la mayoría de las aceleraciones producidas por el procesamiento paralelo ocurren cuando el procesamiento utiliza menos núcleos físicos.\n\nPara `fit_resamples()` y otras funciones en <span class=\"pkg\">tune</span>, el procesamiento paralelo ocurre cuando el usuario registra un paquete backend paralelo. Estos paquetes de R definen cómo ejecutar el procesamiento paralelo. En los sistemas operativos Unix y macOS, un método para dividir los cálculos es bifurcar subprocesos. Para habilitar esto, cargue el paquete <span class=\"pkg\">doMC</span> y registre el número de núcleos paralelos con <span class=\"pkg\">foreach</span>:\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-mc_54c9bd94aa62553324f67aa6d0e7793c'}\n\n```{.r .cell-code}\n# Solo Unix y macOS\nlibrary(doMC)\nregisterDoMC(cores = 2)\n\n# Ahora ejecute fit_resamples()...\n```\n:::\n\n\nEsto indica a `fit_resamples()` que ejecute la mitad de los cálculos en cada uno de los dos núcleos. Para restablecer los cálculos al procesamiento secuencial:\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-seq_d02d37db8800809fc06e30d53af003c7'}\n\n```{.r .cell-code}\nregisterDoSEQ()\n```\n:::\n\n\nAlternativamente, un enfoque diferente para paralelizar los cálculos utiliza sockets de red. El paquete <span class=\"pkg\">doParallel</span> habilita este método (utilizable en todos los sistemas operativos):\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-psock_3149859db45f268f98058df4f549bf62'}\n\n```{.r .cell-code}\n# Todos los sistemas operativos\nlibrary(doParallel)\n\n# Cree un objeto de clúster y luego regístrelo: \ncl <- makePSOCKcluster(2)\nregisterDoParallel(cl)\n\n# Ahora ejecute fit_resamples()`...\n\nstopCluster(cl)\n```\n:::\n\n\nOtro paquete de R que facilita el procesamiento paralelo es el paquete [<span class=\"pkg\">future</span>](https://future.futureverse.org/). Al igual que <span class=\"pkg\">foreach</span>, proporciona un marco para el paralelismo. Este paquete se usa junto con <span class=\"pkg\">foreach</span> a través del paquete <span class=\"pkg\">doFuture</span>.\n\n::: rmdnote\nLos paquetes R con backends paralelos para <span class=\"pkg\">foreach</span> comienzan con el prefijo `\"do\"`.\n:::\n\nEl procesamiento paralelo con el paquete <span class=\"pkg\">tune</span> tiende a proporcionar aceleraciones lineales para los primeros núcleos. Esto significa que, con dos núcleos, los cálculos son el doble de rápidos. Dependiendo de los datos y del tipo de modelo, la aceleración lineal se deteriora después de cuatro o cinco núcleos. El uso de más núcleos seguirá reduciendo el tiempo necesario para completar la tarea; simplemente hay rendimientos decrecientes para los núcleos adicionales.\n\nConcluyamos con una nota final sobre el paralelismo. Para cada una de estas tecnologías, los requisitos de memoria se multiplican por cada núcleo adicional utilizado. Por ejemplo, si el conjunto de datos actual tiene 2 GB de memoria y se utilizan tres núcleos, el requisito total de memoria es de 8 GB (2 para cada proceso de trabajo más el original). El uso de demasiados núcleos puede hacer que los cálculos (y la computadora) se ralenticen considerablemente.\n\n## Guardar los objetos remuestreados {#sec-extract}\n\nLos modelos creados durante el remuestreo no se conservan. Estos modelos se entrenan con el fin de evaluar el desempeño y, por lo general, no los necesitamos después de haber calculado las estadísticas de desempeño. Si un enfoque de modelado particular resulta ser la mejor opción para nuestro conjunto de datos, entonces la mejor opción es ajustar nuevamente todo el conjunto de entrenamiento para que los parámetros del modelo puedan estimarse con más datos.\n\nSi bien estos modelos creados durante el remuestreo no se conservan, existe un método para conservarlos o algunos de sus componentes. La opción `extract` de `control_resamples()` especifica una función que toma un solo argumento; Usaremos `x`. Cuando se ejecuta, `x` da como resultado un objeto de flujo de trabajo ajustado, independientemente de si proporcionó a `fit_resamples()` un flujo de trabajo. Recuerde que el paquete <span class=\"pkg\">workflows</span> tiene funciones que pueden extraer los diferentes componentes de los objetos (por ejemplo, el modelo, la receta, etc.).\n\nAjustemos un modelo de regresión lineal usando la receta que desarrollamos en el [Capítulo @sec-recipes]:\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-lm-ames_2dd1c72c292ee7c872a180ecef2f36f6'}\n\n```{.r .cell-code}\names_rec <- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %>%\n  step_other(Neighborhood, threshold = 0.01) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %>% \n  step_ns(Latitude, Longitude, deg_free = 20)\n\nlm_wflow <-  \n  workflow() %>% \n  add_recipe(ames_rec) %>% \n  add_model(linear_reg() %>% set_engine(\"lm\")) \n\nlm_fit <- lm_wflow %>% fit(data = ames_train)\n\n# Selecciona la receta:\nextract_recipe(lm_fit, estimated = TRUE)\n## \n## ── Recipe ───────────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:   1\n## predictor: 6\n## \n## ── Training information\n## Training data contained 2342 data points and no incomplete rows.\n## \n## ── Operations\n## • Collapsing factor levels for: Neighborhood | Trained\n## • Dummy variables from: Neighborhood, Bldg_Type | Trained\n## • Interactions with: Gr_Liv_Area:(Bldg_Type_TwoFmCon + Bldg_Type_Duplex +\n##   Bldg_Type_Twnhs + Bldg_Type_TwnhsE) | Trained\n## • Natural splines on: Latitude, Longitude | Trained\n```\n:::\n\n\nPodemos guardar los coeficientes del modelo lineal para un objeto de modelo ajustado desde un flujo de trabajo:\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-extract-func_6f907e7065c6bff56e9c560cdcf42f69'}\n\n```{.r .cell-code}\nget_model <- function(x) {\n  extract_fit_parsnip(x) %>% tidy()\n}\n\n# Pruébelo usando:\n# get_model(lm_fit)\n```\n:::\n\n\nAhora apliquemos esta función a los diez ajustes remuestreados. Los resultados de la función de extracción se envuelven en un objeto de lista y se devuelven en un tibble:\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-extract-all_2c42b02b5ae643db206c37311d3616ec'}\n\n```{.r .cell-code}\nctrl <- control_resamples(extract = get_model)\n\nlm_res <- lm_wflow %>%  fit_resamples(resamples = ames_folds, control = ctrl)\nlm_res\n## # Resampling results\n## # 10-fold cross-validation \n## # A tibble: 10 × 5\n##   splits             id     .metrics         .notes           .extracts       \n##   <list>             <chr>  <list>           <list>           <list>          \n## 1 <split [2107/235]> Fold01 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [1 × 2]>\n## 2 <split [2107/235]> Fold02 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [1 × 2]>\n## 3 <split [2108/234]> Fold03 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [1 × 2]>\n## 4 <split [2108/234]> Fold04 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [1 × 2]>\n## 5 <split [2108/234]> Fold05 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [1 × 2]>\n## 6 <split [2108/234]> Fold06 <tibble [2 × 4]> <tibble [0 × 3]> <tibble [1 × 2]>\n## # ℹ 4 more rows\n```\n:::\n\n\nAhora hay una columna `.extracts` con tibbles anidados. ¿Qué contienen estos? Averigüemos subconjuntos.\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-extract-res_b1546c373ea5a27fabafd2235bed214a'}\n\n```{.r .cell-code}\nlm_res$.extracts[[1]]\n## # A tibble: 1 × 2\n##   .extracts         .config             \n##   <list>            <chr>               \n## 1 <tibble [73 × 5]> Preprocessor1_Model1\n\n# Para obtener los resultados\nlm_res$.extracts[[1]][[1]]\n## [[1]]\n## # A tibble: 73 × 5\n##   term                        estimate  std.error statistic   p.value\n##   <chr>                          <dbl>      <dbl>     <dbl>     <dbl>\n## 1 (Intercept)                 1.48     0.320         4.62   4.11e-  6\n## 2 Gr_Liv_Area                 0.000158 0.00000476   33.2    9.72e-194\n## 3 Year_Built                  0.00180  0.000149     12.1    1.57e- 32\n## 4 Neighborhood_College_Creek -0.00163  0.0373       -0.0438 9.65e-  1\n## 5 Neighborhood_Old_Town      -0.0757   0.0138       -5.47   4.92e-  8\n## 6 Neighborhood_Edwards       -0.109    0.0310       -3.53   4.21e-  4\n## # ℹ 67 more rows\n```\n:::\n\n\nEsto podría parecer un método complicado para guardar los resultados del modelo. Sin embargo, `extract` es flexible y no supone que el usuario solo guardará un tibble por remuestreo. Por ejemplo, el método `tidy()` podría ejecutarse tanto en la receta como en el modelo. En este caso, se devolverá una lista de dos tibbles.\n\nPara nuestro ejemplo más simple, todos los resultados se pueden aplanar y recopilar usando:\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-extract-fraction_e1ddb0c1f8a75c3f2f1c908d902cd708'}\n\n```{.r .cell-code}\nall_coef <- map_dfr(lm_res$.extracts, ~ .x[[1]][[1]])\n# Muestre las réplicas de un único predictor:\nfilter(all_coef, term == \"Year_Built\")\n## # A tibble: 10 × 5\n##   term       estimate std.error statistic  p.value\n##   <chr>         <dbl>     <dbl>     <dbl>    <dbl>\n## 1 Year_Built  0.00180  0.000149      12.1 1.57e-32\n## 2 Year_Built  0.00180  0.000151      12.0 6.45e-32\n## 3 Year_Built  0.00185  0.000150      12.3 1.00e-33\n## 4 Year_Built  0.00183  0.000147      12.5 1.90e-34\n## 5 Year_Built  0.00184  0.000150      12.2 2.47e-33\n## 6 Year_Built  0.00180  0.000150      12.0 3.35e-32\n## # ℹ 4 more rows\n```\n:::\n\n\nLos capítulos [-@sec-grid-search] y [-@sec-iterative-search] analizan un conjunto de funciones para ajustar modelos. Sus interfaces son similares a `fit_resamples()` y muchas de las características descritas aquí se aplican a esas funciones.\n\n## Resumen del capítulo {#sec-resampling-summary}\n\nEste capítulo describe una de las herramientas fundamentales del análisis de datos, la capacidad de medir el rendimiento y la variación en los resultados del modelo. El remuestreo nos permite determinar qué tan bien funciona el modelo sin utilizar el conjunto de prueba.\n\nSe introdujo una función importante del paquete <span class=\"pkg\">tune</span>, llamada `fit_resamples()`. La interfaz para esta función también se utiliza en capítulos futuros que describen las herramientas de ajuste de modelos.\n\nEl código de análisis de datos, hasta ahora, para los datos de Ames es:\n\n\n::: {.cell layout-align=\"center\" hash='10-resampling_cache/html/resampling-summary_fee6ae4206f3db10a3a95664ffd6410a'}\n\n```{.r .cell-code}\nlibrary(tidymodels)\ndata(ames)\names <- mutate(ames, Sale_Price = log10(Sale_Price))\n\nset.seed(502)\names_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train <- training(ames_split)\names_test  <-  testing(ames_split)\n\names_rec <- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %>%\n  step_log(Gr_Liv_Area, base = 10) %>% \n  step_other(Neighborhood, threshold = 0.01) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %>% \n  step_ns(Latitude, Longitude, deg_free = 20)\n\nlm_model <- linear_reg() %>% set_engine(\"lm\")\n\nlm_wflow <- \n  workflow() %>% \n  add_model(lm_model) %>% \n  add_recipe(ames_rec)\n\nlm_fit <- fit(lm_wflow, ames_train)\n\nrf_model <- \n  rand_forest(trees = 1000) %>% \n  set_engine(\"ranger\") %>% \n  set_mode(\"regression\")\n\nrf_wflow <- \n  workflow() %>% \n  add_formula(\n    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n      Latitude + Longitude) %>% \n  add_model(rf_model) \n\nset.seed(1001)\names_folds <- vfold_cv(ames_train, v = 10)\n\nkeep_pred <- control_resamples(save_pred = TRUE, save_workflow = TRUE)\n\nset.seed(1003)\nrf_res <- rf_wflow %>% fit_resamples(resamples = ames_folds, control = keep_pred)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}