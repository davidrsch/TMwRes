{
  "hash": "d33fda03d6a3583f260cefb9b1a52467",
  "result": {
    "markdown": "\n\n\n# Feature Engineering with recipes {#sec-recipes}\n\nFeature engineering entails reformatting predictor values to make them easier for a model to use effectively. This includes transformations and encodings of the data to best represent their important characteristics. Imagine that you have two predictors in a data set that can be more effectively represented in your model as a ratio; creating a new predictor from the ratio of the original two is a simple example of feature engineering.\n\nTake the location of a house in Ames as a more involved example. There are a variety of ways that this spatial information can be exposed to a model, including neighborhood (a qualitative measure), longitude/latitude, distance to the nearest school or Iowa State University, and so on. When choosing how to encode these data in modeling, we might choose an option we believe is most associated with the outcome. The original format of the data, for example numeric (e.g., distance) versus categorical (e.g., neighborhood), is also a driving factor in feature engineering choices.\n\nOther examples of preprocessing to build better features for modeling include:\n\n-   Correlation between predictors can be reduced via feature extraction or the removal of some predictors.\n\n-   When some predictors have missing values, they can be imputed using a sub-model.\n\n-   Models that use variance-type measures may benefit from coercing the distribution of some skewed predictors to be symmetric by estimating a transformation.\n\nFeature engineering and data preprocessing can also involve reformatting that may be required by the model. Some models use geometric distance metrics and, consequently, numeric predictors should be centered and scaled so that they are all in the same units. Otherwise, the distance values would be biased by the scale of each column.\n\n::: rmdnote\nDifferent models have different preprocessing requirements and some, such as tree-based models, require very little preprocessing at all. [Appendix @sec-pre-proc-table] contains a small table of recommended preprocessing techniques for different models.\n:::\n\nIn this chapter, we introduce the [<span class=\"pkg\">recipes</span>](https://recipes.tidymodels.org/) package that you can use to combine different feature engineering and preprocessing tasks into a single object and then apply these transformations to different data sets. The <span class=\"pkg\">recipes</span> package is, like <span class=\"pkg\">parsnip</span> for models, one of the core tidymodels packages.\n\nThis chapter uses the Ames housing data and the R objects created in the book so far, as summarized in @sec-workflows-summary.\n\n## A Simple `recipe()` for the Ames Housing Data\n\nIn this section, we will focus on a small subset of the predictors available in the Ames housing data:\n\n-   The neighborhood (qualitative, with 29 neighborhoods in the training set)\n\n-   The gross above-grade living area (continuous, named `Gr_Liv_Area`)\n\n-   The year built (`Year_Built`)\n\n-   The type of building (`Bldg_Type` with values `OneFam` ($n = 1,936$), `TwoFmCon` ($n =    50$), `Duplex` ($n =    88$), `Twnhs` ($n =    77$), and `TwnhsE` ($n =   191$))\n\nSuppose that an initial ordinary linear regression model were fit to these data. Recalling that, in [Chapter @sec-ames], the sale prices were pre-logged, a standard call to `lm()` might look like:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm(Sale_Price ~ Neighborhood + log10(Gr_Liv_Area) + Year_Built + Bldg_Type, data = ames)\n```\n:::\n\n\nWhen this function is executed, the data are converted from a data frame to a numeric *design matrix* (also called a *model matrix*) and then the least squares method is used to estimate parameters. In @sec-formula we listed the multiple purposes of the R model formula; let's focus only on the data manipulation aspects for now. What this formula does can be decomposed into a series of steps:\n\n1.  Sale price is defined as the outcome while neighborhood, gross living area, the year built, and building type variables are all defined as predictors.\n\n2.  A log transformation is applied to the gross living area predictor.\n\n3.  The neighborhood and building type columns are converted from a non-numeric format to a numeric format (since least squares requires numeric predictors).\n\nAs mentioned in [Chapter @sec-base-r], the formula method will apply these data manipulations to any data, including new data, that are passed to the `predict()` function.\n\nA recipe is also an object that defines a series of steps for data processing. Unlike the formula method inside a modeling function, the recipe defines the steps via `step_*()` functions without immediately executing them; it is only a specification of what should be done. Here is a recipe equivalent to the previous formula that builds on the code summary in @sec-splitting-summary:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels) # Includes the recipes package\ntidymodels_prefer()\n\nsimple_ames <- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,\n         data = ames_train) %>%\n  step_log(Gr_Liv_Area, base = 10) %>% \n  step_dummy(all_nominal_predictors())\nsimple_ames\n## \n## ── Recipe ───────────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:   1\n## predictor: 4\n## \n## ── Operations\n## • Log transformation on: Gr_Liv_Area\n## • Dummy variables from: all_nominal_predictors()\n```\n:::\n\n\nLet's break this down:\n\n1.  The call to `recipe()` with a formula tells the recipe the *roles* of the \"ingredients\" or variables (e.g., predictor, outcome). It only uses the data `ames_train` to determine the data types for the columns.\n\n2.  `step_log()` declares that `Gr_Liv_Area` should be log transformed.\n\n3.  `step_dummy()` specifies which variables should be converted from a qualitative format to a quantitative format, in this case, using dummy or indicator variables. An indicator or dummy variable is a binary numeric variable (a column of ones and zeroes) that encodes qualitative information; we will dig deeper into these kinds of variables in @sec-dummies.\n\nThe function `all_nominal_predictors()` captures the names of any predictor columns that are currently factor or character (i.e., nominal) in nature. This is a <span class=\"pkg\">dplyr</span>-like selector function similar to `starts_with()` or `matches()` but that can only be used inside of a recipe.\n\n::: rmdnote\nOther selectors specific to the <span class=\"pkg\">recipes</span> package are: `all_numeric_predictors()`, `all_numeric()`, `all_predictors()`, and `all_outcomes()`. As with <span class=\"pkg\">dplyr</span>, one or more unquoted expressions, separated by commas, can be used to select which columns are affected by each step.\n:::\n\nWhat is the advantage to using a recipe, over a formula or raw predictors? There are a few, including:\n\n-   These computations can be recycled across models since they are not tightly coupled to the modeling function.\n\n-   A recipe enables a broader set of data processing choices than formulas can offer.\n\n-   The syntax can be very compact. For example, `all_nominal_predictors()` can be used to capture many variables for specific types of processing while a formula would require each to be explicitly listed.\n\n-   All data processing can be captured in a single R object instead of in scripts that are repeated, or even spread across different files.\n\n## Using Recipes\n\nAs we discussed in [Chapter @sec-workflows], preprocessing choices and feature engineering should typically be considered part of a modeling workflow, not a separate task. The <span class=\"pkg\">workflows</span> package contains high level functions to handle different types of preprocessors. Our previous workflow (`lm_wflow`) used a simple set of <span class=\"pkg\">dplyr</span> selectors. To improve on that approach with more complex feature engineering, let's use the `simple_ames` recipe to preprocess data for modeling.\n\nThis object can be attached to the workflow:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm_wflow %>% \n  add_recipe(simple_ames)\n## Error in `add_recipe()`:\n## ! A recipe cannot be added when variables already exist.\n```\n:::\n\n\nThat did not work! We can have only one preprocessing method at a time, so we need to remove the existing preprocessor before adding the recipe.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm_wflow <- \n  lm_wflow %>% \n  remove_variables() %>% \n  add_recipe(simple_ames)\nlm_wflow\n## ══ Workflow ═════════════════════════════════════════════════════════════════════════\n## Preprocessor: Recipe\n## Model: linear_reg()\n## \n## ── Preprocessor ─────────────────────────────────────────────────────────────────────\n## 2 Recipe Steps\n## \n## • step_log()\n## • step_dummy()\n## \n## ── Model ────────────────────────────────────────────────────────────────────────────\n## Linear Regression Model Specification (regression)\n## \n## Computational engine: lm\n```\n:::\n\n\nLet's estimate both the recipe and model using a simple call to `fit()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm_fit <- fit(lm_wflow, ames_train)\n```\n:::\n\n\nThe `predict()` method applies the same preprocessing that was used on the training set to the new data before passing them along to the model's `predict()` method:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(lm_fit, ames_test %>% slice(1:3))\n## # A tibble: 3 × 1\n##   .pred\n##   <dbl>\n## 1  5.08\n## 2  5.32\n## 3  5.28\n```\n:::\n\n\nIf we need the bare model object or recipe, there are `extract_*` functions that can retrieve them:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Get the recipe after it has been estimated:\nlm_fit %>% \n  extract_recipe(estimated = TRUE)\n## \n## ── Recipe ───────────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:   1\n## predictor: 4\n## \n## ── Training information\n## Training data contained 2342 data points and no incomplete rows.\n## \n## ── Operations\n## • Log transformation on: Gr_Liv_Area | Trained\n## • Dummy variables from: Neighborhood, Bldg_Type | Trained\n\n# To tidy the model fit: \nlm_fit %>% \n  # This returns the parsnip object:\n  extract_fit_parsnip() %>% \n  # Now tidy the linear model object:\n  tidy() %>% \n  slice(1:5)\n## # A tibble: 5 × 5\n##   term                       estimate std.error statistic   p.value\n##   <chr>                         <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)                -0.669    0.231        -2.90 3.80e-  3\n## 2 Gr_Liv_Area                 0.620    0.0143       43.2  2.63e-299\n## 3 Year_Built                  0.00200  0.000117     17.1  6.16e- 62\n## 4 Neighborhood_College_Creek  0.0178   0.00819       2.17 3.02e-  2\n## 5 Neighborhood_Old_Town      -0.0330   0.00838      -3.93 8.66e-  5\n```\n:::\n\n\n::: rmdnote\nTools for using (and debugging) recipes outside of workflow objects are described in @sec-recipe-functions.\n:::\n\n## How Data Are Used by the `recipe()`\n\nData are passed to recipes at different stages.\n\nFirst, when calling `recipe(..., data)`, the data set is used to determine the data types of each column so that selectors such as `all_numeric()` or `all_numeric_predictors()` can be used.\n\nSecond, when preparing the data using `fit(workflow, data)`, the training data are used for all estimation operations including a recipe that may be part of the `workflow`, from determining factor levels to computing PCA components and everything in between.\n\n::: rmdwarning\nAll preprocessing and feature engineering steps use *only* the training data. Otherwise, information leakage can negatively impact the model's performance when used with new data.\n:::\n\nFinally, when using `predict(workflow, new_data)`, no model or preprocessor parameters like those from recipes are re-estimated using the values in `new_data`. Take centering and scaling using `step_normalize()` as an example. Using this step, the means and standard deviations from the appropriate columns are determined from the training set; new samples at prediction time are standardized using these values from training when `predict()` is invoked.\n\n## Examples of Recipe Steps {#sec-example-steps}\n\nBefore proceeding, let's take an extended tour of the capabilities of <span class=\"pkg\">recipes</span> and explore some of the most important `step_*()` functions. These recipe step functions each specify a specific possible step in a feature engineering process, and different recipe steps can have different effects on columns of data.\n\n### Encoding qualitative data in a numeric format {#sec-dummies}\n\nOne of the most common feature engineering tasks is transforming nominal or qualitative data (factors or characters) so that they can be encoded or represented numerically. Sometimes we can alter the factor levels of a qualitative column in helpful ways prior to such a transformation. For example, `step_unknown()` can be used to change missing values to a dedicated factor level. Similarly, if we anticipate that a new factor level may be encountered in future data, `step_novel()` can allot a new level for this purpose.\n\nAdditionally, `step_other()` can be used to analyze the frequencies of the factor levels in the training set and convert infrequently occurring values to a catch-all level of \"other,\" with a threshold that can be specified. A good example is the `Neighborhood` predictor in our data, shown in @fig-ames-neighborhoods.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Frequencies of neighborhoods in the Ames training set](figures/fig-ames-neighborhoods-1.png){#fig-ames-neighborhoods fig-align='center' fig-alt='A bar chart of the frequencies of neighborhoods in the Ames training set. The most homes are in North Ames while the Greens, Green Hills, and Landmark neighborhood have very few instances.' width=672}\n:::\n:::\n\n\nHere we see that two neighborhoods have less than five properties in the training data (Landmark and Green Hills); in this case, no houses at all in the Landmark neighborhood were included in the testing set. For some models, it may be problematic to have dummy variables with a single nonzero entry in the column. At a minimum, it is highly improbable that these features would be important to a model. If we add `step_other(Neighborhood, threshold = 0.01)` to our recipe, the bottom 1% of the neighborhoods will be lumped into a new level called \"other.\" In this training set, this will catch seven neighborhoods.\n\nFor the Ames data, we can amend the recipe to use:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsimple_ames <- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,\n         data = ames_train) %>%\n  step_log(Gr_Liv_Area, base = 10) %>% \n  step_other(Neighborhood, threshold = 0.01) %>% \n  step_dummy(all_nominal_predictors())\n```\n:::\n\n\n::: rmdnote\nMany, but not all, underlying model calculations require predictor values to be encoded as numbers. Notable exceptions include tree-based models, rule-based models, and naive Bayes models.\n:::\n\nThe most common method for converting a factor predictor to a numeric format is to create dummy or indicator variables. Let's take the predictor in the Ames data for the building type, which is a factor variable with five levels (see @tbl-dummy-vars). For dummy variables, the single `Bldg_Type` column would be replaced with four numeric columns whose values are either zero or one. These binary variables represent specific factor level values. In R, the convention is to exclude a column for the first factor level (`OneFam`, in this case). The `Bldg_Type` column would be replaced with a column called `TwoFmCon` that is one when the row has that value and zero otherwise. Three other columns are similarly created:\n\n\n::: {#tbl-dummy-vars .cell layout-align=\"center\" tbl-cap='Illustration of binary encodings (i.e., dummy variables) for a qualitative predictor.'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Raw Data </th>\n   <th style=\"text-align:right;\"> TwoFmCon </th>\n   <th style=\"text-align:right;\"> Duplex </th>\n   <th style=\"text-align:right;\"> Twnhs </th>\n   <th style=\"text-align:right;\"> TwnhsE </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> OneFam </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> TwoFmCon </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Duplex </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Twnhs </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> TwnhsE </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nWhy not all five? The most basic reason is simplicity; if you know the value for these four columns, you can determine the last value because these are mutually exclusive categories. More technically, the classical justification is that a number of models, including ordinary linear regression, have numerical issues when there are linear dependencies between columns. If all five building type indicator columns are included, they would add up to the intercept column (if there is one). This would cause an issue, or perhaps an outright error, in the underlying matrix algebra.\n\nThe full set of encodings can be used for some models. This is traditionally called the one-hot encoding and can be achieved using the `one_hot` argument of `step_dummy()`.\n\nOne helpful feature of `step_dummy()` is that there is more control over how the resulting dummy variables are named. In base R, dummy variable names mash the variable name with the level, resulting in names like `NeighborhoodVeenker`. Recipes, by default, use an underscore as the separator between the name and level (e.g., `Neighborhood_Veenker`) and there is an option to use custom formatting for the names. The default naming convention in <span class=\"pkg\">recipes</span> makes it easier to capture those new columns in future steps using a selector, such as `starts_with(\"Neighborhood_\")`.\n\nTraditional dummy variables require that all of the possible categories be known to create a full set of numeric features. There are other methods for doing this transformation to a numeric format. *Feature hashing* methods only consider the value of the category to assign it to a predefined pool of dummy variables. *Effect* or *likelihood encodings* replace the original data with a single numeric column that measures the *effect* of those data. Both feature hashing and effect encoding can seamlessly handle situations where a novel factor level is encountered in the data. [Chapter @sec-categorical] explores these and other methods for encoding categorical data, beyond straightforward dummy or indicator variables.\n\n::: rmdnote\nDifferent recipe steps behave differently when applied to variables in the data. For example, `step_log()` modifies a column in place without changing the name. Other steps, such as `step_dummy()`, eliminate the original data column and replace it with one or more columns with different names. The effect of a recipe step depends on the type of feature engineering transformation being done.\n:::\n\n### Interaction terms\n\nInteraction effects involve two or more predictors. Such an effect occurs when one predictor has an effect on the outcome that is contingent on one or more other predictors. For example, if you were trying to predict how much traffic there will be during your commute, two potential predictors could be the specific time of day you commute and the weather. However, the relationship between the amount of traffic and bad weather is different for different times of day. In this case, you could add an interaction term between the two predictors to the model along with the original two predictors (which are called the main effects). Numerically, an interaction term between predictors is encoded as their product. Interactions are defined in terms of their effect on the outcome and can be combinations of different types of data (e.g., numeric, categorical, etc). [Chapter 7](https://bookdown.org/max/FES/detecting-interaction-effects.html) of @fes discusses interactions and how to detect them in greater detail.\n\nAfter exploring the Ames training set, we might find that the regression slopes for the gross living area differ for different building types, as shown in @fig-building-type-interactions.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(ames_train, aes(x = Gr_Liv_Area, y = 10^Sale_Price)) + \n  geom_point(alpha = .2) + \n  facet_wrap(~ Bldg_Type) + \n  geom_smooth(method = lm, formula = y ~ x, se = FALSE, color = \"lightblue\") + \n  scale_x_log10() + \n  scale_y_log10() + \n  labs(x = \"Gross Living Area\", y = \"Sale Price (USD)\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Gross living area (in log-10 units) versus sale price (also in log-10 units) for five different building types](figures/fig-building-type-interactions-1.png){#fig-building-type-interactions fig-align='center' fig-alt='Scatter plots of gross living area (in log-10 units) versus sale price (also in log-10 units) for five different building types. All trends are linear but appear to have different slopes and intercepts for the different building types.' width=672}\n:::\n:::\n\n\nHow are interactions specified in a recipe? A base R formula would take an interaction using a `:`, so we would use:\n\n``` r\nSale_Price ~ Neighborhood + log10(Gr_Liv_Area) + Bldg_Type + \n  log10(Gr_Liv_Area):Bldg_Type\n# or\nSale_Price ~ Neighborhood + log10(Gr_Liv_Area) * Bldg_Type \n```\n\nwhere `*` expands those columns to the main effects and interaction term. Again, the formula method does many things simultaneously and understands that a factor variable (such as `Bldg_Type`) should be expanded into dummy variables first and that the interaction should involve all of the resulting binary columns.\n\nRecipes are more explicit and sequential, and they give you more control. With the current recipe, `step_dummy()` has already created dummy variables. How would we combine these for an interaction? The additional step would look like `step_interact(~ interaction terms)` where the terms on the right-hand side of the tilde are the interactions. These can include selectors, so it would be appropriate to use:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsimple_ames <- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type,\n         data = ames_train) %>%\n  step_log(Gr_Liv_Area, base = 10) %>% \n  step_other(Neighborhood, threshold = 0.01) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  # Gr_Liv_Area is on the log scale from a previous step\n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") )\n```\n:::\n\n\nAdditional interactions can be specified in this formula by separating them by `+`. Also note that the recipe will only use interactions between different variables; if the formula uses `var_1:var_1`, this term will be ignored.\n\nSuppose that, in a recipe, we had not yet made dummy variables for building types. It would be inappropriate to include a factor column in this step, such as:\n\n``` r\n step_interact( ~ Gr_Liv_Area:Bldg_Type )\n```\n\nThis is telling the underlying (base R) code used by `step_interact()` to make dummy variables and then form the interactions. In fact, if this occurs, a warning states that this might generate unexpected results.\n\n\n::: {.cell layout-align=\"center\" type='rmdwarning'}\n<div class=\"rmdwarning\">\n<p>This behavior gives you more control, but it is different from R’s\nstandard model formula.</p>\n</div>\n:::\n\n\nAs with naming dummy variables, <span class=\"pkg\">recipes</span> provides more coherent names for interaction terms. In this case, the interaction is named `Gr_Liv_Area_x_Bldg_Type_Duplex` instead of `Gr_Liv_Area:Bldg_TypeDuplex` (which is not a valid column name for a data frame).\n\n::: rmdnote\n*Remember that order matters*. The gross living area is log transformed prior to the interaction term. Subsequent interactions with this variable will also use the log scale.\n:::\n\n### Spline functions\n\nWhen a predictor has a nonlinear relationship with the outcome, some types of predictive models can adaptively approximate this relationship during training. However, simpler is usually better and it is not uncommon to try to use a simple model, such as a linear fit, and add in specific nonlinear features for predictors that may need them, such as longitude and latitude for the Ames housing data. One common method for doing this is to use *spline* functions to represent the data. Splines replace the existing numeric predictor with a set of columns that allow a model to emulate a flexible, nonlinear relationship. As more spline terms are added to the data, the capacity to nonlinearly represent the relationship increases. Unfortunately, it may also increase the likelihood of picking up on data trends that occur by chance (i.e., overfitting).\n\nIf you have ever used `geom_smooth()` within a `ggplot`, you have probably used a spline representation of the data. For example, each panel in @fig-ames-latitude-splines uses a different number of smooth splines for the latitude predictor:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(patchwork)\nlibrary(splines)\n\nplot_smoother <- function(deg_free) {\n  ggplot(ames_train, aes(x = Latitude, y = 10^Sale_Price)) + \n    geom_point(alpha = .2) + \n    scale_y_log10() +\n    geom_smooth(\n      method = lm,\n      formula = y ~ ns(x, df = deg_free),\n      color = \"lightblue\",\n      se = FALSE\n    ) +\n    labs(title = paste(deg_free, \"Spline Terms\"),\n         y = \"Sale Price (USD)\")\n}\n\n( plot_smoother(2) + plot_smoother(5) ) / ( plot_smoother(20) + plot_smoother(100) )\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Sale price versus latitude, with trend lines using natural splines with different degrees of freedom](figures/fig-ames-latitude-splines-1.png){#fig-ames-latitude-splines fig-align='center' fig-alt='Scatter plots of sale price versus latitude with trend lines using natural splines with different degrees of freedom. As the degrees of freedom increase, the lines are more responsive to trends in the data but begin to become excessively complex with 100 spline terms.' width=672}\n:::\n:::\n\n\nThe `ns()` function in the <span class=\"pkg\">splines</span> package generates feature columns using functions called *natural splines*.\n\nSome panels in @fig-ames-latitude-splines clearly fit poorly; two terms *underfit* the data while 100 terms *overfit*. The panels with five and twenty terms seem like reasonably smooth fits that catch the main patterns of the data. This indicates that the proper amount of \"nonlinear-ness\" matters. The number of spline terms could then be considered a *tuning parameter* for this model. These types of parameters are explored in [Chapter @sec-tuning].\n\nIn <span class=\"pkg\">recipes</span>, multiple steps can create these types of terms. To add a natural spline representation for this predictor:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrecipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Latitude,\n         data = ames_train) %>%\n  step_log(Gr_Liv_Area, base = 10) %>% \n  step_other(Neighborhood, threshold = 0.01) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %>% \n  step_ns(Latitude, deg_free = 20)\n```\n:::\n\n\nThe user would need to determine if both neighborhood and latitude should be in the model since they both represent the same underlying data in different ways.\n\n### Feature extraction\n\nAnother common method for representing multiple features at once is called *feature extraction*. Most of these techniques create new features from the predictors that capture the information in the broader set as a whole. For example, principal component analysis (PCA) tries to extract as much of the original information in the predictor set as possible using a smaller number of features. PCA is a linear extraction method, meaning that each new feature is a linear combination of the original predictors. One nice aspect of PCA is that each of the new features, called the principal components or PCA scores, are uncorrelated with one another. Because of this, PCA can be very effective at reducing the correlation between predictors. Note that PCA is only aware of the predictors; the new PCA features might not be associated with the outcome.\n\nIn the Ames data, several predictors measure size of the property, such as the total basement size (`Total_Bsmt_SF`), size of the first floor (`First_Flr_SF`), the gross living area (`Gr_Liv_Area`), and so on. PCA might be an option to represent these potentially redundant variables as a smaller feature set. Apart from the gross living area, these predictors have the suffix `SF` in their names (for square feet) so a recipe step for PCA might look like:\n\n``` r\n  # Use a regular expression to capture house size predictors: \n  step_pca(matches(\"(SF$)|(Gr_Liv)\"))\n```\n\nNote that all of these columns are measured in square feet. PCA assumes that all of the predictors are on the same scale. That's true in this case, but often this step can be preceded by `step_normalize()`, which will center and scale each column.\n\nThere are existing recipe steps for other extraction methods, such as: independent component analysis (ICA), non-negative matrix factorization (NNMF), multidimensional scaling (MDS), uniform manifold approximation and projection (UMAP), and others.\n\n### Row sampling steps\n\nRecipe steps can affect the rows of a data set as well. For example, *subsampling* techniques for class imbalances change the class proportions in the data being given to the model; these techniques often don't improve overall performance but can generate better behaved distributions of the predicted class probabilities. These are approaches to try when subsampling your data with class imbalance:\n\n-   *Downsampling* the data keeps the minority class and takes a random sample of the majority class so that class frequencies are balanced.\n\n-   *Upsampling* replicates samples from the minority class to balance the classes. Some techniques do this by synthesizing new samples that resemble the minority class data while other methods simply add the same minority samples repeatedly.\n\n-   *Hybrid methods* do a combination of both.\n\nThe [<span class=\"pkg\">themis</span>](https://themis.tidymodels.org/) package has recipe steps that can be used to address class imbalance via subsampling. For simple downsampling, we would use:\n\n``` r\n  step_downsample(outcome_column_name)\n```\n\n::: rmdwarning\nOnly the training set should be affected by these techniques. The test set or other holdout samples should be left as-is when processed using the recipe. For this reason, all of the subsampling steps default the `skip` argument to have a value of `TRUE` (@sec-skip-equals-true).\n:::\n\nOther step functions are row-based as well: `step_filter()`, `step_sample()`, `step_slice()`, and `step_arrange()`. In almost all uses of these steps, the `skip` argument should be set to `TRUE`.\n\n### General transformations\n\nMirroring the original <span class=\"pkg\">dplyr</span> operation, `step_mutate()` can be used to conduct a variety of basic operations to the data. It is best used for straightforward transformations like computing a ratio of two variables, such as `Bedroom_AbvGr / Full_Bath`, the ratio of bedrooms to bathrooms for the Ames housing data.\n\n::: rmdwarning\nWhen using this flexible step, use extra care to avoid data leakage in your preprocessing. Consider, for example, the transformation `x = w > mean(w)`. When applied to new data or testing data, this transformation would use the mean of `w` from the *new* data, not the mean of `w` from the training data.\n:::\n\n### Natural language processing\n\nRecipes can also handle data that are not in the traditional structure where the columns are features. For example, the [<span class=\"pkg\">textrecipes</span>](https://textrecipes.tidymodels.org/) package can apply natural language processing methods to the data. The input column is typically a string of text, and different steps can be used to tokenize the data (e.g., split the text into separate words), filter out tokens, and create new features appropriate for modeling.\n\n## Skipping Steps for New Data {#sec-skip-equals-true}\n\nThe sale price data are already log-transformed in the `ames` data frame. Why not use:\n\n``` r\n step_log(Sale_Price, base = 10)\n```\n\nThis will cause a failure when the recipe is applied to new properties with an unknown sale price. Since price is what we are trying to predict, there probably won't be a column in the data for this variable. In fact, to avoid information leakage, many tidymodels packages isolate the data being used when making any predictions. This means that the training set and any outcome columns are not available for use at prediction time.\n\n::: rmdnote\nFor simple transformations of the outcome column(s), we strongly suggest that those operations be *conducted outside of the recipe*.\n:::\n\nHowever, there are other circumstances where this is not an adequate solution. For example, in classification models where there is a severe class imbalance, it is common to conduct *subsampling* of the data that are given to the modeling function. For example, suppose that there were two classes and a 10% event rate. A simple, albeit controversial, approach would be to *downsample* the data so that the model is provided with all of the events and a random 10% of the nonevent samples.\n\nThe problem is that the same subsampling process should not be applied to the data being predicted. As a result, when using a recipe, we need a mechanism to ensure that some operations are applied only to the data that are given to the model. Each step function has an option called `skip` that, when set to `TRUE`, will be ignored by the `predict()` function. In this way, you can isolate the steps that affect the modeling data without causing errors when applied to new samples. However, all steps are applied when using `fit()`.\n\n\n\n\n\nAt the time of this writing, the step functions in the <span class=\"pkg\">recipes</span> and <span class=\"pkg\">themis</span> packages that are only applied to the training data are: `step_adasyn()`, `step_bsmote()`, `step_downsample()`, `step_filter()`, `step_naomit()`, `step_nearmiss()`, `step_rose()`, `step_sample()`, `step_slice()`, `step_smote()`, `step_smotenc()`, `step_tomek()`, and `step_upsample()`.\n\n## Tidy a `recipe()`\n\nIn @sec-tidiness-modeling, we introduced the `tidy()` verb for statistical objects. There is also a `tidy()` method for recipes, as well as individual recipe steps. Before proceeding, let's create an extended recipe for the Ames data using some of the new steps we've discussed in this chapter:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\names_rec <- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %>%\n  step_log(Gr_Liv_Area, base = 10) %>% \n  step_other(Neighborhood, threshold = 0.01) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %>% \n  step_ns(Latitude, Longitude, deg_free = 20)\n```\n:::\n\n\nThe `tidy()` method, when called with the recipe object, gives a summary of the recipe steps:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy(ames_rec)\n## # A tibble: 5 × 6\n##   number operation type     trained skip  id            \n##    <int> <chr>     <chr>    <lgl>   <lgl> <chr>         \n## 1      1 step      log      FALSE   FALSE log_66JTU     \n## 2      2 step      other    FALSE   FALSE other_ePfcw   \n## 3      3 step      dummy    FALSE   FALSE dummy_Z18Cl   \n## 4      4 step      interact FALSE   FALSE interact_JLU36\n## 5      5 step      ns       FALSE   FALSE ns_rvsqQ\n```\n:::\n\n\nThis result can be helpful for identifying individual steps, perhaps to then be able to execute the `tidy()` method on one specific step.\n\nWe can specify the `id` argument in any step function call; otherwise it is generated using a random suffix. Setting this value can be helpful if the same type of step is added to the recipe more than once. Let's specify the `id` ahead of time for `step_other()`, since we'll want to `tidy()` it:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\names_rec <- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %>%\n  step_log(Gr_Liv_Area, base = 10) %>% \n  step_other(Neighborhood, threshold = 0.01, id = \"my_id\") %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %>% \n  step_ns(Latitude, Longitude, deg_free = 20)\n```\n:::\n\n\nWe'll refit the workflow with this new recipe:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlm_wflow <- \n  workflow() %>% \n  add_model(lm_model) %>% \n  add_recipe(ames_rec)\n\nlm_fit <- fit(lm_wflow, ames_train)\n```\n:::\n\n\nThe `tidy()` method can be called again along with the `id` identifier we specified to get our results for applying `step_other()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nestimated_recipe <- \n  lm_fit %>% \n  extract_recipe(estimated = TRUE)\n\ntidy(estimated_recipe, id = \"my_id\")\n## # A tibble: 22 × 3\n##   terms        retained           id   \n##   <chr>        <chr>              <chr>\n## 1 Neighborhood North_Ames         my_id\n## 2 Neighborhood College_Creek      my_id\n## 3 Neighborhood Old_Town           my_id\n## 4 Neighborhood Edwards            my_id\n## 5 Neighborhood Somerset           my_id\n## 6 Neighborhood Northridge_Heights my_id\n## # ℹ 16 more rows\n```\n:::\n\n\nThe `tidy()` results we see here for using `step_other()` show which factor levels were retained, i.e., not added to the new \"other\" category.\n\nThe `tidy()` method can be called with the `number` identifier as well, if we know which step in the recipe we need:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy(estimated_recipe, number = 2)\n## # A tibble: 22 × 3\n##   terms        retained           id   \n##   <chr>        <chr>              <chr>\n## 1 Neighborhood North_Ames         my_id\n## 2 Neighborhood College_Creek      my_id\n## 3 Neighborhood Old_Town           my_id\n## 4 Neighborhood Edwards            my_id\n## 5 Neighborhood Somerset           my_id\n## 6 Neighborhood Northridge_Heights my_id\n## # ℹ 16 more rows\n```\n:::\n\n\nEach `tidy()` method returns the relevant information about that step. For example, the `tidy()` method for `step_dummy()` returns a column with the variables that were converted to dummy variables and another column with all of the known levels for each column.\n\n## Column Roles\n\nWhen a formula is used with the initial call to `recipe()` it assigns *roles* to each of the columns, depending on which side of the tilde they are on. Those roles are either `\"predictor\"` or `\"outcome\"`. However, other roles can be assigned as needed.\n\nFor example, in our Ames data set, the original raw data contained a column for address.[^08-feature-engineering-1] It may be useful to keep that column in the data so that, after predictions are made, problematic results can be investigated in detail. In other words, the column could be important even when it isn't a predictor or outcome.\n\n[^08-feature-engineering-1]: Our version of these data does not contain that column.\n\nTo solve this, the `add_role()`, `remove_role()`, and `update_role()` functions can be helpful. For example, for the house price data, the role of the street address column could be modified using:\n\n``` r\names_rec %>% update_role(address, new_role = \"street address\")\n```\n\nAfter this change, the `address` column in the dataframe will no longer be a predictor but instead will be a `\"street address\"` according to the recipe. Any character string can be used as a role. Also, columns can have multiple roles (additional roles are added via `add_role()`) so that they can be selected under more than one context.\n\nThis can be helpful when the data are *resampled*. It helps to keep the columns that are not involved with the model fit in the same data frame (rather than in an external vector). Resampling, described in [Chapter @sec-resampling], creates alternate versions of the data mostly by row subsampling. If the street address were in another column, additional subsampling would be required and might lead to more complex code and a higher likelihood of errors.\n\nFinally, all step functions have a `role` field that can assign roles to the results of the step. In many cases, columns affected by a step retain their existing role. For example, the `step_log()` calls to our `ames_rec` object affected the `Gr_Liv_Area` column. For that step, the default behavior is to keep the existing role for this column since no new column is created. As a counter-example, the step to produce splines defaults new columns to have a role of `\"predictor\"` since that is usually how spline columns are used in a model. Most steps have sensible defaults but, since the defaults can be different, be sure to check the documentation page to understand which role(s) will be assigned.\n\n## Chapter Summary {#sec-recipes-summary}\n\nIn this chapter, you learned about using <span class=\"pkg\">recipes</span> for flexible feature engineering and data preprocessing, from creating dummy variables to handling class imbalance and more. Feature engineering is an important part of the modeling process where information leakage can easily occur and good practices must be adopted. Between the <span class=\"pkg\">recipes</span> package and other packages that extend recipes, there are over 100 available steps. All possible recipe steps are enumerated at [`tidymodels.org/find`](https://www.tidymodels.org/find/). The <span class=\"pkg\">recipes</span> framework provides a rich data manipulation environment for preprocessing and transforming data prior to modeling. Additionally, [`tidymodels.org/learn/develop/recipes/`](https://www.tidymodels.org/learn/develop/recipes/) shows how custom steps can be created.\n\nOur work here has used recipes solely inside of a workflow object. For modeling, that is the recommended use because feature engineering should be estimated together with a model. However, for visualization and other activities, a workflow may not be appropriate; more recipe-specific functions may be required. [Chapter @sec-dimensionality] discusses lower-level APIs for fitting, using, and troubleshooting recipes.\n\nThe code that we will use in later chapters is:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels)\ndata(ames)\names <- mutate(ames, Sale_Price = log10(Sale_Price))\n\nset.seed(502)\names_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)\names_train <- training(ames_split)\names_test  <-  testing(ames_split)\n\names_rec <- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %>%\n  step_log(Gr_Liv_Area, base = 10) %>% \n  step_other(Neighborhood, threshold = 0.01) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %>% \n  step_ns(Latitude, Longitude, deg_free = 20)\n  \nlm_model <- linear_reg() %>% set_engine(\"lm\")\n\nlm_wflow <- \n  workflow() %>% \n  add_model(lm_model) %>% \n  add_recipe(ames_rec)\n\nlm_fit <- fit(lm_wflow, ames_train)\n```\n:::\n\n\n",
    "supporting": [
      "08-feature-engineering_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}