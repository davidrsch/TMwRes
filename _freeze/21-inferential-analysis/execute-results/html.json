{
  "hash": "563fa49516f4aa036ed984c5779cd9da",
  "result": {
    "markdown": "\n\n\n# Inferential Analysis {#inferential}\n\n::: rmdnote\nIn Section \\@ref(model-types), we outlined a taxonomy of models and said that most models can be categorized as descriptive, inferential, and/or predictive.\n:::\n\nMost of the chapters in this book have focused on models from the perspective of the accuracy of predicted values, an important quality of models for all purposes but most relevant for predictive models. Inferential models are usually created not only for their predictions, but also to make inferences or judgments about some component of the model, such as a coefficient value or other parameter. These results are often used to answer some (hopefully) pre-defined questions or hypotheses. In predictive models, predictions on hold-out data are used to validate or characterize the quality of the model. Inferential methods focus on validating the probabilistic or structural assumptions that are made prior to fitting the model.\n\nFor example, in ordinary linear regression, the common assumption is that the residual values are independent and follow a Gaussian distribution with a constant variance. While you may have scientific or domain knowledge to lend credence to this assumption for your model analysis, the residuals from the fitted model are usually examined to determine if the assumption was a good idea. As a result, the methods for determining if the model's assumptions have been met are not as simple as looking at holdout predictions, although that can be very useful as well.\n\nWe will use p-values in this chapter. However, the tidymodels framework tends to promote confidence intervals over p-values as a method for quantifying the evidence for an alternative hypothesis. As previously shown in Section \\@ref(tidyposterior), Bayesian methods are often superior to both p-values and confidence intervals in terms of ease of interpretation (but they can be more computationally expensive).\n\n::: rmdwarning\nThere has been a push in recent years to move away from p-values in favor of other methods [@pvalue]. See Volume 73 of [*The American Statistician*](https://www.tandfonline.com/toc/utas20/73/) for more information and discussion.\n:::\n\nIn this chapter, we describe how to use <span class=\"pkg\">tidymodels</span> for fitting and assessing inferential models. In some cases, the tidymodels framework can help users work with the objects produced by their models. In others, it can help assess the quality of a given model.\n\n## Inference for Count Data\n\nTo understand how tidymodels packages can be used for inferential modeling, let's focus on an example with count data. We'll use biochemistry publication data from the <span class=\"pkg\">pscl</span> package. These data consist of information on 915 Ph.D. biochemistry graduates and tries to explain factors that impact their academic productivity (measured via number or count of articles published within three years). The predictors include the gender of the graduate, their marital status, the number of children of the graduate that are at least five years old, the prestige of their department, and the number of articles produced by their mentor in the same time period. The data reflect biochemistry doctorates who finished their education between 1956 and 1963. The data are a somewhat biased sample of all of the biochemistry doctorates given during this period (based on completeness of information).\n\n::: rmdnote\nRecall that in Chapter \\@ref(trust) we asked the question \"Is our model applicable for predicting a specific data point?\" It is very important to define what populations an inferential analysis applies to. For these data, the results would likely apply to biochemistry doctorates given around the time frame that the data were collected. Does it also apply to other chemistry doctorate types (e.g., medicinal chemistry, etc)? These are important questions to address (and document) when conducting inferential analyses.\n:::\n\nA plot of the data shown in Figure \\@ref(fig:counts) indicates that many graduates did not publish any articles in this time and that the outcome follows a right-skewed distribution:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels)\ntidymodels_prefer()\n\ndata(\"bioChemists\", package = \"pscl\")\n\nggplot(bioChemists, aes(x = art)) + \n  geom_histogram(binwidth = 1, color = \"white\") + \n  labs(x = \"Number of articles within 3y of graduation\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Distribution of the number of articles written within 3 years of graduation](figures/counts-1.png){fig-align='center' fig-alt='The distribution of the number of articles written within 3 years of graduation. The distribution is right-skewed and most of the data have counts of zero or one.' width=80%}\n:::\n:::\n\n\nSince the outcome data are counts, the most common distribution assumption to make is that the outcome has a Poisson distribution. This chapter will use these data for several types of analyses.\n\n## Comparisons with Two-Sample Tests\n\nWe can start with hypothesis testing. The original author's goal with this data set on biochemistry publication data was to determine if there is a difference in publications between men and women [@Long1992]. The data from the study show:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbioChemists %>% \n  group_by(fem) %>% \n  summarize(counts = sum(art), n = length(art))\n## # A tibble: 2 × 3\n##   fem   counts     n\n##   <fct>  <int> <int>\n## 1 Men      930   494\n## 2 Women    619   421\n```\n:::\n\n\nThere were many more publications by men, although there were also more men in the data. The simplest approach to analyzing these data would be to do a two-sample comparison using the `poisson.test()` function in the <span class=\"pkg\">stats</span> package. It requires the counts for one or two groups.\n\nFor our application, the hypotheses to compare the two sexes are:\n\n\n```{=tex}\n\\begin{align}\nH_0&: \\lambda_m = \\lambda_f \\notag \\\\\nH_a&: \\lambda_m \\ne \\lambda_f \\notag\n\\end{align}\n```\n\nwhere the $\\lambda$ values are the rates of publications (over the same time period).\n\nA basic application of the test is:[^21-inferential-analysis-1]\n\n[^21-inferential-analysis-1]: The `T` argument allows us to account for the time when the events (publications) were counted, which was three years for both men and women. There are more men than women in these data, but `poisson.test()` has limited functionality so more sophisticated analysis can be used to account for this difference.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoisson.test(c(930, 619), T = 3)\n## \n## \tComparison of Poisson rates\n## \n## data:  c(930, 619) time base: 3\n## count1 = 930, expected count1 = 774, p-value = 3e-15\n## alternative hypothesis: true rate ratio is not equal to 1\n## 95 percent confidence interval:\n##  1.356 1.666\n## sample estimates:\n## rate ratio \n##      1.502\n```\n:::\n\n\nThe function reports a p-value as well as a confidence interval for the ratio of the publication rates. The results indicate that the observed difference is greater than the experiential noise and favors $H_a$.\n\nOne issue with using this function is that the results come back as an `htest` object. While this type of object has a well-defined structure, it can be difficult to consume for subsequent operations such as reporting or visualizations. The most impactful tool that tidymodels offers for inferential models is the `tidy()` functions in the <span class=\"pkg\">broom</span> package. As previously seen, this function makes a well-formed, predictably named tibble from the object. We can `tidy()` the results of our two-sample comparison test:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npoisson.test(c(930, 619)) %>% \n  tidy()\n## # A tibble: 1 × 8\n##   estimate statistic  p.value parameter conf.low conf.high method        alternative\n##      <dbl>     <dbl>    <dbl>     <dbl>    <dbl>     <dbl> <chr>         <chr>      \n## 1     1.50       930 2.73e-15      774.     1.36      1.67 Comparison o… two.sided\n```\n:::\n\n\n::: rmdnote\nBetween the [<span class=\"pkg\">broom</span>](https://broom.tidymodels.org/) and [<span class=\"pkg\">broom.mixed</span>](https://CRAN.R-project.org/package=broom.mixed) packages, there are `tidy()` methods for more than 150 models.\n:::\n\nWhile the Poisson distribution is reasonable, we might also want to assess using fewer distributional assumptions. Two methods that might be helpful are the bootstrap and permutation tests [@davison1997bootstrap].\n\nThe <span class=\"pkg\">infer</span> package, part of the tidymodels framework, is a powerful and intuitive tool for hypothesis testing [@ModernDive]. Its syntax is concise and designed for nonstatisticians.\n\nFirst, we `specify()` that we will use the difference in the mean number of articles between the sexes and then `calculate()` the statistic from the data. Recall that the maximum likelihood estimator for the Poisson mean is the sample mean. The hypotheses tested here are the same as the previous test (but are conducted using a different testing procedure).\n\nWith <span class=\"pkg\">infer</span>, we specify the outcome and covariate, then state the statistic of interest:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(infer)\n\nobserved <- \n  bioChemists %>%\n  specify(art ~ fem) %>%\n  calculate(stat = \"diff in means\", order = c(\"Men\", \"Women\"))\nobserved\n## Response: art (numeric)\n## Explanatory: fem (factor)\n## # A tibble: 1 × 1\n##    stat\n##   <dbl>\n## 1 0.412\n```\n:::\n\n\nFrom here, we compute a confidence interval for this mean by creating the bootstrap distribution via `generate()`; the same statistic is computed for each resampled version of the data:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(2101)\nbootstrapped <- \n  bioChemists %>%\n  specify(art ~ fem)  %>%\n  generate(reps = 2000, type = \"bootstrap\") %>%\n  calculate(stat = \"diff in means\", order = c(\"Men\", \"Women\"))\nbootstrapped\n## Response: art (numeric)\n## Explanatory: fem (factor)\n## # A tibble: 2,000 × 2\n##   replicate  stat\n##       <int> <dbl>\n## 1         1 0.467\n## 2         2 0.107\n## 3         3 0.467\n## 4         4 0.308\n## 5         5 0.369\n## 6         6 0.428\n## # ℹ 1,994 more rows\n```\n:::\n\n\nA percentile interval is calculated using:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npercentile_ci <- get_ci(bootstrapped)\npercentile_ci\n## # A tibble: 1 × 2\n##   lower_ci upper_ci\n##      <dbl>    <dbl>\n## 1    0.158    0.653\n```\n:::\n\n\nThe <span class=\"pkg\">infer</span> package has a high-level API for showing the analysis results, as shown in Figure \\@ref(fig:bootstrapped-mean).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvisualize(bootstrapped) +\n    shade_confidence_interval(endpoints = percentile_ci)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![The bootstrap distribution of the difference in means. The highlighted region is the confidence interval.](figures/bootstrapped-mean-1.png){fig-align='center' fig-alt='The bootstrap distribution of the difference in means. The highlighted region is the confidence interval, which does not include a value of zero.' width=80%}\n:::\n:::\n\n\nSince the interval visualized in in Figure \\@ref(fig:bootstrapped-mean) does not include zero, these results indicate that men have published more articles than women.\n\nIf we require a p-value, the <span class=\"pkg\">infer</span> package can compute the value via a permutation test, shown in the following code. The syntax is very similar to the bootstrapping code we used earlier. We add a `hypothesize()` verb to state the type of assumption to test and the `generate()` call contains an option to shuffle the data.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(2102)\npermuted <- \n  bioChemists %>%\n  specify(art ~ fem)  %>%\n  hypothesize(null = \"independence\") %>%\n  generate(reps = 2000, type = \"permute\") %>%\n  calculate(stat = \"diff in means\", order = c(\"Men\", \"Women\"))\npermuted\n## Response: art (numeric)\n## Explanatory: fem (factor)\n## Null Hypothesis: independence\n## # A tibble: 2,000 × 2\n##   replicate     stat\n##       <int>    <dbl>\n## 1         1  0.201  \n## 2         2 -0.133  \n## 3         3  0.109  \n## 4         4 -0.195  \n## 5         5 -0.00128\n## 6         6 -0.102  \n## # ℹ 1,994 more rows\n```\n:::\n\n\nThe following visualization code is also very similar to the bootstrap approach. This code generates Figure \\@ref(fig:permutation-dist) where the vertical line signifies the observed value:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvisualize(permuted) +\n    shade_p_value(obs_stat = observed, direction = \"two-sided\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Empirical distribution of the test statistic under the null hypothesis. The vertical line indicates the observed test statistic.](figures/permutation-dist-1.png){fig-align='center' fig-alt='The empirical distribution of the test statistic under the null hypothesis. The vertical line indicates the observed test statistic and is far away form the mainstream of the distribution.' width=80%}\n:::\n:::\n\n\nThe actual p-value is:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npermuted %>%\n  get_p_value(obs_stat = observed, direction = \"two-sided\")\n## # A tibble: 1 × 1\n##   p_value\n##     <dbl>\n## 1   0.002\n```\n:::\n\n\nThe vertical line representing the null hypothesis in Figure \\@ref(fig:permutation-dist) is far away from the permutation distribution. This means, if in fact the null hypothesis were true, the likelihood is exceedingly small of observing data at least as extreme as what is at hand.\n\nThe two-sample tests shown in this section are probably suboptimal because they do not account for other factors that might explain the observed relationship between publication rate and sex. Let's move to a more complex model that can consider additional covariates.\n\n## Log-Linear Models\n\nThe focus of the rest of this chapter will be on a generalized linear model [@Dobson99] where we assume the counts follow a Poisson distribution. For this model, the covariates/predictors enter the model in a log-linear fashion:\n\n$$\n\\log(\\lambda) = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_px_p\n$$\n\nwhere $\\lambda$ is the expected value of the counts.\n\nLet's fit a simple model that contains all of the predictor columns. The <span class=\"pkg\">poissonreg</span> package, a <span class=\"pkg\">parsnip</span> extension package in tidymodels, will fit this model specification:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(poissonreg)\n\n# default engine is 'glm'\nlog_lin_spec <- poisson_reg()\n\nlog_lin_fit <- \n  log_lin_spec %>% \n  fit(art ~ ., data = bioChemists)\nlog_lin_fit\n## parsnip model object\n## \n## \n## Call:  stats::glm(formula = art ~ ., family = stats::poisson, data = data)\n## \n## Coefficients:\n## (Intercept)     femWomen   marMarried         kid5          phd         ment  \n##      0.3046      -0.2246       0.1552      -0.1849       0.0128       0.0255  \n## \n## Degrees of Freedom: 914 Total (i.e. Null);  909 Residual\n## Null Deviance:\t    1820 \n## Residual Deviance: 1630 \tAIC: 3310\n```\n:::\n\n\nThe `tidy()` method succinctly summarizes the coefficients for the model (along with 90% confidence intervals):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy(log_lin_fit, conf.int = TRUE, conf.level = 0.90)\n## # A tibble: 6 × 7\n##   term        estimate std.error statistic  p.value conf.low conf.high\n##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n## 1 (Intercept)   0.305    0.103       2.96  3.10e- 3   0.134     0.473 \n## 2 femWomen     -0.225    0.0546     -4.11  3.92e- 5  -0.315    -0.135 \n## 3 marMarried    0.155    0.0614      2.53  1.14e- 2   0.0545    0.256 \n## 4 kid5         -0.185    0.0401     -4.61  4.08e- 6  -0.251    -0.119 \n## 5 phd           0.0128   0.0264      0.486 6.27e- 1  -0.0305    0.0563\n## 6 ment          0.0255   0.00201    12.7   3.89e-37   0.0222    0.0288\n```\n:::\n\n\nIn this output, the p-values correspond to separate hypothesis tests for each parameter:\n\n\n```{=tex}\n\\begin{align}\nH_0&: \\beta_j = 0 \\notag \\\\\nH_a&: \\beta_j \\ne 0 \\notag\n\\end{align}\n```\n\nfor each of the model parameters. Looking at these results, `phd` (the prestige of their department) may not have any relationship with the outcome.\n\nWhile the Poisson distribution is the routine assumption for data like these, it may be beneficial to conduct a rough check of the model assumptions by fitting the models without using the Poisson likelihood to calculate the confidence intervals. The <span class=\"pkg\">rsample</span> package has a convenience function to compute bootstrap confidence intervals for `lm()` and `glm()` models. We can use this function, while explicitly declaring `family = poisson`, to compute a large number of model fits. By default, we compute a 90% confidence bootstrap-t interval (percentile intervals are also available):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(2103)\nglm_boot <- \n  reg_intervals(art ~ ., data = bioChemists, model_fn = \"glm\", family = poisson)\nglm_boot\n## # A tibble: 5 × 6\n##   term          .lower .estimate  .upper .alpha .method  \n##   <chr>          <dbl>     <dbl>   <dbl>  <dbl> <chr>    \n## 1 femWomen   -0.358      -0.226  -0.0856   0.05 student-t\n## 2 kid5       -0.298      -0.184  -0.0789   0.05 student-t\n## 3 marMarried  0.000264    0.155   0.317    0.05 student-t\n## 4 ment        0.0182      0.0256  0.0322   0.05 student-t\n## 5 phd        -0.0707      0.0130  0.102    0.05 student-t\n```\n:::\n\n\n::: rmdwarning\nWhen we compare these results (in Figure \\@ref(fig:glm-intervals)) to the purely parametric results from `glm()`, the bootstrap intervals are somewhat wider. If the data were truly Poisson, these intervals would have more similar widths.\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Two types of confidence intervals for the Poisson regression model](figures/glm-intervals-1.png){fig-align='center' fig-alt='Two types of confidence intervals for the Poisson regression model. the interval for the PhD model is the only interval overlapping zero. The parametric intervals tend to be wider than the bootstrap intervals. ' width=672}\n:::\n:::\n\n\nDetermining which predictors to include in the model is a difficult problem. One approach is to conduct likelihood ratio tests (LRT) [@McCullaghNelder89] between nested models. Based on the confidence intervals, we have evidence that a simpler model without `phd` may be sufficient. Let's fit a smaller model, then conduct a statistical test:\n\n\n```{=tex}\n\\begin{align}\nH_0&: \\beta_{phd} = 0 \\notag \\\\\nH_a&: \\beta_{phd} \\ne 0 \\notag\n\\end{align}\n```\n\nThis hypothesis was previously tested when we showed the tidied results for `log_lin_fit`. That particular approach used results from a single model fit via a Wald statistic (i.e., the parameter divided by its standard error). For that approach, the p-value was 0.63. We can tidy the results for the LRT to get the p-value:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlog_lin_reduced <- \n  log_lin_spec %>% \n  fit(art ~ ment + kid5 + fem + mar, data = bioChemists)\n\nanova(\n  extract_fit_engine(log_lin_reduced),\n  extract_fit_engine(log_lin_fit),\n  test = \"LRT\"\n) %>%\n  tidy()\n## # A tibble: 2 × 6\n##   term                          df.residual residual.deviance    df deviance p.value\n##   <chr>                               <dbl>             <dbl> <dbl>    <dbl>   <dbl>\n## 1 art ~ ment + kid5 + fem + mar         910             1635.    NA   NA      NA    \n## 2 art ~ fem + mar + kid5 + phd…         909             1634.     1    0.236   0.627\n```\n:::\n\n\nThe results are the same and, based on these and the confidence interval for this parameter, we'll exclude `phd` from further analyses since it does not appear to be associated with the outcome.\n\n## A More Complex Model\n\nWe can move into even more complex models within our tidymodels approach. For count data, there are occasions where the number of zero counts is larger than what a simple Poisson distribution would prescribe. A more complex model appropriate for this situation is the zero-inflated Poisson (ZIP) model; see @Mullahy, @Lambert1992, and @JSSv027i08. Here, there are two sets of covariates: one for the count data and others that affect the probability (denoted as $\\pi$) of zeros. The equation for the mean $\\lambda$ is:\n\n$$\\lambda = 0 \\pi + (1 - \\pi) \\lambda_{nz}$$\n\nwhere\n\n\n```{=tex}\n\\begin{align}\n\\log(\\lambda_{nz}) &= \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_px_p \\notag \\\\\n\\log\\left(\\frac{\\pi}{1-\\pi}\\right) &= \\gamma_0 + \\gamma_1z_1 + \\ldots + \\gamma_qz_q \\notag \n\\end{align}\n```\n\nand the $x$ covariates affect the count values while the $z$ covariates influence the probability of a zero. The two sets of predictors do not need to be mutually exclusive.\n\nWe'll fit a model with a full set of $z$ covariates:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nzero_inflated_spec <- poisson_reg() %>% set_engine(\"zeroinfl\")\n\nzero_inflated_fit <- \n  zero_inflated_spec %>% \n  fit(art ~ fem + mar + kid5 + ment | fem + mar + kid5 + phd + ment,\n      data = bioChemists)\n\nzero_inflated_fit\n## parsnip model object\n## \n## \n## Call:\n## pscl::zeroinfl(formula = art ~ fem + mar + kid5 + ment | fem + mar + kid5 + \n##     phd + ment, data = data)\n## \n## Count model coefficients (poisson with log link):\n## (Intercept)     femWomen   marMarried         kid5         ment  \n##       0.621       -0.209        0.105       -0.143        0.018  \n## \n## Zero-inflation model coefficients (binomial with logit link):\n## (Intercept)     femWomen   marMarried         kid5          phd         ment  \n##     -0.6086       0.1093      -0.3529       0.2195       0.0124      -0.1351\n```\n:::\n\n\nSince the coefficients for this model are also estimated using maximum likelihood, let's try to use another likelihood ratio test to understand if the new model terms are helpful. We will *simultaneously* test that:\n\n\n```{=tex}\n\\begin{align}\nH_0&: \\gamma_1 = 0, \\gamma_2 = 0, \\cdots, \\gamma_5 = 0 \\notag \\\\\nH_a&: \\text{at least one } \\gamma \\ne 0  \\notag\n\\end{align}\n```\n\nLet's try ANOVA again:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nanova(\n  extract_fit_engine(zero_inflated_fit),\n  extract_fit_engine(log_lin_reduced),\n  test = \"LRT\"\n) %>%\n  tidy()\n## Error in UseMethod(\"anova\"): no applicable method for 'anova' applied to an object of class \"zeroinfl\"\n```\n:::\n\n\nAn `anova()` method isn't implemented for `zeroinfl` objects!\n\nAn alternative is to use an *information criterion statistic*, such as the Akaike information criterion (AIC) [@claeskens2016statistical]. This computes the log-likelihood (from the training set) and penalizes that value based on the training set size and the number of model parameters. In R's parameterization, smaller AIC values are better. In this case, we are not conducting a formal statistical test but *estimating* the ability of the data to fit the data.\n\nThe results indicate that the ZIP model is preferable:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nzero_inflated_fit %>% extract_fit_engine() %>% AIC()\n## [1] 3232\nlog_lin_reduced   %>% extract_fit_engine() %>% AIC()\n## [1] 3312\n```\n:::\n\n\nHowever, it's hard to contextualize this pair of single values and assess *how* different they actually are. To solve this problem, we'll resample a large number of each of these two models. From these, we can compute the AIC values for each and determine how often the results favor the ZIP model. Basically, we will be characterizing the uncertainty of the AIC statistics to gauge their difference relative to the noise in the data.\n\nWe'll also compute more bootstrap confidence intervals for the parameters in a bit so we specify the `apparent = TRUE` option when creating the bootstrap samples. This is required for some types of intervals.\n\nFirst, we create the 4,000 model fits:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nzip_form <- art ~ fem + mar + kid5 + ment | fem + mar + kid5 + phd + ment\nglm_form <- art ~ fem + mar + kid5 + ment\n\nset.seed(2104)\nbootstrap_models <-\n  bootstraps(bioChemists, times = 2000, apparent = TRUE) %>%\n  mutate(\n    glm = map(splits, ~ fit(log_lin_spec,       glm_form, data = analysis(.x))),\n    zip = map(splits, ~ fit(zero_inflated_spec, zip_form, data = analysis(.x)))\n  )\nbootstrap_models\n## # Bootstrap sampling with apparent sample \n## # A tibble: 2,001 × 4\n##   splits            id            glm      zip     \n##   <list>            <chr>         <list>   <list>  \n## 1 <split [915/355]> Bootstrap0001 <fit[+]> <fit[+]>\n## 2 <split [915/333]> Bootstrap0002 <fit[+]> <fit[+]>\n## 3 <split [915/337]> Bootstrap0003 <fit[+]> <fit[+]>\n## 4 <split [915/344]> Bootstrap0004 <fit[+]> <fit[+]>\n## 5 <split [915/351]> Bootstrap0005 <fit[+]> <fit[+]>\n## 6 <split [915/354]> Bootstrap0006 <fit[+]> <fit[+]>\n## # ℹ 1,995 more rows\n```\n:::\n\n\nNow we can extract the model fits and their corresponding AIC values:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbootstrap_models <-\n  bootstrap_models %>%\n  mutate(\n    glm_aic = map_dbl(glm, ~ extract_fit_engine(.x) %>% AIC()),\n    zip_aic = map_dbl(zip, ~ extract_fit_engine(.x) %>% AIC())\n  )\nmean(bootstrap_models$zip_aic < bootstrap_models$glm_aic)\n## [1] 1\n```\n:::\n\n\nIt seems definitive from these results that accounting for the excessive number of zero counts is a good idea.\n\n::: rmdnote\nWe could have used `fit_resamples()` or a workflow set to conduct these computations. In this section, we used `mutate()` and `map()` to compute the models to demonstrate how one might use tidymodels tools for models that are not supported by one of the <span class=\"pkg\">parsnip</span> packages.\n:::\n\nSince we have computed the resampled model fits, let's create bootstrap intervals for the zero probability model coefficients (i.e., the $\\gamma_j$). We can extract these with the `tidy()` method and use the `type = \"zero\"` option to obtain these estimates:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbootstrap_models <-\n  bootstrap_models %>%\n  mutate(zero_coefs  = map(zip, ~ tidy(.x, type = \"zero\")))\n\n# One example:\nbootstrap_models$zero_coefs[[1]]\n## # A tibble: 6 × 6\n##   term        type  estimate std.error statistic   p.value\n##   <chr>       <chr>    <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept) zero   -0.128     0.497     -0.257 0.797    \n## 2 femWomen    zero   -0.0764    0.319     -0.240 0.811    \n## 3 marMarried  zero   -0.112     0.365     -0.307 0.759    \n## 4 kid5        zero    0.270     0.186      1.45  0.147    \n## 5 phd         zero   -0.178     0.132     -1.35  0.177    \n## 6 ment        zero   -0.123     0.0315    -3.91  0.0000935\n```\n:::\n\n\nIt's a good idea to visualize the bootstrap distributions of the coefficients, as in Figure \\@ref(fig:zip-bootstrap).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbootstrap_models %>% \n  unnest(zero_coefs) %>% \n  ggplot(aes(x = estimate)) +\n  geom_histogram(bins = 25, color = \"white\") + \n  facet_wrap(~ term, scales = \"free_x\") + \n  geom_vline(xintercept = 0, lty = 2, color = \"gray70\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Bootstrap distributions of the ZIP model coefficients. The vertical lines indicate the observed estimates. ](figures/zip-bootstrap-1.png){fig-align='center' fig-alt='Bootstrap distributions of the ZIP model coefficients. The vertical lines indicate the observed estimates. The ment predictor that appears to be important to the model.' width=672}\n:::\n:::\n\n\nOne of the covariates (`ment`) that appears to be important has a very skewed distribution. The extra space in some of the facets indicates there are some outliers in the estimates. This *might* occur when models did not converge; those results probably should be excluded from the resamples. For the results visualized in Figure \\@ref(fig:zip-bootstrap), the outliers are due only to extreme parameter estimates; all of the models converged.\n\nThe <span class=\"pkg\">rsample</span> package contains a set of functions named `int_*()` that compute different types of bootstrap intervals. Since the `tidy()` method contains standard error estimates, the bootstrap-t intervals can be computed. We'll also compute the standard percentile intervals. By default, 90% confidence intervals are computed.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbootstrap_models %>% int_pctl(zero_coefs)\n## # A tibble: 6 × 6\n##   term        .lower .estimate  .upper .alpha .method   \n##   <chr>        <dbl>     <dbl>   <dbl>  <dbl> <chr>     \n## 1 (Intercept) -1.75    -0.621   0.423    0.05 percentile\n## 2 femWomen    -0.521    0.115   0.818    0.05 percentile\n## 3 kid5        -0.327    0.218   0.677    0.05 percentile\n## 4 marMarried  -1.20    -0.381   0.362    0.05 percentile\n## 5 ment        -0.401   -0.162  -0.0513   0.05 percentile\n## 6 phd         -0.276    0.0220  0.327    0.05 percentile\nbootstrap_models %>% int_t(zero_coefs)\n## # A tibble: 6 × 6\n##   term        .lower .estimate  .upper .alpha .method  \n##   <chr>        <dbl>     <dbl>   <dbl>  <dbl> <chr>    \n## 1 (Intercept) -1.61    -0.621   0.321    0.05 student-t\n## 2 femWomen    -0.482    0.115   0.671    0.05 student-t\n## 3 kid5        -0.211    0.218   0.599    0.05 student-t\n## 4 marMarried  -0.988   -0.381   0.290    0.05 student-t\n## 5 ment        -0.324   -0.162  -0.0275   0.05 student-t\n## 6 phd         -0.274    0.0220  0.291    0.05 student-t\n```\n:::\n\n\nFrom these results, we can get a good idea of which predictor(s) to include in the zero count probability model. It may be sensible to refit a smaller model to assess if the bootstrap distribution for `ment` is still skewed.\n\n## More Inferential Analysis {#inference-options}\n\nThis chapter demonstrated just a small subset of what is available for inferential analysis in tidymodels and has focused on resampling and frequentist methods. Arguably, Bayesian analysis is a very effective and often superior approach for inference. A variety of Bayesian models are available via <span class=\"pkg\">parsnip</span>. Additionally, the <span class=\"pkg\">multilevelmod</span> package enables users to fit hierarchical Bayesian and non-Bayesian models (e.g., mixed models). The <span class=\"pkg\">broom.mixed</span> and <span class=\"pkg\">tidybayes</span> packages are excellent tools for extracting data for plots and summaries. Finally, for data sets with a single hierarchy, such as simple longitudinal or repeated measures data, <span class=\"pkg\">rsample</span>'s `group_vfold_cv()` function facilitates straightforward out-of-sample characterizations of model performance.\n\n## Chapter Summary {#inference-summary}\n\nThe tidymodels framework is for more than predictive modeling alone. Packages and functions from tidymodels can be used for hypothesis testing, as well as fitting and assessing inferential models. The tidymodels framework provides support for working with non-tidymodels R models, and can help assess the statistical qualities of your models.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}