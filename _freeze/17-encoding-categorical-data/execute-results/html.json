{
  "hash": "b077e3f70c9402768529110ffc209f18",
  "result": {
    "markdown": "\n\n\n# Encoding Categorical Data {#sec-categorical}\n\nFor statistical modeling in R, the preferred representation for categorical or nominal data is a *factor*, which is a variable that can take on a limited number of different values; internally, factors are stored as a vector of integer values together with a set of text labels.[^17-encoding-categorical-data-1] In Section \\@ref(dummies) we introduced feature engineering approaches to encode or transform qualitative or nominal data into a representation better suited for most model algorithms. We discussed how to transform a categorical variable, such as the `Bldg_Type` in our Ames housing data (with levels `OneFam`, `TwoFmCon`, `Duplex`, `Twnhs`, and `TwnhsE`), to a set of dummy or indicator variables like those shown in Table \\@ref(tab:encoding-dummies).\n\n[^17-encoding-categorical-data-1]: This is in contrast to statistical modeling in Python, where categorical variables are often directly represented by integers alone, such as `0, 1, 2` representing red, blue, and green.\n\n\n::: {.cell layout-align=\"center\" hash='17-encoding-categorical-data_cache/html/encoding-dummies_6fb9ecac20d262b57ee6a082470158cd'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption>Dummy or indicator variable encodings for the building type predictor in the Ames training set.</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Raw Data </th>\n   <th style=\"text-align:right;\"> TwoFmCon </th>\n   <th style=\"text-align:right;\"> Duplex </th>\n   <th style=\"text-align:right;\"> Twnhs </th>\n   <th style=\"text-align:right;\"> TwnhsE </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> OneFam </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> TwoFmCon </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Duplex </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Twnhs </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> TwnhsE </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nMany model implementations require such a transformation to a numeric representation for categorical data.\n\n::: rmdnote\nAppendix \\@ref(pre-proc-table) presents a table of recommended preprocessing techniques for different models; notice how many of the models in the table require a numeric encoding for all predictors.\n:::\n\nHowever, for some realistic data sets, straightforward dummy variables are not a good fit. This often happens because there are *too many* categories or there are *new* categories at prediction time. In this chapter, we discuss more sophisticated options for encoding categorical predictors that address these issues. These options are available as tidymodels recipe steps in the [<span class=\"pkg\">embed</span>](https://embed.tidymodels.org/) and [<span class=\"pkg\">textrecipes</span>](https://textrecipes.tidymodels.org/) packages.\n\n## Is an Encoding Necessary?\n\nA minority of models, such as those based on trees or rules, can handle categorical data natively and do not require encoding or transformation of these kinds of features. A tree-based model can natively partition a variable like `Bldg_Type` into groups of factor levels, perhaps `OneFam` alone in one group and `Duplex` and `Twnhs` together in another group. Naive Bayes models are another example where the structure of the model can deal with categorical variables natively; distributions are computed within each level, for example, for all the different kinds of `Bldg_Type` in the data set.\n\nThese models that can handle categorical features natively can *also* deal with numeric, continuous features, making the transformation or encoding of such variables optional. Does this help in some way, perhaps with model performance or time to train models? Typically no, as Section 5.7 of @fes shows using benchmark data sets with untransformed factor variables compared with transformed dummy variables for those same features. In short, using dummy encodings did not typically result in better model performance but often required more time to train the models.\n\n::: rmdnote\nWe advise starting with untransformed categorical variables when a model allows it; note that more complex encodings often do not result in better performance for such models.\n:::\n\n## Encoding Ordinal Predictors\n\nSometimes qualitative columns can be *ordered*, such as \"low,\" \"medium,\" and \"high\". In base R, the default encoding strategy is to make new numeric columns that are polynomial expansions of the data. For columns that have five ordinal values, like the example shown in Table \\@ref(tab:encoding-ordered-table), the factor column is replaced with columns for linear, quadratic, cubic, and quartic terms:\n\n\n::: {.cell layout-align=\"center\" hash='17-encoding-categorical-data_cache/html/encoding-ordered-table_6b0045a367f7ceac4e8ea416a14b05ef'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption>Polynominal expansions for encoding an ordered variable.</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Raw Data </th>\n   <th style=\"text-align:right;\"> Linear </th>\n   <th style=\"text-align:right;\"> Quadratic </th>\n   <th style=\"text-align:right;\"> Cubic </th>\n   <th style=\"text-align:right;\"> Quartic </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> none </td>\n   <td style=\"text-align:right;\"> -0.63 </td>\n   <td style=\"text-align:right;\"> 0.53 </td>\n   <td style=\"text-align:right;\"> -0.32 </td>\n   <td style=\"text-align:right;\"> 0.12 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> a little </td>\n   <td style=\"text-align:right;\"> -0.32 </td>\n   <td style=\"text-align:right;\"> -0.27 </td>\n   <td style=\"text-align:right;\"> 0.63 </td>\n   <td style=\"text-align:right;\"> -0.48 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> some </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n   <td style=\"text-align:right;\"> -0.53 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n   <td style=\"text-align:right;\"> 0.72 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> a bunch </td>\n   <td style=\"text-align:right;\"> 0.32 </td>\n   <td style=\"text-align:right;\"> -0.27 </td>\n   <td style=\"text-align:right;\"> -0.63 </td>\n   <td style=\"text-align:right;\"> -0.48 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> copious amounts </td>\n   <td style=\"text-align:right;\"> 0.63 </td>\n   <td style=\"text-align:right;\"> 0.53 </td>\n   <td style=\"text-align:right;\"> 0.32 </td>\n   <td style=\"text-align:right;\"> 0.12 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nWhile this is not unreasonable, it is not an approach that people tend to find useful. For example, an 11-degree polynomial is probably not the most effective way of encoding an ordinal factor for the months of the year. Instead, consider trying recipe steps related to ordered factors, such as `step_unorder()`, to convert to regular factors, and `step_ordinalscore()`, which maps specific numeric values to each factor level.\n\n## Using the Outcome for Encoding Predictors\n\nThere are multiple options for encodings more complex than dummy or indicator variables. One method called *effect* or *likelihood encodings* replaces the original categorical variables with a single numeric column that measures the effect of those data [@MicciBarreca2001; @Zumel2019]. For example, for the neighborhood predictor in the Ames housing data, we can compute the mean or median sale price for each neighborhood (as shown in Figure \\@ref(fig:encoding-mean-price)) and substitute these means for the original data values:\n\n\n::: {.cell layout-align=\"center\" hash='17-encoding-categorical-data_cache/html/encoding-mean-price_6756ba0d93af1592720176b70752fd4d'}\n\n```{.r .cell-code}\names_train %>%\n  group_by(Neighborhood) %>%\n  summarize(mean = mean(Sale_Price),\n            std_err = sd(Sale_Price) / sqrt(length(Sale_Price))) %>% \n  ggplot(aes(y = reorder(Neighborhood, mean), x = mean)) + \n  geom_point() +\n  geom_errorbar(aes(xmin = mean - 1.64 * std_err, xmax = mean + 1.64 * std_err)) +\n  labs(y = NULL, x = \"Price (mean, log scale)\")\n```\n\n::: {.cell-output-display}\n![Mean home price for neighborhoods in the Ames training set, which can be used as an effect encoding for this categorical variable](17-encoding-categorical-data_files/figure-html/encoding-mean-price-1.png){fig-align='center' fig-alt='A chart with points and error bars for the mean home price for neighborhoods in the Ames training set. The most expensive neighborhoods are Northridge and Stone Brook, while the least expensive are Iowa DOT and Railroad and Meadow Village.' width=672}\n:::\n:::\n\n\nThis kind of effect encoding works well when your categorical variable has many levels. In tidymodels, the <span class=\"pkg\">embed</span> package includes several recipe step functions for different kinds of effect encodings, such as `step_lencode_glm()`, `step_lencode_mixed()`, and `step_lencode_bayes()`. These steps use a generalized linear model to estimate the effect of each level in a categorical predictor on the outcome. When using a recipe step like `step_lencode_glm()`, specify the variable being encoded first and then the outcome using `vars()`:\n\n\n::: {.cell layout-align=\"center\" hash='17-encoding-categorical-data_cache/html/unnamed-chunk-1_6a7a06f8d0708c93cf5644f4fae94295'}\n\n```{.r .cell-code}\nlibrary(embed)\n\names_glm <- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %>%\n  step_log(Gr_Liv_Area, base = 10) %>% \n  step_lencode_glm(Neighborhood, outcome = vars(Sale_Price)) %>%\n  step_dummy(all_nominal_predictors()) %>% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %>% \n  step_ns(Latitude, Longitude, deg_free = 20)\n\names_glm\n## \n## ── Recipe ───────────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:   1\n## predictor: 6\n## \n## ── Operations\n## • Log transformation on: Gr_Liv_Area\n## • Linear embedding for factors via GLM for: Neighborhood\n## • Dummy variables from: all_nominal_predictors()\n## • Interactions with: Gr_Liv_Area:starts_with(\"Bldg_Type_\")\n## • Natural splines on: Latitude, Longitude\n```\n:::\n\n\nAs detailed in Section \\@ref(recipe-functions), we can `prep()` our recipe to fit or estimate parameters for the preprocessing transformations using training data. We can then `tidy()` this prepared recipe to see the results:\n\n\n::: {.cell layout-align=\"center\" hash='17-encoding-categorical-data_cache/html/unnamed-chunk-2_fee6614a76947d8a5db21bf555d4336b'}\n\n```{.r .cell-code}\nglm_estimates <-\n  prep(ames_glm) %>%\n  tidy(number = 2)\n\nglm_estimates\n## # A tibble: 29 × 4\n##   level              value terms        id               \n##   <chr>              <dbl> <chr>        <chr>            \n## 1 North_Ames          5.15 Neighborhood lencode_glm_ZsXdy\n## 2 College_Creek       5.29 Neighborhood lencode_glm_ZsXdy\n## 3 Old_Town            5.07 Neighborhood lencode_glm_ZsXdy\n## 4 Edwards             5.09 Neighborhood lencode_glm_ZsXdy\n## 5 Somerset            5.35 Neighborhood lencode_glm_ZsXdy\n## 6 Northridge_Heights  5.49 Neighborhood lencode_glm_ZsXdy\n## # ℹ 23 more rows\n```\n:::\n\n\nWhen we use the newly encoded `Neighborhood` numeric variable created via this method, we substitute the original level (such as `\"North_Ames\"`) with the estimate for `Sale_Price` from the GLM.\n\nEffect encoding methods like this one can also seamlessly handle situations where a novel factor level is encountered in the data. This `value` is the predicted price from the GLM when we don't have any specific neighborhood information:\n\n\n::: {.cell layout-align=\"center\" hash='17-encoding-categorical-data_cache/html/unnamed-chunk-3_584b2ee9105b773d4d71cb53872b5277'}\n\n```{.r .cell-code}\nglm_estimates %>%\n  filter(level == \"..new\")\n## # A tibble: 1 × 4\n##   level value terms        id               \n##   <chr> <dbl> <chr>        <chr>            \n## 1 ..new  5.23 Neighborhood lencode_glm_ZsXdy\n```\n:::\n\n\n::: rmdwarn\nEffect encodings can be powerful but should be used with care. The effects should be computed from the training set, after data splitting. This type of supervised preprocessing should be rigorously resampled to avoid overfitting (see Chapter \\@ref(resampling)).\n:::\n\nWhen you create an effect encoding for your categorical variable, you are effectively layering a mini-model inside your actual model. The possibility of overfitting with effect encodings is a representative example for why feature engineering *must* be considered part of the model process, as described in Chapter \\@ref(workflows), and why feature engineering must be estimated together with model parameters inside resampling.\n\n### Effect encodings with partial pooling\n\nCreating an effect encoding with `step_lencode_glm()` estimates the effect separately for each factor level (in this example, neighborhood). However, some of these neighborhoods have many houses in them, and some have only a few. There is much more uncertainty in our measurement of price for the single training set home in the Landmark neighborhood than the 354 training set homes in North Ames. We can use *partial pooling* to adjust these estimates so that levels with small sample sizes are shrunken toward the overall mean. The effects for each level are modeled all at once using a mixed or hierarchical generalized linear model:\n\n\n::: {.cell layout-align=\"center\" hash='17-encoding-categorical-data_cache/html/unnamed-chunk-4_70cca7d0e8361ddb663ee989918038f0'}\n\n```{.r .cell-code}\names_mixed <- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %>%\n  step_log(Gr_Liv_Area, base = 10) %>% \n  step_lencode_mixed(Neighborhood, outcome = vars(Sale_Price)) %>%\n  step_dummy(all_nominal_predictors()) %>% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %>% \n  step_ns(Latitude, Longitude, deg_free = 20)\n\names_mixed\n## \n## ── Recipe ───────────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:   1\n## predictor: 6\n## \n## ── Operations\n## • Log transformation on: Gr_Liv_Area\n## • Linear embedding for factors via mixed effects for: Neighborhood\n## • Dummy variables from: all_nominal_predictors()\n## • Interactions with: Gr_Liv_Area:starts_with(\"Bldg_Type_\")\n## • Natural splines on: Latitude, Longitude\n```\n:::\n\n\nLet's `prep()` and `tidy()` this recipe to see the results:\n\n\n::: {.cell layout-align=\"center\" hash='17-encoding-categorical-data_cache/html/unnamed-chunk-5_b1d3aac805ca32a18a712d946c069153'}\n\n```{.r .cell-code}\nmixed_estimates <-\n  prep(ames_mixed) %>%\n  tidy(number = 2)\n\nmixed_estimates\n## # A tibble: 29 × 4\n##   level              value terms        id                 \n##   <chr>              <dbl> <chr>        <chr>              \n## 1 North_Ames          5.15 Neighborhood lencode_mixed_SC9hi\n## 2 College_Creek       5.29 Neighborhood lencode_mixed_SC9hi\n## 3 Old_Town            5.07 Neighborhood lencode_mixed_SC9hi\n## 4 Edwards             5.10 Neighborhood lencode_mixed_SC9hi\n## 5 Somerset            5.35 Neighborhood lencode_mixed_SC9hi\n## 6 Northridge_Heights  5.49 Neighborhood lencode_mixed_SC9hi\n## # ℹ 23 more rows\n```\n:::\n\n\nNew levels are then encoded at close to the same value as with the GLM:\n\n\n::: {.cell layout-align=\"center\" hash='17-encoding-categorical-data_cache/html/unnamed-chunk-6_8ba02c7792e287708d6620c7461d3780'}\n\n```{.r .cell-code}\nmixed_estimates %>%\n  filter(level == \"..new\")\n## # A tibble: 1 × 4\n##   level value terms        id                 \n##   <chr> <dbl> <chr>        <chr>              \n## 1 ..new  5.23 Neighborhood lencode_mixed_SC9hi\n```\n:::\n\n\n::: rmdnote\nYou can use a fully Bayesian hierarchical model for the effects in the same way with `step_lencode_bayes()`.\n:::\n\nLet's visually compare the effects using partial pooling vs. no pooling in Figure \\@ref(fig:encoding-compare-pooling):\n\n\n::: {.cell layout-align=\"center\" hash='17-encoding-categorical-data_cache/html/encoding-compare-pooling_3810ffc09d523123de888632d559673c'}\n\n```{.r .cell-code}\nglm_estimates %>%\n  rename(`no pooling` = value) %>%\n  left_join(\n    mixed_estimates %>%\n      rename(`partial pooling` = value), by = \"level\"\n  ) %>%\n  left_join(\n    ames_train %>% \n      count(Neighborhood) %>% \n      mutate(level = as.character(Neighborhood))\n  ) %>%\n  ggplot(aes(`no pooling`, `partial pooling`, size = sqrt(n))) +\n  geom_abline(color = \"gray50\", lty = 2) +\n  geom_point(alpha = 0.7) +\n  coord_fixed()\n## Warning: Removed 1 rows containing missing values (`geom_point()`).\n```\n\n::: {.cell-output-display}\n![Comparing the effect encodings for neighborhood estimated without pooling to those with partial pooling](17-encoding-categorical-data_files/figure-html/encoding-compare-pooling-1.png){fig-align='center' fig-alt='A scatter chart comparing the effect encodings for neighborhood estimated without pooling to those with partial pooling. Almost all neighborhoods are very close to the slope = 1 line, but the neighborhoods with the fewest homes are further away.' width=672}\n:::\n:::\n\n\nNotice in Figure \\@ref(fig:encoding-compare-pooling) that most estimates for neighborhood effects are about the same when we compare pooling to no pooling. However, the neighborhoods with the fewest homes in them have been pulled (either up or down) toward the mean effect. When we use pooling, we shrink the effect estimates toward the mean because we don't have as much evidence about the price in those neighborhoods.\n\n## Feature Hashing\n\nTraditional dummy variables as described in Section \\@ref(dummies) require that all of the possible categories be known to create a full set of numeric features. *Feature hashing* methods [@weinberger2009feature] also create dummy variables, but only consider the value of the category to assign it to a predefined pool of dummy variables. Let's look at the `Neighborhood` values in Ames again and use the `rlang::hash()` function to understand more:\n\n\n::: {.cell layout-align=\"center\" hash='17-encoding-categorical-data_cache/html/unnamed-chunk-7_72f829f9509b278cb007a625079e0c01'}\n\n```{.r .cell-code}\nlibrary(rlang)\n\names_hashed <-\n  ames_train %>%\n  mutate(Hash = map_chr(Neighborhood, hash))\n\names_hashed %>%\n  select(Neighborhood, Hash)\n## # A tibble: 2,342 × 2\n##   Neighborhood    Hash                            \n##   <fct>           <chr>                           \n## 1 North_Ames      076543f71313e522efe157944169d919\n## 2 North_Ames      076543f71313e522efe157944169d919\n## 3 Briardale       b598bec306983e3e68a3118952df8cf0\n## 4 Briardale       b598bec306983e3e68a3118952df8cf0\n## 5 Northpark_Villa 6af95b5db968bf393e78188a81e0e1e4\n## 6 Northpark_Villa 6af95b5db968bf393e78188a81e0e1e4\n## # ℹ 2,336 more rows\n```\n:::\n\n\nIf we input Briardale to this hashing function, we will always get the same output. The neighborhoods in this case are called the \"keys,\" while the outputs are the \"hashes.\"\n\n::: rmdnote\nA hashing function takes an input of variable size and maps it to an output of fixed size. Hashing functions are commonly used in cryptography and databases.\n:::\n\nThe `rlang::hash()` function generates a 128-bit hash, which means there are `2^128` possible hash values. This is great for some applications but doesn't help with feature hashing of *high cardinality* variables (variables with many levels). In feature hashing, the number of possible hashes is a hyperparameter and is set by the model developer through computing the modulo of the integer hashes. We can get sixteen possible hash values by using `Hash %% 16`:\n\n\n::: {.cell layout-align=\"center\" hash='17-encoding-categorical-data_cache/html/unnamed-chunk-8_8f5cca417cb73231aa40945992a0ad6d'}\n\n```{.r .cell-code}\names_hashed %>%\n  ## first make a smaller hash for integers that R can handle\n  mutate(Hash = strtoi(substr(Hash, 26, 32), base = 16L),  \n         ## now take the modulo\n         Hash = Hash %% 16) %>%\n  select(Neighborhood, Hash)\n## # A tibble: 2,342 × 2\n##   Neighborhood     Hash\n##   <fct>           <dbl>\n## 1 North_Ames          9\n## 2 North_Ames          9\n## 3 Briardale           0\n## 4 Briardale           0\n## 5 Northpark_Villa     4\n## 6 Northpark_Villa     4\n## # ℹ 2,336 more rows\n```\n:::\n\n\nNow instead of the 28 neighborhoods in our original data or an incredibly huge number of the original hashes, we have sixteen hash values. This method is very fast and memory efficient, and it can be a good strategy when there are a large number of possible categories.\n\n::: rmdnote\nFeature hashing is useful for text data as well as high cardinality categorical data. See Section 6.7 of @Hvitfeldt2021 for a case study demonstration with text predictors.\n:::\n\nWe can implement feature hashing using a tidymodels recipe step from the <span class=\"pkg\">textrecipes</span> package:\n\n\n::: {.cell layout-align=\"center\" hash='17-encoding-categorical-data_cache/html/unnamed-chunk-9_dcbe8e99e90b0b1971b4c47b10e56282'}\n\n```{.r .cell-code}\nlibrary(textrecipes)\names_hash <- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train) %>%\n  step_log(Gr_Liv_Area, base = 10) %>% \n  step_dummy_hash(Neighborhood, signed = FALSE, num_terms = 16L) %>%\n  step_dummy(all_nominal_predictors()) %>% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %>% \n  step_ns(Latitude, Longitude, deg_free = 20)\n\names_hash\n## \n## ── Recipe ───────────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:   1\n## predictor: 6\n## \n## ── Operations\n## • Log transformation on: Gr_Liv_Area\n## • Feature hashing with: Neighborhood\n## • Dummy variables from: all_nominal_predictors()\n## • Interactions with: Gr_Liv_Area:starts_with(\"Bldg_Type_\")\n## • Natural splines on: Latitude, Longitude\n```\n:::\n\n\nFeature hashing is fast and efficient but has a few downsides. For example, different category values often map to the same hash value. This is called a *collision* or *aliasing*. How often did this happen with our neighborhoods in Ames? Table \\@ref(tab:encoding-hash) presents the distribution of number of neighborhoods per hash value.\n\n\n::: {.cell layout-align=\"center\" hash='17-encoding-categorical-data_cache/html/encoding-hash_9c3e3b09ae5fb1ee7a34d67afde12b70'}\n\n```\n## 'as(<dgTMatrix>, \"dgCMatrix\")' is deprecated.\n## Use 'as(., \"CsparseMatrix\")' instead.\n## See help(\"Deprecated\") and help(\"Matrix-deprecated\").\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n<caption>The number of hash features at each number of neighborhoods.</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> Number of neighborhoods within a hash feature </th>\n   <th style=\"text-align:right;\"> Number of occurrences </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 7 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 4 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 3 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThe number of neighborhoods mapped to each hash value varies between zero and four. All of the hash values greater than one are examples of hash collisions.\n\nWhat are some things to consider when using feature hashing?\n\n-   Feature hashing is not directly interpretable because hash functions cannot be reversed. We can't determine what the input category levels were from the hash value, or if a collision occurred.\n\n-   The number of hash values is a *tuning parameter* of this preprocessing technique, and you should try several values to determine what is best for your particular modeling approach. A lower number of hash values results in more collisions, but a high number may not be an improvement over your original high cardinality variable.\n\n-   Feature hashing can handle new category levels at prediction time, since it does not rely on pre-determined dummy variables.\n\n-   You can reduce hash collisions with a *signed* hash by using `signed = TRUE`. This expands the values from only 1 to either +1 or -1, depending on the sign of the hash.\n\n::: rmdwarn\nIt is likely that some hash columns will contain all zeros, as we see in this example. We recommend a zero-variance filter via `step_zv()` to filter out such columns.\n:::\n\n## More Encoding Options\n\nEven more options are available for transforming factors to a numeric representation.\n\nWe can build a full set of *entity embeddings* [@Guo2016] to transform a categorical variable with many levels to a set of lower-dimensional vectors. This approach is best suited to a nominal variable with many category levels, many more than the example we've used with neighborhoods in Ames.\n\n::: rmdnote\nThe idea of entity embeddings comes from the methods used to create word embeddings from text data. See Chapter 5 of @Hvitfeldt2021 for more on word embeddings.\n:::\n\nEmbeddings for a categorical variable can be learned via a TensorFlow neural network with the `step_embed()` function in <span class=\"pkg\">embed</span>. We can use the outcome alone or optionally the outcome plus a set of additional predictors. Like in feature hashing, the number of new encoding columns to create is a hyperparameter of the feature engineering. We also must make decisions about the neural network structure (the number of hidden units) and how to fit the neural network (how many epochs to train, how much of the data to use for validation in measuring metrics).\n\nYet one more option available for dealing with a binary outcome is to transform a set of category levels based on their association with the binary outcome. This *weight of evidence* (WoE) transformation [@Good1985] uses the logarithm of the \"Bayes factor\" (the ratio of the posterior odds to the prior odds) and creates a dictionary mapping each category level to a WoE value. WoE encodings can be determined with the `step_woe()` function in <span class=\"pkg\">embed</span>.\n\n## Chapter Summary {#sec-categorical-summary}\n\nIn this chapter, you learned about using preprocessing recipes for encoding categorical predictors. The most straightforward option for transforming a categorical variable to a numeric representation is to create dummy variables from the levels, but this option does not work well when you have a variable with high cardinality (too many levels) or when you may see novel values at prediction time (new levels). One option in such a situation is to create *effect encodings*, a supervised encoding method that uses the outcome. Effect encodings can be learned with or without pooling the categories. Another option uses a *hashing* function to map category levels to a new, smaller set of dummy variables. Feature hashing is fast and has a low-memory footprint. Other options include entity embeddings (learned via a neural network) and weight of evidence transformation.\n\nMost model algorithms require some kind of transformation or encoding of this type for categorical variables. A minority of models, including those based on trees and rules, can handle categorical variables natively and do not require such encodings.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}