{
  "hash": "bf772ba1ebc1faf9cbfa9a6d6cbd809a",
  "result": {
    "markdown": "\n\n\n# Juzgar La Eficacia Del Modelo {#sec-performance}\n\nUna vez que tenemos un modelo, necesitamos saber qué tan bien funciona. Un enfoque cuantitativo para estimar la efectividad nos permite comprender el modelo, comparar diferentes modelos o modificar el modelo para mejorar el rendimiento. Nuestro enfoque en tidymodels está en la validación empírica; Esto generalmente significa usar datos que no se usaron para crear el modelo como sustrato para medir la efectividad.\n\n::: rmdwarning\nEl mejor enfoque para la validación empírica implica el uso de métodos de *resampling* que se presentarán en el [Capítulo @sec-resampling]. En este capítulo, motivaremos la necesidad de una validación empírica mediante el uso del conjunto de pruebas. Tenga en cuenta que el conjunto de prueba solo se puede utilizar una vez, como se explica en @sec-splitting-methods.\n:::\n\nAl juzgar la eficacia del modelo, su decisión sobre qué métricas examinar puede ser fundamental. En capítulos posteriores, se optimizarán empíricamente ciertos parámetros del modelo y se utilizará una métrica de rendimiento primaria para elegir el mejor submodelo. Elegir la métrica incorrecta puede fácilmente tener consecuencias no deseadas. Por ejemplo, dos métricas comunes para los modelos de regresión son la raíz del error cuadrático medio (RMSE) y el coeficiente de determinación (también conocido como $R^2$). El primero mide la *precisión* mientras que el segundo mide la *correlación*. Estos no son necesariamente lo mismo. @fig-Performance-reg-metrics demuestra la diferencia entre los dos.\n\n\n::: {.cell layout-align=\"center\" hash='09-judging-model-effectiveness_cache/html/fig-performance-reg-metrics_3e2c2c29d1bf7eb50f22f984b7e3ed9c'}\n::: {.cell-output-display}\n![Valores observados versus valores predichos para modelos optimizados utilizando el RMSE en comparación con el coeficiente de determinación](09-judging-model-effectiveness_files/figure-html/fig-performance-reg-metrics-1.png){#fig-performance-reg-metrics fig-align='center' fig-alt='Gráficos de dispersión de valores numéricos observados versus valores predichos para modelos optimizados utilizando el RMSE y el coeficiente de determinación. El primero produce resultados cercanos a la línea de identidad de 45 grados, mientras que el segundo muestra resultados con una estrecha correlación lineal pero se aleja mucho de la línea de identidad.' width=672}\n:::\n:::\n\n\nUn modelo optimizado para RMSE tiene más variabilidad pero tiene una precisión relativamente uniforme en todo el rango del resultado. El panel derecho muestra que existe una correlación más estrecha entre los valores observados y predichos, pero este modelo funciona mal en las colas.\n\nEste capítulo demostrará el paquete <span class=\"pkg\">yardstick</span>, un paquete central de tidymodels cuyo objetivo es medir el rendimiento del modelo. Antes de ilustrar la sintaxis, exploremos si vale la pena la validación empírica mediante métricas de rendimiento cuando un modelo se centra en la inferencia en lugar de la predicción.\n\n## Métricas De Rendimiento E Inferencia\n\n\n\n\n\nLa eficacia de cualquier modelo depende de cómo se utilizará. Un modelo inferencial se utiliza principalmente para comprender las relaciones y normalmente enfatiza la elección (y validez) de distribuciones probabilísticas y otras cualidades generativas que definen el modelo. Por el contrario, para un modelo utilizado principalmente para la predicción, la fuerza predictiva es de primordial importancia y otras preocupaciones sobre las cualidades estadísticas subyacentes pueden ser menos importantes. La fuerza predictiva generalmente está determinada por qué tan cerca están nuestras predicciones de los datos observados, es decir, la fidelidad de las predicciones del modelo a los resultados reales. Este capítulo se centra en funciones que se pueden utilizar para medir la fuerza predictiva. Sin embargo, nuestro consejo para quienes desarrollan modelos inferenciales es utilizar estas técnicas incluso cuando el modelo no se utilizará con el objetivo principal de predicción.\n\nUn problema de larga data con la práctica de la estadística inferencial es que, centrándose exclusivamente en la inferencia, es difícil evaluar la credibilidad de un modelo. Por ejemplo, considere los datos sobre la enfermedad de Alzheimer de @CraigSchapiro cuando se estudiaron pacientes con 333 para determinar los factores que influyen en el deterioro cognitivo. Un análisis podría tomar los factores de riesgo conocidos y construir un modelo de regresión logística donde el resultado sea binario (deteriorado/no deteriorado). Consideremos los predictores de edad, sexo y genotipo de apolipoproteína E. Esta última es una variable categórica con las seis combinaciones posibles de las tres variantes principales de este gen. Se sabe que la apolipoproteína E tiene una asociación con la demencia [@Kim:2009p4370].\n\nUn enfoque superficial, pero no infrecuente, para este análisis sería ajustar un modelo grande con efectos e interacciones principales y luego utilizar pruebas estadísticas para encontrar el conjunto mínimo de términos del modelo que sean estadísticamente significativos en algún nivel predefinido. Si se utilizara un modelo completo con los tres factores y sus interacciones de dos y tres vías, una fase inicial sería probar las interacciones utilizando pruebas de índice de probabilidad secuencial [@HosmerLemeshow]. Analicemos este tipo de enfoque para el ejemplo de datos sobre la enfermedad de Alzheimer:\n\n-   Al comparar el modelo con todas las interacciones de dos vías con uno con la interacción de tres vías adicional, las pruebas de razón de verosimilitud producen un valor p de 0.888. Esto implica que no hay evidencia de que los términos del modelo adicionales four asociados con la interacción de tres vías expliquen suficiente variación en los datos para mantenerlos en el modelo.\n\n-   A continuación, las interacciones bidireccionales se evalúan de manera similar con respecto al modelo sin interacciones. El valor p aquí es 0.0382. Esto es algo dudoso, pero, dado el pequeño tamaño de la muestra, sería prudente concluir que hay evidencia de que algunas de las posibles interacciones bidireccionales 10 son importantes para la modelo.\n\n-   A partir de aquí, construiríamos alguna explicación de los resultados. Sería particularmente importante discutir las interacciones, ya que pueden generar hipótesis fisiológicas o neurológicas interesantes que se explorarán más a fondo.\n\nSi bien superficial, esta estrategia de análisis es común tanto en la práctica como en la literatura. Esto es especialmente cierto si el profesional tiene una formación formal limitada en análisis de datos.\n\nUn dato que falta en este enfoque es qué tan cerca se ajusta este modelo a los datos reales. Usando métodos de remuestreo, discutidos en el [Capítulo @sec-resampling], podemos estimar que la precisión de este modelo es aproximadamente 73%. La precisión es a menudo una mala medida del rendimiento del modelo; Lo usamos aquí porque se entiende comúnmente. Si el modelo tiene 73% de fidelidad a los datos, ¿deberíamos confiar en las conclusiones que produce? Podríamos pensar así hasta que nos demos cuenta de que la tasa inicial de pacientes no deteriorados en los datos es 72.7%. Esto significa que, a pesar de nuestro análisis estadístico, el modelo de dos factores parece ser sólo 0.3% mejor que una simple heurística que siempre predice que los pacientes no sufrirán daños, independientemente de los datos observados.\n\n::: rmdnote\nEl objetivo de este análisis es demostrar la idea de que la optimización de las características estadísticas del modelo no implica que el modelo se ajuste bien a los datos. Incluso para modelos puramente inferenciales, alguna medida de fidelidad a los datos debería acompañar a los resultados inferenciales. Con esto, los consumidores de los análisis pueden calibrar sus expectativas sobre los resultados.\n:::\n\nEn el resto de este capítulo, discutiremos enfoques generales para evaluar modelos mediante validación empírica. Estos enfoques se agrupan según la naturaleza de los datos de resultados: puramente numéricos, clases binarias y tres o más niveles de clase.\n\n## Métricas De Regresión\n\nRecuerde de @sec-parsnip-predictions que las funciones de predicción de tidymodels producen tibbles con columnas para los valores predichos. Estas columnas tienen nombres consistentes y las funciones en el paquete <span class=\"pkg\">yardstick</span> que producen métricas de rendimiento tienen interfaces consistentes. Las funciones están basadas en marcos de datos, a diferencia de vectores, con la sintaxis general de:\n\n``` r\nfunction(data, truth, ...)\n```\n\ndonde `data` es un marco de datos o tibble y `truth` es la columna con los valores de resultados observados. Las elipses u otros argumentos se utilizan para especificar las columnas que contienen las predicciones.\n\nPara ilustrar, tomemos el modelo de @sec-recipes-summary. Este modelo `lm_wflow_fit` combina un modelo de regresión lineal con un conjunto de predictores complementado con una interacción y funciones spline para longitud y latitud. Fue creado a partir de un conjunto de entrenamiento (llamado `ames_train`). Aunque no recomendamos utilizar el conjunto de pruebas en este punto del proceso de modelado, se utilizará aquí para ilustrar la funcionalidad y la sintaxis. El marco de datos `ames_test` consta de las propiedades 588. Para empezar, hagamos predicciones:\n\n\n::: {.cell layout-align=\"center\" hash='09-judging-model-effectiveness_cache/html/performance-predict-ames_12083a946a51b3078583dea7057543a1'}\n\n```{.r .cell-code}\names_test_res <- predict(lm_fit, new_data = ames_test %>% select(-Sale_Price))\names_test_res\n## # A tibble: 588 × 1\n##   .pred\n##   <dbl>\n## 1  5.07\n## 2  5.31\n## 3  5.28\n## 4  5.33\n## 5  5.30\n## 6  5.24\n## # ℹ 582 more rows\n```\n:::\n\n\nEl resultado numérico previsto por el modelo de regresión se denomina `.pred`. Hagamos coincidir los valores predichos con sus correspondientes valores de resultado observados:\n\n\n::: {.cell layout-align=\"center\" hash='09-judging-model-effectiveness_cache/html/performance-ames-outcome_d218996d2524f6b61b93b56c5186c466'}\n\n```{.r .cell-code}\names_test_res <- bind_cols(ames_test_res, ames_test %>% select(Sale_Price))\names_test_res\n## # A tibble: 588 × 2\n##   .pred Sale_Price\n##   <dbl>      <dbl>\n## 1  5.07       5.02\n## 2  5.31       5.39\n## 3  5.28       5.28\n## 4  5.33       5.28\n## 5  5.30       5.28\n## 6  5.24       5.26\n## # ℹ 582 more rows\n```\n:::\n\n\nVemos que estos valores en su mayoría parecen cercanos, pero aún no tenemos una comprensión cuantitativa de cómo funciona el modelo porque no hemos calculado ninguna métrica de rendimiento. Tenga en cuenta que tanto los resultados previstos como los observados están en unidades log-10. Es una buena práctica analizar las predicciones en la escala transformada (si se usara una), incluso si las predicciones se informan utilizando las unidades originales.\n\nTrazamos los datos en @fig-ames-performance-plot antes de calcular las métricas:\n\n\n::: {.cell layout-align=\"center\" hash='09-judging-model-effectiveness_cache/html/performance-ames-plot_6a1fc9df51c96f989a173faa2f1a1ef3'}\n\n```{.r .cell-code}\nggplot(ames_test_res, aes(x = Sale_Price, y = .pred)) + \n  # Crear una línea diagonal:\n  geom_abline(lty = 2) + \n  geom_point(alpha = 0.5) + \n  labs(y = \"Precio de Venta Predecido (log10)\", x = \"Precio de Venta (log10)\") +\n  # Escale y dimensione los ejes x e y de manera uniforme:\n  coord_obs_pred()\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='09-judging-model-effectiveness_cache/html/fig-ames-performance-plot_c9f38063a9cb53b38e9883f8b0cff727'}\n::: {.cell-output-display}\n![Valores observados versus valores predichos para un modelo de regresión de Ames, con unidades log-10 en ambos ejes](09-judging-model-effectiveness_files/figure-html/fig-ames-performance-plot-1.png){#fig-ames-performance-plot fig-align='center' fig-alt='Gráficos de dispersión de valores numéricos observados versus valores predichos para un modelo de regresión de Ames. Ambos ejes utilizan unidades log-10. El modelo muestra una buena concordancia con algunos puntos que no se ajustan bien a precios altos y bajos.' width=672}\n:::\n:::\n\n\nHay una propiedad de bajo precio que está sustancialmente sobreestimada, es decir, bastante por encima de la línea discontinua.\n\nCalculemos la raíz del error cuadrático medio para este modelo usando la función `rmse()`:\n\n\n::: {.cell layout-align=\"center\" hash='09-judging-model-effectiveness_cache/html/performance-ames-rmse_291018087092374da50156468613fff1'}\n\n```{.r .cell-code}\nrmse(ames_test_res, truth = Sale_Price, estimate = .pred)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard      0.0736\n```\n:::\n\n\nEsto nos muestra el formato estándar de salida de las funciones <span class=\"pkg\">yardstick</span>. Las métricas para resultados numéricos suelen tener un valor \"estándar\" para la columna `.estimator`. En las siguientes secciones se muestran ejemplos con diferentes valores para esta columna.\n\nPara calcular varias métricas a la vez, podemos crear un *conjunto de métricas*. Sumemos $R^2$ y el error absoluto medio:\n\n\n::: {.cell layout-align=\"center\" hash='09-judging-model-effectiveness_cache/html/performance-metric-set_9b75d2f08819ab9697de178c3ea6079b'}\n\n```{.r .cell-code}\names_metrics <- metric_set(rmse, rsq, mae)\names_metrics(ames_test_res, truth = Sale_Price, estimate = .pred)\n## # A tibble: 3 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard      0.0736\n## 2 rsq     standard      0.836 \n## 3 mae     standard      0.0549\n```\n:::\n\n\nEste formato de datos ordenado apila las métricas verticalmente. Las métricas del error cuadrático medio y del error absoluto medio están en la escala del resultado (por lo tanto, `log10(Sale_Price)` para nuestro ejemplo) y miden la diferencia entre los valores previstos y observados. El valor de $R^2$ mide la correlación al cuadrado entre los valores previstos y observados, por lo que los valores más cercanos a uno son mejores.\n\n::: rmdwarning\nEl paquete <span class=\"pkg\">yardstick</span> *no* contiene una función para $R^2$ ajustado. Esta modificación del coeficiente de determinación se utiliza comúnmente cuando los mismos datos utilizados para ajustar el modelo se utilizan para evaluar el modelo. Esta métrica no es totalmente compatible con tidymodels porque siempre es un mejor enfoque para calcular el rendimiento en un conjunto de datos separado que el utilizado para ajustar el modelo.\n:::\n\n## Métricas De Clasificación Binaria\n\nPara ilustrar otras formas de medir el rendimiento del modelo, cambiaremos a un ejemplo diferente. El paquete <span class=\"pkg\">modeldata</span> (otro de los paquetes tidymodels) contiene predicciones de ejemplo de un conjunto de datos de prueba con dos clases (\"Class1\" y \"Class2\"):\n\n\n::: {.cell layout-align=\"center\" hash='09-judging-model-effectiveness_cache/html/performance-two-class-example_9625434ebca4ebc0532b3c0d238de87e'}\n\n```{.r .cell-code}\ndata(two_class_example)\ntibble(two_class_example)\n## # A tibble: 500 × 4\n##   truth   Class1   Class2 predicted\n##   <fct>    <dbl>    <dbl> <fct>    \n## 1 Class2 0.00359 0.996    Class2   \n## 2 Class1 0.679   0.321    Class1   \n## 3 Class2 0.111   0.889    Class2   \n## 4 Class1 0.735   0.265    Class1   \n## 5 Class2 0.0162  0.984    Class2   \n## 6 Class1 0.999   0.000725 Class1   \n## # ℹ 494 more rows\n```\n:::\n\n\nLa segunda y tercera columnas son las probabilidades de clase predichas para el conjunto de prueba, mientras que `predicted` son las predicciones discretas.\n\nPara las predicciones de clases difíciles, una variedad de funciones <span class=\"pkg\">yardstick</span> son útiles:\n\n\n::: {.cell layout-align=\"center\" hash='09-judging-model-effectiveness_cache/html/performance-class-metrics_fc40ecb98152779af052a9b5a20e1cfd'}\n\n```{.r .cell-code}\n# Una matriz de confusión:\nconf_mat(two_class_example, truth = truth, estimate = predicted)\n##           Truth\n## Prediction Class1 Class2\n##     Class1    227     50\n##     Class2     31    192\n\n# Exactitud:\naccuracy(two_class_example, truth, predicted)\n## # A tibble: 1 × 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary         0.838\n\n# Coeficiente de correlación de Matthews:\nmcc(two_class_example, truth, predicted)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 mcc     binary         0.677\n\n# Métrica F1:\nf_meas(two_class_example, truth, predicted)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 f_meas  binary         0.849\n\n# Combinando estas tres métricas de clasificación juntas\nclassification_metrics <- metric_set(accuracy, mcc, f_meas)\nclassification_metrics(two_class_example, truth = truth, estimate = predicted)\n## # A tibble: 3 × 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary         0.838\n## 2 mcc      binary         0.677\n## 3 f_meas   binary         0.849\n```\n:::\n\n\nEl coeficiente de correlación de Matthews y la puntuación F1 resumen la matriz de confusión, pero en comparación con `mcc()`, que mide la calidad de ejemplos tanto positivos como negativos, la métrica `f_meas()` enfatiza la clase positiva, es decir, el evento de interés. Para conjuntos de datos de clasificación binaria como este ejemplo, las funciones <span class=\"pkg\">yardstick</span> tienen un argumento estándar llamado `event_level` para distinguir los niveles positivos y negativos. El valor predeterminado (que utilizamos en este código) es que el *primer* nivel del factor de resultado es el evento de interés.\n\n::: rmdnote\nExiste cierta heterogeneidad en las funciones R a este respecto; algunos utilizan el primer nivel y otros el segundo para denotar el evento de interés. Consideramos más intuitivo que el primer nivel es el más importante. La lógica de segundo nivel surge de codificar el resultado como 0/1 (en cuyo caso el segundo valor es el evento) y desafortunadamente permanece en algunos paquetes. Sin embargo, tidymodels (junto con muchos otros paquetes de R) requieren que se codifique un resultado categórico como factor y, por esta razón, la justificación heredada para el segundo nivel como evento se vuelve irrelevante.\n:::\n\nComo ejemplo donde el segundo nivel es el evento:\n\n\n::: {.cell layout-align=\"center\" hash='09-judging-model-effectiveness_cache/html/performance-2nd-level_17ac2875da08a2bc84a2feed44e03448'}\n\n```{.r .cell-code}\nf_meas(two_class_example, truth, predicted, event_level = \"second\")\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 f_meas  binary         0.826\n```\n:::\n\n\nEn este resultado, el valor `.estimator` de \"binario\" indica que se utilizará la fórmula estándar para clases binarias.\n\nExisten numerosas métricas de clasificación que utilizan las probabilidades predichas como entradas en lugar de las predicciones de clase estrictas. Por ejemplo, la curva de características operativas del receptor (ROC) calcula la sensibilidad y la especificidad sobre un continuo de diferentes umbrales de eventos. La columna de clase prevista no se utiliza. Hay dos funciones <span class=\"pkg\">yardstick</span> para este método: `roc_curve()` calcula los puntos de datos que forman la curva ROC y `roc_auc()` calcula el área bajo la curva.\n\nLas interfaces para estos tipos de funciones métricas utilizan el marcador de posición del argumento `...` para pasar la columna de probabilidad de clase apropiada. Para problemas de dos clases, la columna de probabilidad del evento de interés se pasa a la función:\n\n\n::: {.cell layout-align=\"center\" hash='09-judging-model-effectiveness_cache/html/performance-2class-roc_02229861c0b5d38079609db59fee547f'}\n\n```{.r .cell-code}\ntwo_class_curve <- roc_curve(two_class_example, truth, Class1)\ntwo_class_curve\n## # A tibble: 502 × 3\n##   .threshold specificity sensitivity\n##        <dbl>       <dbl>       <dbl>\n## 1 -Inf           0                 1\n## 2    1.79e-7     0                 1\n## 3    4.50e-6     0.00413           1\n## 4    5.81e-6     0.00826           1\n## 5    5.92e-6     0.0124            1\n## 6    1.22e-5     0.0165            1\n## # ℹ 496 more rows\n\nroc_auc(two_class_example, truth, Class1)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 roc_auc binary         0.939\n```\n:::\n\n\nEl objeto `two_class_curve` se puede usar en una llamada `ggplot` para visualizar la curva, como se muestra en @fig-example-roc-curve. Existe un método `autoplot()` que se encargará de los detalles:\n\n\n::: {.cell layout-align=\"center\" hash='09-judging-model-effectiveness_cache/html/performance-2class-roc-curve_608b94d8f3abed5fdafa571ea700905e'}\n\n```{.r .cell-code}\nautoplot(two_class_curve)\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='09-judging-model-effectiveness_cache/html/fig-example-roc-curve_5a95360c4fa44a318532b01497ece211'}\n::: {.cell-output-display}\n![Ejemplo de curva ROC](09-judging-model-effectiveness_files/figure-html/fig-example-roc-curve-1.png){#fig-example-roc-curve fig-align='center' fig-alt='Un ejemplo de curva ROC. El eje x es uno menos la especificidad y el eje y es la sensibilidad. La curva se inclina hacia el lado superior izquierdo del área de la gráfica.' width=672}\n:::\n:::\n\n\nSi la curva estuviera cerca de la línea diagonal, entonces las predicciones del modelo no serían mejores que las conjeturas aleatorias. Dado que la curva está arriba en la esquina superior izquierda, vemos que nuestro modelo funciona bien en diferentes umbrales.\n\nHay otras funciones que utilizan estimaciones de probabilidad, incluidas `gain_curve()`, `lift_curve()` y `pr_curve()`.\n\n## Métricas De Clasificación Multiclase\n\n¿Qué pasa con los datos con tres o más clases? Para demostrarlo, exploremos un conjunto de datos de ejemplo diferente que tiene cuatro clases:\n\n\n::: {.cell layout-align=\"center\" hash='09-judging-model-effectiveness_cache/html/performance-hpc-example_203c646e654f845208ffbc2044efe393'}\n\n```{.r .cell-code}\ndata(hpc_cv)\ntibble(hpc_cv)\n## # A tibble: 3,467 × 7\n##   obs   pred     VF      F       M          L Resample\n##   <fct> <fct> <dbl>  <dbl>   <dbl>      <dbl> <chr>   \n## 1 VF    VF    0.914 0.0779 0.00848 0.0000199  Fold01  \n## 2 VF    VF    0.938 0.0571 0.00482 0.0000101  Fold01  \n## 3 VF    VF    0.947 0.0495 0.00316 0.00000500 Fold01  \n## 4 VF    VF    0.929 0.0653 0.00579 0.0000156  Fold01  \n## 5 VF    VF    0.942 0.0543 0.00381 0.00000729 Fold01  \n## 6 VF    VF    0.951 0.0462 0.00272 0.00000384 Fold01  \n## # ℹ 3,461 more rows\n```\n:::\n\n\nComo antes, hay factores para los resultados observados y previstos junto con otras cuatro columnas de probabilidades previstas para cada clase. (Estos datos también incluyen una columna `Resample`. Estos resultados `hpc_cv` son para predicciones fuera de muestra asociadas con una validación cruzada de 10 veces. Por el momento, esta columna se ignorará y discutiremos el remuestreo en profundidad en [Capítulo @sec-resampling].)\n\nLas funciones para las métricas que utilizan predicciones de clases discretas son idénticas a sus contrapartes binarias:\n\n\n::: {.cell layout-align=\"center\" hash='09-judging-model-effectiveness_cache/html/performance-mutliclass-pred_21f11da1c3996ad31fef9438e62163b3'}\n\n```{.r .cell-code}\naccuracy(hpc_cv, obs, pred)\n## # A tibble: 1 × 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy multiclass     0.709\n\nmcc(hpc_cv, obs, pred)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 mcc     multiclass     0.515\n```\n:::\n\n\nTenga en cuenta que, en estos resultados, aparece un `.estimator` \"multiclase\". Al igual que \"binario\", esto indica que se utilizó la fórmula para resultados con tres o más niveles de clase. El coeficiente de correlación de Matthews se diseñó originalmente para dos clases, pero se ha extendido a casos con más niveles de clase.\n\nExisten métodos para tomar métricas diseñadas para manejar resultados con solo dos clases y extenderlas para resultados con más de dos clases. Por ejemplo, una métrica como la sensibilidad mide la tasa de verdaderos positivos que, por definición, es específica de dos clases (es decir, \"evento\" y \"no evento\"). ¿Cómo se puede utilizar esta métrica en nuestros datos de ejemplo?\n\nExisten métodos contenedores que se pueden utilizar para aplicar sensibilidad a nuestro resultado de cuatro clases. Estas opciones son promedio macro, promedio macroponderado y micropromedio:\n\n-   El promedio macro calcula un conjunto de métricas de uno contra todos utilizando las estadísticas estándar de dos clases. Estos están promediados.\n\n-   El promedio macroponderado hace lo mismo, pero el promedio se pondera según el número de muestras de cada clase.\n\n-   El micropromedio calcula la contribución de cada clase, las agrega y luego calcula una única métrica a partir de los agregados.\n\nConsulte @wu2017unified y @OpitzBurst para obtener más información sobre cómo ampliar las métricas de clasificación a resultados con más de dos clases.\n\nUsando la sensibilidad como ejemplo, el cálculo habitual de dos clases es la relación entre el número de eventos predichos correctamente dividido por el número de eventos verdaderos. Los cálculos manuales para estos métodos de promediación son:\n\n\n::: {.cell layout-align=\"center\" hash='09-judging-model-effectiveness_cache/html/performance-sens-manual_3aa2f02a4c64dbbebd6cd1ee0d73eacf'}\n\n```{.r .cell-code}\nclass_totals <- \n  count(hpc_cv, obs, name = \"totals\") %>% \n  mutate(class_wts = totals / sum(totals))\nclass_totals\n##   obs totals class_wts\n## 1  VF   1769   0.51024\n## 2   F   1078   0.31093\n## 3   M    412   0.11883\n## 4   L    208   0.05999\n\ncell_counts <- \n  hpc_cv %>% \n  group_by(obs, pred) %>% \n  count() %>% \n  ungroup()\n\n# Calcule las cuatro sensibilidades usando 1 contra todos\none_versus_all <- \n  cell_counts %>% \n  filter(obs == pred) %>% \n  full_join(class_totals, by = \"obs\") %>% \n  mutate(sens = n / totals)\none_versus_all\n## # A tibble: 4 × 6\n##   obs   pred      n totals class_wts  sens\n##   <fct> <fct> <int>  <int>     <dbl> <dbl>\n## 1 VF    VF     1620   1769    0.510  0.916\n## 2 F     F       647   1078    0.311  0.600\n## 3 M     M        79    412    0.119  0.192\n## 4 L     L       111    208    0.0600 0.534\n\n# Tres estimaciones diferentes:\none_versus_all %>% \n  summarize(\n    macro = mean(sens), \n    macro_wts = weighted.mean(sens, class_wts),\n    micro = sum(n) / sum(totals)\n  )\n## # A tibble: 1 × 3\n##   macro macro_wts micro\n##   <dbl>     <dbl> <dbl>\n## 1 0.560     0.709 0.709\n```\n:::\n\n\nAfortunadamente, no es necesario implementar manualmente estos métodos de promedio. En cambio, las funciones <span class=\"pkg\">yardstick</span> pueden aplicar automáticamente estos métodos a través del argumento `estimator`:\n\n\n::: {.cell layout-align=\"center\" hash='09-judging-model-effectiveness_cache/html/performance-sens_cce961f074643e8f1b389934057929f1'}\n\n```{.r .cell-code}\nsensitivity(hpc_cv, obs, pred, estimator = \"macro\")\n## # A tibble: 1 × 3\n##   .metric     .estimator .estimate\n##   <chr>       <chr>          <dbl>\n## 1 sensitivity macro          0.560\nsensitivity(hpc_cv, obs, pred, estimator = \"macro_weighted\")\n## # A tibble: 1 × 3\n##   .metric     .estimator     .estimate\n##   <chr>       <chr>              <dbl>\n## 1 sensitivity macro_weighted     0.709\nsensitivity(hpc_cv, obs, pred, estimator = \"micro\")\n## # A tibble: 1 × 3\n##   .metric     .estimator .estimate\n##   <chr>       <chr>          <dbl>\n## 1 sensitivity micro          0.709\n```\n:::\n\n\nCuando se trata de estimaciones de probabilidad, existen algunas métricas con análogos multiclase. Por ejemplo, @HandTill determinó una técnica multiclase para curvas ROC. En este caso, *todas* las columnas de probabilidad de clase deben asignarse a la función:\n\n\n::: {.cell layout-align=\"center\" hash='09-judging-model-effectiveness_cache/html/performance-multi-class-roc_504791ac22dcfe2e962839f37607c05b'}\n\n```{.r .cell-code}\nroc_auc(hpc_cv, obs, VF, F, M, L)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 roc_auc hand_till      0.829\n```\n:::\n\n\nEl promedio macroponderado también está disponible como opción para aplicar esta métrica a un resultado multiclase:\n\n\n::: {.cell layout-align=\"center\" hash='09-judging-model-effectiveness_cache/html/performance-multi-class-roc-macro_0a7c177042351f5c4a30875bee3ea806'}\n\n```{.r .cell-code}\nroc_auc(hpc_cv, obs, VF, F, M, L, estimator = \"macro_weighted\")\n## # A tibble: 1 × 3\n##   .metric .estimator     .estimate\n##   <chr>   <chr>              <dbl>\n## 1 roc_auc macro_weighted     0.868\n```\n:::\n\n\nFinalmente, todas estas métricas de rendimiento se pueden calcular utilizando agrupaciones <span class=\"pkg\">dplyr</span>. Recuerde que estos datos tienen una columna para los grupos de remuestreo. Aún no hemos analizado el remuestreo en detalle, pero observe cómo podemos pasar un marco de datos agrupados a la función métrica para calcular las métricas para cada grupo:\n\n\n::: {.cell layout-align=\"center\" hash='09-judging-model-effectiveness_cache/html/performance-multi-class-acc-grouped_aa08a7fd177fe4bc171f076de6b81e1b'}\n\n```{.r .cell-code}\nhpc_cv %>% \n  group_by(Resample) %>% \n  accuracy(obs, pred)\n## # A tibble: 10 × 4\n##   Resample .metric  .estimator .estimate\n##   <chr>    <chr>    <chr>          <dbl>\n## 1 Fold01   accuracy multiclass     0.726\n## 2 Fold02   accuracy multiclass     0.712\n## 3 Fold03   accuracy multiclass     0.758\n## 4 Fold04   accuracy multiclass     0.712\n## 5 Fold05   accuracy multiclass     0.712\n## 6 Fold06   accuracy multiclass     0.697\n## # ℹ 4 more rows\n```\n:::\n\n\nLas agrupaciones también se traducen a los métodos `autoplot()`, y los resultados se muestran en @fig-grouped-roc-curves.\n\n\n::: {.cell layout-align=\"center\" hash='09-judging-model-effectiveness_cache/html/performance-multi-class-roc-grouped_29d5bb6d7222c3f9cedfe84f8d0c32c9'}\n\n```{.r .cell-code}\n# Cuatro curvas ROC 1 contra todos para cada pliegue\nhpc_cv %>% \n  group_by(Resample) %>% \n  roc_curve(obs, VF, F, M, L) %>% \n  autoplot()\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='09-judging-model-effectiveness_cache/html/fig-grouped-roc-curves_d5a09b1b8f15fff4b136a4b4f9ce84af'}\n::: {.cell-output-display}\n![Curvas ROC remuestreadas para cada una de las cuatro clases de resultados.](09-judging-model-effectiveness_files/figure-html/fig-grouped-roc-curves-1.png){#fig-grouped-roc-curves fig-align='center' fig-alt='Curvas ROC remuestreadas para cada una de las cuatro clases de resultados. Hay cuatro paneles para las clases VF, F, M y L. Cada panel contiene diez curvas ROC para cada uno de los conjuntos de datos remuestreados.' width=672}\n:::\n:::\n\n\nEsta visualización nos muestra que todos los diferentes grupos se desempeñan más o menos igual, pero que la clase `VF` se predice mejor que las clases `F` o `M`, ya que las curvas ROC `VF` están más en la esquina superior izquierda. . Este ejemplo utiliza remuestreos como grupos, pero se puede utilizar cualquier agrupación de sus datos. Este método `autoplot()` puede ser un método de visualización rápida para la efectividad del modelo en todas las clases y/o grupos de resultados.\n\n## Resumen Del Capítulo {#sec-performance-summary}\n\nDiferentes métricas miden diferentes aspectos del ajuste de un modelo, por ejemplo, RMSE mide la precisión mientras que $R^2$ mide la correlación. Medir el rendimiento del modelo es importante incluso cuando un modelo determinado no se utilizará principalmente para predicción; el poder predictivo también es importante para los modelos inferenciales o descriptivos. Las funciones del paquete <span class=\"pkg\">yardstick</span> miden la efectividad de un modelo utilizando datos. La interfaz principal de tidymodels utiliza principios de tidyverse y marcos de datos (en lugar de tener argumentos vectoriales). Diferentes métricas son apropiadas para las métricas de regresión y clasificación y, dentro de ellas, a veces hay diferentes formas de estimar las estadísticas, como para resultados multiclase.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}