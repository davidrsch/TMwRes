{
  "hash": "b91734b813d4ad8dca8ffd149e0e0555",
  "result": {
    "markdown": "\n\n\n# Judging Model Effectiveness {#sec-performance}\n\nOnce we have a model, we need to know how well it works. A quantitative approach for estimating effectiveness allows us to understand the model, to compare different models, or to tweak the model to improve performance. Our focus in tidymodels is on empirical validation; this usually means using data that were not used to create the model as the substrate to measure effectiveness.\n\n::: rmdwarning\nThe best approach to empirical validation involves using *resampling* methods that will be introduced in [Chapter @sec-resampling]. In this chapter, we will motivate the need for empirical validation by using the test set. Keep in mind that the test set can only be used once, as explained in @sec-splitting-methods.\n:::\n\nWhen judging model effectiveness, your decision about which metrics to examine can be critical. In later chapters, certain model parameters will be empirically optimized and a primary performance metric will be used to choose the best sub-model. Choosing the wrong metric can easily result in unintended consequences. For example, two common metrics for regression models are the root mean squared error (RMSE) and the coefficient of determination (a.k.a. $R^2$). The former measures *accuracy* while the latter measures *correlation*. These are not necessarily the same thing. @fig-performance-reg-metrics demonstrates the difference between the two.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Observed versus predicted values for models that are optimized using the RMSE compared to the coefficient of determination](figures/fig-performance-reg-metrics-1.png){#fig-performance-reg-metrics fig-align='center' fig-alt='Scatter plots of numeric observed versus predicted values for models that are optimized using the RMSE and the coefficient of determination. The former results in results that are close to the 45 degree line of identity while the latter shows results with a tight linear correlation but falls well off of the line of identity.' width=672}\n:::\n:::\n\n\nA model optimized for RMSE has more variability but has relatively uniform accuracy across the range of the outcome. The right panel shows that there is a tighter correlation between the observed and predicted values but this model performs poorly in the tails.\n\nThis chapter will demonstrate the <span class=\"pkg\">yardstick</span> package, a core tidymodels packages with the focus of measuring model performance. Before illustrating syntax, let's explore whether empirical validation using performance metrics is worthwhile when a model is focused on inference rather than prediction.\n\n## Performance Metrics and Inference\n\n\n\n\n\nThe effectiveness of any given model depends on how the model will be used. An inferential model is used primarily to understand relationships, and typically emphasizes the choice (and validity) of probabilistic distributions and other generative qualities that define the model. For a model used primarily for prediction, by contrast, predictive strength is of primary importance and other concerns about underlying statistical qualities may be less important. Predictive strength is usually determined by how close our predictions come to the observed data, i.e., fidelity of the model predictions to the actual results. This chapter focuses on functions that can be used to measure predictive strength. However, our advice for those developing inferential models is to use these techniques even when the model will not be used with the primary goal of prediction.\n\nA longstanding issue with the practice of inferential statistics is that, with a focus purely on inference, it is difficult to assess the credibility of a model. For example, consider the Alzheimer's disease data from @CraigSchapiro when 333 patients were studied to determine the factors that influence cognitive impairment. An analysis might take the known risk factors and build a logistic regression model where the outcome is binary (impaired/non-impaired). Let's consider predictors for age, sex, and the Apolipoprotein E genotype. The latter is a categorical variable with the six possible combinations of the three main variants of this gene. Apolipoprotein E is known to have an association with dementia [@Kim:2009p4370].\n\nA superficial, but not uncommon, approach to this analysis would be to fit a large model with main effects and interactions, then use statistical tests to find the minimal set of model terms that are statistically significant at some pre-defined level. If a full model with the three factors and their two- and three-way interactions were used, an initial phase would be to test the interactions using sequential likelihood ratio tests [@HosmerLemeshow]. Let's step through this kind of approach for the example Alzheimer's disease data:\n\n-   When comparing the model with all two-way interactions to one with the additional three-way interaction, the likelihood ratio tests produces a p-value of 0.888. This implies that there is no evidence that the four additional model terms associated with the three-way interaction explain enough of the variation in the data to keep them in the model.\n\n-   Next, the two-way interactions are similarly evaluated against the model with no interactions. The p-value here is 0.0382. This is somewhat borderline, but, given the small sample size, it would be prudent to conclude that there is evidence that some of the 10 possible two-way interactions are important to the model.\n\n-   From here, we would build some explanation of the results. The interactions would be particularly important to discuss since they may spark interesting physiological or neurological hypotheses to be explored further.\n\nWhile shallow, this analysis strategy is common in practice as well as in the literature. This is especially true if the practitioner has limited formal training in data analysis.\n\nOne missing piece of information in this approach is how closely this model fits the actual data. Using resampling methods, discussed in [Chapter @sec-resampling], we can estimate the accuracy of this model to be about 73%. Accuracy is often a poor measure of model performance; we use it here because it is commonly understood. If the model has 73% fidelity to the data, should we trust conclusions it produces? We might think so until we realize that the baseline rate of nonimpaired patients in the data is 72.7%. This means that, despite our statistical analysis, the two-factor model appears to be only 0.3% better than a simple heuristic that always predicts patients to be unimpaired, regardless of the observed data.\n\n::: rmdnote\nThe point of this analysis is to demonstrate the idea that optimization of statistical characteristics of the model does not imply that the model fits the data well. Even for purely inferential models, some measure of fidelity to the data should accompany the inferential results. Using this, the consumers of the analyses can calibrate their expectations of the results.\n:::\n\nIn the remainder of this chapter, we will discuss general approaches for evaluating models via empirical validation. These approaches are grouped by the nature of the outcome data: purely numeric, binary classes, and three or more class levels.\n\n## Regression Metrics\n\nRecall from @sec-parsnip-predictions that tidymodels prediction functions produce tibbles with columns for the predicted values. These columns have consistent names, and the functions in the <span class=\"pkg\">yardstick</span> package that produce performance metrics have consistent interfaces. The functions are data frame-based, as opposed to vector-based, with the general syntax of:\n\n``` r\nfunction(data, truth, ...)\n```\n\nwhere `data` is a data frame or tibble and `truth` is the column with the observed outcome values. The ellipses or other arguments are used to specify the column(s) containing the predictions.\n\nTo illustrate, let's take the model from @sec-recipes-summary. This model `lm_wflow_fit` combines a linear regression model with a predictor set supplemented with an interaction and spline functions for longitude and latitude. It was created from a training set (named `ames_train`). Although we do not advise using the test set at this juncture of the modeling process, it will be used here to illustrate functionality and syntax. The data frame `ames_test` consists of 588 properties. To start, let's produce predictions:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\names_test_res <- predict(lm_fit, new_data = ames_test %>% select(-Sale_Price))\names_test_res\n## # A tibble: 588 × 1\n##   .pred\n##   <dbl>\n## 1  5.07\n## 2  5.31\n## 3  5.28\n## 4  5.33\n## 5  5.30\n## 6  5.24\n## # ℹ 582 more rows\n```\n:::\n\n\nThe predicted numeric outcome from the regression model is named `.pred`. Let's match the predicted values with their corresponding observed outcome values:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\names_test_res <- bind_cols(ames_test_res, ames_test %>% select(Sale_Price))\names_test_res\n## # A tibble: 588 × 2\n##   .pred Sale_Price\n##   <dbl>      <dbl>\n## 1  5.07       5.02\n## 2  5.31       5.39\n## 3  5.28       5.28\n## 4  5.33       5.28\n## 5  5.30       5.28\n## 6  5.24       5.26\n## # ℹ 582 more rows\n```\n:::\n\n\nWe see that these values mostly look close, but we don't yet have a quantitative understanding of how the model is doing because we haven't computed any performance metrics. Note that both the predicted and observed outcomes are in log-10 units. It is best practice to analyze the predictions on the transformed scale (if one were used) even if the predictions are reported using the original units.\n\nLet's plot the data in @fig-ames-performance-plot before computing metrics:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(ames_test_res, aes(x = Sale_Price, y = .pred)) + \n  # Create a diagonal line:\n  geom_abline(lty = 2) + \n  geom_point(alpha = 0.5) + \n  labs(y = \"Predicted Sale Price (log10)\", x = \"Sale Price (log10)\") +\n  # Scale and size the x- and y-axis uniformly:\n  coord_obs_pred()\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Observed versus predicted values for an Ames regression model, with log-10 units on both axes](figures/fig-ames-performance-plot-1.png){#fig-ames-performance-plot fig-align='center' fig-alt='Scatter plots of numeric observed versus predicted values for an Ames regression model. Both axes use log-10 units. The model shows good concordance with some poorly fitting points at high and low prices.' width=672}\n:::\n:::\n\n\nThere is one low-price property that is substantially over-predicted, i.e., quite high above the dashed line.\n\nLet's compute the root mean squared error for this model using the `rmse()` function:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrmse(ames_test_res, truth = Sale_Price, estimate = .pred)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard      0.0736\n```\n:::\n\n\nThis shows us the standard format of the output of <span class=\"pkg\">yardstick</span> functions. Metrics for numeric outcomes usually have a value of \"standard\" for the `.estimator` column. Examples with different values for this column are shown in the next sections.\n\nTo compute multiple metrics at once, we can create a *metric set*. Let's add $R^2$ and the mean absolute error:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\names_metrics <- metric_set(rmse, rsq, mae)\names_metrics(ames_test_res, truth = Sale_Price, estimate = .pred)\n## # A tibble: 3 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard      0.0736\n## 2 rsq     standard      0.836 \n## 3 mae     standard      0.0549\n```\n:::\n\n\nThis tidy data format stacks the metrics vertically. The root mean squared error and mean absolute error metrics are both on the scale of the outcome (so `log10(Sale_Price)` for our example) and measure the difference between the predicted and observed values. The value for $R^2$ measures the squared correlation between the predicted and observed values, so values closer to one are better.\n\n::: rmdwarning\nThe <span class=\"pkg\">yardstick</span> package does *not* contain a function for adjusted $R^2$. This modification of the coefficient of determination is commonly used when the same data used to fit the model are used to evaluate the model. This metric is not fully supported in tidymodels because it is always a better approach to compute performance on a separate data set than the one used to fit the model.\n:::\n\n## Binary Classification Metrics\n\nTo illustrate other ways to measure model performance, we will switch to a different example. The <span class=\"pkg\">modeldata</span> package (another one of the tidymodels packages) contains example predictions from a test data set with two classes (\"Class1\" and \"Class2\"):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(two_class_example)\ntibble(two_class_example)\n## # A tibble: 500 × 4\n##   truth   Class1   Class2 predicted\n##   <fct>    <dbl>    <dbl> <fct>    \n## 1 Class2 0.00359 0.996    Class2   \n## 2 Class1 0.679   0.321    Class1   \n## 3 Class2 0.111   0.889    Class2   \n## 4 Class1 0.735   0.265    Class1   \n## 5 Class2 0.0162  0.984    Class2   \n## 6 Class1 0.999   0.000725 Class1   \n## # ℹ 494 more rows\n```\n:::\n\n\nThe second and third columns are the predicted class probabilities for the test set while `predicted` are the discrete predictions.\n\nFor the hard class predictions, a variety of <span class=\"pkg\">yardstick</span> functions are helpful:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# A confusion matrix: \nconf_mat(two_class_example, truth = truth, estimate = predicted)\n##           Truth\n## Prediction Class1 Class2\n##     Class1    227     50\n##     Class2     31    192\n\n# Accuracy:\naccuracy(two_class_example, truth, predicted)\n## # A tibble: 1 × 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary         0.838\n\n# Matthews correlation coefficient:\nmcc(two_class_example, truth, predicted)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 mcc     binary         0.677\n\n# F1 metric:\nf_meas(two_class_example, truth, predicted)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 f_meas  binary         0.849\n\n# Combining these three classification metrics together\nclassification_metrics <- metric_set(accuracy, mcc, f_meas)\nclassification_metrics(two_class_example, truth = truth, estimate = predicted)\n## # A tibble: 3 × 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy binary         0.838\n## 2 mcc      binary         0.677\n## 3 f_meas   binary         0.849\n```\n:::\n\n\nThe Matthews correlation coefficient and F1 score both summarize the confusion matrix, but compared to `mcc()`, which measures the quality of both positive and negative examples, the `f_meas()` metric emphasizes the positive class, i.e., the event of interest. For binary classification data sets like this example, <span class=\"pkg\">yardstick</span> functions have a standard argument called `event_level` to distinguish positive and negative levels. The default (which we used in this code) is that the *first* level of the outcome factor is the event of interest.\n\n::: rmdnote\nThere is some heterogeneity in R functions in this regard; some use the first level and others the second to denote the event of interest. We consider it more intuitive that the first level is the most important. The second level logic is borne of encoding the outcome as 0/1 (in which case the second value is the event) and unfortunately remains in some packages. However, tidymodels (along with many other R packages) require a categorical outcome to be encoded as a factor and, for this reason, the legacy justification for the second level as the event becomes irrelevant.\n:::\n\nAs an example where the second level is the event:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nf_meas(two_class_example, truth, predicted, event_level = \"second\")\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 f_meas  binary         0.826\n```\n:::\n\n\nIn this output, the `.estimator` value of \"binary\" indicates that the standard formula for binary classes will be used.\n\nThere are numerous classification metrics that use the predicted probabilities as inputs rather than the hard class predictions. For example, the receiver operating characteristic (ROC) curve computes the sensitivity and specificity over a continuum of different event thresholds. The predicted class column is not used. There are two <span class=\"pkg\">yardstick</span> functions for this method: `roc_curve()` computes the data points that make up the ROC curve and `roc_auc()` computes the area under the curve.\n\nThe interfaces to these types of metric functions use the `...` argument placeholder to pass in the appropriate class probability column. For two-class problems, the probability column for the event of interest is passed into the function:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntwo_class_curve <- roc_curve(two_class_example, truth, Class1)\ntwo_class_curve\n## # A tibble: 502 × 3\n##   .threshold specificity sensitivity\n##        <dbl>       <dbl>       <dbl>\n## 1 -Inf           0                 1\n## 2    1.79e-7     0                 1\n## 3    4.50e-6     0.00413           1\n## 4    5.81e-6     0.00826           1\n## 5    5.92e-6     0.0124            1\n## 6    1.22e-5     0.0165            1\n## # ℹ 496 more rows\n\nroc_auc(two_class_example, truth, Class1)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 roc_auc binary         0.939\n```\n:::\n\n\nThe `two_class_curve` object can be used in a `ggplot` call to visualize the curve, as shown in @fig-example-roc-curve. There is an `autoplot()` method that will take care of the details:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nautoplot(two_class_curve)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Example ROC curve](figures/fig-example-roc-curve-1.png){#fig-example-roc-curve fig-align='center' fig-alt='An example ROC curve. The x-axis is one minus specificity and the y-axis is sensitivity. The curve bows towards the upper left-hand side of the plot area.' width=672}\n:::\n:::\n\n\nIf the curve was close to the diagonal line, then the model's predictions would be no better than random guessing. Since the curve is up in the top, left-hand corner, we see that our model performs well at different thresholds.\n\nThere are a number of other functions that use probability estimates, including `gain_curve()`, `lift_curve()`, and `pr_curve()`.\n\n## Multiclass Classification Metrics\n\nWhat about data with three or more classes? To demonstrate, let's explore a different example data set that has four classes:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(hpc_cv)\ntibble(hpc_cv)\n## # A tibble: 3,467 × 7\n##   obs   pred     VF      F       M          L Resample\n##   <fct> <fct> <dbl>  <dbl>   <dbl>      <dbl> <chr>   \n## 1 VF    VF    0.914 0.0779 0.00848 0.0000199  Fold01  \n## 2 VF    VF    0.938 0.0571 0.00482 0.0000101  Fold01  \n## 3 VF    VF    0.947 0.0495 0.00316 0.00000500 Fold01  \n## 4 VF    VF    0.929 0.0653 0.00579 0.0000156  Fold01  \n## 5 VF    VF    0.942 0.0543 0.00381 0.00000729 Fold01  \n## 6 VF    VF    0.951 0.0462 0.00272 0.00000384 Fold01  \n## # ℹ 3,461 more rows\n```\n:::\n\n\nAs before, there are factors for the observed and predicted outcomes along with four other columns of predicted probabilities for each class. (These data also include a `Resample` column. These `hpc_cv` results are for out-of-sample predictions associated with 10-fold cross-validation. For the time being, this column will be ignored and we'll discuss resampling in depth in [Chapter @sec-resampling].)\n\nThe functions for metrics that use the discrete class predictions are identical to their binary counterparts:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\naccuracy(hpc_cv, obs, pred)\n## # A tibble: 1 × 3\n##   .metric  .estimator .estimate\n##   <chr>    <chr>          <dbl>\n## 1 accuracy multiclass     0.709\n\nmcc(hpc_cv, obs, pred)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 mcc     multiclass     0.515\n```\n:::\n\n\nNote that, in these results, a \"multiclass\" `.estimator` is listed. Like \"binary,\" this indicates that the formula for outcomes with three or more class levels was used. The Matthews correlation coefficient was originally designed for two classes but has been extended to cases with more class levels.\n\nThere are methods for taking metrics designed to handle outcomes with only two classes and extend them for outcomes with more than two classes. For example, a metric such as sensitivity measures the true positive rate which, by definition, is specific to two classes (i.e., \"event\" and \"nonevent\"). How can this metric be used in our example data?\n\nThere are wrapper methods that can be used to apply sensitivity to our four-class outcome. These options are macro-averaging, macro-weighted averaging, and micro-averaging:\n\n-   Macro-averaging computes a set of one-versus-all metrics using the standard two-class statistics. These are averaged.\n\n-   Macro-weighted averaging does the same but the average is weighted by the number of samples in each class.\n\n-   Micro-averaging computes the contribution for each class, aggregates them, then computes a single metric from the aggregates.\n\nSee @wu2017unified and @OpitzBurst for more on extending classification metrics to outcomes with more than two classes.\n\nUsing sensitivity as an example, the usual two-class calculation is the ratio of the number of correctly predicted events divided by the number of true events. The manual calculations for these averaging methods are:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nclass_totals <- \n  count(hpc_cv, obs, name = \"totals\") %>% \n  mutate(class_wts = totals / sum(totals))\nclass_totals\n##   obs totals class_wts\n## 1  VF   1769   0.51024\n## 2   F   1078   0.31093\n## 3   M    412   0.11883\n## 4   L    208   0.05999\n\ncell_counts <- \n  hpc_cv %>% \n  group_by(obs, pred) %>% \n  count() %>% \n  ungroup()\n\n# Compute the four sensitivities using 1-vs-all\none_versus_all <- \n  cell_counts %>% \n  filter(obs == pred) %>% \n  full_join(class_totals, by = \"obs\") %>% \n  mutate(sens = n / totals)\none_versus_all\n## # A tibble: 4 × 6\n##   obs   pred      n totals class_wts  sens\n##   <fct> <fct> <int>  <int>     <dbl> <dbl>\n## 1 VF    VF     1620   1769    0.510  0.916\n## 2 F     F       647   1078    0.311  0.600\n## 3 M     M        79    412    0.119  0.192\n## 4 L     L       111    208    0.0600 0.534\n\n# Three different estimates:\none_versus_all %>% \n  summarize(\n    macro = mean(sens), \n    macro_wts = weighted.mean(sens, class_wts),\n    micro = sum(n) / sum(totals)\n  )\n## # A tibble: 1 × 3\n##   macro macro_wts micro\n##   <dbl>     <dbl> <dbl>\n## 1 0.560     0.709 0.709\n```\n:::\n\n\nThankfully, there is no need to manually implement these averaging methods. Instead, <span class=\"pkg\">yardstick</span> functions can automatically apply these methods via the `estimator` argument:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsensitivity(hpc_cv, obs, pred, estimator = \"macro\")\n## # A tibble: 1 × 3\n##   .metric     .estimator .estimate\n##   <chr>       <chr>          <dbl>\n## 1 sensitivity macro          0.560\nsensitivity(hpc_cv, obs, pred, estimator = \"macro_weighted\")\n## # A tibble: 1 × 3\n##   .metric     .estimator     .estimate\n##   <chr>       <chr>              <dbl>\n## 1 sensitivity macro_weighted     0.709\nsensitivity(hpc_cv, obs, pred, estimator = \"micro\")\n## # A tibble: 1 × 3\n##   .metric     .estimator .estimate\n##   <chr>       <chr>          <dbl>\n## 1 sensitivity micro          0.709\n```\n:::\n\n\nWhen dealing with probability estimates, there are some metrics with multiclass analogs. For example, @HandTill determined a multiclass technique for ROC curves. In this case, *all* of the class probability columns must be given to the function:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nroc_auc(hpc_cv, obs, VF, F, M, L)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 roc_auc hand_till      0.829\n```\n:::\n\n\nMacro-weighted averaging is also available as an option for applying this metric to a multiclass outcome:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nroc_auc(hpc_cv, obs, VF, F, M, L, estimator = \"macro_weighted\")\n## # A tibble: 1 × 3\n##   .metric .estimator     .estimate\n##   <chr>   <chr>              <dbl>\n## 1 roc_auc macro_weighted     0.868\n```\n:::\n\n\nFinally, all of these performance metrics can be computed using <span class=\"pkg\">dplyr</span> groupings. Recall that these data have a column for the resampling groups. We haven't yet discussed resampling in detail, but notice how we can pass a grouped data frame to the metric function to compute the metrics for each group:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhpc_cv %>% \n  group_by(Resample) %>% \n  accuracy(obs, pred)\n## # A tibble: 10 × 4\n##   Resample .metric  .estimator .estimate\n##   <chr>    <chr>    <chr>          <dbl>\n## 1 Fold01   accuracy multiclass     0.726\n## 2 Fold02   accuracy multiclass     0.712\n## 3 Fold03   accuracy multiclass     0.758\n## 4 Fold04   accuracy multiclass     0.712\n## 5 Fold05   accuracy multiclass     0.712\n## 6 Fold06   accuracy multiclass     0.697\n## # ℹ 4 more rows\n```\n:::\n\n\nThe groupings also translate to the `autoplot()` methods, with results shown in @fig-grouped-roc-curves.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Four 1-vs-all ROC curves for each fold\nhpc_cv %>% \n  group_by(Resample) %>% \n  roc_curve(obs, VF, F, M, L) %>% \n  autoplot()\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Resampled ROC curves for each of the four outcome classes](figures/fig-grouped-roc-curves-1.png){#fig-grouped-roc-curves fig-align='center' fig-alt='Resampled ROC curves for each of the four outcome classes. There are four panels for classes VF, F, M, and L. Each panel contains ten ROC curves for each of the resampled data sets.' width=672}\n:::\n:::\n\n\nThis visualization shows us that the different groups all perform about the same, but that the `VF` class is predicted better than the `F` or `M` classes, since the `VF` ROC curves are more in the top-left corner. This example uses resamples as the groups, but any grouping in your data can be used. This `autoplot()` method can be a quick visualization method for model effectiveness across outcome classes and/or groups.\n\n## Chapter Summary {#sec-performance-summary}\n\nDifferent metrics measure different aspects of a model fit, e.g., RMSE measures accuracy while the $R^2$ measures correlation. Measuring model performance is important even when a given model will not be used primarily for prediction; predictive power is also important for inferential or descriptive models. Functions from the <span class=\"pkg\">yardstick</span> package measure the effectiveness of a model using data. The primary tidymodels interface uses tidyverse principles and data frames (as opposed to having vector arguments). Different metrics are appropriate for regression and classification metrics and, within these, there are sometimes different ways to estimate the statistics, such as for multiclass outcomes.\n",
    "supporting": [
      "09-judging-model-effectiveness_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}