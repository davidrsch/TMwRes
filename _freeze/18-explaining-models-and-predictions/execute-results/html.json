{
  "hash": "5f0bb3071c8403a5c916912100e15897",
  "result": {
    "markdown": "\n\n\n# Explicando Modelos Y Predicciones {#sec-explain}\n\nEn @sec-model-types, describimos una taxonomía de modelos y sugerimos que los modelos generalmente se construyen como uno o más descriptivos, inferenciales o predictivos. Sugerimos que el rendimiento del modelo, medido mediante métricas apropiadas (como RMSE para regresión o área bajo la curva ROC para clasificación), puede ser importante para todas las aplicaciones de modelado. De manera similar, las explicaciones del modelo, que responden *por qué* un modelo hace las predicciones que hace, pueden ser importantes ya sea que el propósito de su modelo sea en gran medida descriptivo, probar una hipótesis o hacer una predicción. Respondiendo a la pregunta \"¿por qué?\" permite a los profesionales del modelado comprender qué características fueron importantes en las predicciones e incluso cómo cambiarían las predicciones del modelo bajo diferentes valores de las características. Este capítulo cubre cómo preguntarle a un modelo por qué hace las predicciones que hace.\n\nPara algunos modelos, como la regresión lineal, suele estar claro cómo explicar por qué el modelo hace sus predicciones. La estructura de un modelo lineal contiene coeficientes para cada predictor que normalmente son sencillos de interpretar. Para otros modelos, como los bosques aleatorios que pueden capturar el comportamiento no lineal por diseño, es menos transparente cómo explicar las predicciones del modelo únicamente a partir de la estructura del modelo mismo. En cambio, podemos aplicar algoritmos explicativos de modelos para generar comprensión de las predicciones.\n\n::: rmdnote\nHay dos tipos de explicaciones de modelos, *global* y *local*. Las explicaciones del modelo global proporcionan una comprensión general agregada de un conjunto completo de observaciones; Las explicaciones del modelo local proporcionan información sobre una predicción para una sola observación.\n:::\n\n## Software Para Explicaciones De Modelos\n\nEl marco tidymodels no contiene software para explicaciones de modelos. En cambio, los modelos entrenados y evaluados con tidymodels se pueden explicar con otro software complementario en paquetes R como [<span class=\"pkg\">lime</span>](https://lime.data-imaginist.com/), [<span class=\"pkg\">vip</span>](https://koalaverse.github.io/vip/) y [<span class=\"pkg\">DALEX</span>](https://dalex.drwhy.ai/). A menudo elegimos:\n\n-   Las funciones de <span class=\"pkg\">vip</span> cuando queremos usar métodos *basados en modelos* que aprovechan la estructura del modelo (y a menudo son más rápidos)\n-   Las funciones de <span class=\"pkg\">DALEX</span> cuando queremos usar métodos *independientes del modelo* que se pueden aplicar a cualquier modelo\n\nEn los capítulos [-@sec-resampling] y [-@sec-compare], entrenamos y comparamos varios modelos para predecir el precio de las viviendas en Ames, IA, incluido un modelo lineal con interacciones y un modelo forestal aleatorio, y los resultados se muestran en @fig-explain-obs-pred.\n\n\n::: {.cell layout-align=\"center\" hash='18-explaining-models-and-predictions_cache/html/fig-explain-obs-pred_0fb629897bff14a85a552e85f445e2cb'}\n::: {.cell-output-display}\n![Comparación de precios previstos para un modelo lineal con interacciones y un modelo forestal aleatorio](18-explaining-models-and-predictions_files/figure-html/fig-explain-obs-pred-1.png){#fig-explain-obs-pred fig-align='center' fig-alt='Comparación de precios previstos para un modelo lineal con interacciones y un modelo forestal aleatorio. El bosque aleatorio da como resultado predicciones más precisas.' width=672}\n:::\n:::\n\n\nConstruyamos explicadores independientes del modelo para ambos modelos para descubrir por qué hacen estas predicciones. Podemos usar el paquete complementario <span class=\"pkg\">DALEXtra</span> para <span class=\"pkg\">DALEX</span>, que proporciona soporte para tidymodels. @Biecek2021 proporciona una exploración exhaustiva de cómo utilizar <span class=\"pkg\">DALEX</span> para explicaciones de modelos; Este capítulo sólo resume algunos enfoques importantes, específicos de tidymodels. Para calcular cualquier tipo de explicación de modelo, global o local, usando <span class=\"pkg\">DALEX</span>, primero preparamos los datos apropiados y luego creamos un *explicador* para cada modelo:\n\n\n::: {.cell layout-align=\"center\" hash='18-explaining-models-and-predictions_cache/html/explain-explainers_142ffbc822e645f6f1351360943899bb'}\n\n```{.r .cell-code}\nlibrary(DALEXtra)\nvip_features <- c(\"Neighborhood\", \"Gr_Liv_Area\", \"Year_Built\", \n                  \"Bldg_Type\", \"Latitude\", \"Longitude\")\n\nvip_train <- \n  ames_train %>% \n  select(all_of(vip_features))\n\nexplainer_lm <- \n  explain_tidymodels(\n    lm_fit, \n    data = vip_train, \n    y = ames_train$Sale_Price,\n    label = \"lm + interactions\",\n    verbose = FALSE\n  )\n\nexplainer_rf <- \n  explain_tidymodels(\n    rf_fit, \n    data = vip_train, \n    y = ames_train$Sale_Price,\n    label = \"random forest\",\n    verbose = FALSE\n  )\n```\n:::\n\n\n::: rmdwarning\nUn modelo lineal suele ser sencillo de interpretar y explicar; Es posible que no se encuentre frecuentemente utilizando algoritmos de explicación de modelos separados para un modelo lineal. Sin embargo, a veces puede resultar difícil comprender o explicar las predicciones incluso de un modelo lineal una vez que tiene splines y términos de interacción.\n:::\n\nTratar con importantes transformaciones de ingeniería de características durante la explicabilidad del modelo resalta algunas opciones que tenemos (o, a veces, la ambigüedad en dichos análisis). Podemos cuantificar las explicaciones del modelo global o local en términos de:\n\n-   *predictores básicos originales* tal como existían sin transformaciones significativas de ingeniería de características, o\n-   *características derivadas*, como las creadas mediante reducción de dimensionalidad ([Capítulo @sec-dimensionality]) o interacciones y términos spline, como en este ejemplo.\n\n## Explicaciones Locales\n\nLas explicaciones de los modelos locales proporcionan información sobre una predicción para una sola observación. Por ejemplo, consideremos un dúplex antiguo en el vecindario de North Ames (@sec-exploring-features-of-homes-in-ames):\n\n\n::: {.cell layout-align=\"center\" hash='18-explaining-models-and-predictions_cache/html/explain-duplex_53a41bf67e10c0ac5f4a5faf79aae9af'}\n\n```{.r .cell-code}\nduplex <- vip_train[120,]\nduplex\n## # A tibble: 1 × 6\n##   Neighborhood Gr_Liv_Area Year_Built Bldg_Type Latitude Longitude\n##   <fct>              <dbl>      <dbl> <fct>        <dbl>     <dbl>\n## 1 North_Ames          1040       1949 Duplex        42.0     -93.6\n```\n:::\n\n\nExisten múltiples enfoques posibles para comprender por qué un modelo predice un precio determinado para este dúplex. Una es una explicación detallada, implementada con la función <span class=\"pkg\">DALEX</span> `predict_parts()`; calcula cómo las contribuciones atribuidas a características individuales cambian la predicción del modelo medio para una observación particular, como nuestro dúplex. Para el modelo lineal, el estado dúplex (`Bldg_Type = 3`),[^18-explaining-models-and-predictions-1] el tamaño, la longitud y la antigüedad son los que más contribuyen a que el precio baje desde la intersección:\n\n[^18-explaining-models-and-predictions-1]: Tenga en cuenta que este paquete de explicaciones de modelos se centra en el *nivel* de predictores categóricos en este tipo de salida, como `Bldg_Type = 3` para dúplex y `Neighborhood = 1` para North Ames.\n\n\n::: {.cell layout-align=\"center\" hash='18-explaining-models-and-predictions_cache/html/explain-duplex-lm-breakdown_5ada345fe65baae865c817c211a8071a'}\n\n```{.r .cell-code}\nlm_breakdown <- predict_parts(explainer = explainer_lm, new_observation = duplex)\nlm_breakdown\n##                                           contribution\n## lm + interactions: intercept                     5.221\n## lm + interactions: Gr_Liv_Area = 1040           -0.082\n## lm + interactions: Bldg_Type = 3                -0.049\n## lm + interactions: Longitude = -93.608903       -0.043\n## lm + interactions: Year_Built = 1949            -0.039\n## lm + interactions: Latitude = 42.035841         -0.007\n## lm + interactions: Neighborhood = 1              0.001\n## lm + interactions: prediction                    5.002\n```\n:::\n\n\nDado que este modelo lineal se entrenó utilizando términos spline para latitud y longitud, la contribución al precio de \"Longitud\", `Longitude` que se muestra aquí combina los efectos de todos sus términos spline individuales. La contribución se realiza en términos de la característica \"Longitud\" original, no de las características spline derivadas.\n\nLas características más importantes son ligeramente diferentes para el modelo de bosque aleatorio, siendo el tamaño, la edad y el estado dúplex los más importantes:\n\n\n::: {.cell layout-align=\"center\" hash='18-explaining-models-and-predictions_cache/html/explain-duplex-rf-breakdown_8ac14688ee2ef998a3f5a73196689c57'}\n\n```{.r .cell-code}\nrf_breakdown <- predict_parts(explainer = explainer_rf, new_observation = duplex)\nrf_breakdown\n##                                       contribution\n## random forest: intercept                     5.221\n## random forest: Year_Built = 1949            -0.076\n## random forest: Gr_Liv_Area = 1040           -0.075\n## random forest: Bldg_Type = 3                -0.027\n## random forest: Longitude = -93.608903       -0.043\n## random forest: Latitude = 42.035841         -0.028\n## random forest: Neighborhood = 1             -0.003\n## random forest: prediction                    4.969\n```\n:::\n\n\n::: rmdwarning\nLas explicaciones de desglose del modelo como estas dependen del *orden* de las características.\n:::\n\nSi elegimos que el orden, `order`, para la explicación del modelo de bosque aleatorio sea el mismo que el predeterminado para el modelo lineal (elegido mediante una heurística), podemos cambiar la importancia relativa de las características:\n\n\n::: {.cell layout-align=\"center\" hash='18-explaining-models-and-predictions_cache/html/explain-duplex-rf-breakdown-reorder_e8bea3ab059ff4d0c4409a3d7a02e6a1'}\n\n```{.r .cell-code}\npredict_parts(\n  explainer = explainer_rf, \n  new_observation = duplex,\n  order = lm_breakdown$variable_name\n)\n##                                       contribution\n## random forest: intercept                     5.221\n## random forest: Gr_Liv_Area = 1040           -0.075\n## random forest: Bldg_Type = 3                -0.019\n## random forest: Longitude = -93.608903       -0.023\n## random forest: Year_Built = 1949            -0.104\n## random forest: Latitude = 42.035841         -0.028\n## random forest: Neighborhood = 1             -0.003\n## random forest: prediction                    4.969\n```\n:::\n\n\nPodemos utilizar el hecho de que estas explicaciones desglosadas cambian según el orden para calcular las características más importantes en todos (o muchos) ordenamientos posibles. Esta es la idea detrás de Shapley Additive Explanations [@Lundberg2017], donde las contribuciones promedio de las características se calculan bajo diferentes combinaciones o \"coaliciones\" de ordenamiento de características. Calculemos las atribuciones SHAP para nuestro dúplex, usando ordenamientos aleatorios `B = 20`:\n\n\n::: {.cell layout-align=\"center\" hash='18-explaining-models-and-predictions_cache/html/explain-duplex-rf-shap-calc_7721e9b36168ca4f0bf96bd493dedd32'}\n\n```{.r .cell-code}\nset.seed(1801)\nshap_duplex <- \n  predict_parts(\n    explainer = explainer_rf, \n    new_observation = duplex, \n    type = \"shap\",\n    B = 20\n  )\n```\n:::\n\n\nPodríamos usar el método de trazado predeterminado de <span class=\"pkg\">DALEX</span> llamando a `plot(shap_duplex)`, o podemos acceder a los datos subyacentes y crear un gráfico personalizado. Los diagramas de caja en @fig-duplex-rf-shap muestran la distribución de las contribuciones en todos los ordenamientos que probamos, y las barras muestran la atribución promedio de cada característica:\n\n\n::: {.cell layout-align=\"center\" hash='18-explaining-models-and-predictions_cache/html/explain-duplex-rf-shap_2e05596b827e25401b219f028e0281f4'}\n\n```{.r .cell-code}\nlibrary(forcats)\nshap_duplex %>%\n  group_by(variable) %>%\n  mutate(mean_val = mean(contribution)) %>%\n  ungroup() %>%\n  mutate(variable = fct_reorder(variable, abs(mean_val))) %>%\n  ggplot(aes(contribution, variable, fill = mean_val > 0)) +\n  geom_col(data = ~distinct(., variable, mean_val), \n           aes(mean_val, variable), \n           alpha = 0.5) +\n  geom_boxplot(width = 0.5) +\n  theme(legend.position = \"none\") +\n  scale_fill_viridis_d() +\n  labs(y = NULL)\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='18-explaining-models-and-predictions_cache/html/fig-duplex-rf-shap_15bf7d8c3c8189d992026a4f75d3d8a3'}\n::: {.cell-output-display}\n![Explicaciones aditivas de Shapley del modelo de bosque aleatorio para una propiedad dúplex](18-explaining-models-and-predictions_files/figure-html/fig-duplex-rf-shap-1.png){#fig-duplex-rf-shap fig-align='center' fig-alt='Explicaciones aditivas de Shapley a partir del modelo de bosque aleatorio para una propiedad dúplex. El año de construcción y la superficie habitable bruta tienen las mayores contribuciones.' width=672}\n:::\n:::\n\n\n¿Qué pasa con una observación diferente en nuestro conjunto de datos? Veamos una casa unifamiliar más grande y nueva en el vecindario de Gilbert:\n\n\n::: {.cell layout-align=\"center\" hash='18-explaining-models-and-predictions_cache/html/explain-gilbert_78d5ffd7fbdc62bcd2b61778d23ce868'}\n\n```{.r .cell-code}\nbig_house <- vip_train[1269,]\nbig_house\n## # A tibble: 1 × 6\n##   Neighborhood Gr_Liv_Area Year_Built Bldg_Type Latitude Longitude\n##   <fct>              <dbl>      <dbl> <fct>        <dbl>     <dbl>\n## 1 Gilbert             2267       2002 OneFam        42.1     -93.6\n```\n:::\n\n\nPodemos calcular las atribuciones promedio SHAP para esta casa de la misma manera:\n\n\n::: {.cell layout-align=\"center\" hash='18-explaining-models-and-predictions_cache/html/explain-gilbert-shap-calc_22f521dd1970657727a19f603e3084bc'}\n\n```{.r .cell-code}\nset.seed(1802)\nshap_house <- \n  predict_parts(\n    explainer = explainer_rf, \n    new_observation = big_house, \n    type = \"shap\",\n    B = 20\n  )\n```\n:::\n\n\nLos resultados se muestran en @fig-gilbert-shap; a diferencia del dúplex, el tamaño y antigüedad de esta vivienda contribuyen a que su precio sea más elevado.\n\n\n::: {.cell layout-align=\"center\" hash='18-explaining-models-and-predictions_cache/html/fig-gilbert-shap_004503655b67556a72d967fcddef5532'}\n::: {.cell-output-display}\n![Explicaciones aditivas de Shapley a partir del modelo de bosque aleatorio para una vivienda unifamiliar en Gilbert](18-explaining-models-and-predictions_files/figure-html/fig-gilbert-shap-1.png){#fig-gilbert-shap fig-align='center' fig-alt='Explicaciones aditivas de Shapley a partir del modelo de bosque aleatorio para una vivienda unifamiliar en Gilbert. La superficie habitable bruta y el año de construcción tienen las mayores contribuciones, pero en la dirección opuesta a la explicación anterior.' width=672}\n:::\n:::\n\n\n## Explicaciones Globales\n\nLas explicaciones del modelo global, también llamadas importancia de característica global o importancia variable, nos ayudan a comprender qué características son más importantes para impulsar las predicciones de los modelos forestales lineales y aleatorios en general, agregados en todo el conjunto de entrenamiento. Si bien la sección anterior abordó qué variables o características son más importantes para predecir el precio de venta de una vivienda individual, la importancia de las características globales aborda las variables más importantes para un modelo en conjunto.\n\n::: rmdnote\nUna forma de calcular la importancia de una variable es *permutar* las características [@breiman2001random]. Podemos permutar o mezclar los valores de una característica, predecir a partir del modelo y luego medir cuánto peor se ajusta el modelo a los datos en comparación con antes de la mezcla.\n:::\n\nSi barajar una columna provoca una gran degradación en el rendimiento del modelo, es importante; Si mezclar los valores de una columna no supone mucha diferencia en el rendimiento del modelo, no debe ser una variable importante. Este enfoque se puede aplicar a cualquier tipo de modelo (es *independiente del modelo*) y los resultados son fáciles de entender.\n\nUsando <span class=\"pkg\">DALEX</span>, calculamos este tipo de importancia variable mediante la función `model_parts()`.\n\n\n::: {.cell layout-align=\"center\" hash='18-explaining-models-and-predictions_cache/html/explain-global-calcs_d64c5c57c37086b134137fd1aa0ea21a'}\n\n```{.r .cell-code}\nset.seed(1803)\nvip_lm <- model_parts(explainer_lm, loss_function = loss_root_mean_square)\nset.seed(1804)\nvip_rf <- model_parts(explainer_rf, loss_function = loss_root_mean_square)\n```\n:::\n\n\nNuevamente, podríamos usar el método de trazado predeterminado de <span class=\"pkg\">DALEX</span> llamando a `plot(vip_lm, vip_rf)` pero los datos subyacentes están disponibles para exploración, análisis y trazado. Creemos una función para trazar:\n\n\n::: {.cell layout-align=\"center\" hash='18-explaining-models-and-predictions_cache/html/explain-global-func_96212bcf6505bbce1f70b6aeef8e4edc'}\n\n```{.r .cell-code}\nggplot_imp <- function(...) {\n  obj <- list(...)\n  metric_name <- attr(obj[[1]], \"loss_name\")\n  metric_lab <- paste(metric_name, \n                      \"después de permutaciones\\n(más alto indica más importante)\")\n  \n  full_vip <- bind_rows(obj) %>%\n    filter(variable != \"_baseline_\")\n  \n  perm_vals <- full_vip %>% \n    filter(variable == \"_full_model_\") %>% \n    group_by(label) %>% \n    summarise(dropout_loss = mean(dropout_loss))\n  \n  p <- full_vip %>%\n    filter(variable != \"_full_model_\") %>% \n    mutate(variable = fct_reorder(variable, dropout_loss)) %>%\n    ggplot(aes(dropout_loss, variable)) \n  if(length(obj) > 1) {\n    p <- p + \n      facet_wrap(vars(label)) +\n      geom_vline(data = perm_vals, aes(xintercept = dropout_loss, color = label),\n                 linewidth = 1.4, lty = 2, alpha = 0.7) +\n      geom_boxplot(aes(color = label, fill = label), alpha = 0.2)\n  } else {\n    p <- p + \n      geom_vline(data = perm_vals, aes(xintercept = dropout_loss),\n                 linewidth = 1.4, lty = 2, alpha = 0.7) +\n      geom_boxplot(fill = \"#91CBD765\", alpha = 0.4)\n    \n  }\n  p +\n    theme(legend.position = \"none\") +\n    labs(x = metric_lab, \n         y = NULL,  fill = NULL,  color = NULL)\n}\n```\n:::\n\n\nEl uso de `ggplot_imp(vip_lm, vip_rf)` produce @fig-global-rf.\n\n\n::: {.cell layout-align=\"center\" hash='18-explaining-models-and-predictions_cache/html/fig-global-rf_d88a9334f71dfd3c4f25f50f55e6149a'}\n::: {.cell-output-display}\n![Explicador global para los modelos de regresión lineal y de bosque aleatorio](18-explaining-models-and-predictions_files/figure-html/fig-global-rf-1.png){#fig-global-rf fig-align='center' fig-alt='Explicador global para los modelos de bosque aleatorio y de regresión lineal. Para ambos modelos, la superficie habitable bruta y el año de construcción tienen las mayores contribuciones, pero el modelo lineal utiliza el predictor de vecindario en gran medida.' width=768}\n:::\n:::\n\n\nLa línea discontinua en cada panel de @fig-global-rf muestra el RMSE para el modelo completo, ya sea el modelo lineal o el modelo de bosque aleatorio. Las características más a la derecha son más importantes porque permutarlas da como resultado un RMSE más alto. Hay bastante información interesante que aprender de esta trama; por ejemplo, la vecindad es bastante importante en el modelo lineal con interacciones/splines, pero es la segunda característica menos importante para el modelo de bosque aleatorio.\n\n## Construyendo Explicaciones Globales A Partir De Explicaciones Locales\n\nHasta ahora en este capítulo, nos hemos centrado en explicaciones de modelos locales para una sola observación (a través de explicaciones aditivas de Shapley) y explicaciones de modelos globales para un conjunto de datos en su conjunto (a través de características de permutación). También es posible construir explicaciones de modelos globales agregando explicaciones de modelos locales, como con los *perfiles de dependencia parcial*.\n\n::: rmdnote\nLos perfiles de dependencia parcial muestran cómo el valor esperado de la predicción de un modelo, como el precio previsto de una casa en Ames, cambia en función de una característica, como la edad o la superficie habitable bruta.\n:::\n\nUna forma de crear dicho perfil es agregando o promediando perfiles para observaciones individuales. Un perfil que muestra cómo cambia la predicción de una observación individual en función de una característica determinada se denomina perfil ICE (expectativa condicional individual) o perfil CP (*ceteris paribus*). Podemos calcular dichos perfiles individuales (para 500 de las observaciones en nuestro conjunto de entrenamiento) y luego agregarlos usando la función <span class=\"pkg\">DALEX</span> `model_profile()`:\n\n\n::: {.cell layout-align=\"center\" hash='18-explaining-models-and-predictions_cache/html/explain-year-built-pdp-calc_ec4d44d906b440c87c6c9f5152b8ad2d'}\n\n```{.r .cell-code}\nset.seed(1805)\npdp_age <- model_profile(explainer_rf, N = 500, variables = \"Year_Built\")\n```\n:::\n\n\nCreemos otra función para trazar los datos subyacentes en este objeto:\n\n\n::: {.cell layout-align=\"center\" hash='18-explaining-models-and-predictions_cache/html/explain-pdp-func_d280163bf906beadd48e19e9fb957db0'}\n\n```{.r .cell-code}\nggplot_pdp <- function(obj, x) {\n  \n  p <- \n    as_tibble(obj$agr_profiles) %>%\n    mutate(`_label_` = stringr::str_remove(`_label_`, \"^[^_]*_\")) %>%\n    ggplot(aes(`_x_`, `_yhat_`)) +\n    geom_line(data = as_tibble(obj$cp_profiles),\n              aes(x = {{ x }}, group = `_ids_`),\n              linewidth = 0.5, alpha = 0.05, color = \"gray50\")\n  \n  num_colors <- n_distinct(obj$agr_profiles$`_label_`)\n  \n  if (num_colors > 1) {\n    p <- p + geom_line(aes(color = `_label_`), linewidth = 1.2, alpha = 0.8)\n  } else {\n    p <- p + geom_line(color = \"midnightblue\", linewidth = 1.2, alpha = 0.8)\n  }\n  \n  p\n}\n```\n:::\n\n\nEl uso de esta función genera @fig-year-built, donde podemos ver el comportamiento no lineal del modelo de bosque aleatorio.\n\n\n::: {.cell layout-align=\"center\" hash='18-explaining-models-and-predictions_cache/html/explain-year-built_b7ccd6e749f222f6c7122c8d37e4524b'}\n\n```{.r .cell-code}\nggplot_pdp(pdp_age, Year_Built)  +\n  labs(x = \"Year built\", \n       y = \"Sale Price (log)\", \n       color = NULL)\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='18-explaining-models-and-predictions_cache/html/fig-year-built_e449b471fe362bcf2a38dbc232ac482a'}\n::: {.cell-output-display}\n![Perfiles de dependencia parcial para el modelo de bosque aleatorio centrados en el predictor del año de construcción](18-explaining-models-and-predictions_files/figure-html/fig-year-built-1.png){#fig-year-built fig-align='center' fig-alt='Perfiles de dependencia parcial para el modelo de bosque aleatorio que se centran en el predictor del año de construcción. Los perfiles son en su mayoría relativamente constantes, pero luego aumentan linealmente alrededor de 1950.' width=672}\n:::\n:::\n\n\nEl precio de venta de las casas construidas en diferentes años es en su mayor parte estable, con un modesto aumento después de 1960 aproximadamente. Se pueden calcular perfiles de dependencia parcial para cualquier otra característica del modelo, y también para grupos en los datos, como `Bldg_Type`. Utilicemos 1000 observaciones para estos perfiles.\n\n\n::: {.cell layout-align=\"center\" hash='18-explaining-models-and-predictions_cache/html/explain-build-type-pdp_adc9701614b7eb8a065f154d984c5c7d'}\n\n```{.r .cell-code}\nset.seed(1806)\npdp_liv <- model_profile(explainer_rf, N = 1000, \n                         variables = \"Gr_Liv_Area\", \n                         groups = \"Bldg_Type\")\n\nggplot_pdp(pdp_liv, Gr_Liv_Area) +\n  scale_x_log10() +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(x = \"Superficie habitable bruta\", \n       y = \"Precio de venta (registro)\", \n       color = NULL)\n```\n:::\n\n\nEste código produce @fig-building-type-profiles, donde vemos que el precio de venta aumenta más entre aproximadamente 1,000 y 3,000 pies cuadrados de área habitable, y que los diferentes tipos de viviendas (como casas unifamiliares o diferentes tipos de casas adosadas) exhiben en su mayoría tendencias crecientes similares en el precio con más espacio habitable.\n\n\n::: {.cell layout-align=\"center\" hash='18-explaining-models-and-predictions_cache/html/fig-building-type-profiles_40527a4889fbb23522dd361b43c5d8f9'}\n::: {.cell-output-display}\n![Perfiles de dependencia parcial para el modelo de bosque aleatorio centrado en los tipos de construcción y la superficie habitable bruta](18-explaining-models-and-predictions_files/figure-html/fig-building-type-profiles-1.png){#fig-building-type-profiles fig-align='center' fig-alt='Perfiles de dependencia parcial para el modelo de bosque aleatorio que se centran en los tipos de construcción y la superficie habitable bruta. Los perfiles tipo edificio son, en su mayor parte, paralelos sobre la superficie habitable bruta.' width=672}\n:::\n:::\n\n\nTenemos la opción de usar `plot(pdp_liv)` para los gráficos predeterminados <span class=\"pkg\">DALEX</span>, pero dado que aquí estamos haciendo gráficos con los datos subyacentes, incluso podemos facetar una de las características para visualizar si las predicciones cambian. de manera diferente y resaltando el desequilibrio en estos subgrupos (como se muestra en @fig-building-type-facets).\n\n\n::: {.cell layout-align=\"center\" hash='18-explaining-models-and-predictions_cache/html/explain-build-type-facet_d340f2e3b13d7d721baee4f6a96e0d4f'}\n\n```{.r .cell-code}\nas_tibble(pdp_liv$agr_profiles) %>%\n  mutate(Bldg_Type = stringr::str_remove(`_label_`, \"random forest_\")) %>%\n  ggplot(aes(`_x_`, `_yhat_`, color = Bldg_Type)) +\n  geom_line(data = as_tibble(pdp_liv$cp_profiles),\n            aes(x = Gr_Liv_Area, group = `_ids_`),\n            linewidth = 0.5, alpha = 0.1, color = \"gray50\") +\n  geom_line(linewidth = 1.2, alpha = 0.8, show.legend = FALSE) +\n  scale_x_log10() +\n  facet_wrap(~Bldg_Type) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(x = \"Superficie habitable bruta\", \n       y = \"Precio de venta (registro)\", \n       color = NULL)\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='18-explaining-models-and-predictions_cache/html/fig-building-type-facets_ab7972c1e51714fb72af192d8895e90d'}\n::: {.cell-output-display}\n![Perfiles de dependencia parcial para el modelo de bosque aleatorio que se centran en los tipos de construcción y la superficie habitable bruta mediante facetas](18-explaining-models-and-predictions_files/figure-html/fig-building-type-facets-1.png){#fig-building-type-facets fig-align='center' fig-alt='Perfiles de dependencia parcial para el modelo de bosque aleatorio que se centran en los tipos de construcción y la superficie habitable bruta mediante facetas. Los perfiles tipo edificio son, en su mayor parte, paralelos sobre la superficie habitable bruta.' width=672}\n:::\n:::\n\n\nNo existe un único enfoque correcto para crear explicaciones de modelos y las opciones descritas en este capítulo no son exhaustivas. Hemos destacado buenas opciones para explicaciones tanto a nivel individual como global, así como también cómo tender un puente entre uno y otro, y le remitimos a @Biecek2021 y @Molnar2021 para obtener más información.\n\n## ¡De Vuelta A Los Frijoles!\n\nEn el [Capítulo @sec-dimensionality], analizamos cómo utilizar la reducción de dimensionalidad como un paso de preprocesamiento o ingeniería de características al modelar datos de alta dimensión. Para nuestro conjunto de datos de ejemplo de medidas de morfología de frijoles secos que predicen el tipo de frijol, vimos excelentes resultados de la reducción de la dimensionalidad de mínimos cuadrados parciales (PLS) combinada con un modelo de análisis discriminante regularizado. ¿Cuáles de esas características morfológicas fueron *más* importantes en las predicciones del tipo de frijol? Podemos usar el mismo enfoque descrito a lo largo de este capítulo para crear un explicador independiente del modelo y calcular, por ejemplo, explicaciones del modelo global a través de `model_parts()`:\n\n\n::: {.cell layout-align=\"center\" hash='18-explaining-models-and-predictions_cache/html/explain-bean-data_df03a4c2ce1e82d0e7f93d57b1d3f4ae'}\n\n:::\n\n::: {.cell layout-align=\"center\" hash='18-explaining-models-and-predictions_cache/html/explain-bea-vip_85135173ef4324222c9cc79f678b5856'}\n\n```{.r .cell-code}\nset.seed(1807)\nvip_beans <- \n  explain_tidymodels(\n    rda_wflow_fit, \n    data = bean_train %>% select(-class), \n    y = bean_train$class,\n    label = \"RDA\",\n    verbose = FALSE\n  ) %>% \n  model_parts() \n```\n:::\n\n\nUsando nuestra función de trazado de importancia previamente definida, `ggplot_imp(vip_beans)` produce @fig-bean-explainer.\n\n\n::: {.cell layout-align=\"center\" hash='18-explaining-models-and-predictions_cache/html/fig-bean-explainer_ab3a692604b8b4a4921eae97ec8ced76'}\n::: {.cell-output-display}\n![Explicador global del modelo de análisis discriminante regularizado de los datos de frijoles](18-explaining-models-and-predictions_files/figure-html/fig-bean-explainer-1.png){#fig-bean-explainer fig-align='center' fig-alt='Explicador global del modelo de análisis discriminante regularizado de los datos de frijoles. Casi todos los predictores tienen una contribución significativa, siendo los factores de forma uno y cuatro los que más contribuyen. ' width=768}\n:::\n:::\n\n\n::: rmdwarning\nLas medidas de importancia de características globales que vemos en @fig-bean-explainer incorporan los efectos de todos los componentes de PLS, pero en términos de las variables originales.\n:::\n\n@fig-bean-explainer nos muestra que los factores de forma se encuentran entre las características más importantes para predecir el tipo de frijol, especialmente el factor de forma 4, una medida de solidez que toma en cuenta el área $A$, el eje mayor $L$ y el eje menor $l$:\n\n$$\\text{SF4} = \\frac{A}{\\pi(L/2)(l/2)}$$\n\nPodemos ver en @fig-bean-explainer que el factor de forma 1 (la relación entre el eje mayor y el área), la longitud del eje menor y la redondez son las siguientes características más importantes del frijol para predecir la variedad de frijol.\n\n## Resumen Del Capítulo {#sec-explain-summary}\n\nPara algunos tipos de modelos, la respuesta a por qué un modelo hizo una determinada predicción es sencilla, pero para otros tipos de modelos, debemos usar algoritmos explicativos separados para comprender qué características son relativamente más importantes para las predicciones. Puede generar dos tipos principales de explicaciones de modelo a partir de un modelo entrenado. Las explicaciones globales brindan información agregada sobre un conjunto de datos completo, mientras que las explicaciones locales brindan comprensión sobre las predicciones de un modelo para una sola observación.\n\nPaquetes como <span class=\"pkg\">DALEX</span> y su paquete de soporte <span class=\"pkg\">DALEXtra</span>, <span class=\"pkg\">vip</span> y <span class=\"pkg\">lime</span> se pueden integrar en un análisis de tidymodels para proporcionar explicaciones de estos modelos. . Las explicaciones del modelo son sólo una parte de la comprensión de si su modelo es apropiado y eficaz, junto con las estimaciones del rendimiento del modelo; El @sec-trust explora más a fondo la calidad y confiabilidad de las predicciones.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}