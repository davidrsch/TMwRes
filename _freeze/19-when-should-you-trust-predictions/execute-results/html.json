{
  "hash": "99bd3e9542e6aea2311c82594ff6fef4",
  "result": {
    "markdown": "\n\n\n# When Should You Trust Your Predictions? {#sec-trust}\n\nA predictive model can almost always produce a prediction, given input data. However, in plenty of situations it is inappropriate to produce such a prediction. When a new data point is well outside of the range of data used to create the model, making a prediction may be an inappropriate *extrapolation*. A more qualitative example of an inappropriate prediction would be when the model is used in a completely different context. The cell segmentation data used in [Chapter @sec-iterative-search] flags when human breast cancer cells can or cannot be accurately isolated inside an image. A model built from these data could be inappropriately applied to stomach cells for the same purpose. We can produce a prediction but it is unlikely to be applicable to the different cell type.\n\nThis chapter discusses two methods for quantifying the potential quality of a prediction:\n\n-   *Equivocal zones* use the predicted values to alert the user that results may be suspect.\n-   *Applicability* uses the predictors to measure the amount of extrapolation (if any) for new samples.\n\n## Equivocal Results {#sec-equivocal-zones}\n\n::: rmdwarning\nIn some cases, the amount of uncertainty associated with a prediction is too high to be trusted.\n:::\n\nIf a model result indicated that you had a 51% chance of having contracted COVID-19, it would be natural to view the diagnosis with some skepticism. In fact, regulatory bodies often require many medical diagnostics to have an *equivocal zone*. This zone is a range of results in which the prediction should not be reported to patients, for example, some range of COVID-19 test results that are too uncertain to be reported to a patient. See @Danowski524 and @Kerleguer1783 for examples. The same notion can be applied to models created outside of medical diagnostics.\n\nLet's use a function that can simulate classification data with two classes and two predictors (`x` and `y`). The true model is a logistic regression model with the equation:\n\n$$\n\\mathrm{logit}(p) = -1 - 2x - \\frac{x^2}{5} + 2y^2 \n$$\n\nThe two predictors follow a bivariate normal distribution with a correlation of 0.70. We'll create a training set of 200 samples and a test set of 50:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-simulation_112c4655ff2f6a35b895cd0d53d531ff'}\n\n```{.r .cell-code}\nlibrary(tidymodels)\ntidymodels_prefer()\n\nsimulate_two_classes <- \n  function (n, error = 0.1, eqn = quote(-1 - 2 * x - 0.2 * x^2 + 2 * y^2))  {\n    # Slightly correlated predictors\n    sigma <- matrix(c(1, 0.7, 0.7, 1), nrow = 2, ncol = 2)\n    dat <- MASS::mvrnorm(n = n, mu = c(0, 0), Sigma = sigma)\n    colnames(dat) <- c(\"x\", \"y\")\n    cls <- paste0(\"class_\", 1:2)\n    dat <- \n      as_tibble(dat) %>% \n      mutate(\n        linear_pred = !!eqn,\n        # Add some misclassification noise\n        linear_pred = linear_pred + rnorm(n, sd = error),\n        prob = binomial()$linkinv(linear_pred),\n        class = ifelse(prob > runif(n), cls[1], cls[2]),\n        class = factor(class, levels = cls)\n      )\n    dplyr::select(dat, x, y, class)\n  }\n\nset.seed(1901)\ntraining_set <- simulate_two_classes(200)\ntesting_set  <- simulate_two_classes(50)\n```\n:::\n\n\nWe estimate a logistic regression model using Bayesian methods (using the default Gaussian prior distributions for the parameters):\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-bayes-glm_0ba020bc077240480ba10228cec64312'}\n\n```{.r .cell-code}\ntwo_class_mod <- \n  logistic_reg() %>% \n  set_engine(\"stan\", seed = 1902) %>% \n  fit(class ~ . + I(x^2)+ I(y^2), data = training_set)\nprint(two_class_mod, digits = 3)\n## parsnip model object\n## \n## stan_glm\n##  family:       binomial [logit]\n##  formula:      class ~ . + I(x^2) + I(y^2)\n##  observations: 200\n##  predictors:   5\n## ------\n##             Median MAD_SD\n## (Intercept)  1.092  0.287\n## x            2.290  0.423\n## y            0.314  0.354\n## I(x^2)       0.077  0.307\n## I(y^2)      -2.465  0.424\n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\n```\n:::\n\n\nThe fitted class boundary is overlaid onto the test set in @fig-glm-boundaries. The data points closest to the class boundary are the most uncertain. If their values changed slightly, their predicted class might change. One simple method for disqualifying some results is to call them \"equivocal\" if the values are within some range around 50% (or the appropriate probability cutoff for a certain situation). Depending on the problem the model is being applied to, this might indicate we should collect another measurement or we require more information before a trustworthy prediction is possible.\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/fig-glm-boundaries_843d3a5f36a8c0374eac13d5526aa6ba'}\n::: {.cell-output-display}\n![Simulated two-class data set with a logistic regression fit and decision boundary.](19-when-should-you-trust-predictions_files/figure-html/fig-glm-boundaries-1.png){#fig-glm-boundaries fig-align='center' fig-alt='Simulated two-class data set with a logistic regression fit and decision boundary. The scatter plot of the two classes shows fairly correlated data. The decision boundary is a parabola in the x axis that does a good job of separating the classes.' width=70%}\n:::\n:::\n\n\nWe could base the width of the band around the cutoff on how performance improves when the uncertain results are removed. However, we should also estimate the reportable rate (the expected proportion of usable results). For example, it would not be useful in real-world situations to have perfect performance but release predictions on only 2% of the samples passed to the model.\n\nLet's use the test set to determine the balance between improving performance and having enough reportable results. The predictions are created using:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-bayes-glm-pred_bb55a105d49208ddae015ca155cac098'}\n\n```{.r .cell-code}\ntest_pred <- augment(two_class_mod, testing_set)\ntest_pred %>% head()\n## # A tibble: 6 × 6\n##   .pred_class .pred_class_1 .pred_class_2      x      y class  \n##   <fct>               <dbl>         <dbl>  <dbl>  <dbl> <fct>  \n## 1 class_2           0.0256          0.974  1.12  -0.176 class_2\n## 2 class_1           0.555           0.445 -0.126 -0.582 class_2\n## 3 class_2           0.00620         0.994  1.92   0.615 class_2\n## 4 class_2           0.472           0.528 -0.400  0.252 class_2\n## 5 class_2           0.163           0.837  1.30   1.09  class_1\n## 6 class_2           0.0317          0.968  2.59   1.36  class_2\n```\n:::\n\n\nWith tidymodels, the <span class=\"pkg\">probably</span> package contains functions for equivocal zones. For cases with two classes, the `make_two_class_pred()` function creates a factor-like column that has the predicted classes with an equivocal zone:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-make-eq_62e6d28aa388291d4f811d69e441f770'}\n\n```{.r .cell-code}\nlibrary(probably)\n\nlvls <- levels(training_set$class)\n\ntest_pred <- \n  test_pred %>% \n  mutate(.pred_with_eqz = make_two_class_pred(.pred_class_1, lvls, buffer = 0.15))\n\ntest_pred %>% count(.pred_with_eqz)\n## # A tibble: 3 × 2\n##   .pred_with_eqz     n\n##       <clss_prd> <int>\n## 1           [EQ]     9\n## 2        class_1    20\n## 3        class_2    21\n```\n:::\n\n\nRows that are within $0.50\\pm0.15$ are given a value of `[EQ]`.\n\n::: rmdnote\nThe notation `[EQ]` in this example is not a factor level but an attribute of that column.\n:::\n\nSince the factor levels are the same as the original data, confusion matrices and other statistics can be computed without error. When using standard functions from the <span class=\"pkg\">yardstick</span> package, the equivocal results are converted to `NA` and are not used in the calculations that use the hard class predictions. Notice the differences in these confusion matrices:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-conf-mat_dfa69832ebe8f7b3f161a46c0c5aaeb6'}\n\n```{.r .cell-code}\n# All data\ntest_pred %>% conf_mat(class, .pred_class)\n##           Truth\n## Prediction class_1 class_2\n##    class_1      20       6\n##    class_2       5      19\n\n# Reportable results only: \ntest_pred %>% conf_mat(class, .pred_with_eqz)\n##           Truth\n## Prediction class_1 class_2\n##    class_1      17       3\n##    class_2       5      16\n```\n:::\n\n\nAn `is_equivocal()` function is also available for filtering these rows from the data.\n\nDoes the equivocal zone help improve accuracy? Let's look at different buffer sizes, as shown in @fig-equivocal-zone-results:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-eq-calcs_a26157f441fe7e35c0ead15393052068'}\n\n```{.r .cell-code}\n# A function to change the buffer then compute performance.\neq_zone_results <- function(buffer) {\n  test_pred <- \n    test_pred %>% \n    mutate(.pred_with_eqz = make_two_class_pred(.pred_class_1, lvls, buffer = buffer))\n  acc <- test_pred %>% accuracy(class, .pred_with_eqz)\n  rep_rate <- reportable_rate(test_pred$.pred_with_eqz)\n  tibble(accuracy = acc$.estimate, reportable = rep_rate, buffer = buffer)\n}\n\n# Evaluate a sequence of buffers and plot the results. \nmap(seq(0, .1, length.out = 40), eq_zone_results) %>% \n  list_rbind() %>% \n  pivot_longer(c(-buffer), names_to = \"statistic\", values_to = \"value\") %>% \n  ggplot(aes(x = buffer, y = value, lty = statistic)) + \n  geom_step(linewidth = 1.2, alpha = 0.8) + \n  labs(y = NULL, lty = NULL)\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/fig-equivocal-zone-results_5686d894b12f02d80744fac426ddcfcf'}\n::: {.cell-output-display}\n![The effect of equivocal zones on model performance](19-when-should-you-trust-predictions_files/figure-html/fig-equivocal-zone-results-1.png){#fig-equivocal-zone-results fig-align='center' fig-alt='The effect of equivocal zones on model performance. There is a slight increase in accuracy at the expense of a falling reportable rate.' width=80%}\n:::\n:::\n\n\n@fig-equivocal-zone-results shows us that accuracy improves by a few percentage points but at the cost of nearly 10% of predictions being unusable! The value of such a compromise depends on how the model predictions will be used.\n\nThis analysis focused on using the predicted class probability to disqualify points, since this is a fundamental measure of uncertainty in classification models. A slightly better approach would be to use the standard error of the class probability. Since we used a Bayesian model, the probability estimates we found are actually the mean of the posterior predictive distribution. In other words, the Bayesian model gives us a distribution for the class probability. Measuring the standard deviation of this distribution gives us a *standard error of prediction* of the probability. In most cases, this value is directly related to the mean class probability. You might recall that, for a Bernoulli random variable with probability $p$, the variance is $p(1-p)$. Because of this relationship, the standard error is largest when the probability is 50%. Instead of assigning an equivocal result using the class probability, we could instead use a cutoff on the standard error of prediction.\n\nOne important aspect of the standard error of prediction is that it takes into account more than just the class probability. In cases where there is significant extrapolation or aberrant predictor values, the standard error might increase. The benefit of using the standard error of prediction is that it might also flag predictions that are problematic (as opposed to simply uncertain). One reason we used the Bayesian model is that it naturally estimates the standard error of prediction; not many models can calculate this. For our test set, using `type = \"pred_int\"` will produce upper and lower limits and the `std_error` adds a column for that quantity. For 80% intervals:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-pred-int_9b2b3cb54ab796ab2bf1820435861f47'}\n\n```{.r .cell-code}\ntest_pred <- \n  test_pred %>% \n  bind_cols(\n    predict(two_class_mod, testing_set, type = \"pred_int\", std_error = TRUE)\n  )\n```\n:::\n\n\nFor our example where the model and data are well behaved, @fig-std-errors shows the standard error of prediction across the space:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/fig-std-errors_e84c6b24eee108fbe1acfc30acecf4dd'}\n::: {.cell-output-display}\n![The effect of the standard error of prediction overlaid with the test set data](19-when-should-you-trust-predictions_files/figure-html/fig-std-errors-1.png){#fig-std-errors fig-align='center' fig-alt='The effect of the standard error of prediction overlaid with the test set data. The region of large variation is very similar to the class boundary space. Additionally, there is a large amount of variation to the west of the inflection point of the boundary curve.' width=70%}\n:::\n:::\n\n\nUsing the standard error as a measure to preclude samples from being predicted can also be applied to models with numeric outcomes. However, as shown in the next section, this may not always work.\n\n## Determining Model Applicability {#sec-applicability-domains}\n\nEquivocal zones try to measure the reliability of a prediction based on the model outputs. It may be that model statistics, such as the standard error of prediction, cannot measure the impact of extrapolation, and so we need another way to assess whether to trust a prediction and answer, \"Is our model applicable for predicting a specific data point?\" Let's take the Chicago train data used extensively in [Kuhn and Johnson (2019)](https://bookdown.org/max/FES/chicago-intro.html) and first shown in @sec-examples-of-tidyverse-syntax. The goal is to predict the number of customers entering the Clark and Lake train station each day.\n\nThe data set in the <span class=\"pkg\">modeldata</span> package (a tidymodels package with example data sets) has daily values between enero 22, 2001 and agosto 28, 2016. Let's create a small test set using the last two weeks of the data:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-chicago-data_79a404e0d21fbde172e7008714330d3e'}\n\n```{.r .cell-code}\n## loads both `Chicago` data set as well as `stations`\ndata(Chicago)\n\nChicago <- Chicago %>% select(ridership, date, one_of(stations))\n\nn <- nrow(Chicago)\n\nChicago_train <- Chicago %>% slice(1:(n - 14))\nChicago_test  <- Chicago %>% slice((n - 13):n)\n```\n:::\n\n\nThe main predictors are lagged ridership data at different train stations, including Clark and Lake, as well as the date. The ridership predictors are highly correlated with one another. In the following recipe, the date column is expanded into several new features, and the ridership predictors are represented using partial least squares (PLS) components. PLS [@Geladi:1986], as we discussed in Section \\@ref(partial-least-squares), is a supervised version of principal component analysis where the new features have been decorrelated but are predictive of the outcome data.\n\nUsing the preprocessed data, we fit a standard linear model:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-chicago-model_637a64821bb63502a6542b3df01786b5'}\n\n```{.r .cell-code}\nbase_recipe <-\n  recipe(ridership ~ ., data = Chicago_train) %>%\n  # Create date features\n  step_date(date) %>%\n  step_holiday(date, keep_original_cols = FALSE) %>%\n  # Create dummy variables from factor columns\n  step_dummy(all_nominal()) %>%\n  # Remove any columns with a single unique value\n  step_zv(all_predictors()) %>%\n  step_normalize(!!!stations)%>%\n  step_pls(!!!stations, num_comp = 10, outcome = vars(ridership))\n\nlm_spec <-\n  linear_reg() %>%\n  set_engine(\"lm\") \n\nlm_wflow <-\n  workflow() %>%\n  add_recipe(base_recipe) %>%\n  add_model(lm_spec)\n\nset.seed(1902)\nlm_fit <- fit(lm_wflow, data = Chicago_train)\n```\n:::\n\n\nHow well do the data fit on the test set? We can `predict()` for the test set to find both predictions and prediction intervals:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-chicago-test-res_b774bf811d8179cd766398cee4001db9'}\n\n```{.r .cell-code}\nres_test <-\n  predict(lm_fit, Chicago_test) %>%\n  bind_cols(\n    predict(lm_fit, Chicago_test, type = \"pred_int\"),\n    Chicago_test\n  )\n\nres_test %>% select(date, ridership, starts_with(\".pred\"))\n## # A tibble: 14 × 5\n##   date       ridership .pred .pred_lower .pred_upper\n##   <date>         <dbl> <dbl>       <dbl>       <dbl>\n## 1 2016-08-15     20.6  20.3        16.2         24.5\n## 2 2016-08-16     21.0  21.3        17.1         25.4\n## 3 2016-08-17     21.0  21.4        17.3         25.6\n## 4 2016-08-18     21.3  21.4        17.3         25.5\n## 5 2016-08-19     20.4  20.9        16.7         25.0\n## 6 2016-08-20      6.22  7.52        3.34        11.7\n## # ℹ 8 more rows\nres_test %>% rmse(ridership, .pred)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard       0.865\n```\n:::\n\n\nThese are fairly good results. @fig-chicago-2016 visualizes the predictions along with 95% prediction intervals.\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/fig-chicago-2016_2b688e3d92251a242a3fb5f23d272b40'}\n::: {.cell-output-display}\n![Two weeks of 2016 predictions for the Chicago data along with 95% prediction intervals](19-when-should-you-trust-predictions_files/figure-html/fig-chicago-2016-1.png){#fig-chicago-2016 fig-align='center' fig-alt='Two weeks of 2016 predictions for the Chicago data along with 95% prediction intervals. The model fit the data fairly well with reasonable error estimates.' width=80%}\n:::\n:::\n\n\nGiven the scale of the ridership numbers, these results look particularly good for such a simple model. If this model were deployed, how well would it have done a few years later in June 2020? The model successfully makes a prediction, as a predictive model almost always will when given input data:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-chicago-2020-res_1ee8d4f33fb31e1a3992bb02b2de0cf7'}\n\n```{.r .cell-code}\nres_2020 <-\n  predict(lm_fit, Chicago_2020) %>%\n  bind_cols(\n    predict(lm_fit, Chicago_2020, type = \"pred_int\"),\n    Chicago_2020\n  ) \n\nres_2020 %>% select(date, contains(\".pred\"))\n## # A tibble: 14 × 4\n##   date       .pred .pred_lower .pred_upper\n##   <date>     <dbl>       <dbl>       <dbl>\n## 1 2020-06-01 20.1        15.9         24.3\n## 2 2020-06-02 21.4        17.2         25.6\n## 3 2020-06-03 21.5        17.3         25.6\n## 4 2020-06-04 21.3        17.1         25.4\n## 5 2020-06-05 20.7        16.6         24.9\n## 6 2020-06-06  9.04        4.88        13.2\n## # ℹ 8 more rows\n```\n:::\n\n\nThe prediction intervals are about the same width, even though these data are well beyond the time period of the original training set. However, given the global pandemic in 2020, the performance on these data are abysmal:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-chicago-2020-stats_92fd9892f02fbb79c8f47d9b0b5a1938'}\n\n```{.r .cell-code}\nres_2020 %>% select(date, ridership, starts_with(\".pred\"))\n## # A tibble: 14 × 5\n##   date       ridership .pred .pred_lower .pred_upper\n##   <date>         <dbl> <dbl>       <dbl>       <dbl>\n## 1 2020-06-01     0.002 20.1        15.9         24.3\n## 2 2020-06-02     0.005 21.4        17.2         25.6\n## 3 2020-06-03     0.566 21.5        17.3         25.6\n## 4 2020-06-04     1.66  21.3        17.1         25.4\n## 5 2020-06-05     1.95  20.7        16.6         24.9\n## 6 2020-06-06     1.08   9.04        4.88        13.2\n## # ℹ 8 more rows\nres_2020 %>% rmse(ridership, .pred)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard        17.2\n```\n:::\n\n\nYou can see this terrible model performance visually in @fig-chicago-2020.\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/fig-chicago-2020_93ea7b6e985ed24c7a1cc670e8d7ce0f'}\n::: {.cell-output-display}\n![Two weeks of 2020 predictions for the Chicago data along with 95% prediction intervals](19-when-should-you-trust-predictions_files/figure-html/fig-chicago-2020-1.png){#fig-chicago-2020 fig-align='center' fig-alt='Two weeks of 2016 predictions for the Chicago data along with 95% prediction intervals. The model fit the data fairly well with reasonable error estimates.' width=80%}\n:::\n:::\n\n\nConfidence and prediction intervals for linear regression expand as the data become more and more removed from the center of the training set. However, that effect is not dramatic enough to flag these predictions as being poor.\n\n::: rmdwarning\nSometimes the statistics produced by models don't measure the quality of predictions very well.\n:::\n\nThis situation can be avoided by having a secondary methodology that can quantify how applicable the model is for any new prediction (i.e., the model's *applicability domain*). There are a variety of methods to compute an applicability domain model, such as @Jaworska or @Netzeva. The approach used in this chapter is a fairly simple unsupervised method that attempts to measure how much (if any) a new data point is beyond the training data.[^19-when-should-you-trust-predictions-1]\n\n[^19-when-should-you-trust-predictions-1]: @Bartley shows yet another method and applies it to ecological studies.\n\n::: rmdnote\nThe idea is to accompany a prediction with a score that measures how similar the new point is to the training set.\n:::\n\nOne method that works well uses principal component analysis (PCA) on the numeric predictor values. We'll illustrate the process by using only two of the predictors that correspond to ridership at different stations (California and Austin stations). The training set are shown in panel (a) in @fig-pca-reference-dist. The ridership data for these stations are highly correlated, and the two distributions shown in the scatter plot correspond to ridership on the weekends and week days.\n\nThe first step is to conduct PCA on the training data. The PCA scores for the training set are shown in panel (b) in @fig-pca-reference-dist. Next, using these results, we measure the distance of each training set point to the center of the PCA data (panel (c) of @fig-pca-reference-dist). We can then use this *reference distribution* (panel (d) of @fig-pca-reference-dist) to estimate how far a data point is from the mainstream of the training data.\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/fig-pca-reference-dist_c797a62935838ddbbb765f14699e84bd'}\n::: {.cell-output-display}\n![The PCA reference distribution based on the training set](19-when-should-you-trust-predictions_files/figure-html/fig-pca-reference-dist-1.png){#fig-pca-reference-dist fig-align='center' fig-alt='The PCA reference distribution based on the training set. The majority of the distances to the center of the PCA distribution are below a value of three.' width=100%}\n:::\n:::\n\n\nFor a new sample, the PCA scores are computed along with the distance to the center of the training set.\n\nHowever, what does it mean when a new sample has a distance of *X*? Since the PCA components can have different ranges from data set to data set, there is no obvious limit to say that a distance is too large.\n\nOne approach is to treat the distances from the training set data as \"normal.\" For new samples, we can determine how the new distance compares to the range in the reference distribution (from the training set). A percentile can be computed for new samples that reflect how much of the training set is less extreme than the new samples.\n\n::: rmdnote\nA percentile of 90% means that most of the training set data are closer to the data center than the new sample.\n:::\n\nThe plot in @fig-two-new-points overlays a testing set sample (triangle and dashed line) and a 2020 sample (circle and solid line) with the PCA distances from the training set.\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/fig-two-new-points_f5c639c48b8914c94a099f216651999a'}\n::: {.cell-output-display}\n![The reference distribution with two new points: one using the test set and one from the 2020 data](19-when-should-you-trust-predictions_files/figure-html/fig-two-new-points-1.png){#fig-two-new-points fig-align='center' fig-alt='The reference distribution with two new points: one using the test set and one from the 2020 data. The test set point is snugly within the data mainstream while the 2020 point is outside of the reference distribution.' width=100%}\n:::\n:::\n\n\nThe test set point has a distance of 1.28. It is in the 51.8% percentile of the training set distribution, indicating that it is snugly within the mainstream of the training set.\n\nThe 2020 sample is farther from the center than any of the training set samples (with a percentile of 100%). This indicates the sample is very extreme and that its corresponding prediction would be a severe extrapolation (and probably should not be reported).\n\nThe <span class=\"pkg\">applicable</span> package can develop an applicability domain model using PCA. We'll use the 20 lagged station ridership predictors as inputs into the PCA analysis. There is an additional argument called `threshold` that determines how many components are used in the distance calculation. For our example, we'll use a large value that indicates we should use enough components to account for 99% of the variation in the ridership predictors:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-apd-pca_68d6db190272d0b3c1c038654fd0ad20'}\n\n```{.r .cell-code}\nlibrary(applicable)\npca_stat <- apd_pca(~ ., data = Chicago_train %>% select(one_of(stations)), \n                    threshold = 0.99)\npca_stat\n## # Predictors:\n##    20\n## # Principal Components:\n##    9 components were needed\n##    to capture at least 99% of the\n##    total variation in the predictors.\n```\n:::\n\n\nThe `autoplot()` method plots the reference distribution. It has an optional argument for which data to plot. We'll add a value of `distance` to plot only the training set distance distribution. This code generates the plot in @fig-ap-autoplot:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-ref-dist_d740f615fbdd1187d175ca6ffbe8a3e3'}\n\n```{.r .cell-code}\nautoplot(pca_stat, distance) + labs(x = \"distance\")\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/fig-ap-autoplot_74f7578466ee19e8809d8d20cd365b8f'}\n::: {.cell-output-display}\n![The results of using the `autoplot()` method on an applicable object](19-when-should-you-trust-predictions_files/figure-html/fig-ap-autoplot-1.png){#fig-ap-autoplot fig-align='center' fig-alt='The results of using the `autoplot()` method on an applicable object.' width=70%}\n:::\n:::\n\n\nThe x-axis shows the values of the distance and the y-axis displays the distribution's percentiles. For example, half of the training set samples had distances less than 3.7.\n\nTo compute the percentiles for new data, the `score()` function works in the same way as `predict()`:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-apd-test-scores_e5975e0df542a60a214e35257c8bb161'}\n\n```{.r .cell-code}\nscore(pca_stat, Chicago_test) %>% select(starts_with(\"distance\"))\n## # A tibble: 14 × 2\n##   distance distance_pctl\n##      <dbl>         <dbl>\n## 1     4.88          66.7\n## 2     5.21          71.4\n## 3     5.19          71.1\n## 4     5.00          68.5\n## 5     4.36          59.3\n## 6     4.10          55.2\n## # ℹ 8 more rows\n```\n:::\n\n\nThese seem fairly reasonable. For the 2020 data:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-apd-2020-scores_d7c32eb6e0ac65d165e8ac8ab85d6318'}\n\n```{.r .cell-code}\nscore(pca_stat, Chicago_2020) %>% select(starts_with(\"distance\"))\n## # A tibble: 14 × 2\n##   distance distance_pctl\n##      <dbl>         <dbl>\n## 1     9.39          99.8\n## 2     9.40          99.8\n## 3     9.30          99.7\n## 4     9.30          99.7\n## 5     9.29          99.7\n## 6    10.1            1  \n## # ℹ 8 more rows\n```\n:::\n\n\nThe 2020 distance values indicate that these predictor values are outside of the vast majority of data seen by the model at training time. These should be flagged so that the predictions are either not reported at all or viewed with skepticism.\n\n::: rmdnote\nOne important aspect of this analysis concerns which predictors are used to develop the applicability domain model. In our analysis, we used the raw predictor columns. However, in building the model, PLS score features were used in their place. Which of these should `apd_pca()` use? The `apd_pca()` function can also take a recipe as the input (instead of a formula) so that the distances reflect the PLS scores instead of the individual predictor columns. You can evaluate both methods to understand which one gives more relevant results.\n:::\n\n## Chapter Summary {#sec-trust-summary}\n\nThis chapter showed two methods for evaluating whether predictions should be reported to the consumers of models. Equivocal zones deal with outcomes/predictions and can be helpful when the amount of uncertainty in a prediction is too large.\n\nApplicability domain models deal with features/predictors and quantify the amount of extrapolation (if any) that occurs when making a prediction. This chapter showed a basic method using principal component analysis, although there are many other ways to measure applicability. The <span class=\"pkg\">applicable</span> package also contains specialized methods for data sets where all of the predictors are binary. This method computes similarity scores between training set data points to define the reference distribution.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}