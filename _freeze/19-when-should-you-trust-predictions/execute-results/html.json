{
  "hash": "a8a7038bda4ba0fce46c2b5e10b149a8",
  "result": {
    "markdown": "\n\n\n# ¿Cuándo debería confiar en sus predicciones? {#sec-trust}\n\nUn modelo predictivo casi siempre puede producir una predicción, dados los datos de entrada. Sin embargo, en muchas situaciones resulta inapropiado realizar tal predicción. Cuando un nuevo punto de datos está muy fuera del rango de datos utilizados para crear el modelo, hacer una predicción puede ser una *extrapolación* inapropiada. Un ejemplo más cualitativo de una predicción inapropiada sería cuando el modelo se utiliza en un contexto completamente diferente. Los datos de segmentación celular utilizados en el [Capítulo @sec-iterative-search] indican cuándo las células de cáncer de mama humano pueden o no aislarse con precisión dentro de una imagen. Un modelo construido a partir de estos datos podría aplicarse de manera inapropiada a las células del estómago con el mismo propósito. Podemos producir una predicción, pero es poco probable que sea aplicable a los diferentes tipos de células.\n\nEste capítulo analiza dos métodos para cuantificar la calidad potencial de una predicción:\n\n-   Las *zonas equívocas* utilizan los valores previstos para alertar al usuario de que los resultados pueden ser sospechosos.\n-   *Aplicabilidad* utiliza los predictores para medir la cantidad de extrapolación (si la hay) para nuevas muestras.\n\n## Resultados equívocos {#sec-equivocal-zones}\n\n::: rmdwarning\nEn algunos casos, la cantidad de incertidumbre asociada con una predicción es demasiado alta para confiar en ella.\n:::\n\nSi el resultado de un modelo indicara que usted tenía un 51% de posibilidades de haber contraído COVID-19, sería natural ver el diagnóstico con cierto escepticismo. De hecho, los organismos reguladores suelen exigir que muchos diagnósticos médicos tengan una *zona equívoca*. Esta zona es un rango de resultados en los que la predicción no debe informarse a los pacientes, por ejemplo, algún rango de resultados de pruebas de COVID-19 que son demasiado inciertos para informarse a un paciente. Consulte @Danowski524 y @Kerleguer1783 para ver ejemplos. La misma noción se puede aplicar a modelos creados fuera del diagnóstico médico.\n\nUsemos una función que pueda simular datos de clasificación con dos clases y dos predictores (`x` e `y`). El verdadero modelo es un modelo de regresión logística con la ecuación:\n\n$$\n\\mathrm{logit}(p) = -1 - 2x - \\frac{x^2}{5} + 2y^2 \n$$\n\nLos dos predictores siguen una distribución normal bivariada con una correlación de 0,70. Crearemos un conjunto de entrenamiento de 200 muestras y un conjunto de prueba de 50:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-simulation_930511c4f6119770600410423acf0f57'}\n\n```{.r .cell-code}\nlibrary(tidymodels)\ntidymodels_prefer()\n\nsimulate_two_classes <- \n  function (n, error = 0.1, eqn = quote(-1 - 2 * x - 0.2 * x^2 + 2 * y^2))  {\n    # Predictores ligeramente correlacionados\n    sigma <- matrix(c(1, 0.7, 0.7, 1), nrow = 2, ncol = 2)\n    dat <- MASS::mvrnorm(n = n, mu = c(0, 0), Sigma = sigma)\n    colnames(dat) <- c(\"x\", \"y\")\n    cls <- paste0(\"class_\", 1:2)\n    dat <- \n      as_tibble(dat) %>% \n      mutate(\n        linear_pred = !!eqn,\n        # Agregue algo de ruido de clasificación errónea\n        linear_pred = linear_pred + rnorm(n, sd = error),\n        prob = binomial()$linkinv(linear_pred),\n        class = ifelse(prob > runif(n), cls[1], cls[2]),\n        class = factor(class, levels = cls)\n      )\n    dplyr::select(dat, x, y, class)\n  }\n\nset.seed(1901)\ntraining_set <- simulate_two_classes(200)\ntesting_set  <- simulate_two_classes(50)\n```\n:::\n\n\nEstimamos un modelo de regresión logística utilizando métodos bayesianos (utilizando las distribuciones previas gaussianas predeterminadas para los parámetros):\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-bayes-glm_0ba020bc077240480ba10228cec64312'}\n\n```{.r .cell-code}\ntwo_class_mod <- \n  logistic_reg() %>% \n  set_engine(\"stan\", seed = 1902) %>% \n  fit(class ~ . + I(x^2)+ I(y^2), data = training_set)\nprint(two_class_mod, digits = 3)\n## parsnip model object\n## \n## stan_glm\n##  family:       binomial [logit]\n##  formula:      class ~ . + I(x^2) + I(y^2)\n##  observations: 200\n##  predictors:   5\n## ------\n##             Median MAD_SD\n## (Intercept)  1.092  0.287\n## x            2.290  0.423\n## y            0.314  0.354\n## I(x^2)       0.077  0.307\n## I(y^2)      -2.465  0.424\n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\n```\n:::\n\n\nEl límite de clase ajustado se superpone al conjunto de prueba en @fig-glm-boundaries. Los puntos de datos más cercanos al límite de clase son los más inciertos. Si sus valores cambiaran ligeramente, su clase prevista podría cambiar. Un método simple para descalificar algunos resultados es llamarlos \"equívocos\" si los valores están dentro de algún rango alrededor del 50% (o el límite de probabilidad apropiado para una situación determinada). Dependiendo del problema al que se aplique el modelo, esto podría indicar que debemos recopilar otra medición o que necesitamos más información antes de que sea posible una predicción confiable.\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/fig-glm-boundaries_069884f070a289d97840f43ac6465797'}\n::: {.cell-output-display}\n![Conjunto de datos simulado de dos clases con ajuste de regresión logística y límite de decisión.](19-when-should-you-trust-predictions_files/figure-html/fig-glm-boundaries-1.png){#fig-glm-boundaries fig-align='center' fig-alt='Conjunto de datos simulado de dos clases con ajuste de regresión logística y límite de decisión. El diagrama de dispersión de las dos clases muestra datos bastante correlacionados. El límite de decisión es una parábola en el eje x que hace un buen trabajo al separar las clases.' width=70%}\n:::\n:::\n\n\nPodríamos basar el ancho de la banda alrededor del límite en cómo mejora el rendimiento cuando se eliminan los resultados inciertos. Sin embargo, también debemos estimar la tasa reportable (la proporción esperada de resultados utilizables). Por ejemplo, no sería útil en situaciones del mundo real tener un rendimiento perfecto pero publicar predicciones en solo el 2% de las muestras pasadas al modelo.\n\nUtilicemos el conjunto de pruebas para determinar el equilibrio entre mejorar el rendimiento y tener suficientes resultados reportables. Las predicciones se crean usando:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-bayes-glm-pred_bb55a105d49208ddae015ca155cac098'}\n\n```{.r .cell-code}\ntest_pred <- augment(two_class_mod, testing_set)\ntest_pred %>% head()\n## # A tibble: 6 × 6\n##   .pred_class .pred_class_1 .pred_class_2      x      y class  \n##   <fct>               <dbl>         <dbl>  <dbl>  <dbl> <fct>  \n## 1 class_2           0.0256          0.974  1.12  -0.176 class_2\n## 2 class_1           0.555           0.445 -0.126 -0.582 class_2\n## 3 class_2           0.00620         0.994  1.92   0.615 class_2\n## 4 class_2           0.472           0.528 -0.400  0.252 class_2\n## 5 class_2           0.163           0.837  1.30   1.09  class_1\n## 6 class_2           0.0317          0.968  2.59   1.36  class_2\n```\n:::\n\n\nCon tidymodels, el paquete <span class=\"pkg\">probably</span> contiene funciones para zonas equívocas. Para casos con dos clases, la función `make_two_class_pred()` crea una columna similar a un factor que tiene las clases predichas con una zona equívoca:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-make-eq_62e6d28aa388291d4f811d69e441f770'}\n\n```{.r .cell-code}\nlibrary(probably)\n\nlvls <- levels(training_set$class)\n\ntest_pred <- \n  test_pred %>% \n  mutate(.pred_with_eqz = make_two_class_pred(.pred_class_1, lvls, buffer = 0.15))\n\ntest_pred %>% count(.pred_with_eqz)\n## # A tibble: 3 × 2\n##   .pred_with_eqz     n\n##       <clss_prd> <int>\n## 1           [EQ]     9\n## 2        class_1    20\n## 3        class_2    21\n```\n:::\n\n\nRows that are within $0.50\\pm0.15$ are given a value of `[EQ]`.\n\n::: rmdnote\nLa notación `[EQ]` en este ejemplo no es un nivel de factor sino un atributo de esa columna.\n:::\n\nDado que los niveles de los factores son los mismos que los de los datos originales, las matrices de confusión y otras estadísticas se pueden calcular sin errores. Cuando se utilizan funciones estándar del paquete <span class=\"pkg\">yardstick</span>, los resultados equívocos se convierten a `NA` y no se utilizan en los cálculos que utilizan predicciones de clases estrictas. Observe las diferencias en estas matrices de confusión:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-conf-mat_e7f0758ecd6b1feb65c6235e78be8b29'}\n\n```{.r .cell-code}\n# Toda la información\ntest_pred %>% conf_mat(class, .pred_class)\n##           Truth\n## Prediction class_1 class_2\n##    class_1      20       6\n##    class_2       5      19\n\n# Sólo resultados reportables:\ntest_pred %>% conf_mat(class, .pred_with_eqz)\n##           Truth\n## Prediction class_1 class_2\n##    class_1      17       3\n##    class_2       5      16\n```\n:::\n\n\nTambién está disponible una función `is_equivocal()` para filtrar estas filas de los datos.\n\n¿La zona equívoca ayuda a mejorar la precisión? Veamos diferentes tamaños de búfer, como se muestra en @fig-equivocal-zone-results:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-eq-calcs_5414687198a814e747ef847dae53490a'}\n\n```{.r .cell-code}\n# Una función para cambiar el búfer y luego calcular el rendimiento.\neq_zone_results <- function(buffer) {\n  test_pred <- \n    test_pred %>% \n    mutate(.pred_with_eqz = make_two_class_pred(.pred_class_1, lvls, buffer = buffer))\n  acc <- test_pred %>% accuracy(class, .pred_with_eqz)\n  rep_rate <- reportable_rate(test_pred$.pred_with_eqz)\n  tibble(accuracy = acc$.estimate, reportable = rep_rate, buffer = buffer)\n}\n\n# Evaluar una secuencia de buffers y trazar los resultados.\nmap(seq(0, .1, length.out = 40), eq_zone_results) %>% \n  list_rbind() %>% \n  pivot_longer(c(-buffer), names_to = \"statistic\", values_to = \"value\") %>% \n  ggplot(aes(x = buffer, y = value, lty = statistic)) + \n  geom_step(linewidth = 1.2, alpha = 0.8) + \n  labs(y = NULL, lty = NULL)\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/fig-equivocal-zone-results_7497b534d7c5798eae1899fd8a75e157'}\n::: {.cell-output-display}\n![El efecto de las zonas equívocas en el rendimiento del modelo.](19-when-should-you-trust-predictions_files/figure-html/fig-equivocal-zone-results-1.png){#fig-equivocal-zone-results fig-align='center' fig-alt='El efecto de las zonas equívocas en el rendimiento del modelo. Hay un ligero aumento en la precisión a expensas de una tasa de declaración decreciente.' width=80%}\n:::\n:::\n\n\n@fig-equivocal-zone-results nos muestra que la precisión mejora en unos pocos puntos porcentuales, ¡pero a costa de que casi el 10% de las predicciones sean inutilizables! El valor de tal compromiso depende de cómo se utilizarán las predicciones del modelo.\n\nEste análisis se centró en utilizar la probabilidad de clase prevista para descalificar puntos, ya que esta es una medida fundamental de incertidumbre en los modelos de clasificación. Un enfoque ligeramente mejor sería utilizar el error estándar de la probabilidad de clase. Dado que utilizamos un modelo bayesiano, las estimaciones de probabilidad que encontramos son en realidad la media de la distribución predictiva posterior. En otras palabras, el modelo bayesiano nos da una distribución para la probabilidad de clase. Medir la desviación estándar de esta distribución nos da un *error estándar de predicción* de la probabilidad. En la mayoría de los casos, este valor está directamente relacionado con la probabilidad de clase media. Quizás recuerdes que, para una variable aleatoria de Bernoulli con probabilidad $p$, la varianza es $p(1-p)$. Debido a esta relación, el error estándar es mayor cuando la probabilidad es del 50%. En lugar de asignar un resultado equívoco utilizando la probabilidad de clase, podríamos utilizar un límite en el error estándar de predicción.\n\nUn aspecto importante del error estándar de predicción es que tiene en cuenta algo más que la probabilidad de clase. En los casos en los que hay una extrapolación significativa o valores predictivos aberrantes, el error estándar podría aumentar. El beneficio de utilizar el error estándar de predicción es que también podría señalar predicciones que son problemáticas (en lugar de simplemente inciertas). Una razón por la que utilizamos el modelo bayesiano es que estima naturalmente el error estándar de predicción; No muchos modelos pueden calcular esto. Para nuestro conjunto de prueba, usar `type = \"pred_int\"` producirá límites superior e inferior y `std_error` agrega una columna para esa cantidad. Para intervalos del 80%:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-pred-int_9b2b3cb54ab796ab2bf1820435861f47'}\n\n```{.r .cell-code}\ntest_pred <- \n  test_pred %>% \n  bind_cols(\n    predict(two_class_mod, testing_set, type = \"pred_int\", std_error = TRUE)\n  )\n```\n:::\n\n\nPara nuestro ejemplo donde el modelo y los datos se comportan bien, @fig-std-errors muestra el error estándar de predicción en todo el espacio:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/fig-std-errors_46c61ef178cb9dd7d60e236ecc1e09a6'}\n::: {.cell-output-display}\n![El efecto del error estándar de predicción superpuesto con los datos del conjunto de pruebas](19-when-should-you-trust-predictions_files/figure-html/fig-std-errors-1.png){#fig-std-errors fig-align='center' fig-alt='El efecto del error estándar de predicción superpuesto con los datos del conjunto de pruebas. La región de gran variación es muy similar al espacio límite de clases. Además, hay una gran variación hacia el oeste del punto de inflexión de la curva límite.' width=70%}\n:::\n:::\n\n\nEl uso del error estándar como medida para evitar que se predigan muestras también se puede aplicar a modelos con resultados numéricos. Sin embargo, como se muestra en la siguiente sección, es posible que esto no siempre funcione.\n\n## Determinación de la aplicabilidad del modelo {#sec-applicability-domains}\n\nLas zonas equívocas intentan medir la confiabilidad de una predicción basada en los resultados del modelo. Puede ser que las estadísticas del modelo, como el error estándar de predicción, no puedan medir el impacto de la extrapolación, por lo que necesitamos otra forma de evaluar si debemos confiar en una predicción y responder: \"¿Es nuestro modelo aplicable para predecir un punto de datos específico? \" Tomemos los datos del tren de Chicago utilizados ampliamente en [Kuhn y Johnson (2019)](https://bookdown.org/max/FES/chicago-intro.html) y mostrados por primera vez en @sec-examples-of-tidyverse-syntax . El objetivo es predecir la cantidad de clientes que ingresan a la estación de tren de Clark y Lake cada día.\n\nEl conjunto de datos en el paquete <span class=\"pkg\">modeldata</span> (un paquete tidymodels con conjuntos de datos de ejemplo) tiene valores diarios entre enero 22, 2001 y ` formato r(max(Chicago$fecha), \"%B %d, %Y\")`. Creemos un pequeño conjunto de pruebas utilizando las últimas dos semanas de datos:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-chicago-data_f32d5a3d851efaab82e59bedfb4bd59b'}\n\n```{.r .cell-code}\n## loads tanto el conjunto de datos de \"Chicago\" como las \"estaciones\"\ndata(Chicago)\n\nChicago <- Chicago %>% select(ridership, date, one_of(stations))\n\nn <- nrow(Chicago)\n\nChicago_train <- Chicago %>% slice(1:(n - 14))\nChicago_test  <- Chicago %>% slice((n - 13):n)\n```\n:::\n\n\nLos principales predictores son los datos retrasados ​​sobre el número de pasajeros en diferentes estaciones de tren, incluidas Clark y Lake, así como la fecha. Los predictores de número de pasajeros están altamente correlacionados entre sí. En la siguiente receta, la columna de fecha se amplía con varias características nuevas y los predictores de número de pasajeros se representan mediante componentes de mínimos cuadrados parciales (PLS). PLS [@Geladi:1986], como comentamos en @sec-partial-least-squares, es una versión supervisada del análisis de componentes principales donde las nuevas características han sido descorrelacionadas pero predicen los datos de resultado.\n\nUtilizando los datos preprocesados, ajustamos un modelo lineal estándar:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-chicago-model_7a9459125564078878d90a8a1111211d'}\n\n```{.r .cell-code}\nbase_recipe <-\n  recipe(ridership ~ ., data = Chicago_train) %>%\n  # Crear funciones de fecha\n  step_date(date) %>%\n  step_holiday(date, keep_original_cols = FALSE) %>%\n  # Crear variables ficticias a partir de columnas de factores\n  step_dummy(all_nominal()) %>%\n  # Elimine cualquier columna con un único valor único\n  step_zv(all_predictors()) %>%\n  step_normalize(!!!stations)%>%\n  step_pls(!!!stations, num_comp = 10, outcome = vars(ridership))\n\nlm_spec <-\n  linear_reg() %>%\n  set_engine(\"lm\") \n\nlm_wflow <-\n  workflow() %>%\n  add_recipe(base_recipe) %>%\n  add_model(lm_spec)\n\nset.seed(1902)\nlm_fit <- fit(lm_wflow, data = Chicago_train)\n```\n:::\n\n\n¿Qué tan bien encajan los datos en el conjunto de prueba? Podemos `predecit()` para que el conjunto de pruebas encuentre tanto predicciones como intervalos de predicción:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-chicago-test-res_b774bf811d8179cd766398cee4001db9'}\n\n```{.r .cell-code}\nres_test <-\n  predict(lm_fit, Chicago_test) %>%\n  bind_cols(\n    predict(lm_fit, Chicago_test, type = \"pred_int\"),\n    Chicago_test\n  )\n\nres_test %>% select(date, ridership, starts_with(\".pred\"))\n## # A tibble: 14 × 5\n##   date       ridership .pred .pred_lower .pred_upper\n##   <date>         <dbl> <dbl>       <dbl>       <dbl>\n## 1 2016-08-15     20.6  20.3        16.2         24.5\n## 2 2016-08-16     21.0  21.3        17.1         25.4\n## 3 2016-08-17     21.0  21.4        17.3         25.6\n## 4 2016-08-18     21.3  21.4        17.3         25.5\n## 5 2016-08-19     20.4  20.9        16.7         25.0\n## 6 2016-08-20      6.22  7.52        3.34        11.7\n## # ℹ 8 more rows\nres_test %>% rmse(ridership, .pred)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard       0.865\n```\n:::\n\n\nEstos son resultados bastante buenos. @fig-chicago-2016 visualiza las predicciones junto con intervalos de predicción del 95%.\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/fig-chicago-2016_159ee0836b5130a121949c344ebeb4fa'}\n::: {.cell-output-display}\n![Dos semanas de predicciones de 2016 para los datos de Chicago junto con intervalos de predicción del 95%](19-when-should-you-trust-predictions_files/figure-html/fig-chicago-2016-1.png){#fig-chicago-2016 fig-align='center' fig-alt='Dos semanas de predicciones de 2016 para los datos de Chicago junto con intervalos de predicción del 95%. El modelo se ajusta bastante bien a los datos con estimaciones de error razonables.' width=80%}\n:::\n:::\n\n\nDada la escala del número de usuarios, estos resultados parecen particularmente buenos para un modelo tan simple. Si se hubiera implementado este modelo, ¿qué tan bien habría funcionado unos años más tarde, en junio de 2020? El modelo realiza con éxito una predicción, como casi siempre lo hace un modelo predictivo cuando se le dan datos de entrada:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-chicago-2020-res_1ee8d4f33fb31e1a3992bb02b2de0cf7'}\n\n```{.r .cell-code}\nres_2020 <-\n  predict(lm_fit, Chicago_2020) %>%\n  bind_cols(\n    predict(lm_fit, Chicago_2020, type = \"pred_int\"),\n    Chicago_2020\n  ) \n\nres_2020 %>% select(date, contains(\".pred\"))\n## # A tibble: 14 × 4\n##   date       .pred .pred_lower .pred_upper\n##   <date>     <dbl>       <dbl>       <dbl>\n## 1 2020-06-01 20.1        15.9         24.3\n## 2 2020-06-02 21.4        17.2         25.6\n## 3 2020-06-03 21.5        17.3         25.6\n## 4 2020-06-04 21.3        17.1         25.4\n## 5 2020-06-05 20.7        16.6         24.9\n## 6 2020-06-06  9.04        4.88        13.2\n## # ℹ 8 more rows\n```\n:::\n\n\nLos intervalos de predicción tienen aproximadamente el mismo ancho, aunque estos datos están mucho más allá del período de tiempo del conjunto de entrenamiento original. Sin embargo, dada la pandemia mundial en 2020, el desempeño según estos datos es pésimo:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-chicago-2020-stats_92fd9892f02fbb79c8f47d9b0b5a1938'}\n\n```{.r .cell-code}\nres_2020 %>% select(date, ridership, starts_with(\".pred\"))\n## # A tibble: 14 × 5\n##   date       ridership .pred .pred_lower .pred_upper\n##   <date>         <dbl> <dbl>       <dbl>       <dbl>\n## 1 2020-06-01     0.002 20.1        15.9         24.3\n## 2 2020-06-02     0.005 21.4        17.2         25.6\n## 3 2020-06-03     0.566 21.5        17.3         25.6\n## 4 2020-06-04     1.66  21.3        17.1         25.4\n## 5 2020-06-05     1.95  20.7        16.6         24.9\n## 6 2020-06-06     1.08   9.04        4.88        13.2\n## # ℹ 8 more rows\nres_2020 %>% rmse(ridership, .pred)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   <chr>   <chr>          <dbl>\n## 1 rmse    standard        17.2\n```\n:::\n\n\nPuedes ver visualmente el terrible rendimiento de este modelo en @fig-chicago-2020.\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/fig-chicago-2020_dfc2f3a258b366f6a2207381312fb751'}\n::: {.cell-output-display}\n![Dos semanas de predicciones de 2020 para los datos de Chicago junto con intervalos de predicción del 95%](19-when-should-you-trust-predictions_files/figure-html/fig-chicago-2020-1.png){#fig-chicago-2020 fig-align='center' fig-alt='Dos semanas de predicciones de 2016 para los datos de Chicago junto con intervalos de predicción del 95%. El modelo se ajusta bastante bien a los datos con estimaciones de error razonables.' width=80%}\n:::\n:::\n\n\nLos intervalos de confianza y predicción para la regresión lineal se expanden a medida que los datos se alejan cada vez más del centro del conjunto de entrenamiento. Sin embargo, ese efecto no es lo suficientemente dramático como para señalar que estas predicciones son deficientes.\n\n::: rmdwarning\nA veces las estadísticas producidas por los modelos no miden muy bien la calidad de las predicciones.\n:::\n\nEsta situación se puede evitar teniendo una metodología secundaria que pueda cuantificar qué tan aplicable es el modelo para cualquier nueva predicción (es decir, el *dominio de aplicabilidad* del modelo). Existe una variedad de métodos para calcular un modelo de dominio de aplicabilidad, como @Jaworska o @Netzeva. El enfoque utilizado en este capítulo es un método no supervisado bastante simple que intenta medir cuánto (si lo hay) un nuevo punto de datos está más allá de los datos de entrenamiento.[^19-when-should-you-trust-predictions-1]\n\n[^19-when-should-you-trust-predictions-1]: @Bartley muestra otro método más y lo aplica a estudios ecológicos.\n\n::: rmdnote\nLa idea es acompañar una predicción con una puntuación que mida qué tan similar es el nuevo punto al conjunto de entrenamiento.\n:::\n\nUn método que funciona bien utiliza el análisis de componentes principales (PCA) en los valores predictivos numéricos. Ilustraremos el proceso utilizando sólo dos de los predictores que corresponden al número de pasajeros en diferentes estaciones (estaciones de California y Austin). El conjunto de entrenamiento se muestra en el panel (a) en @fig-pca-reference-dist. Los datos sobre el número de pasajeros de estas estaciones están altamente correlacionados y las dos distribuciones que se muestran en el diagrama de dispersión corresponden al número de pasajeros los fines de semana y días laborables.\n\nEl primer paso es realizar PCA con los datos de entrenamiento. Las puntuaciones de PCA para el conjunto de entrenamiento se muestran en el panel (b) en @fig-pca-reference-dist. A continuación, utilizando estos resultados, medimos la distancia de cada punto de ajuste de entrenamiento al centro de los datos de PCA (panel (c) de @fig-pca-reference-dist). Luego podemos usar esta *distribución de referencia* (panel (d) de @fig-pca-reference-dist) para estimar qué tan lejos está un punto de datos de la corriente principal de los datos de entrenamiento.\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/fig-pca-reference-dist_b179afa5d808ec557d91be2e3d476238'}\n::: {.cell-output-display}\n![La distribución de referencia de PCA basada en el conjunto de entrenamiento.](19-when-should-you-trust-predictions_files/figure-html/fig-pca-reference-dist-1.png){#fig-pca-reference-dist fig-align='center' fig-alt='La distribución de referencia de PCA basada en el conjunto de entrenamiento. La mayoría de las distancias al centro de la distribución PCA están por debajo de un valor de tres.' width=100%}\n:::\n:::\n\n\nPara una nueva muestra, las puntuaciones de PCA se calculan junto con la distancia al centro del conjunto de entrenamiento.\n\nSin embargo, ¿qué significa que una nueva muestra tenga una distancia de *X*? Dado que los componentes de PCA pueden tener diferentes rangos de un conjunto de datos a otro, no existe un límite obvio para decir que una distancia es demasiado grande.\n\nUn enfoque es tratar las distancias de los datos del conjunto de entrenamiento como \"normales\". Para muestras nuevas, podemos determinar cómo se compara la nueva distancia con el rango en la distribución de referencia (del conjunto de entrenamiento). Se puede calcular un percentil para nuevas muestras que refleje qué parte del conjunto de entrenamiento es menos extremo que las nuevas muestras.\n\n::: rmdnote\nUn percentil del 90% significa que la mayoría de los datos del conjunto de entrenamiento están más cerca del centro de datos que la nueva muestra.\n:::\n\nEl gráfico en @fig-two-new-points superpone una muestra del conjunto de prueba (triángulo y línea discontinua) y una muestra de 2020 (círculo y línea continua) con las distancias PCA del conjunto de entrenamiento.\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/fig-two-new-points_9b42f0caf64ba5cd076876cd53df0a3c'}\n::: {.cell-output-display}\n![La distribución de referencia con dos nuevos puntos: uno usando el conjunto de prueba y otro a partir de los datos de 2020](19-when-should-you-trust-predictions_files/figure-html/fig-two-new-points-1.png){#fig-two-new-points fig-align='center' fig-alt='La distribución de referencia con dos nuevos puntos: uno usando el conjunto de prueba y otro a partir de los datos de 2020. El punto de ajuste de la prueba se encuentra perfectamente dentro de la corriente principal de datos, mientras que el punto de 2020 está fuera de la distribución de referencia.' width=100%}\n:::\n:::\n\n\nEl punto de ajuste de prueba tiene una distancia de 1.28. Está en el percentil 51.8% de la distribución del conjunto de entrenamiento, lo que indica que está cómodamente dentro de la corriente principal del conjunto de entrenamiento.\n\nLa muestra de 2020 está más alejada del centro que cualquiera de las muestras del conjunto de entrenamiento (con un percentil de 100%). Esto indica que la muestra es muy extrema y que su predicción correspondiente sería una extrapolación severa (y probablemente no debería informarse).\n\nEl paquete <span class=\"pkg\">applicable</span> puede desarrollar un modelo de dominio de aplicabilidad utilizando PCA. Usaremos los predictores de número de pasajeros de 20 estaciones retrasadas como datos de entrada para el análisis de PCA. Hay un argumento adicional llamado umbral, `threshold`, que determina cuántos componentes se utilizan en el cálculo de la distancia. Para nuestro ejemplo, usaremos un valor grande que indica que debemos usar suficientes componentes para representar el 99 % de la variación en los predictores de número de pasajeros:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-apd-pca_68d6db190272d0b3c1c038654fd0ad20'}\n\n```{.r .cell-code}\nlibrary(applicable)\npca_stat <- apd_pca(~ ., data = Chicago_train %>% select(one_of(stations)), \n                    threshold = 0.99)\npca_stat\n## # Predictors:\n##    20\n## # Principal Components:\n##    9 components were needed\n##    to capture at least 99% of the\n##    total variation in the predictors.\n```\n:::\n\n\nEl método `autoplot()` traza la distribución de referencia. Tiene un argumento opcional para qué datos trazar. Agregaremos un valor de distancia, `distance` para trazar solo la distribución de distancia del conjunto de entrenamiento. Este código genera la trama en @fig-ap-autoplot:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-ref-dist_d740f615fbdd1187d175ca6ffbe8a3e3'}\n\n```{.r .cell-code}\nautoplot(pca_stat, distance) + labs(x = \"distance\")\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/fig-ap-autoplot_4a6f4221d944c9ff715a0b22c2582af9'}\n::: {.cell-output-display}\n![Los resultados de usar el método `autoplot()` en un objeto aplicable](19-when-should-you-trust-predictions_files/figure-html/fig-ap-autoplot-1.png){#fig-ap-autoplot fig-align='center' fig-alt='Los resultados de usar el método `autoplot()` en un objeto aplicable' width=70%}\n:::\n:::\n\n\nEl eje x muestra los valores de la distancia y el eje y muestra los percentiles de la distribución. Por ejemplo, la mitad de las muestras del conjunto de entrenamiento tenían distancias menores que 3.7.\n\nPara calcular los percentiles de datos nuevos, la función `score()` funciona de la misma manera que `predict()`:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-apd-test-scores_e5975e0df542a60a214e35257c8bb161'}\n\n```{.r .cell-code}\nscore(pca_stat, Chicago_test) %>% select(starts_with(\"distance\"))\n## # A tibble: 14 × 2\n##   distance distance_pctl\n##      <dbl>         <dbl>\n## 1     4.88          66.7\n## 2     5.21          71.4\n## 3     5.19          71.1\n## 4     5.00          68.5\n## 5     4.36          59.3\n## 6     4.10          55.2\n## # ℹ 8 more rows\n```\n:::\n\n\nEstos parecen bastante razonables. Para los datos de 2020:\n\n\n::: {.cell layout-align=\"center\" hash='19-when-should-you-trust-predictions_cache/html/trust-apd-2020-scores_d7c32eb6e0ac65d165e8ac8ab85d6318'}\n\n```{.r .cell-code}\nscore(pca_stat, Chicago_2020) %>% select(starts_with(\"distance\"))\n## # A tibble: 14 × 2\n##   distance distance_pctl\n##      <dbl>         <dbl>\n## 1     9.39          99.8\n## 2     9.40          99.8\n## 3     9.30          99.7\n## 4     9.30          99.7\n## 5     9.29          99.7\n## 6    10.1            1  \n## # ℹ 8 more rows\n```\n:::\n\n\nLos valores de distancia de 2020 indican que estos valores predictivos están fuera de la gran mayoría de los datos vistos por el modelo en el momento del entrenamiento. Estos deben señalarse para que las predicciones no se informen en absoluto o se vean con escepticismo.\n\n::: rmdnote\nUn aspecto importante de este análisis se refiere a qué predictores se utilizan para desarrollar el modelo de dominio de aplicabilidad. En nuestro análisis, utilizamos las columnas predictoras sin procesar. Sin embargo, al construir el modelo, se utilizaron en su lugar características de puntuación PLS. ¿Cuál de estos debería usar `apd_pca()`? La función `apd_pca()` también puede tomar una receta como entrada (en lugar de una fórmula) para que las distancias reflejen las puntuaciones PLS en lugar de las columnas predictivas individuales. Puede evaluar ambos métodos para comprender cuál ofrece resultados más relevantes.\n:::\n\n## Resumen del capítulo {#sec-trust-summary}\n\nEste capítulo mostró dos métodos para evaluar si las predicciones deben informarse a los consumidores de modelos. Las zonas equívocas se ocupan de resultados/predicciones y pueden resultar útiles cuando la cantidad de incertidumbre en una predicción es demasiado grande.\n\nLos modelos de dominio de aplicabilidad tratan con características/predictores y cuantifican la cantidad de extrapolación (si la hay) que ocurre al hacer una predicción. Este capítulo mostró un método básico que utiliza el análisis de componentes principales, aunque hay muchas otras formas de medir la aplicabilidad. El paquete <span class=\"pkg\">applicable</span> también contiene métodos especializados para conjuntos de datos donde todos los predictores son binarios. Este método calcula puntuaciones de similitud entre los puntos de datos del conjunto de entrenamiento para definir la distribución de referencia.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}