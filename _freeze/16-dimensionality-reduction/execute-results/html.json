{
  "hash": "1f7db68be1909e72504b76c26b2713ed",
  "result": {
    "markdown": "\n\n\n# Dimensionality Reduction {#dimensionality}\n\nDimensionality reduction transforms a data set from a high-dimensional space into a low-dimensional space, and can be a good choice when you suspect there are \"too many\" variables. An excess of variables, usually predictors, can be a problem because it is difficult to understand or visualize data in higher dimensions.\n\n## What Problems Can Dimensionality Reduction Solve?\n\nDimensionality reduction can be used either in feature engineering or in exploratory data analysis. For example, in high-dimensional biology experiments, one of the first tasks, before any modeling, is to determine if there are any unwanted trends in the data (e.g., effects not related to the question of interest, such as lab-to-lab differences). Debugging the data is difficult when there are hundreds of thousands of dimensions, and dimensionality reduction can be an aid for exploratory data analysis.\n\nAnother potential consequence of having a multitude of predictors is possible harm to a model. The simplest example is a method like ordinary linear regression where the number of predictors should be less than the number of data points used to fit the model. Another issue is multicollinearity, where between-predictor correlations can negatively impact the mathematical operations used to estimate a model. If there are an extremely large number of predictors, it is fairly unlikely that there are an equal number of real underlying effects. Predictors may be measuring the same latent effect(s), and thus such predictors will be highly correlated. Many dimensionality reduction techniques thrive in this situation. In fact, most can be effective only when there are such relationships between predictors that can be exploited.\n\n::: rmdnote\nWhen starting a new modeling project, reducing the dimensions of the data may provide some intuition about how hard the modeling problem may be.\n:::\n\nPrincipal component analysis (PCA) is one of the most straightforward methods for reducing the number of columns in the data set because it relies on linear methods and is unsupervised (i.e., does not consider the outcome data). For a high-dimensional classification problem, an initial plot of the main PCA components might show a clear separation between the classes. If this is the case, then it is fairly safe to assume that a linear classifier might do a good job. However, the converse is not true; a lack of separation does not mean that the problem is insurmountable.\n\nThe dimensionality reduction methods discussed in this chapter are generally *not* feature selection methods. Methods such as PCA represent the original predictors using a smaller subset of new features. All of the original predictors are required to compute these new features. The exception to this are sparse methods that have the ability to completely remove the impact of predictors when creating the new features.\n\n::: rmdnote\nThis chapter has two goals:\n\n-   Demonstrate how to use recipes to create a small set of features that capture the main aspects of the original predictor set.\n\n-   Describe how recipes can be used on their own (as opposed to being used in a workflow object, as in Section \\@ref(using-recipes)).\n:::\n\nThe latter is helpful when testing or debugging a recipe. However, as described in Section \\@ref(using-recipes), the best way to use a recipe for modeling is from within a workflow object.\n\nIn addition to the <span class=\"pkg\">tidymodels</span> package, this chapter uses the following packages: <span class=\"pkg\">baguette</span>, <span class=\"pkg\">beans</span>, <span class=\"pkg\">bestNormalize</span>, <span class=\"pkg\">corrplot</span>, <span class=\"pkg\">discrim</span>, <span class=\"pkg\">embed</span>, <span class=\"pkg\">ggforce</span>, <span class=\"pkg\">klaR</span>, <span class=\"pkg\">learntidymodels</span>,[^16-dimensionality-reduction-1] <span class=\"pkg\">mixOmics</span>,[^16-dimensionality-reduction-2] and <span class=\"pkg\">uwot</span>.\n\n[^16-dimensionality-reduction-1]: The <span class=\"pkg\">learntidymodels</span> package can be found at its GitHub site: <https://github.com/tidymodels/learntidymodels>\n\n[^16-dimensionality-reduction-2]: The <span class=\"pkg\">mixOmics</span> package is not available on CRAN, but instead on Bioconductor: <https://doi.org/doi:10.18129/B9.bioc.mixOmics>\n\n## A Picture Is Worth a Thousand... Beans {#beans}\n\nLet's walk through how to use dimensionality reduction with <span class=\"pkg\">recipes</span> for an example data set. @beans published a data set of visual characteristics of dried beans and described methods for determining the varieties of dried beans in an image. While the dimensionality of these data is not very large compared to many real-world modeling problems, it does provide a nice working example to demonstrate how to reduce the number of features. From their manuscript:\n\n> The primary objective of this study is to provide a method for obtaining uniform seed varieties from crop production, which is in the form of population, so the seeds are not certified as a sole variety. Thus, a computer vision system was developed to distinguish seven different registered varieties of dry beans with similar features in order to obtain uniform seed classification. For the classification model, images of 13,611 grains of 7 different registered dry beans were taken with a high-resolution camera.\n\nEach image contains multiple beans. The process of determining which pixels correspond to a particular bean is called *image segmentation*. These pixels can be analyzed to produce features for each bean, such as color and morphology (i.e., shape). These features are then used to model the outcome (bean variety) because different bean varieties look different. The training data come from a set of manually labeled images, and this data set is used to create a predictive model that can distinguish between seven bean varieties: Cali, Horoz, Dermason, Seker, Bombay, Barbunya, and Sira. Producing an effective model can help manufacturers quantify the homogeneity of a batch of beans.\n\nThere are numerous methods for quantifying shapes of objects [@Mingqiang08]. Many are related to the boundaries or regions of the object of interest. Example of features include:\n\n-   The *area* (or size) can be estimated using the number of pixels in the object or the size of the convex hull around the object.\n\n-   We can measure the *perimeter* using the number of pixels in the boundary as well as the area of the bounding box (the smallest rectangle enclosing an object).\n\n-   The *major axis* quantifies the longest line connecting the most extreme parts of the object. The *minor axis* is perpendicular to the major axis.\n\n-   We can measure the *compactness* of an object using the ratio of the object's area to the area of a circle with the same perimeter. For example, the symbols \"•\" and \"×\" have very different compactness.\n\n-   There are also different measures of how *elongated* or oblong an object is. For example, the *eccentricity* statistic is the ratio of the major and minor axes. There are also related estimates for roundness and convexity.\n\nNotice the eccentricity for the different shapes in Figure \\@ref(fig:eccentricity).\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Some example shapes and their eccentricity statistics](premade/morphology.svg){fig-align='center' fig-alt='Some example shapes and their eccentricity statistics. Circles and squares have the smallest eccentricity values while X shapes and lightning bolts have the largest. Also, the eccentricity is the same when shapes are rotated.' width=95%}\n:::\n:::\n\n\nShapes such as circles and squares have low eccentricity while oblong shapes have high values. Also, the metric is unaffected by the rotation of the object.\n\nMany of these image features have high correlations; objects with large areas are more likely to have large perimeters. There are often multiple methods to quantify the same underlying characteristics (e.g., size).\n\nIn the bean data, 16 morphology features were computed: area, perimeter, major axis length, minor axis length, aspect ratio, eccentricity, convex area, equiv diameter, extent, solidity, roundness, compactness, shape factor 1, shape factor 2, shape factor 3, and shape factor 4. The latter four are described in @symons1988211.\n\nWe can begin by loading the data:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidymodels)\ntidymodels_prefer()\nlibrary(beans)\n```\n:::\n\n\n::: rmdwarning\nIt is important to maintain good data discipline when evaluating dimensionality reduction techniques, especially if you will use them within a model.\n:::\n\nFor our analyses, we start by holding back a testing set with `initial_split()`. The remaining data are split into training and validation sets:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(1601)\nbean_split <- initial_validation_split(beans, strata = class, prop = c(0.75, 0.125))\n## Warning: Too little data to stratify.\n## • Resampling will be unstratified.\nbean_split\n## <Training/Validation/Testing/Total>\n## <10206/1702/1703/13611>\n\n# Return data frames:\nbean_train <- training(bean_split)\nbean_test <- testing(bean_split)\nbean_validation <- validation(bean_split)\n\nset.seed(1602)\n# Return an 'rset' object to use with the tune functions:\nbean_val <- validation_set(bean_split)\nbean_val$splits[[1]]\n## <Training/Validation/Total>\n## <10206/1702/11908>\n```\n:::\n\n\nTo visually assess how well different methods perform, we can estimate the methods on the training set (n = 10,206 beans) and display the results using the validation set (n = 1,702).\n\nBefore beginning any dimensionality reduction, we can spend some time investigating our data. Since we know that many of these shape features are probably measuring similar concepts, let's take a look at the correlation structure of the data in Figure \\@ref(fig:beans-corr-plot) using this code.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(corrplot)\ntmwr_cols <- colorRampPalette(c(\"#91CBD765\", \"#CA225E\"))\nbean_train %>% \n  select(-class) %>% \n  cor() %>% \n  corrplot(col = tmwr_cols(200), tl.col = \"black\", method = \"ellipse\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Correlation matrix of the predictors with variables ordered via clustering](16-dimensionality-reduction_files/figure-html/beans-corr-plot-1.png){fig-align='center' fig-alt='A correlation matrix of the predictors with variables ordered via clustering. There are two to three clusters that have high within cluster correlations.' width=70%}\n:::\n:::\n\n\nMany of these predictors are highly correlated, such as area and perimeter or shape factors 2 and 3. While we don't take the time to do it here, it is also important to see if this correlation structure significantly changes across the outcome categories. This can help create better models.\n\n## A Starter Recipe\n\nIt's time to look at the beans data in a smaller space. We can start with a basic recipe to preprocess the data prior to any dimensionality reduction steps. Several predictors are ratios and so are likely to have skewed distributions. Such distributions can wreak havoc on variance calculations (such as the ones used in PCA). The [<span class=\"pkg\">bestNormalize</span> package](https://petersonr.github.io/bestNormalize/) has a step that can enforce a symmetric distribution for the predictors. We'll use this to mitigate the issue of skewed distributions:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(bestNormalize)\nbean_rec <-\n  # Use the training data from the bean_val split object\n  recipe(class ~ ., data = bean_train) %>%\n  step_zv(all_numeric_predictors()) %>%\n  step_orderNorm(all_numeric_predictors()) %>% \n  step_normalize(all_numeric_predictors())\n```\n:::\n\n\n::: rmdnote\nRemember that when invoking the `recipe()` function, the steps are not estimated or executed in any way.\n:::\n\nThis recipe will be extended with additional steps for the dimensionality reduction analyses. Before doing so, let's go over how a recipe can be used outside of a workflow.\n\n## Recipes in the Wild {#recipe-functions}\n\nAs mentioned in Section \\@ref(using-recipes), a workflow containing a recipe uses `fit()` to estimate the recipe and model, then `predict()` to process the data and make model predictions. There are analogous functions in the <span class=\"pkg\">recipes</span> package that can be used for the same purpose:\n\n-   `prep(recipe, training)` fits the recipe to the training set.\n-   `bake(recipe, new_data)` applies the recipe operations to `new_data`.\n\nFigure \\@ref(fig:recipe-process) summarizes this. Let's look at each of these functions in more detail.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Summary of recipe-related functions](premade/recipes-process.svg){fig-align='center' fig-alt='A summary of the recipe-related functions.' width=80%}\n:::\n:::\n\n\n### Preparing a recipe {#prep}\n\nLet's estimate `bean_rec` using the training set data, with `prep(bean_rec)`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbean_rec_trained <- prep(bean_rec)\nbean_rec_trained\n## \n## ── Recipe ───────────────────────────────────────────────────────────────────────────\n## \n## ── Inputs\n## Number of variables by role\n## outcome:    1\n## predictor: 16\n## \n## ── Training information\n## Training data contained 10206 data points and no incomplete rows.\n## \n## ── Operations\n## • Zero variance filter removed: <none> | Trained\n## • orderNorm transformation on: area, perimeter, major_axis_length, ... | Trained\n## • Centering and scaling for: area, perimeter, major_axis_length, ... | Trained\n```\n:::\n\n\n::: rmdnote\nRemember that `prep()` for a recipe is like `fit()` for a model.\n:::\n\nNote in the output that the steps have been trained and that the selectors are no longer general (i.e., `all_numeric_predictors()`); they now show the actual columns that were selected. Also, `prep(bean_rec)` does not require the `training` argument. You can pass any data into that argument, but omitting it means that the original `data` from the call to `recipe()` will be used. In our case, this was the training set data.\n\nOne important argument to `prep()` is `retain`. When `retain = TRUE` (the default), the estimated version of the training set is kept within the recipe. This data set has been pre-processed using all of the steps listed in the recipe. Since `prep()` has to execute the recipe as it proceeds, it may be advantageous to keep this version of the training set so that, if that data set is to be used later, redundant calculations can be avoided. However, if the training set is big, it may be problematic to keep such a large amount of data in memory. Use `retain = FALSE` to avoid this.\n\nOnce new steps are added to this estimated recipe, reapplying `prep()` will estimate only the untrained steps. This will come in handy when we try different feature extraction methods.\n\n::: rmdwarning\nIf you encounter errors when working with a recipe, `prep()` can be used with its `verbose` option to troubleshoot:\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbean_rec_trained %>% \n  step_dummy(cornbread) %>%  # <- not a real predictor\n  prep(verbose = TRUE)\n## oper 1 step zv [pre-trained]\n## oper 2 step orderNorm [pre-trained]\n## oper 3 step normalize [pre-trained]\n## oper 4 step dummy [training]\n## Error in `step_dummy()`:\n## Caused by error in `prep()`:\n## ! Can't subset columns that don't exist.\n## ✖ Column `cornbread` doesn't exist.\n```\n:::\n\n\nAnother option that can help you understand what happens in the analysis is `log_changes`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nshow_variables <- \n  bean_rec %>% \n  prep(log_changes = TRUE)\n## step_zv (zv_RLYwH): same number of columns\n## \n## step_orderNorm (orderNorm_Jx8oD): same number of columns\n## \n## step_normalize (normalize_GU75D): same number of columns\n```\n:::\n\n\n### Baking the recipe {#bake}\n\n::: rmdnote\nUsing `bake()` with a recipe is much like using `predict()` with a model; the operations estimated from the training set are applied to any data, like testing data or new data at prediction time.\n:::\n\nFor example, the validation set samples can be processed:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbean_val_processed <- bake(bean_rec_trained, new_data = bean_validation)\n```\n:::\n\n\nFigure \\@ref(fig:bean-area) shows histograms of the `area` predictor before and after the recipe was prepared.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(patchwork)\np1 <- \n  bean_validation %>% \n  ggplot(aes(x = area)) + \n  geom_histogram(bins = 30, color = \"white\", fill = \"blue\", alpha = 1/3) + \n  ggtitle(\"Original validation set data\")\n\np2 <- \n  bean_val_processed %>% \n  ggplot(aes(x = area)) + \n  geom_histogram(bins = 30, color = \"white\", fill = \"red\", alpha = 1/3) + \n  ggtitle(\"Processed validation set data\")\n\np1 + p2\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![The `area` predictor before and after preprocessing](16-dimensionality-reduction_files/figure-html/bean-area-1.png){fig-align='center' fig-alt='The `area` predictor before and after preprocessing. The before panel shows a right-skewed, slightly bimodal distribution. The after panel has a distribution that is fairly bell shaped.' width=672}\n:::\n:::\n\n\nTwo important aspects of `bake()` are worth noting here.\n\nFirst, as previously mentioned, using `prep(recipe, retain = TRUE)` keeps the existing processed version of the training set in the recipe. This enables the user to use `bake(recipe, new_data = NULL)`, which returns that data set without further computations. For example:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbake(bean_rec_trained, new_data = NULL) %>% nrow()\n## [1] 10206\nbean_train %>% nrow()\n## [1] 10206\n```\n:::\n\n\nIf the training set is not pathologically large, using this value of `retain` can save a lot of computational time.\n\nSecond, additional selectors can be used in the call to specify which columns to return. The default selector is `everything()`, but more specific directives can be used.\n\nWe will use `prep()` and `bake()` in the next section to illustrate some of these options.\n\n## Feature Extraction Techniques\n\nSince recipes are the primary option in tidymodels for dimensionality reduction, let's write a function that will estimate the transformation and plot the resulting data in a scatter plot matrix via the <span class=\"pkg\">ggforce</span> package:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggforce)\nplot_validation_results <- function(recipe, dat = bean_validation) {\n  recipe %>%\n    # Estimate any additional steps\n    prep() %>%\n    # Process the data (the validation set by default)\n    bake(new_data = dat) %>%\n    # Create the scatterplot matrix\n    ggplot(aes(x = .panel_x, y = .panel_y, color = class, fill = class)) +\n    geom_point(alpha = 0.4, size = 0.5) +\n    geom_autodensity(alpha = .3) +\n    facet_matrix(vars(-class), layer.diag = 2) + \n    scale_color_brewer(palette = \"Dark2\") + \n    scale_fill_brewer(palette = \"Dark2\")\n}\n```\n:::\n\n\nWe will reuse this function several times in this chapter.\n\nA series of several feature extraction methodologies are explored here. An overview of most can be found in [Section 6.3.1](https://bookdown.org/max/FES/numeric-many-to-many.html#linear-projection-methods) of @fes and the references therein. The UMAP method is described in @mcinnes2020umap.\n\n### Principal component analysis\n\nWe've mentioned PCA several times already in this book, and it's time to go into more detail. PCA is an unsupervised method that uses linear combinations of the predictors to define new features. These features attempt to account for as much variation as possible in the original data. We add `step_pca()` to the original recipe and use our function to visualize the results on the validation set in Figure \\@ref(fig:bean-pca) using:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbean_rec_trained %>%\n  step_pca(all_numeric_predictors(), num_comp = 4) %>%\n  plot_validation_results() + \n  ggtitle(\"Principal Component Analysis\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Principal component scores for the bean validation set, colored by class](16-dimensionality-reduction_files/figure-html/bean-pca-1.png){fig-align='center' fig-alt='Principal component scores for the bean validation set, colored by class. The classes separate when the first two components are plotted against one another.' width=672}\n:::\n:::\n\n\nWe see that the first two components `PC1` and `PC2`, especially when used together, do an effective job distinguishing between or separating the classes. This may lead us to expect that the overall problem of classifying these beans will not be especially difficult.\n\nRecall that PCA is unsupervised. For these data, it turns out that the PCA components that explain the most variation in the predictors also happen to be predictive of the classes. What features are driving performance? The <span class=\"pkg\">learntidymodels</span> package has functions that can help visualize the top features for each component. We'll need the prepared recipe; the PCA step is added in the following code along with a call to `prep()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(learntidymodels)\nbean_rec_trained %>%\n  step_pca(all_numeric_predictors(), num_comp = 4) %>% \n  prep() %>% \n  plot_top_loadings(component_number <= 4, n = 5) + \n  scale_fill_brewer(palette = \"Paired\") +\n  ggtitle(\"Principal Component Analysis\")\n```\n:::\n\n\nThis produces Figure \\@ref(fig:pca-loadings).\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Predictor loadings for the PCA transformation](16-dimensionality-reduction_files/figure-html/pca-loadings-1.png){fig-align='center' fig-alt='Predictor loadings for the PCA transformation. For the first component, the major axis length, second shape factor, convex area, and area have the largest effect. ' width=672}\n:::\n:::\n\n\nThe top loadings are mostly related to the cluster of correlated predictors shown in the top-left portion of the previous correlation plot: perimeter, area, major axis length, and convex area. These are all related to bean size. Shape factor 2, from @symons1988211, is the area over the cube of the major axis length and is therefore also related to bean size. Measures of elongation appear to dominate the second PCA component.\n\n### Partial least squares\n\nPLS, which we introduced in Section \\@ref(submodel-trick), is a supervised version of PCA. It tries to find components that simultaneously maximize the variation in the predictors while also maximizing the relationship between those components and the outcome. Figure \\@ref(fig:bean-pls) shows the results of this slightly modified version of the PCA code:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbean_rec_trained %>%\n  step_pls(all_numeric_predictors(), outcome = \"class\", num_comp = 4) %>%\n  plot_validation_results() + \n  ggtitle(\"Partial Least Squares\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![PLS component scores for the bean validation set, colored by class](16-dimensionality-reduction_files/figure-html/bean-pls-1.png){fig-align='center' fig-alt='PLS component scores for the bean validation set, colored by class. The first two PLS components are nearly identical to the first two PCA components.' width=672}\n:::\n:::\n\n\nThe first two PLS components plotted in Figure \\@ref(fig:bean-pls) are nearly identical to the first two PCA components! We find this result because those PCA components are so effective at separating the varieties of beans. The remaining components are different. Figure \\@ref(fig:pls-loadings) visualizes the loadings, the top features for each component.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbean_rec_trained %>%\n  step_pls(all_numeric_predictors(), outcome = \"class\", num_comp = 4) %>%\n  prep() %>% \n  plot_top_loadings(component_number <= 4, n = 5, type = \"pls\") + \n  scale_fill_brewer(palette = \"Paired\") +\n  ggtitle(\"Partial Least Squares\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Predictor loadings for the PLS transformation](16-dimensionality-reduction_files/figure-html/pls-loadings-1.png){fig-align='center' fig-alt='Predictor loadings for the PLS transformation. For the first component, the major axis length, second shape factor, the equivalent diameter, convex area, and area have the largest effect. ' width=672}\n:::\n:::\n\n\nSolidity (i.e., the density of the bean) drives the third PLS component, along with roundness. Solidity may be capturing bean features related to \"bumpiness\" of the bean surface since it can measure irregularity of the bean boundaries.\n\n### Independent component analysis\n\nICA is slightly different than PCA in that it finds components that are as statistically independent from one another as possible (as opposed to being uncorrelated). It can be thought of as maximizing the \"non-Gaussianity\" of the ICA components, or separating information instead of compressing information like PCA. Let's use `step_ica()` to produce Figure \\@ref(fig:bean-ica):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbean_rec_trained %>%\n  step_ica(all_numeric_predictors(), num_comp = 4) %>%\n  plot_validation_results() + \n  ggtitle(\"Independent Component Analysis\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![ICA component scores for the bean validation set, colored by class](16-dimensionality-reduction_files/figure-html/bean-ica-1.png){fig-align='center' fig-alt='ICA component scores for the bean validation set, colored by class. There is significant overlap in the first two ICA components.' width=672}\n:::\n:::\n\n\nInspecting this plot, there does not appear to be much separation between the classes in the first few components when using ICA. These independent (or as independent as possible) components do not separate the bean types.\n\n### Uniform manifold approximation and projection\n\nUMAP is similar to the popular t-SNE method for nonlinear dimension reduction. In the original high-dimensional space, UMAP uses a distance-based nearest neighbor method to find local areas of the data where the data points are more likely to be related. The relationship between data points is saved as a directed graph model where most points are not connected.\n\nFrom there, UMAP translates points in the graph to the reduced dimensional space. To do this, the algorithm has an optimization process that uses cross-entropy to map data points to the smaller set of features so that the graph is well approximated.\n\nTo create the mapping, the <span class=\"pkg\">embed</span> package contains a step function for this method, visualized in Figure \\@ref(fig:bean-umap).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(embed)\nbean_rec_trained %>%\n  step_umap(all_numeric_predictors(), num_comp = 4) %>%\n  plot_validation_results() +\n  ggtitle(\"UMAP\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![UMAP component scores for the bean validation set, colored by class](16-dimensionality-reduction_files/figure-html/bean-umap-1.png){fig-align='center' fig-alt='UMAP component scores for the bean validation set, colored by class. There is a very high degree of separation between clusters, but several of the clusters contain more than one class.' width=672}\n:::\n:::\n\n\nWhile the between-cluster space is pronounced, the clusters can contain a heterogeneous mixture of classes.\n\nThere is also a supervised version of UMAP:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbean_rec_trained %>%\n  step_umap(all_numeric_predictors(), outcome = \"class\", num_comp = 4) %>%\n  plot_validation_results() +\n  ggtitle(\"UMAP (supervised)\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Supervised UMAP component scores for the bean validation set, colored by class](16-dimensionality-reduction_files/figure-html/bean-umap-supervised-1.png){fig-align='center' fig-alt='Supervised UMAP component scores for the bean validation set, colored by class. There is again a very high degree of separation between clusters, and there are now fewer instances of one cluster containing multiple classes.' width=672}\n:::\n:::\n\n\nThe supervised method shown in Figure \\@ref(fig:bean-umap-supervised) looks promising for modeling the data.\n\nUMAP is a powerful method to reduce the feature space. However, it can be very sensitive to tuning parameters (e.g., the number of neighbors and so on). For this reason, it would help to experiment with a few of the parameters to assess how robust the results are for these data.\n\n## Modeling {#bean-models}\n\nBoth the PLS and UMAP methods are worth investigating in conjunction with different models. Let's explore a variety of different models with these dimensionality reduction techniques (along with no transformation at all): a single layer neural network, bagged trees, flexible discriminant analysis (FDA), naive Bayes, and regularized discriminant analysis (RDA).\n\nNow that we are back in \"modeling mode,\" we'll create a series of model specifications and then use a workflow set to tune the models in the following code. Note that the model parameters are tuned in conjunction with the recipe parameters (e.g., size of the reduced dimension, UMAP parameters).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(baguette)\nlibrary(discrim)\n\nmlp_spec <-\n  mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>%\n  set_engine('nnet') %>%\n  set_mode('classification')\n\nbagging_spec <-\n  bag_tree() %>%\n  set_engine('rpart') %>%\n  set_mode('classification')\n\nfda_spec <-\n  discrim_flexible(\n    prod_degree = tune()\n  ) %>%\n  set_engine('earth')\n\nrda_spec <-\n  discrim_regularized(frac_common_cov = tune(), frac_identity = tune()) %>%\n  set_engine('klaR')\n\nbayes_spec <-\n  naive_Bayes() %>%\n  set_engine('klaR')\n```\n:::\n\n\nWe also need recipes for the dimensionality reduction methods we'll try. Let's start with a base recipe `bean_rec` and then extend it with different dimensionality reduction steps:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbean_rec <-\n  recipe(class ~ ., data = bean_train) %>%\n  step_zv(all_numeric_predictors()) %>%\n  step_orderNorm(all_numeric_predictors()) %>%\n  step_normalize(all_numeric_predictors())\n\npls_rec <- \n  bean_rec %>% \n  step_pls(all_numeric_predictors(), outcome = \"class\", num_comp = tune())\n\numap_rec <-\n  bean_rec %>%\n  step_umap(\n    all_numeric_predictors(),\n    outcome = \"class\",\n    num_comp = tune(),\n    neighbors = tune(),\n    min_dist = tune()\n  )\n```\n:::\n\n\nOnce again, the <span class=\"pkg\">workflowsets</span> package takes the preprocessors and models and crosses them. The `control` option `parallel_over` is set so that the parallel processing can work simultaneously across tuning parameter combinations. The `workflow_map()` function applies grid search to optimize the model/preprocessing parameters (if any) across 10 parameter combinations. The multiclass area under the ROC curve is estimated on the validation set.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nctrl <- control_grid(parallel_over = \"everything\")\nbean_res <- \n  workflow_set(\n    preproc = list(basic = class ~., pls = pls_rec, umap = umap_rec), \n    models = list(bayes = bayes_spec, fda = fda_spec,\n                  rda = rda_spec, bag = bagging_spec,\n                  mlp = mlp_spec)\n  ) %>% \n  workflow_map(\n    verbose = TRUE,\n    seed = 1603,\n    resamples = bean_val,\n    grid = 10,\n    metrics = metric_set(roc_auc),\n    control = ctrl\n  )\n```\n:::\n\n\nWe can rank the models by their validation set estimates of the area under the ROC curve:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrankings <- \n  rank_results(bean_res, select_best = TRUE) %>% \n  mutate(method = map_chr(wflow_id, ~ str_split(.x, \"_\", simplify = TRUE)[1])) \n\ntidymodels_prefer()\nfilter(rankings, rank <= 5) %>% dplyr::select(rank, mean, model, method)\n## # A tibble: 5 × 4\n##    rank  mean model               method\n##   <int> <dbl> <chr>               <chr> \n## 1     1 0.996 mlp                 pls   \n## 2     2 0.996 discrim_regularized pls   \n## 3     3 0.995 discrim_flexible    basic \n## 4     4 0.995 naive_Bayes         pls   \n## 5     5 0.994 naive_Bayes         basic\n```\n:::\n\n\nFigure \\@ref(fig:dimensionality-rankings) illustrates this ranking.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Area under the ROC curve from the validation set](16-dimensionality-reduction_files/figure-html/dimensionality-rankings-1.png){fig-align='center' fig-alt='Area under the ROC curve from the validation set. The three best model configurations use PLS together with regularized discriminant analysis, a multi-layer perceptron, and a naive Bayes model.' width=672}\n:::\n:::\n\n\nIt is clear from these results that most models give very good performance; there are few bad choices here. For demonstration, we'll use the RDA model with PLS features as the final model. We will finalize the workflow with the numerically best parameters, fit it to the training set, then evaluate with the test set:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrda_res <- \n  bean_res %>% \n  extract_workflow(\"pls_rda\") %>% \n  finalize_workflow(\n    bean_res %>% \n      extract_workflow_set_result(\"pls_rda\") %>% \n      select_best(metric = \"roc_auc\")\n  ) %>% \n  last_fit(split = bean_split, metrics = metric_set(roc_auc))\n\nrda_wflow_fit <- extract_workflow(rda_res)\n```\n:::\n\n\nWhat are the results for our metric (multiclass ROC AUC) on the testing set?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncollect_metrics(rda_res)\n## # A tibble: 1 × 4\n##   .metric .estimator .estimate .config             \n##   <chr>   <chr>          <dbl> <chr>               \n## 1 roc_auc hand_till      0.995 Preprocessor1_Model1\n```\n:::\n\n\nPretty good! We'll use this model in the next chapter to demonstrate variable importance methods.\n\n\n\n\n\n## Chapter Summary {#dimensionality-summary}\n\nDimensionality reduction can be a helpful method for exploratory data analysis as well as modeling. The <span class=\"pkg\">recipes</span> and <span class=\"pkg\">embed</span> packages contain steps for a variety of different methods and <span class=\"pkg\">workflowsets</span> facilitates choosing an appropriate method for a data set. This chapter also discussed how recipes can be used on their own, either for debugging problems with a recipe or directly for exploratory data analysis and data visualization.\n",
    "supporting": [
      "16-dimensionality-reduction_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}