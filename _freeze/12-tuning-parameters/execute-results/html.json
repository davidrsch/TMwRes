{
  "hash": "b2cb62c7529e5d1a1a988a42ebe4ee15",
  "result": {
    "markdown": "\n\n\n# Model Tuning and the Dangers of Overfitting {#sec-tuning}\n\nIn order to use a model for prediction, the parameters for that model must be estimated. Some of these parameters can be estimated directly from the training data, but other parameters, called *tuning parameters* or *hyperparameters*, must be specified ahead of time and can't be directly found from training data. These are unknown structural or other kind of values that have significant impact on the model but cannot be directly estimated from the data. This chapter will provide examples of tuning parameters and show how we use tidymodels functions to create and handle tuning parameters. We'll also demonstrate how poor choices of these values lead to overfitting and introduce several tactics for finding optimal tuning parameters values. Chapters [-@sec-grid-search] and [-@sec-iterative-search] go into more detail on specific optimization methods for tuning.\n\n## Model Parameters\n\nIn ordinary linear regression, there are two parameters $\\beta_0$ and $\\beta_1$ of the model:\n\n$$ y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$$\n\nWhen we have the outcome ($y$) and predictor ($x$) data, we can estimate the two parameters $\\beta_0$ and $\\beta_1$:\n\n$$\\hat \\beta_1 = \\frac{\\sum_i (y_i-\\bar{y})(x_i-\\bar{x})}{\\sum_i(x_i-\\bar{x})^2}$$\n\nand\n\n$$\\hat \\beta_0 = \\bar{y}-\\hat \\beta_1 \\bar{x}.$$\n\nWe can directly estimate these values from the data for this example model because they are analytically tractable; if we have the data, then we can estimate these model parameters.\n\n::: rmdnote\nThere are many situations where a model has parameters that *can't* be directly estimated from the data.\n:::\n\nFor the KNN model, the prediction equation for a new value $x_0$ is\n\n$$\\hat y = \\frac{1}{K}\\sum_{\\ell = 1}^K x_\\ell^*$$\n\nwhere $K$ is the number of neighbors and the $x_\\ell^*$ are the $K$ closest values to $x_0$ in the training set. The model itself is not defined by a model equation; the previous prediction equation instead defines it. This characteristic, along with the possible intractability of the distance measure, makes it impossible to create a set of equations that can be solved for $K$ (iteratively or otherwise). The number of neighbors has a profound impact on the model; it governs the flexibility of the class boundary. For small values of $K$, the boundary is very elaborate while for large values, it might be quite smooth.\n\nThe number of nearest neighbors is a good example of a tuning parameter or hyperparameter that cannot be directly estimated from the data.\n\n## Tuning Parameters for Different Types of Models {#sec-tuning-parameter-examples}\n\nThere are many examples of tuning parameters or hyperparameters in different statistical and machine learning models:\n\n-   Boosting is an ensemble method that combines a series of base models, each of which is created sequentially and depends on the previous models. The number of boosting iterations is an important tuning parameter that usually requires optimization.\n\n-   In the classic single-layer artificial neural network (a.k.a. the multilayer perceptron), the predictors are combined using two or more hidden units. The hidden units are linear combinations of the predictors that are captured in an *activation function* (typically a nonlinear function, such as a sigmoid). The hidden units are then connected to the outcome units; one outcome unit is used for regression models, and multiple outcome units are required for classification. The number of hidden units and the type of activation function are important structural tuning parameters.\n\n-   Modern gradient descent methods are improved by finding the right optimization parameters. Examples of such hyperparameters are learning rates, momentum, and the number of optimization iterations/epochs [@Goodfellow]. Neural networks and some ensemble models use gradient descent to estimate the model parameters. While the tuning parameters associated with gradient descent are not structural parameters, they often require tuning.\n\nIn some cases, preprocessing techniques require tuning:\n\n-   In principal component analysis, or its supervised cousin called partial least squares, the predictors are replaced with new, artificial features that have better properties related to collinearity. The number of extracted components can be tuned.\n\n-   Imputation methods estimate missing predictor values using the complete values of one or more predictors. One effective imputation tool uses $K$-nearest neighbors of the complete columns to predict the missing value. The number of neighbors modulates the amount of averaging and can be tuned.\n\nSome classical statistical models also have structural parameters:\n\n-   In binary regression, the logit link is commonly used (i.e., logistic regression). Other link functions, such as the probit and complementary log-log, are also available [@Dobson99]. This example is described in more detail in the @sec-what-to-optimize.\n\n-   Non-Bayesian longitudinal and repeated measures models require a specification for the covariance or correlation structure of the data. Options include compound symmetric (a.k.a. exchangeable), autoregressive, Toeplitz, and others [@littell2000modelling].\n\nA counterexample where it is inappropriate to tune a parameter is the prior distribution required for Bayesian analysis. The prior encapsulates the analyst's belief about the distribution of a quantity before evidence or data are taken into account. For example, in @sec-tidyposterior, we used a Bayesian ANOVA model, and we were unclear about what the prior should be for the regression parameters (beyond being a symmetric distribution). We chose a t-distribution with one degree of freedom for the prior since it has heavier tails; this reflects our added uncertainty. Our prior beliefs should not be subject to optimization. Tuning parameters are typically optimized for performance whereas priors should not be tweaked to get \"the right results.\"\n\n::: rmdwarning\nAnother (perhaps more debatable) counterexample of a parameter that does *not* need to be tuned is the number of trees in a random forest or bagging model. This value should instead be chosen to be large enough to ensure numerical stability in the results; tuning it cannot improve performance as long as the value is large enough to produce reliable results. For random forests, this value is typically in the thousands while the number of trees needed for bagging is around 50 to 100.\n:::\n\n## What do we Optimize? {#sec-what-to-optimize}\n\nHow should we evaluate models when we optimize tuning parameters? It depends on the model and the purpose of the model.\n\nFor cases where the statistical properties of the tuning parameter are tractable, common statistical properties can be used as the objective function. For example, in the case of binary logistic regression, the link function can be chosen by maximizing the likelihood or information criteria. However, these statistical properties may not align with the results achieved using accuracy-oriented properties. As an example, @FriedmanGFA optimized the number of trees in a boosted tree ensemble and found different results when maximizing the likelihood and accuracy:\n\n> degrading the likelihood by overfitting actually improves misclassification error rate. Although perhaps counterintuitive, this is not a contradiction; likelihood and error rate measure different aspects of fit quality.\n\nTo demonstrate, consider the classification data shown in @fig-two-class-dat with two predictors, two classes, and a training set of 593 data points.\n\n\n::: {.cell layout-align=\"center\" hash='12-tuning-parameters_cache/html/fig-two-class-dat_3dc6cae8570af9cd9958c5e2de70e3e8'}\n::: {.cell-output-display}\n![An example two-class classification data set with two predictors](figures/fig-two-class-dat-1.png){#fig-two-class-dat fig-align='center' fig-alt='An example two-class classification data set with two predictors. The two predictors have a moderate correlation and there is some locations of separation between the classes.' width=672}\n:::\n:::\n\n\nWe could start by fitting a linear class boundary to these data. The most common method for doing this is to use a generalized linear model in the form of *logistic regression*. This model relates the *log odds* of a sample being Class 1 using the *logit* transformation:\n\n$$ \\log\\left(\\frac{\\pi}{1 - \\pi}\\right) = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_px_p$$\n\nIn the context of generalized linear models, the logit function is the *link function* between the outcome ($\\pi$) and the predictors. There are other link functions that include the *probit* model:\n\n$$\\Phi^{-1}(\\pi) = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_px_p$$\n\nwhere $\\Phi$ is the cumulative standard normal function, as well as the *complementary log-log* model:\n\n$$\\log(-\\log(1-\\pi)) = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_px_p$$\n\nEach of these models results in linear class boundaries. Which one should we use? Since, for these data, the number of model parameters does not vary, the statistical approach is to compute the (log) likelihood for each model and determine the model with the largest value. Traditionally, the likelihood is computed using the same data that were used to estimate the parameters, not using approaches like data splitting or resampling from Chapters [-@sec-splitting] and [-@sec-resampling].\n\nFor a data frame `training_set`, let's create a function to compute the different models and extract the likelihood statistics for the training set (using `broom::glance()`):\n\n\n::: {.cell layout-align=\"center\" hash='12-tuning-parameters_cache/html/tuning-likelihood_a2c62d6f4188554549f0c0ddc9ec2b51'}\n\n```{.r .cell-code}\nlibrary(tidymodels)\ntidymodels_prefer()\n\nllhood <- function(...) {\n  logistic_reg() %>% \n    set_engine(\"glm\", ...) %>% \n    fit(Class ~ ., data = training_set) %>% \n    glance() %>% \n    select(logLik)\n}\n\nbind_rows(\n  llhood(),\n  llhood(family = binomial(link = \"probit\")),\n  llhood(family = binomial(link = \"cloglog\"))\n) %>% \n  mutate(link = c(\"logit\", \"probit\", \"c-log-log\"))  %>% \n  arrange(desc(logLik))\n## # A tibble: 3 × 2\n##   logLik link     \n##    <dbl> <chr>    \n## 1  -258. logit    \n## 2  -262. probit   \n## 3  -270. c-log-log\n```\n:::\n\n\nAccording to these results, the logistic model has the best statistical properties.\n\nFrom the scale of the log-likelihood values, it is difficult to understand if these differences are important or negligible. One way of improving this analysis is to resample the statistics and separate the modeling data from the data used for performance estimation. With this small data set, repeated 10-fold cross-validation is a good choice for resampling. In the <span class=\"pkg\">yardstick</span> package, the `mn_log_loss()` function is used to estimate the negative log-likelihood, with our results shown in @fig-resampled-log-lhood.\n\n\n::: {.cell layout-align=\"center\" hash='12-tuning-parameters_cache/html/tuning-resampled-log-lhood_b9ba22bbf7ba1e9ba1dd8dc9a6b905f4'}\n\n```{.r .cell-code}\nset.seed(1201)\nrs <- vfold_cv(training_set, repeats = 10)\n\n# Return the individual resampled performance estimates:\nlloss <- function(...) {\n  perf_meas <- metric_set(roc_auc, mn_log_loss)\n    \n  logistic_reg() %>% \n    set_engine(\"glm\", ...) %>% \n    fit_resamples(Class ~ A + B, rs, metrics = perf_meas) %>% \n    collect_metrics(summarize = FALSE) %>%\n    select(id, id2, .metric, .estimate)\n}\n\nresampled_res <- \n  bind_rows(\n    lloss()                                    %>% mutate(model = \"logistic\"),\n    lloss(family = binomial(link = \"probit\"))  %>% mutate(model = \"probit\"),\n    lloss(family = binomial(link = \"cloglog\")) %>% mutate(model = \"c-log-log\")     \n  ) %>%\n  # Convert log-loss to log-likelihood:\n  mutate(.estimate = ifelse(.metric == \"mn_log_loss\", -.estimate, .estimate)) %>% \n  group_by(model, .metric) %>% \n  summarize(\n    mean = mean(.estimate, na.rm = TRUE),\n    std_err = sd(.estimate, na.rm = TRUE) / sqrt(n()), \n    .groups = \"drop\"\n  )\n\nresampled_res %>% \n  filter(.metric == \"mn_log_loss\") %>% \n  ggplot(aes(x = mean, y = model)) + \n  geom_point() + \n  geom_errorbar(aes(xmin = mean - 1.64 * std_err, xmax = mean + 1.64 * std_err),\n                width = .1) + \n  labs(y = NULL, x = \"log-likelihood\")\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='12-tuning-parameters_cache/html/fig-resampled-log-lhood_f6c209ebfe5ef7c9b5fdd1a8f9d60afc'}\n\n```\n## → A | warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n## There were issues with some computations   A: x1\n## There were issues with some computations   A: x1\n## \n```\n\n::: {.cell-output-display}\n![Means and approximate 90% confidence intervals for the resampled binomial log-likelihood with three different link functions](figures/fig-resampled-log-lhood-1.png){#fig-resampled-log-lhood fig-align='center' fig-alt='Means and approximate 90% confidence intervals for the resampled binomial log-likelihood with three different link functions. The logit link has the largest value, followed by the probit link. The complementary log log link has far lower values.' width=672}\n:::\n:::\n\n\n::: rmdnote\nThe scale of these values is different than the previous values since they are computed on a smaller data set; the value produced by `broom::glance()` is a sum while `yardstick::mn_log_loss()` is an average.\n:::\n\nThese results exhibit evidence that the choice of the link function matters somewhat. Although there is an overlap in the confidence intervals, the logistic model has the best results.\n\nWhat about a different metric? We also calculated the area under the ROC curve for each resample. These results, which reflect the discriminative ability of the models across numerous probability thresholds, show a lack of difference in @fig-resampled-roc.\n\n\n::: {.cell layout-align=\"center\" hash='12-tuning-parameters_cache/html/fig-resampled-roc_7a2c3f4fd911d8fb73bde59a27678e48'}\n::: {.cell-output-display}\n![Means and approximate 90% confidence intervals for the resampled area under the ROC curve with three different link functions](figures/fig-resampled-roc-1.png){#fig-resampled-roc fig-align='center' fig-alt='Means and approximate 90% confidence intervals for the resampled area under the ROC curve with three different link functions. The logit link has the largest value, followed by the probit link. The confidence intervals show a large amount of overlap between the two methods.' width=672}\n:::\n:::\n\n\nGiven the overlap of the intervals, as well as the scale of the x-axis, any of these options could be used. We see this again when the class boundaries for the three models are overlaid on the test set of 198 data points in @fig-three-link-fits.\n\n\n::: {.cell layout-align=\"center\" hash='12-tuning-parameters_cache/html/fig-three-link-fits_cd46fa7a0363447f024f10cf1aeabcb8'}\n::: {.cell-output-display}\n![The linear class boundary fits for three link functions](figures/fig-three-link-fits-1.png){#fig-three-link-fits fig-align='center' fig-alt='The linear class boundary fits for three link functions. The lines have very similar slopes with the complementary log log having a slightly different intercept than the other two links.' width=672}\n:::\n:::\n\n\n::: rmdwarning\nThis exercise emphasizes that different metrics might lead to different decisions about the choice of tuning parameter values. In this case, one metric indicates the models are somewhat different while another metric shows no difference at all.\n:::\n\nMetric optimization is thoroughly discussed by @thomas2020problem who explore several issues, including the gaming of metrics. They warn that:\n\n> The unreasonable effectiveness of metric optimization in current AI approaches is a fundamental challenge to the field, and yields an inherent contradiction: solely optimizing metrics leads to far from optimal outcomes.\n\n## The consequences of poor parameter estimates {#sec-overfitting-bad}\n\nMany tuning parameters modulate the amount of model complexity. More complexity often implies more malleability in the patterns that a model can emulate. For example, as shown in @sec-spline-functions, adding degrees of freedom in a spline function increases the intricacy of the prediction equation. While this is an advantage when the underlying motifs in the data are complex, it can also lead to overinterpretation of chance patterns that would not reproduce in new data. *Overfitting* is the situation where a model adapts too much to the training data; it performs well for the data used to build the model but poorly for new data.\n\n::: rmdwarning\nSince tuning model parameters can increase model complexity, poor choices can lead to overfitting.\n:::\n\nRecall the single layer neural network model described in @sec-tuning-parameter-examples. With a single hidden unit and sigmoidal activation functions, a neural network for classification is, for all intents and purposes, just logistic regression. However, as the number of hidden units increases, so does the complexity of the model. In fact, when the network model uses sigmoidal activation units, @cybenko1989approximation showed that the model is a universal function approximator as long as there are enough hidden units.\n\nWe fit neural network classification models to the same two-class data from the previous section, varying the number of hidden units. Using the area under the ROC curve as a performance metric, the effectiveness of the model on the training set increases as more hidden units are added. The network model thoroughly and meticulously learns the training set. If the model judges itself on the training set ROC value, it prefers many hidden units so that it can nearly eliminate errors.\n\nChapters [-@sec-splitting] and [-@sec-resampling] demonstrated that simply repredicting the training set is a poor approach to model evaluation. Here, the neural network very quickly begins to overinterpret patterns that it sees in the training set. Compare these three example class boundaries (developed with the training set) overlaid on training and test sets in [@fig-two-class-boundaries].\n\n\n\n\n::: {.cell layout-align=\"center\" hash='12-tuning-parameters_cache/html/fig-two-class-boundaries_32c2cc7f921b6590c8e93aab7494b398'}\n::: {.cell-output-display}\n![Class boundaries for three models with increasing numbers of hidden units. The boundaries are fit on the training set and shown for the training and test sets.](figures/fig-two-class-boundaries-1.png){#fig-two-class-boundaries fig-align='center' fig-alt='Class boundaries for three models with increasing numbers of hidden units. The boundaries are fit on the training set and shown for the training and test sets. After a single hidden unit, the boundaries become wildly complex. The test set plots show that the more complex models do not conform to the data that was not used to fit the model.' width=672}\n:::\n:::\n\n\nThe single unit model does not adapt very flexibly to the data (since it is constrained to be linear). A model with four hidden units begins to show signs of overfitting with an unrealistic boundary for values away from the data mainstream. This is caused by a single data point from the first class in the upper-right corner of the data. By 20 hidden units, the model is beginning to memorize the training set, creating small islands around those data to minimize the resubstitution error rate. These patterns do not repeat in the test set. This last panel is the best illustration of how tuning parameters that control complexity must be modulated so that the model is effective. For a 20-unit model, the training set ROC AUC is 0.945 but the test set value is 0.847.\n\nThis occurrence of overfitting is obvious with two predictors that we can plot. However, in general, we must use a quantitative approach for detecting overfitting.\n\n::: rmdnote\nThe solution for detecting when a model is overemphasizing the training set is using out-of-sample data.\n:::\n\nRather than using the test set, some form of resampling is required. This could mean an iterative approach (e.g., 10-fold cross-validation) or a single data source (e.g., a validation set).\n\n## Two general strategies for optimization\n\nTuning parameter optimization usually falls into one of two categories: grid search and iterative search.\n\n*Grid search* is when we predefine a set of parameter values to evaluate. The main choices involved in grid search are how to make the grid and how many parameter combinations to evaluate. Grid search is often judged as inefficient since the number of grid points required to cover the parameter space can become unmanageable with the curse of dimensionality. There is truth to this concern, but it is most true when the process is not optimized. This is discussed more in [Chapter @sec-grid-search].\n\n*Iterative search* or sequential search is when we sequentially discover new parameter combinations based on previous results. Almost any nonlinear optimization method is appropriate, although some are more efficient than others. In some cases, an initial set of results for one or more parameter combinations is required to start the optimization process. Iterative search is discussed more in [Chapter @sec-iterative-search].\n\n@fig-tuning-strategies shows two panels that demonstrate these two approaches for a situation with two tuning parameters that range between zero and one. In each, a set of contours shows the true (simulated) relationship between the parameters and the outcome. The optimal results are in the upper-right-hand corners.\n\n\n::: {.cell layout-align=\"center\" hash='12-tuning-parameters_cache/html/fig-tuning-strategies_e11c9949ae7198c2e3e4dffce9686a79'}\n::: {.cell-output-display}\n![Examples of pre-defined grid tuning and an iterative search method. The lines represent contours of a performance metric; it is best in the upper-right-hand side of the plot.](figures/fig-tuning-strategies-1.png){#fig-tuning-strategies fig-align='center' fig-alt='Examples of pre-defined grid tuning and an iterative search method. The lines represent contours of some performance metric that is best in the upper-right-hand side of the plot. The grid search shows points that cover the space well and has one point near the optimum. The iterative search method has many more points and meanders to the optimum where many points zero in on the best value.' width=672}\n:::\n:::\n\n\nThe left-hand panel of @fig-tuning-strategies shows a type of grid called a space-filling design. This is a type of experimental design devised for covering the parameter space such that tuning parameter combinations are not close to one another. The results for this design do not place any points exactly at the truly optimal location. However, one point is in the general vicinity and would probably have performance metric results that are within the noise of the most optimal value.\n\nThe right-hand panel of @fig-tuning-strategies illustrates the results of a global search method: the Nelder-Mead simplex method [@Olsson:1975p3609]. The starting point is in the lower-left part of the parameter space. The search meanders across the space until it reaches the optimum location, where it strives to come as close as possible to the numerically best value. This particular search method, while effective, is not known for its efficiency; it requires many function evaluations, especially near the optimal values. [Chapter @sec-iterative-search] discusses more efficient search algorithms.\n\n::: rmdnote\nHybrid strategies are also an option and can work well. After an initial grid search, a sequential optimization can start from the best grid combination.\n:::\n\nExamples of these strategies are discussed in detail in the next two chapters. Before moving on, let's learn how to work with tuning parameter objects in tidymodels, using the <span class=\"pkg\">dials</span> package.\n\n## Tuning Parameters in tidymodels {#sec-tuning-params-tidymodels}\n\nWe've already dealt with quite a number of arguments that correspond to tuning parameters for recipe and model specifications in previous chapters. It is possible to tune:\n\n-   the threshold for combining neighborhoods into an \"other\" category (with argument name `threshold`) discussed in @sec-dummies\n\n-   the number of degrees of freedom in a natural spline (`deg_free`, @sec-spline-functions)\n\n-   the number of data points required to execute a split in a tree-based model (`min_n`, @sec-create-a-model)\n\n-   the amount of regularization in penalized models (`penalty`, @sec-create-a-model)\n\nFor <span class=\"pkg\">parsnip</span> model specifications, there are two kinds of parameter arguments. *Main arguments* are those that are most often optimized for performance and are available in multiple engines. The main tuning parameters are top-level arguments to the model specification function. For example, the `rand_forest()` function has main arguments `trees`, `min_n`, and `mtry` since these are most frequently specified or optimized.\n\nA secondary set of tuning parameters are *engine specific*. These are either infrequently optimized or are specific only to certain engines. Again using random forests as an example, the <span class=\"pkg\">ranger</span> package contains some arguments that are not used by other packages. One example is gain penalization, which regularizes the predictor selection in the tree induction process. This parameter can help modulate the trade-off between the number of predictors used in the ensemble and performance [@wundervald2020generalizing]. The name of this argument in `ranger()` is `regularization.factor`. To specify a value via a <span class=\"pkg\">parsnip</span> model specification, it is added as a supplemental argument to `set_engine()`:\n\n\n::: {.cell layout-align=\"center\" hash='12-tuning-parameters_cache/html/tuning-args_e7dec5c1727fd7b5bf829d03300247b0'}\n\n```{.r .cell-code}\nrand_forest(trees = 2000, min_n = 10) %>%                   # <- main arguments\n  set_engine(\"ranger\", regularization.factor = 0.5)         # <- engine-specific\n```\n:::\n\n\n::: rmdwarning\nThe main arguments use a harmonized naming system to remove inconsistencies across engines while engine-specific arguments do not.\n:::\n\nHow can we signal to tidymodels functions which arguments should be optimized? Parameters are marked for tuning by assigning them a value of `tune()`. For the single layer neural network used in @sec-overfitting-bad, the number of hidden units is designated for tuning using:\n\n\n::: {.cell layout-align=\"center\" hash='12-tuning-parameters_cache/html/tuning-mlp-units_80a4df1c98a39ef5c23023af463a85fe'}\n\n```{.r .cell-code}\nneural_net_spec <- \n  mlp(hidden_units = tune()) %>%\n  set_mode(\"regression\") %>%\n  set_engine(\"keras\")\n```\n:::\n\n\nThe `tune()` function doesn't execute any particular parameter value; it only returns an expression:\n\n\n::: {.cell layout-align=\"center\" hash='12-tuning-parameters_cache/html/tuning-tune-exprs_8baaf204113a3c41856e714855eb528c'}\n\n```{.r .cell-code}\ntune()\n## tune()\n```\n:::\n\n\nEmbedding this `tune()` value in an argument will tag the parameter for optimization. The model tuning functions shown in the next two chapters parse the model specification and/or recipe to discover the tagged parameters. These functions can automatically configure and process these parameters since they understand their characteristics (e.g., the range of possible values, etc.).\n\nTo enumerate the tuning parameters for an object, use the `extract_parameter_set_dials()` function:\n\n\n::: {.cell layout-align=\"center\" hash='12-tuning-parameters_cache/html/tuning-mlp-units-param_70ac7d25be89155fc2ca9ee77faf3583'}\n\n```{.r .cell-code}\nextract_parameter_set_dials(neural_net_spec)\n## Collection of 1 parameters for tuning\n## \n##    identifier         type    object\n##  hidden_units hidden_units nparam[+]\n```\n:::\n\n\nThe results show a value of `nparam[+]`, indicating that the number of hidden units is a numeric parameter.\n\nThere is an optional identification argument that associates a name with the parameters. This can come in handy when the same kind of parameter is being tuned in different places. For example, with the Ames housing data from @sec-resampling-summary, the recipe encoded both longitude and latitude with spline functions. If we want to tune the two spline functions to potentially have different levels of smoothness, we call `step_ns()` twice, once for each predictor. To make the parameters identifiable, the identification argument can take any character string:\n\n\n::: {.cell layout-align=\"center\" hash='12-tuning-parameters_cache/html/tuning-id_eaa80d3fd683a530c76dca96f5c6ff56'}\n\n```{.r .cell-code}\names_rec <- \n  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + \n           Latitude + Longitude, data = ames_train)  %>%\n  step_log(Gr_Liv_Area, base = 10) %>% \n  step_other(Neighborhood, threshold = tune()) %>% \n  step_dummy(all_nominal_predictors()) %>% \n  step_interact( ~ Gr_Liv_Area:starts_with(\"Bldg_Type_\") ) %>% \n  step_ns(Longitude, deg_free = tune(\"longitude df\")) %>% \n  step_ns(Latitude,  deg_free = tune(\"latitude df\"))\n\nrecipes_param <- extract_parameter_set_dials(ames_rec)\nrecipes_param\n## Collection of 3 parameters for tuning\n## \n##    identifier      type    object\n##     threshold threshold nparam[+]\n##  longitude df  deg_free nparam[+]\n##   latitude df  deg_free nparam[+]\n```\n:::\n\n\nNote that the `identifier` and `type` columns are not the same for both of the spline parameters.\n\nWhen a recipe and model specification are combined using a workflow, both sets of parameters are shown:\n\n\n::: {.cell layout-align=\"center\" hash='12-tuning-parameters_cache/html/tuning-mlp-rec_ba1630c87f7cac0fd93ee53c45b23a21'}\n\n```{.r .cell-code}\nwflow_param <- \n  workflow() %>% \n  add_recipe(ames_rec) %>% \n  add_model(neural_net_spec) %>% \n  extract_parameter_set_dials()\nwflow_param\n## Collection of 4 parameters for tuning\n## \n##    identifier         type    object\n##  hidden_units hidden_units nparam[+]\n##     threshold    threshold nparam[+]\n##  longitude df     deg_free nparam[+]\n##   latitude df     deg_free nparam[+]\n```\n:::\n\n\n::: rmdwarning\nNeural networks are exquisitely capable of emulating nonlinear patterns. Adding spline terms to this type of model is unnecessary; we combined this model and recipe for illustration only.\n:::\n\nEach tuning parameter argument has a corresponding function in the <span class=\"pkg\">dials</span> package. In the vast majority of the cases, the function has the same name as the parameter argument:\n\n\n::: {.cell layout-align=\"center\" hash='12-tuning-parameters_cache/html/tuning-dials-unit_330198a7e8e902272d5b9150d7141e42'}\n\n```{.r .cell-code}\nhidden_units()\n## # Hidden Units (quantitative)\n## Range: [1, 10]\nthreshold()\n## Threshold (quantitative)\n## Range: [0, 1]\n```\n:::\n\n\nThe `deg_free` parameter is a counterexample; the notion of degrees of freedom comes up in a variety of different contexts. When used with splines, there is a specialized <span class=\"pkg\">dials</span> function called `spline_degree()` that is, by default, invoked for splines:\n\n\n::: {.cell layout-align=\"center\" hash='12-tuning-parameters_cache/html/tuning-spline-df_653489d28b439fbe7994983b9e898937'}\n\n```{.r .cell-code}\nspline_degree()\n## Spline Degrees of Freedom (quantitative)\n## Range: [1, 10]\n```\n:::\n\n\nThe <span class=\"pkg\">dials</span> package also has a convenience function for extracting a particular parameter object:\n\n\n::: {.cell layout-align=\"center\" hash='12-tuning-parameters_cache/html/tuning-extract_f857b10b568f4e23eb85a763470ed2cf'}\n\n```{.r .cell-code}\n# identify the parameter using the id value:\nwflow_param %>% extract_parameter_dials(\"threshold\")\n## Threshold (quantitative)\n## Range: [0, 0.1]\n```\n:::\n\n\nInside the parameter set, the range of the parameters can also be updated in place:\n\n\n::: {.cell layout-align=\"center\" hash='12-tuning-parameters_cache/html/tuning-update_449a91c7d2873ba837894c49e53170c0'}\n\n```{.r .cell-code}\nextract_parameter_set_dials(ames_rec) %>% \n  update(threshold = threshold(c(0.8, 1.0)))\n## Collection of 3 parameters for tuning\n## \n##    identifier      type    object\n##     threshold threshold nparam[+]\n##  longitude df  deg_free nparam[+]\n##   latitude df  deg_free nparam[+]\n```\n:::\n\n\nThe *parameter sets* created by `extract_parameter_set_dials()` are consumed by the tidymodels tuning functions (when needed). If the defaults for the tuning parameter objects require modification, a modified parameter set is passed to the appropriate tuning function.\n\n::: rmdnote\nSome tuning parameters depend on the dimensions of the data. For example, the number of nearest neighbors must be between one and the number of rows in the data.\n:::\n\nIn some cases, it is easy to have reasonable defaults for the range of possible values. In other cases, the parameter range is critical and cannot be assumed. The primary tuning parameter for random forest models is the number of predictor columns that are randomly sampled for each split in the tree, usually denoted as `mtry()`. Without knowing the number of predictors, this parameter range cannot be preconfigured and requires finalization.\n\n\n::: {.cell layout-align=\"center\" hash='12-tuning-parameters_cache/html/tuning-rf_bd60cbdf7b88098101a734f5d00f6b3f'}\n\n```{.r .cell-code}\nrf_spec <- \n  rand_forest(mtry = tune()) %>% \n  set_engine(\"ranger\", regularization.factor = tune(\"regularization\")) %>%\n  set_mode(\"regression\")\n\nrf_param <- extract_parameter_set_dials(rf_spec)\nrf_param\n## Collection of 2 parameters for tuning\n## \n##      identifier                  type    object\n##            mtry                  mtry nparam[?]\n##  regularization regularization.factor nparam[+]\n## \n## Model parameters needing finalization:\n##    # Randomly Selected Predictors ('mtry')\n## \n## See `?dials::finalize` or `?dials::update.parameters` for more information.\n```\n:::\n\n\nComplete parameter objects have `[+]` in their summary; a value of `[?]` indicates that at least one end of the possible range is missing. There are two methods for handling this. The first is to use `update()`, to add a range based on what you know about the data dimensions:\n\n\n::: {.cell layout-align=\"center\" hash='12-tuning-parameters_cache/html/tuning-rfupdate_dfa6ed22ad9bfc76ed3f1a9b0cf57a57'}\n\n```{.r .cell-code}\nrf_param %>% \n  update(mtry = mtry(c(1, 70)))\n## Collection of 2 parameters for tuning\n## \n##      identifier                  type    object\n##            mtry                  mtry nparam[+]\n##  regularization regularization.factor nparam[+]\n```\n:::\n\n\nHowever, this approach might not work if a recipe is attached to a workflow that uses steps that either add or subtract columns. If those steps are not slated for tuning, the `finalize()` function can execute the recipe once to obtain the dimensions:\n\n\n::: {.cell layout-align=\"center\" hash='12-tuning-parameters_cache/html/tuning-finalize_f6ae029c465ec6f562572d56e4a09b8c'}\n\n```{.r .cell-code}\npca_rec <- \n  recipe(Sale_Price ~ ., data = ames_train) %>% \n  # Select the square-footage predictors and extract their PCA components:\n  step_normalize(contains(\"SF\")) %>% \n  # Select the number of components needed to capture 95% of\n  # the variance in the predictors. \n  step_pca(contains(\"SF\"), threshold = .95)\n  \nupdated_param <- \n  workflow() %>% \n  add_model(rf_spec) %>% \n  add_recipe(pca_rec) %>% \n  extract_parameter_set_dials() %>% \n  finalize(ames_train)\nupdated_param\n## Collection of 2 parameters for tuning\n## \n##      identifier                  type    object\n##            mtry                  mtry nparam[+]\n##  regularization regularization.factor nparam[+]\nupdated_param %>% extract_parameter_dials(\"mtry\")\n## # Randomly Selected Predictors (quantitative)\n## Range: [1, 74]\n```\n:::\n\n\nWhen the recipe is prepared, the `finalize()` function learns to set the upper range of `mtry` to 74 predictors.\n\nAdditionally, the results of `extract_parameter_set_dials()` will include engine-specific parameters (if any). They are discovered in the same way as the main arguments and included in the parameter set. The <span class=\"pkg\">dials</span> package contains parameter functions for all potentially tunable engine-specific parameters:\n\n\n::: {.cell layout-align=\"center\" hash='12-tuning-parameters_cache/html/tuning-engine-param-set_efb03a25c2b7bb3a49ff66ec6728ed59'}\n\n```{.r .cell-code}\nrf_param\n## Collection of 2 parameters for tuning\n## \n##      identifier                  type    object\n##            mtry                  mtry nparam[?]\n##  regularization regularization.factor nparam[+]\n## \n## Model parameters needing finalization:\n##    # Randomly Selected Predictors ('mtry')\n## \n## See `?dials::finalize` or `?dials::update.parameters` for more information.\nregularization_factor()\n## Gain Penalization (quantitative)\n## Range: [0, 1]\n```\n:::\n\n\nFinally, some tuning parameters are best associated with transformations. A good example of this is the penalty parameter associated with many regularized regression models. This parameter is nonnegative and it is common to vary its values in log units. The primary <span class=\"pkg\">dials</span> parameter object indicates that a transformation is used by default:\n\n\n::: {.cell layout-align=\"center\" hash='12-tuning-parameters_cache/html/tuning-penalty_5ef74c28d4b8d466b640ae77fff3f45d'}\n\n```{.r .cell-code}\npenalty()\n## Amount of Regularization (quantitative)\n## Transformer: log-10 [1e-100, Inf]\n## Range (transformed scale): [-10, 0]\n```\n:::\n\n\nThis is important to know, especially when altering the range. New range values must be in the transformed units:\n\n\n::: {.cell layout-align=\"center\" hash='12-tuning-parameters_cache/html/tuning-penalty-set_9cce9fed575a46aa5bc868b1ac499e91'}\n\n```{.r .cell-code}\n# correct method to have penalty values between 0.1 and 1.0\npenalty(c(-1, 0)) %>% value_sample(1000) %>% summary()\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   0.101   0.181   0.327   0.400   0.589   0.999\n\n# incorrect:\npenalty(c(0.1, 1.0)) %>% value_sample(1000) %>% summary()\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    1.26    2.21    3.68    4.26    5.89   10.00\n```\n:::\n\n\nThe scale can be changed if desired with the `trans` argument. You can use natural units but the same range:\n\n\n::: {.cell layout-align=\"center\" hash='12-tuning-parameters_cache/html/tuning-penalty-natural_3e3301c319dfedaf45f00adfa3369733'}\n\n```{.r .cell-code}\npenalty(trans = NULL, range = 10^c(-10, 0))\n## Amount of Regularization (quantitative)\n## Range: [1e-10, 1]\n```\n:::\n\n\n## Chapter Summary\n\nThis chapter introduced the process of tuning model hyperparameters that cannot be directly estimated from the data. Tuning such parameters can lead to overfitting, often by allowing a model to grow overly complex, so using resampled data sets together with appropriate metrics for evaluation is important. There are two general strategies for determining the right values, grid search and iterative search, which we will explore in depth in the next two chapters. In tidymodels, the `tune()` function is used to identify parameters for optimization, and functions from the <span class=\"pkg\">dials</span> package can extract and interact with tuning parameters objects.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}