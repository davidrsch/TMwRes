```{r ensembles-setup, include = FALSE}
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
library(rules)
library(baguette)
library(stacks)
library(patchwork)
library(kableExtra)

load("RData/concrete_results.RData")
```

# Conjuntos de modelos {#sec-ensembles}

Un conjunto de modelos, donde las predicciones de varios alumnos individuales se agregan para hacer una predicción, puede producir un modelo final de alto rendimiento. Los métodos más populares para crear modelos de conjunto son el embolsado [@breiman1996bagging], el bosque aleatorio [@ho1995random; @breiman2001random] e impulsando [@freund1997decision]. Cada uno de estos métodos combina las predicciones de múltiples versiones del mismo tipo de modelo (por ejemplo, árboles de clasificación). Sin embargo, uno de los primeros métodos para crear conjuntos es *apilamiento de modelos* [@wolpert1992stacked; @breiman1996stacked].

::: rmdnote
El apilamiento de modelos combina las predicciones de múltiples modelos de cualquier tipo. Por ejemplo, en un conjunto de apilamiento se pueden incluir una regresión logística, un árbol de clasificación y una máquina de vectores de soporte.
:::

Este capítulo muestra cómo apilar modelos predictivos usando el paquete `r pkg(stacks)`. Reutilizaremos los resultados del [Capítulo @sec-workflow-sets] donde se evaluaron múltiples modelos para predecir la resistencia a la compresión de las mezclas de concreto.

El proceso de construcción de un conjunto apilado es:

1.  Ensamble el conjunto de entrenamiento de predicciones de reserva (producidas mediante remuestreo).
2.  Cree un modelo para combinar estas predicciones.
3.  Para cada miembro del conjunto, ajuste el modelo en el conjunto de entrenamiento original.

En secciones siguientes, describiremos este proceso. Sin embargo, antes de continuar, aclararemos algunas nomenclaturas para las variaciones de lo que puede significar "el modelo". ¡Esto puede convertirse rápidamente en un término sobrecargado cuando trabajamos en un análisis de modelado complejo! Consideremos el modelo de perceptrón multicapa (MLP) (también conocido como red neuronal) creado en el [Capítulo @sec-workflow-sets].

En general, hablaremos de un modelo MLP como el *tipo* de modelo. Otros tipos de modelos son las máquinas de regresión lineal y de vectores de soporte.

Los parámetros de ajuste son un aspecto importante de un modelo. En el [Capítulo @sec-workflow-sets], el modelo MLP se ajustó en más de 25 valores de parámetros de ajuste. En los capítulos anteriores, hemos llamado a estos valores de *parámetros de ajuste candidatos* o *configuraciones de modelo*. En la literatura sobre ensamblaje, estos también se denominan modelos base.

::: rmdnote
Usaremos el término *miembros candidatos* para describir las posibles configuraciones del modelo (de todos los tipos de modelos) que podrían incluirse en el conjunto de apilamiento.
:::

Esto significa que un modelo de apilamiento puede incluir diferentes tipos de modelos (por ejemplo, árboles y redes neuronales), así como diferentes configuraciones del mismo modelo (por ejemplo, árboles con diferentes profundidades).

## Crear el conjunto de entrenamiento para apilar {#sec-data-stack}

El primer paso para construir un conjunto apilado se basa en las predicciones del conjunto de evaluación a partir de un esquema de remuestreo con múltiples divisiones. Para cada punto de datos en el conjunto de entrenamiento, el apilamiento requiere algún tipo de predicción fuera de la muestra. Para los modelos de regresión, este es el resultado previsto. Para los modelos de clasificación, las clases o probabilidades predichas están disponibles para su uso, aunque estas últimas contienen más información que las predicciones de clases estrictas. Para un conjunto de modelos, se ensambla un conjunto de datos donde las filas son las muestras del conjunto de entrenamiento y las columnas son las predicciones fuera de la muestra del conjunto de múltiples modelos.

En el [Capítulo @sec-workflow-sets], utilizamos cinco repeticiones de validación cruzada 10 veces para volver a muestrear los datos. Este esquema de remuestreo genera cinco predicciones de conjuntos de evaluación para cada muestra de conjunto de entrenamiento. Pueden ocurrir múltiples predicciones fuera de la muestra en varias otras técnicas de remuestreo (por ejemplo, arranque). A los efectos del apilamiento, cualquier predicción replicada para un punto de datos en el conjunto de entrenamiento se promedia de modo que haya una única predicción por muestra del conjunto de entrenamiento por miembro candidato.

::: rmdnote
También se pueden utilizar conjuntos de validación simples con el apilamiento, ya que tidymodels considera que se trata de un remuestreo único.
:::

Para el ejemplo concreto, el conjunto de entrenamiento utilizado para el apilamiento de modelos tiene columnas para todos los resultados de los parámetros de ajuste candidatos. @tbl-ensemble-candidate-preds presenta las primeras seis filas y columnas seleccionadas.

```{r}
#| label: tbl-ensemble-candidate-preds
#| echo: FALSE
#| results: "asis"
#| warning: FALSE
#| message: FALSE
#| tbl-cap: "Predicciones a partir de configuraciones de parámetros de ajuste candidatos."

stacks() %>% 
  add_candidates(grid_results) %>% 
  as_tibble() %>% 
  mutate(
    sample_num = row_number(),
    buffer_1 = "",
    buffer_2 = "") %>% 
  slice_head(n = 6) %>% 
  select(sample_num, CART_bagged_1_1, starts_with("MARS"), Cubist_1_01,
         buffer_1, Cubist_1_18, buffer_2) %>% 
  knitr::kable(
    digits = 2,
    align = rep("c", 8),
    col.names = c("Muestra #", "Bagged Tree", "MARS 1", "MARS 2", "Cubist 1", 
                  "...", "Cubist 25", "...")) %>% 
  kable_styling("striped", full_width = TRUE) %>% 
  add_header_above(c(" ", "Predicciones Candidatas Apiladas" = 7)) %>% 
  row_spec(0, align = "c")
```

Hay una sola columna para el modelo de árbol en bolsas ya que no tiene parámetros de ajuste. Además, recuerde que MARS se sintonizó en función de un único parámetro (el grado del producto) con dos configuraciones posibles, por lo que este modelo está representado por dos columnas. La mayoría de los otros modelos tienen 25 columnas correspondientes, como se muestra para Cubist en este ejemplo.

::: rmdwarning
Para los modelos de clasificación, las columnas de predicción candidatas serían probabilidades de clase predichas. Dado que estas columnas suman una para cada modelo, las probabilidades de una de las clases pueden omitirse.
:::

Para resumir dónde nos encontramos hasta ahora, el primer paso para el apilamiento es ensamblar las predicciones del conjunto de evaluación para el conjunto de entrenamiento de cada modelo candidato. Podemos utilizar estas predicciones de conjuntos de evaluación para avanzar y construir un conjunto apilado.

Para comenzar a ensamblar con el paquete `r pkg(stacks)`, cree una pila de datos vacía usando la función `stacks()` y luego agregue modelos candidatos. Recuerde que utilizamos conjuntos de flujos de trabajo para ajustar una amplia variedad de modelos a estos datos. Usaremos los resultados de las carreras:

```{r ensembles-race}
race_results
```

En este caso, nuestra sintaxis es:

```{r ensembles-data-stack}
library(tidymodels)
library(stacks)
tidymodels_prefer()

concrete_stack <- 
  stacks() %>% 
  add_candidates(race_results)

concrete_stack
```

Recuerde que los métodos de carrera (@sec-racing) son más eficientes ya que es posible que no evalúen todas las configuraciones en todos los remuestreos. El apilamiento requiere que todos los miembros candidatos tengan el conjunto completo de remuestras. `add_candidates()` incluye solo las configuraciones del modelo que tienen resultados completos.

::: rmdnote
¿Por qué utilizar los resultados de las carreras en lugar del conjunto completo de modelos candidatos contenidos en `grid_results`? Se puede utilizar cualquiera de los dos. Encontramos un mejor rendimiento para estos datos utilizando los resultados de las carreras. Esto podría deberse a que el método de carrera preselecciona los mejores modelos de la parrilla más grande.
:::

Si no hubiéramos usado el paquete `r pkg(workflowsets)`, los objetos de `r pkg(tune)` y `r pkg(finetune)` también podrían pasarse a `add_candidates()`. Esto puede incluir objetos de búsqueda iterativos y de cuadrícula.

## Combina las predicciones {#sec-blend-predictions}

Las predicciones del conjunto de entrenamiento y los datos de resultados observados correspondientes se utilizan para crear un *modelo de metaaprendizaje* donde las predicciones del conjunto de evaluación son los predictores de los datos de resultados observados. El metaaprendizaje se puede lograr utilizando cualquier modelo. El modelo más utilizado es un modelo lineal generalizado regularizado, que abarca modelos lineales, logísticos y multinomiales. Específicamente, la regularización mediante la penalización de lazo [@lasso], que utiliza la contracción para atraer puntos hacia un valor central, tiene varias ventajas:

-   El uso de la penalización de lazo puede eliminar candidatos (y, a veces, tipos de modelos completos) del conjunto.
-   La correlación entre los candidatos a conjuntos tiende a ser muy alta y la regularización ayuda a aliviar este problema.

@breiman1996stacked también sugirió que, cuando se utiliza un modelo lineal para combinar las predicciones, podría ser útil restringir los coeficientes de combinación para que no sean negativos. En general, hemos encontrado que este es un buen consejo y es el valor predeterminado para el paquete `r pkg(stacks)` (pero se puede cambiar mediante un argumento opcional).

Dado que nuestro resultado es numérico, se utiliza la regresión lineal para el metamodelo. Ajustar el metamodelo es tan sencillo como usar:

```{r ensembles-initial-blend}
set.seed(2001)
ens <- blend_predictions(concrete_stack)
```

Esto evalúa el modelo de metaaprendizaje sobre una cuadrícula predefinida de valores de penalización de lazo y utiliza un método de remuestreo interno para determinar el mejor valor. El método `autoplot()`, que se muestra en @fig-stacking-autoplot, nos ayuda a comprender si el método de penalización predeterminado fue suficiente:

```{r ensembles-initial-blend-plot, eval=FALSE}
autoplot(ens)
```

```{r}
#| label: fig-stacking-autoplot
#| ref.label: "ensembles-initial-blend-plot"
#| echo: FALSE
#| fig.cap: "Resultados del uso del método `autoplot()` en el objeto de pilas combinadas"
#| fig.alt: "Resultados del uso del método `autoplot()` en el objeto de pilas combinadas."
```

El panel superior de @fig-stacking-autoplot muestra el número promedio de miembros candidatos del conjunto retenidos por el modelo de metaaprendizaje. Podemos ver que el número de miembros es bastante constante y, a medida que aumenta, el RMSE también aumenta.

Es posible que el rango predeterminado no nos haya servido bien aquí. Para evaluar el modelo de metaaprendizaje con penalizaciones mayores, pasemos una opción adicional:

```{r ensembles-second-blend}
set.seed(2002)
ens <- blend_predictions(concrete_stack, penalty = 10^seq(-2, -0.5, length = 20))
```

Ahora, en @fig-stacking-autoplot-redo, vemos un rango en el que el modelo de conjunto se vuelve peor que con nuestra primera combinación (pero no mucho). Los valores de $R^2$ aumentan con más miembros y sanciones mayores.

```{r ensembles-autoplot-calc, eval = FALSE}
autoplot(ens)
```

```{r}
#| label: fig-stacking-autoplot-redo
#| ref.label: "ensembles-autoplot-calc"
#| echo: FALSE
#| fig.cap: "Los resultados del uso del método `autoplot()` en el objeto de pilas combinadas actualizado"
#| fig.alt: "Los resultados del uso del método `autoplot()` en el objeto de pilas combinadas actualizado."
```

Al combinar predicciones utilizando un modelo de regresión, es común restringir los parámetros de combinación para que no sean negativos. Para estos datos, esta restricción tiene el efecto de eliminar muchos de los miembros potenciales del conjunto; Incluso con sanciones bastante bajas, el conjunto se limita a una fracción de los dieciocho originales.

El valor de penalización asociado con el RMSE más pequeño fue `r signif(ens$penalty$penalty, 2)`. Imprimir el objeto muestra los detalles del modelo de metaaprendizaje:

```{r ensembles-second-blend-print}
ens
```

```{r ensembles-details, include = FALSE}
res <- stacks:::top_coefs(ens)
model_key <- 
  tribble(
    ~ type, ~ descr,
    'bag_tree', "bagged tree",
    'boost_tree', "boosted tree",
    'cubist_rules', "Cubist",
    'decision_tree', "decision tree",
    'linear_reg', "linear regression",
    'mars', "multivariate adaptive regression splines",
    'mlp', "neural network",
    'nearest_neighbor', "K-nearest neighbors",
    'rand_forest', "random forest",
    'svm_poly', "support vector machine (polynomial)",
    'svm_rbf', "support vector machine (RBF)"
  )

res <- left_join(res, model_key, by = "type")
top_two <- paste(res$descr[1:2], collapse = " and ")
blending_alt <- 
  glue::glue('fig.alt = "Coeficientes de fusión para el conjunto de apilamiento. Los modelos {top_two} tienen los mayores efectos en las predicciones del conjunto."')

num_coefs <- xfun::numbers_to_words(nrow(res))
num_types <- xfun::numbers_to_words(length(unique(res$type)))
```

El modelo de metaaprendizaje de regresión lineal regularizado contenía coeficientes de combinación `r num_coefs` entre tipos de modelos `r num_types`. El método `autoplot()` se puede utilizar nuevamente para mostrar las contribuciones de cada tipo de modelo, para producir @fig-blending-weights.

```{r ensembles-blending-weights, eval = FALSE}
autoplot(ens, "weights") +
  geom_text(aes(x = weight + 0.01, label = model), hjust = 0) + 
  theme(legend.position = "none") +
  lims(x = c(-0.01, 0.8))
```

```{r}
#| label: fig-blending-weights
#| ref.label: "ensembles-blending-weights"
#| echo: FALSE
#| fig.cap: "Coeficientes de mezcla para el conjunto de apilamiento."
#| fig.alt: blending_alt
```

Los modelos `r top_two` tienen las mayores contribuciones al conjunto. Para este conjunto, el resultado se predice con la ecuación:

```{r ensembles-equation, echo = FALSE, results = "asis", message = FALSE, warning = FALSE}
all_members <- 
  tibble(
    member = unname(unlist(ens$cols_map)), 
    obj = rep(names(ens$cols_map), map_int(ens$cols_map, length))
  ) %>% 
  inner_join(res, by = "member") %>% 
  arrange(type, member)

glmn_int <- 
  tidy(ens$coefs) %>% 
  filter(term == "(Intercept)") %>% 
  mutate(estimate = format(estimate, digits = 2))

config_label <- function(x) {
  x <- dplyr::arrange(x, member)
  x$type <- gsub("_", " ", x$type)
  if (length(unique(x$member)) == 1) {
    x$config <- paste(x$type, "prediction")
  } else {
    congif_chr <- paste0("prediction (config ", 1:nrow(x), ")")
    x$config <- paste(x$type, congif_chr)
  }
  x$weight <- format(x$weight, digits = 2, scientific = FALSE)
  x$term <- paste0(x$weight, " \\times \\text{", x$config, "} \\notag")
  select(x, term, weight)
}
tmp <- 
  all_members %>% 
  group_nest(obj, keep = TRUE) %>% 
  mutate(data = map(data, ~ config_label(.x))) %>% 
  unnest(cols = "data") %>% 
  arrange(desc(weight))

eqn <- paste(c(glmn_int$estimate, tmp$term), collapse = " \\\\\n\t+&")
eqn <- paste0("\n\\begin{align}\n \\text{ensemble prediction} &=", eqn, "\n\\end{align}\n")

cat(eqn)
```

donde los predictores en la ecuación son los valores de resistencia a la compresión pronosticados a partir de esos modelos.

## Ajustar los modelos de miembros {#sec-fit-members}

El conjunto contiene miembros candidatos `r num_coefs` y ahora sabemos cómo se pueden combinar sus predicciones en una predicción final para el conjunto. Sin embargo, estos ajustes de modelos individuales aún no se han creado. Para poder utilizar el modelo de apilamiento, se requieren ajustes de modelo adicionales `r num_coefs`. Estos utilizan todo el conjunto de entrenamiento con los predictores originales.

Los modelos `r num_coefs` a ajustar son:

```{r ensembles-show-members, echo = FALSE, results = "asis"}
param_filter <- function(object, config, stack_obj) {
  res <- 
    collect_parameters(stack_obj, candidates = object) %>% 
    dplyr::filter(member == config) %>% 
    dplyr::select(-coef)
  
  params <- res %>% dplyr::select(-member)
  param_labs <- map_chr(names(params), name_to_label)
  if (length(param_labs) > 0) {
    names(params) <- param_labs
    fmt <- format(as.data.frame(params), digits = 3)
    fmt <- as.matrix(fmt)[1,,drop = FALSE]
    chr_param <- paste0(colnames(fmt), " = ", unname(fmt))
    chr_param <- knitr::combine_words(chr_param)
    items <- paste0("- ", gsub("_", " ", object), ": ", chr_param)
  } else {
    items <- paste0("- ", gsub("_", " ", object))
  }
  items
}
name_to_label <- function(x) {
  if (x %in% c("committees")) {
    ns <- "rules"
  } else {
    ns <- "dials"
  }
  .fn <- rlang::call2(x, .ns = ns)
  object <- rlang::eval_tidy(.fn)
  res <- unname(object$label)
  res <- gsub("^# ", "number of ", tolower(res))
  res
}
config_text <- function(x) {
  if (nrow(x) == 1) {
    res <- "\n\n"
  } else {
    res <- paste0(" (config ", 1:nrow(x), ")\n\n")
  }
  res
}
get_configs <- function(x) {
  dplyr::group_nest(x, type) %>% 
    dplyr::mutate(confg = purrr::map(data, config_text)) %>% 
    dplyr::select(confg) %>% 
    tidyr::unnest(cols = c(confg)) %>% 
    purrr::pluck("confg")
}

param <- map2_chr(all_members$obj, all_members$member, param_filter, ens)
param <- paste0(param, get_configs(all_members))
param <- gsub("full quad linear reg", "linear regression (quadratic features)", param)
param <- gsub("number of observations sampled", "proportion of observations sampled", param)
cat(param, sep = "")

ens_rmse <- dplyr::filter(ens$metrics, penalty == ens$penalty$penalty & .metric == "rmse")$mean
boost_rmse <- collect_metrics(boosting_test_results) %>% dplyr::filter(.metric == "rmse")
```

El paquete `r pkg(stacks)` tiene una función, `fit_members()`, que entrena y devuelve estos modelos:

```{r ensembles-fit-members}
ens <- fit_members(ens)
```

Esto actualiza el objeto de apilamiento con los objetos de flujo de trabajo ajustados para cada miembro. En este punto, el modelo de apilamiento se puede utilizar para la predicción.

## Resultados del conjunto de pruebas

Dado que el proceso de combinación utilizó remuestreo, podemos estimar que el conjunto con miembros `r num_coefs` tenía un RMSE estimado de `r round(ens_rmse, 2)`. Recuerde del [Capítulo @sec-workflow-sets] que el árbol mejor impulsado tenía un conjunto de prueba RMSE de `r round(boost_rmse$.estimate, 2)`. ¿Cómo se comparará el modelo de conjunto en el conjunto de prueba? Podemos usar `predecit()` para averiguarlo:

```{r ensembles-test-set}
reg_metrics <- metric_set(rmse, rsq)
ens_test_pred <- 
  predict(ens, concrete_test) %>% 
  bind_cols(concrete_test)

ens_test_pred %>% 
  reg_metrics(compressive_strength, .pred)
```

Esto es moderadamente mejor que nuestro mejor modelo individual. Es bastante común que el apilamiento produzca beneficios incrementales en comparación con el mejor modelo individual.

## Resumen del capítulo {#sec-ensembles-summary}

Este capítulo demostró cómo combinar diferentes modelos en un conjunto para obtener un mejor rendimiento predictivo. El proceso de creación del conjunto puede eliminar automáticamente los modelos candidatos para encontrar un pequeño subconjunto que mejore el rendimiento. El paquete `r pkg(stacks)` tiene una interfaz fluida para combinar resultados de remuestreo y ajuste en un metamodelo.
