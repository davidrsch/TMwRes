[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modelado Ordenado con R",
    "section": "",
    "text": "Hello World\nWelcome to Tidy Modeling with R! This book is a guide to using a collection of software in the R programming language for model building called tidymodels, and it has two main goals:\nIn Chapter 1, we outline a taxonomy for models and highlight what good software for modeling is like. The ideas and syntax of the tidyverse, which we introduce (or review) in Chapter 2, are the basis for the tidymodels approach to these challenges of methodology and practice. Chapter 3 provides a quick tour of conventional base R modeling functions and summarizes the unmet needs in that area.\nAfter that, this book is separated into parts, starting with the basics of modeling with tidy data principles. Chapters -Chapter 4 through -?sec-performance introduces an example data set on house prices and demonstrates how to use the fundamental tidymodels packages: recipes, parsnip, workflows, yardstick, and others.\nThe next part of the book moves forward with more details on the process of creating an effective model. Chapters -?sec-resampling through -?sec-workflow-sets focus on creating good estimates of performance as well as tuning model hyperparameters.\nFinally, the last section of this book, Chapters -?sec-dimensionality through -?sec-inferential, covers other important topics for model building. We discuss more advanced feature engineering approaches like dimensionality reduction and encoding high cardinality predictors, as well as how to answer questions about why a model makes certain predictions and when to trust your model predictions.\nWe do not assume that readers have extensive experience in model building and statistics. Some statistical knowledge is required, such as random sampling, variance, correlation, basic linear regression, and other topics that are usually found in a basic undergraduate statistics or data analysis course. We do assume that the reader is at least slightly familiar with dplyr, ggplot2, and the %&gt;% “pipe” operator in R, and is interested in applying these tools to modeling. For users who don’t yet have this background R knowledge, we recommend books such as R for Data Science by Wickham and Grolemund (2016). Investigating and analyzing data are an important part of any model process.\nThis book is not intended to be a comprehensive reference on modeling techniques; we suggest other resources to learn more about the statistical methods themselves. For general background on the most common type of model, the linear model, we suggest Fox (2008). For predictive models, Kuhn and Johnson (2013) and Kuhn and Johnson (2020) are good resources. For machine learning methods, Goodfellow, Bengio, and Courville (2016) is an excellent (but formal) source of information. In some cases, we do describe the models we use in some detail, but in a way that is less mathematical, and hopefully more intuitive."
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Modelado Ordenado con R",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nWe are so thankful for the contributions, help, and perspectives of people who have supported us in this project. There are several we would like to thank in particular.\nWe would like to thank our RStudio colleagues on the tidymodels team (Davis Vaughan, Hannah Frick, Emil Hvitfeldt, and Simon Couch) as well as the rest of our coworkers on the RStudio open source team. Thank you to Desirée De Leon for the site design of the online work. We would also like to thank our technical reviewers, Chelsea Parlett-Pelleriti and Dan Simpson, for their detailed, insightful feedback that substantively improved this book, as well as our editors, Nicole Tache and Rita Fernando, for their perspective and guidance during the process of writing and publishing.\n\nThis book was written in the open, and multiple people contributed via pull requests or issues. Special thanks goes to the thirty-eight people who contributed via GitHub pull requests (in alphabetical order by username): @arisp99, Brad Hill (@bradisbrad), Bryce Roney (@bryceroney), Cedric Batailler (@cedricbatailler), Ildikó Czeller (@czeildi), David Kane (@davidkane9), @DavZim, @DCharIAA, Emil Hvitfeldt (@EmilHvitfeldt), Emilio (@emilopezcano), Fgazzelloni (@Fgazzelloni), Hannah Frick (@hfrick), Hlynur (@hlynurhallgrims), Howard Baek (@howardbaek), Jae Yeon Kim (@jaeyk), Jonathan D. Trattner (@jdtrat), Jeffrey Girard (@jmgirard), John W Pickering (@JohnPickering), Jon Harmon (@jonthegeek), Joseph B. Rickert (@joseph-rickert), Maximilian Rohde (@maxdrohde), Michael Grund (@michaelgrund), @MikeJohnPage, Mine Cetinkaya-Rundel (@mine-cetinkaya-rundel), Mohammed Hamdy (@mmhamdy), @nattalides, Y. Yu (@PursuitOfDataScience), Riaz Hedayati (@riazhedayati), Rob Wiederstein (@RobWiederstein), Scott (@scottyd22), Simon Schölzel (@simonschoe), Simon Sayz (@tagasimon), @thrkng, Tanner Stauss (@tmstauss), Tony ElHabr (@tonyelhabr), Dmitry Zotikov (@x1o), Xiaochi (@xiaochi-liu), Zach Bogart (@zachbogart)."
  },
  {
    "objectID": "index.html#using-code-examples",
    "href": "index.html#using-code-examples",
    "title": "Modelado Ordenado con R",
    "section": "Using Code Examples",
    "text": "Using Code Examples\nThis book was written with RStudio using bookdown. The website is hosted via Netlify, and automatically built after every push by GitHub Actions. The complete source is available on GitHub. We generated all plots in this book using ggplot2 and its black and white theme (theme_bw()).\nThis version of the book was built with R version 4.3.1 (2023-06-16 ucrt), pandoc version 3.1.1, and the following packages: applicable (0.1.0, CRAN), av (0.8.5, CRAN), baguette (1.0.1, CRAN), beans (0.1.0, CRAN), bestNormalize (1.9.1, CRAN), bookdown (0.35, CRAN), broom (1.0.5, CRAN), censored (0.2.0, CRAN), corrplot (0.92, CRAN), corrr (0.4.4, CRAN), Cubist (0.4.2.1, CRAN), DALEXtra (2.3.0, CRAN), dials (1.2.0, CRAN), dimRed (0.2.6, CRAN), discrim (1.0.1, CRAN), doMC (1.3.5, R-Forge), dplyr (1.1.3, CRAN), earth (5.3.2, CRAN), embed (1.1.2, CRAN), fastICA (1.2-3, CRAN), finetune (1.1.0, CRAN), forcats (1.0.0, CRAN), ggforce (0.4.1, CRAN), ggplot2 (3.4.3, CRAN), glmnet (4.1-8, CRAN), gridExtra (2.3, CRAN), infer (1.0.5, CRAN), kableExtra (1.3.4, CRAN), kernlab (0.9-32, CRAN), kknn (1.3.1, CRAN), klaR (1.7-2, CRAN), knitr (1.44, CRAN), learntidymodels (0.0.0.9001, Github), lime (0.5.3, CRAN), lme4 (1.1-34, CRAN), lubridate (1.9.3, CRAN), mda (0.5-4, CRAN), mixOmics (6.24.0, Bioconductor), modeldata (1.2.0, CRAN), multilevelmod (1.0.0, CRAN), nlme (3.1-162, CRAN), nnet (7.3-19, CRAN), parsnip (1.1.1, CRAN), patchwork (1.1.3, CRAN), pillar (1.9.0, CRAN), poissonreg (1.0.1, CRAN), prettyunits (1.2.0, CRAN), probably (1.0.2, CRAN), pscl (1.5.5.1, CRAN), purrr (1.0.2, CRAN), ranger (0.15.1, CRAN), recipes (1.0.8, CRAN), rlang (1.1.1, CRAN), rmarkdown (2.25, CRAN), rpart (4.1.19, CRAN), rsample (1.2.0, CRAN), rstanarm (2.26.1, CRAN), rules (1.0.2, CRAN), sessioninfo (1.2.2, CRAN), stacks (1.0.2, CRAN), stringr (1.5.0, CRAN), svglite (2.1.1, CRAN), text2vec (0.6.3, CRAN), textrecipes (1.0.4, CRAN), themis (1.0.2, CRAN), tibble (3.2.1, CRAN), tidymodels (1.1.1, CRAN), tidyposterior (1.0.0, CRAN), tidyverse (2.0.0, CRAN), tune (1.1.2, CRAN), uwot (0.1.16, CRAN), workflows (1.1.3, CRAN), workflowsets (1.0.1, CRAN), xgboost (1.7.5.1, CRAN), and yardstick (1.2.0, CRAN).\n\n\n\n\nFox, J. 2008. Applied Regression Analysis and Generalized Linear Models. Second. Thousand Oaks, CA: Sage.\n\n\nGoodfellow, I, Y Bengio, and A Courville. 2016. Deep Learning. MIT Press.\n\n\nKuhn, M, and K Johnson. 2013. Applied Predictive Modeling. Springer.\n\n\n———. 2020. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press."
  },
  {
    "objectID": "14-iterative-search.html#svm",
    "href": "14-iterative-search.html#svm",
    "title": "14  Iterative Search",
    "section": "\n14.1 A Support Vector Machine Model",
    "text": "14.1 A Support Vector Machine Model\nWe once again use the cell segmentation data, described in Section @ref(evaluating-grid), for modeling, with a support vector machine (SVM) model to demonstrate sequential tuning methods. See Kuhn and Johnson (2013) for more information on this model. The two tuning parameters to optimize are the SVM cost value and the radial basis function kernel parameter \\(\\sigma\\). Both parameters can have a profound effect on the model complexity and performance.\nThe SVM model uses a dot product and, for this reason, it is necessary to center and scale the predictors. Like the multilayer perceptron model, this model would benefit from the use of PCA feature extraction. However, we will not use this third tuning parameter in this chapter so that we can visualize the search process in two dimensions.\nAlong with the previously used objects (shown in Section @ref(grid-summary)), the tidymodels objects svm_rec, svm_spec, and svm_wflow define the model process:\n\nlibrary(tidymodels)\ntidymodels_prefer()\n\nsvm_rec &lt;- \n  recipe(class ~ ., data = cells) %&gt;%\n  step_YeoJohnson(all_numeric_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\nsvm_spec &lt;- \n  svm_rbf(cost = tune(), rbf_sigma = tune()) %&gt;% \n  set_engine(\"kernlab\") %&gt;% \n  set_mode(\"classification\")\n\nsvm_wflow &lt;- \n  workflow() %&gt;% \n  add_model(svm_spec) %&gt;% \n  add_recipe(svm_rec)\n\nThe default parameter ranges for the two tuning parameters cost and rbf_sigma are:\n\ncost()\n## Cost (quantitative)\n## Transformer: log-2 [1e-100, Inf]\n## Range (transformed scale): [-10, 5]\nrbf_sigma()\n## Radial Basis Function sigma (quantitative)\n## Transformer: log-10 [1e-100, Inf]\n## Range (transformed scale): [-10, 0]\n\nFor illustration, let’s slightly change the kernel parameter range, to improve the visualizations of the search:\n\nsvm_param &lt;- \n  svm_wflow %&gt;% \n  extract_parameter_set_dials() %&gt;% \n  Matrix::update(rbf_sigma = rbf_sigma(c(-7, -1)))\n\nBefore discussing specific details about iterative search and how it works, let’s explore the relationship between the two SVM tuning parameters and the area under the ROC curve for this specific data set. We constructed a very large regular grid, composed of 2,500 candidate values, and evaluated the grid using resampling. This is obviously impractical in regular data analysis and tremendously inefficient. However, it elucidates the path that the search process should take and where the numerically optimal value(s) occur.\nFigure @ref(fig:roc-surface) shows the results of evaluating this grid, with lighter color corresponding to higher (better) model performance. There is a large swath in the lower diagonal of the parameter space that is relatively flat with poor performance. A ridge of best performance occurs in the upper-right portion of the space. The black dot indicates the best settings. The transition from the plateau of poor results to the ridge of best performance is very sharp. There is also a sharp drop in the area under the ROC curve just to the right of the ridge.\n\n\n\n\nHeatmap of the mean area under the ROC curve for a high density grid of tuning parameter values. The best point is a solid dot in the upper-right corner.\n\n\n\nThe following search procedures require at least some resampled performance statistics before proceeding. For this purpose, the following code creates a small regular grid that resides in the flat portion of the parameter space. The tune_grid() function resamples this grid:\n\nset.seed(1401)\nstart_grid &lt;- \n  svm_param %&gt;% \n  Matrix::update(\n    cost = cost(c(-6, 1)),\n    rbf_sigma = rbf_sigma(c(-6, -4))\n  ) %&gt;% \n  grid_regular(levels = 2)\n\nset.seed(1402)\nsvm_initial &lt;- \n  svm_wflow %&gt;% \n  tune_grid(resamples = cell_folds, grid = start_grid, metrics = roc_res)\n\ncollect_metrics(svm_initial)\n## # A tibble: 4 × 8\n##     cost rbf_sigma .metric .estimator  mean     n std_err .config             \n##    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n## 1 0.0156  0.000001 roc_auc binary     0.864    10 0.00864 Preprocessor1_Model1\n## 2 2       0.000001 roc_auc binary     0.863    10 0.00867 Preprocessor1_Model2\n## 3 0.0156  0.0001   roc_auc binary     0.863    10 0.00862 Preprocessor1_Model3\n## 4 2       0.0001   roc_auc binary     0.866    10 0.00855 Preprocessor1_Model4\n\nThis initial grid shows fairly equivalent results, with no individual point much better than any of the others. These results can be ingested by the iterative tuning functions discussed in the following sections to be used as initial values."
  },
  {
    "objectID": "14-iterative-search.html#bayesian-optimization",
    "href": "14-iterative-search.html#bayesian-optimization",
    "title": "14  Iterative Search",
    "section": "\n14.2 Bayesian Optimization",
    "text": "14.2 Bayesian Optimization\nBayesian optimization techniques analyze the current resampling results and create a predictive model to suggest tuning parameter values that have yet to be evaluated. The suggested parameter combination is then resampled. These results are then used in another predictive model that recommends more candidate values for testing, and so on. The process proceeds for a set number of iterations or until no further improvements occur. Shahriari et al. (2016) and Frazier (2018) are good introductions to Bayesian optimization.\nWhen using Bayesian optimization, the primary concerns are how to create the model and how to select parameters recommended by that model. First, let’s consider the technique most commonly used for Bayesian optimization, the Gaussian process model.\n\n14.2.1 A Gaussian process model\nGaussian process (GP) (Schulz, Speekenbrink, and Krause 2018) models are well-known statistical techniques that have a history in spatial statistics (under the name of kriging methods). They can be derived in multiple ways, including as a Bayesian model; see Rasmussen and Williams (2006) for an excellent reference.\nMathematically, a GP is a collection of random variables whose joint probability distribution is multivariate Gaussian. In the context of our application, this is the collection of performance metrics for the tuning parameter candidate values. For the previous initial grid of four samples, the realization of these four random variables were 0.8639, 0.8625, 0.8627, and 0.8659. These are assumed to be distributed as multivariate Gaussian. The inputs that define the independent variables/predictors for the GP model are the corresponding tuning parameter values (shown in Table @ref(tab:initial-gp-data)).\n\n\n\nResampling statistics used as the initial substrate to the Gaussian process model.\n\n\n\n\n\n\n\n\noutcome\n\n\npredictors\n\n\n\nROC\ncost\nrbf_sigma\n\n\n\n\n0.8639\n0.01562\n0.000001\n\n\n0.8625\n2.00000\n0.000001\n\n\n0.8627\n0.01562\n0.000100\n\n\n0.8659\n2.00000\n0.000100\n\n\n\n\n\nGaussian process models are specified by their mean and covariance functions, although the latter has the most effect on the nature of the GP model. The covariance function is often parameterized in terms of the input values (denoted as \\(x\\)). As an example, a commonly used covariance function is the squared exponential1 function:\n\\[\\operatorname{cov}(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\exp\\left(-\\frac{1}{2}|\\boldsymbol{x}_i - \\boldsymbol{x}_j|^2\\right) + \\sigma^2_{ij}\\] where \\(\\sigma^2_{ij}\\) is a constant error variance term that is zero when \\(i=j\\). This equation translates to:\n\nAs the distance between two tuning parameter combinations increases, the covariance between the performance metrics increase exponentially.\n\nThe nature of the equation also implies that the variation of the outcome metric is minimized at the points that have already been observed (i.e., when \\(|\\boldsymbol{x}_i - \\boldsymbol{x}_j|^2\\) is zero).\nThe nature of this covariance function allows the Gaussian process to represent highly nonlinear relationships between model performance and the tuning parameters even when only a small amount of data exists.\n\nHowever, fitting these models can be difficult in some cases, and the model becomes more computationally expensive as the number of tuning parameter combinations increases.\n\nAn important virtue of this model is that, since a full probability model is specified, the predictions for new inputs can reflect the entire distribution of the outcome. In other words, new performance statistics can be predicted in terms of both mean and variance.\nSuppose that two new tuning parameters were under consideration. In Table @ref(tab:tuning-candidates), candidate A has a slightly better mean ROC value than candidate B (the current best is 0.8659). However, its variance is four-fold larger than B. Is this good or bad? Choosing option A is riskier but has potentially higher return. The increase in variance also reflects that this new value is farther from the existing data than B. The next section considers these aspects of GP predictions for Bayesian optimization in more detail.\n\n\n\nTwo example tuning parameters considered for further sampling.\n\n\n\n\n\n\n\n\n\nGP Prediction of ROC AUC\n\n\n\ncandidate\nmean\nvariance\n\n\n\n\nA\n0.90\n0.000400\n\n\nB\n0.89\n0.000025\n\n\n\n\n\n\nBayesian optimization is an iterative process.\n\nBased on the initial grid of four results, the GP model is fit, candidates are predicted, and a fifth tuning parameter combination is selected. We compute performance estimates for the new configuration, the GP is refit with the five existing results (and so on).\n\n14.2.2 Acquisition functions\nOnce the Gaussian process is fit to the current data, how is it used? Our goal is to choose the next tuning parameter combination that is most likely to have “better results” than the current best. One approach to do this is to create a large candidate set (perhaps using a space-filling design) and then make mean and variance predictions on each. Using this information, we choose the most advantageous tuning parameter value.\nA class of objective functions, called acquisition functions, facilitate the trade-off between mean and variance. Recall that the predicted variance of the GP models are mostly driven by how far away they are from the existing data. The trade-off between the predicted mean and variance for new candidates is frequently viewed through the lens of exploration and exploitation:\n\nExploration biases the selection towards regions where there are fewer (if any) observed candidate models. This tends to give more weight to candidates with higher variance and focuses on finding new results.\nExploitation principally relies on the mean prediction to find the best (mean) value. It focuses on existing results.\n\nTo demonstrate, let’s look at a toy example with a single parameter that has values between [0, 1] and the performance metric is \\(R^2\\). The true function is shown in Figure @ref(fig:performance-profile), along with five candidate values that have existing results as points.\n\n\n\n\nHypothetical true performance profile over an arbitrary tuning parameter, with five estimated points\n\n\n\nFor these data, the GP model fit is shown in Figure @ref(fig:estimated-profile). The shaded region indicates the mean \\(\\pm\\) 1 standard error. The two vertical lines indicate two candidate points that are examined in more detail later.\nThe shaded confidence region demonstrates the squared exponential variance function; it becomes very large between points and converges to zero at the existing data points.\n\n\n\n\nEstimated performance profile generated by the Gaussian process model. The shaded region shows one-standard-error bounds.\n\n\n\nThis nonlinear trend passes through each observed point, but the model is not perfect. There are no observed points near the true optimum setting and, in this region, the fit could be much better. Despite this, the GP model can effectively point us in the right direction.\nFrom a pure exploitation standpoint, the best choice would select the parameter value that has the best mean prediction. Here, this would be a value of 0.106, just to the right of the existing best observed point at 0.09.\nAs a way to encourage exploration, a simple (but not often used) approach is to find the tuning parameter associated with the largest confidence interval. For example, by using a single standard deviation for the \\(R^2\\) confidence bound, the next point to sample would be 0.236. This is slightly more into the region with no observed results. Increasing the number of standard deviations used in the upper bound would push the selection farther into empty regions.\nOne of the most commonly used acquisition functions is expected improvement. The notion of improvement requires a value for the current best results (unlike the confidence bound approach). Since the GP can describe a new candidate point using a distribution, we can weight the parts of the distribution that show improvement using the probability of the improvement occurring.\nFor example, consider two candidate parameter values of 0.10 and 0.25 (indicated by the vertical lines in Figure @ref(fig:estimated-profile)). Using the fitted GP model, their predicted \\(R^2\\) distributions are shown in Figure @ref(fig:two-candidates) along with a reference line for the current best results.\n\n\n\n\nPredicted performance distributions for two sampled tuning parameter values\n\n\n\nWhen only considering the mean \\(R^2\\) prediction, a parameter value of 0.10 is the better choice (see Table @ref(tab:two-exp-improve)). The tuning parameter recommendation for 0.25 is, on average, predicted to be worse than the current best. However, since it has higher variance, it has more overall probability area above the current best. As a result, it has a larger expected improvement:\n\n\n\nExpected improvement for the two candidate tuning parameters.\n\n\n\n\n\n\n\n\n\n\nPredictions\n\n\n\nParameter Value\nMean\nStd Dev\nExpected Improvment\n\n\n\n\n0.10\n0.8679\n0.0004317\n0.000190\n\n\n0.25\n0.8671\n0.0039301\n0.001216\n\n\n\n\n\nWhen expected improvement is computed across the range of the tuning parameter, the recommended point to sample is much closer to 0.25 than 0.10, as shown in Figure @ref(fig:expected-improvement).\n\n\n\n\nThe estimated performance profile generated by the Gaussian process model (top panel) and the expected improvement (bottom panel). The vertical line indicates the point of maximum improvement.\n\n\n\nNumerous acquisition functions have been proposed and discussed; in tidymodels, expected improvement is the default.\n\n14.2.3 The tune_bayes() function\nTo implement iterative search via Bayesian optimization, use the tune_bayes() function. Its syntax is very similar to tune_grid() but with several additional arguments:\n\niter is the maximum number of search iterations.\ninitial can be either an integer, an object produced using tune_grid(), or one of the racing functions. Using an integer specifies the size of a space-filling design that is sampled prior to the first GP model.\nobjective is an argument for which acquisition function should be used. The tune package contains functions to pass here, such as exp_improve() or conf_bound().\nThe param_info argument, in this case, specifies the range of the parameters as well as any transformations that are used. These are used to define the search space. In situations where the default parameter objects are insufficient, param_info is used to override the defaults.\n\nThe control argument now uses the results of control_bayes(). Some helpful arguments there are:\n\nno_improve is an integer that will stop the search if improved parameters are not discovered within no_improve iterations.\nuncertain is also an integer (or Inf) that will take an uncertainty sample if there is no improvement within uncertain iterations. This will select the next candidate that has large variation. It has the effect of pure exploration since it does not consider the mean prediction.\nverbose is a logical that will print logging information as the search proceeds.\n\nLet’s use the first SVM results from Section @ref(svm) as the initial substrate for the Gaussian process model. Recall that, for this application, we want to maximize the area under the ROC curve. Our code is:\n\nctrl &lt;- control_bayes(verbose = TRUE)\n\nset.seed(1403)\nsvm_bo &lt;-\n  svm_wflow %&gt;%\n  tune_bayes(\n    resamples = cell_folds,\n    metrics = roc_res,\n    initial = svm_initial,\n    param_info = svm_param,\n    iter = 25,\n    control = ctrl\n  )\n\nThe search process starts with an initial best value of 0.8659 for the area under the ROC curve. A Gaussian process model uses these four statistics to create a model. The large candidate set is automatically generated and scored using the expected improvement acquisition function. The first iteration failed to improve the outcome with an ROC value of 0.86315. After fitting another Gaussian process model with the new outcome value, the second iteration also failed to yield an improvement.\nThe log of the first two iterations, produced by the verbose option, was:\nThe search continues. There were a total of 9 improvements in the outcome along the way at iterations 3, 4, 5, 6, 8, 13, 22, 23, and 24. The best result occurred at iteration 24 with an area under the ROC curve of 0.8986.\nThe last step was:\nThe functions that are used to interrogate the results are the same as those used for grid search (e.g., collect_metrics(), etc.). For example:\n\nshow_best(svm_bo)\n## # A tibble: 5 × 9\n##    cost rbf_sigma .metric .estimator  mean     n std_err .config .iter\n##   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;int&gt;\n## 1  31.8   0.00160 roc_auc binary     0.899    10 0.00785 Iter24     24\n## 2  30.8   0.00191 roc_auc binary     0.899    10 0.00791 Iter23     23\n## 3  31.4   0.00166 roc_auc binary     0.899    10 0.00784 Iter22     22\n## 4  31.8   0.00153 roc_auc binary     0.899    10 0.00783 Iter13     13\n## 5  30.8   0.00163 roc_auc binary     0.899    10 0.00782 Iter15     15\n\nThe autoplot() function has several options for iterative search methods. Figure @ref(fig:progress-plot) shows how the outcome changed over the search by using autoplot(svm_bo, type = \"performance\").\n\n\n\n\nThe progress of the Bayesian optimization produced when the autoplot() method is used with type = \"performance\"\n\n\n\nAn additional type of plot uses type = \"parameters\" that shows the parameter values over iterations.\nThe animation below visualizes the results of the search. The black \\(\\times\\) values show the starting values contained in svm_initial. The top-left blue panel shows the predicted mean value of the area under the ROC curve. The red panel on the top-right displays the predicted variation in the ROC values while the bottom plot visualizes the expected improvement. In each panel, darker colors indicate less attractive values (e.g., small mean values, large variation, and small improvements).\nThe surface of the predicted mean surface is very inaccurate in the first few iterations of the search. Despite this, it does help guide the process to the region of good performance. In other words, the Gaussian process model is wrong but shows itself to be very useful. Within the first ten iterations, the search is sampling near the optimum location.\nWhile the best tuning parameter combination is on the boundary of the parameter space, Bayesian optimization will often choose new points on other sides of the boundary. While we can adjust the ratio of exploration and exploitation, the search tends to sample boundary points early on.\n\nIf the search is seeded with an initial grid, a space-filling design would probably be a better choice than a regular design. It samples more unique values of the parameter space and would improve the predictions of the standard deviation in the early iterations.\n\nFinally, if the user interrupts the tune_bayes() computations, the function returns the current results (instead of resulting in an error)."
  },
  {
    "objectID": "14-iterative-search.html#simulated-annealing",
    "href": "14-iterative-search.html#simulated-annealing",
    "title": "14  Iterative Search",
    "section": "\n14.3 Simulated Annealing",
    "text": "14.3 Simulated Annealing\nSimulated annealing (SA) (Kirkpatrick, Gelatt, and Vecchi 1983; Van Laarhoven and Aarts 1987) is a general nonlinear search routine inspired by the process in which metal cools. It is a global search method that can effectively navigate many different types of search landscapes, including discontinuous functions. Unlike most gradient-based optimization routines, simulated annealing can reassess previous solutions.\n\n14.3.1 Simulated annealing search process\nThe process of using simulated annealing starts with an initial value and embarks on a controlled random walk through the parameter space. Each new candidate parameter value is a small perturbation of the previous value that keeps the new point within a local neighborhood.\nThe candidate point is resampled to obtain its corresponding performance value. If this achieves better results than the previous parameters, it is accepted as the new best and the process continues. If the results are worse than the previous value the search procedure may still use this parameter to define further steps. This depends on two factors. First, the likelihood of accepting a bad result decreases as performance becomes worse. In other words, a slightly worse result has a better chance of acceptance than one with a large drop in performance. The other factor is the number of search iterations. Simulated annealing wants to accept fewer suboptimal values as the search proceeds. From these two factors, the acceptance probability for a bad result can be formalized as:\n\\[\\operatorname{Pr}[\\text{accept suboptimal parameters at iteration } i] = \\exp(c\\times D_i \\times i)\\]\nwhere \\(i\\) is the iteration number, \\(c\\) is a user-specified constant, and \\(D_i\\) is the percent difference between the old and new values (where negative values imply worse results). For a bad result, we determine the acceptance probability and compare it to a random uniform number. If the random number is greater than the probability value, the search discards the current parameters and the next iteration creates its candidate value in the neighborhood of the previous value. Otherwise, the next iteration forms the next set of parameters based on the current (suboptimal) values.\n\nThe acceptance probabilities of simulated annealing allow the search to proceed in the wrong direction, at least for the short term, with the potential to find a much better region of the parameter space in the long run.\n\nHow are the acceptance probabilities influenced? The heatmap in Figure @ref(fig:acceptance-prob) shows how the acceptance probability can change over iterations, performance, and the user-specified coefficient.\n\n\n\n\nHeatmap of the simulated annealing acceptance probabilities for different coefficient values\n\n\n\nThe user can adjust the coefficients to find a probability profile that suits their needs. In finetune::control_sim_anneal(), the default for this cooling_coef argument is 0.02. Decreasing this coefficient will encourage the search to be more forgiving of poor results.\nThis process continues for a set amount of iterations but can halt if no globally best results occur within a pre-determined number of iterations. However, it can be very helpful to set a restart threshold. If there are a string of failures, this feature revisits the last globally best parameter settings and starts anew.\nThe main important detail is to define how to perturb the tuning parameters from iteration to iteration. There are a variety of methods in the literature for this. We follow the method given in Bohachevsky, Johnson, and Stein (1986) called generalized simulated annealing. For continuous tuning parameters, we define a small radius to specify the local “neighborhood.” For example, suppose there are two tuning parameters and each is bounded by zero and one. The simulated annealing process generates random values on the surrounding radius and randomly chooses one to be the current candidate value.\nIn our implementation, the neighborhood is determined by scaling the current candidate to be between zero and one based on the range of the parameter object, so radius values between 0.05 and 0.15 seem reasonable. For these values, the fastest that the search could go from one side of the parameter space to the other is about 10 iterations. The size of the radius controls how quickly the search explores the parameter space. In our implementation, a range of radii is specified so different magnitudes of “local” define the new candidate values.\nTo illustrate, we’ll use the two main glmnet tuning parameters:\n\nThe amount of total regularization (penalty). The default range for this parameter is $10^{-10} $ to \\(10^{ 0}\\). It is typical to use a log (base-10) transformation for this parameter.\nThe proportion of the lasso penalty (mixture). This is bounded at zero and one with no transformation.\n\nThe process starts with initial values of penalty = 0.025 and mixture = 0.050. Using a radius that randomly fluctuates between 0.050 and 0.015, the data are appropriately scaled, random values are generated on radii around the initial point, then one is randomly chosen as the candidate. For illustration, we will assume that all candidate values are improvements. Using the new value, a set of new random neighbors are generated, one is chosen, and so on. Figure @ref(fig:iterative-neighborhood) shows six iterations as the search proceeds toward the upper left corner.\n\n\n\n\nAn illustration of how simulated annealing determines what is the local neighborhood for two numeric tuning parameters. The clouds of points show possible next values where one would be selected at random.\n\n\n\nNote that, during some iterations, the candidate sets along the radius exclude points outside of the parameter boundaries. Also, our implementation biases the choice of the next tuning parameter configurations away from new values that are very similar to previous configurations.\nFor non-numeric parameters, we assign a probability for how often the parameter value changes.\n\n14.3.2 The tune_sim_anneal() function\nTo implement iterative search via simulated annealing, use the tune_sim_anneal() function. The syntax for this function is nearly identical to tune_bayes(). There are no options for acquisition functions or uncertainty sampling. The control_sim_anneal() function has some details that define the local neighborhood and the cooling schedule:\n\nno_improve, for simulated annealing, is an integer that will stop the search if no global best or improved results are discovered within no_improve iterations. Accepted suboptimal or discarded parameters count as “no improvement.”\nrestart is the number of iterations with no new best results before starting from the previous best results.\nradius is a numeric vector on (0, 1) that defines the minimum and maximum radius of the local neighborhood around the initial point.\nflip is a probability value that defines the chances of altering the value of categorical or integer parameters.\ncooling_coef is the \\(c\\) coefficient in \\(\\exp(c\\times D_i \\times i)\\) that modulates how quickly the acceptance probability decreases over iterations. Larger values of cooling_coef decrease the probability of accepting a suboptimal parameter setting.\n\nFor the cell segmentation data, the syntax is very consistent with the previously used functions:\n\nctrl_sa &lt;- control_sim_anneal(verbose = TRUE, no_improve = 10L)\n\nset.seed(1404)\nsvm_sa &lt;-\n  svm_wflow %&gt;%\n  tune_sim_anneal(\n    resamples = cell_folds,\n    metrics = roc_res,\n    initial = svm_initial,\n    param_info = svm_param,\n    iter = 50,\n    control = ctrl_sa\n  )\n\nThe simulated annealing process discovered new global optimums at 6 different iterations. The earliest improvement was at iteration 2 and the final optimum occured at iteration 37. The best overall results occured at iteration 37 with a mean area under the ROC curve of 0.898 (compared to an initial best of 0.8659). There were 4 restarts at iterations 13, 26, 34, and 45 as well as 7 discarded candidates during the process.\nThe verbose option prints details of the search process. The output for the first five iterations was:\nThe output for last ten iterations was:\n\n## 40 + better suboptimal  roc_auc=0.89285 (+/-0.008806)\n## \n## i Fold01: preprocessor 1/1\n## \n## ✓ Fold01: preprocessor 1/1\n## \n## i Fold01: preprocessor 1/1, model 1/1\n## \n## ✓ Fold01: preprocessor 1/1, model 1/1\n## \n## i Fold01: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold01: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold02: preprocessor 1/1\n## \n## ✓ Fold02: preprocessor 1/1\n## \n## i Fold02: preprocessor 1/1, model 1/1\n## \n## ✓ Fold02: preprocessor 1/1, model 1/1\n## \n## i Fold02: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold02: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold03: preprocessor 1/1\n## \n## ✓ Fold03: preprocessor 1/1\n## \n## i Fold03: preprocessor 1/1, model 1/1\n## \n## ✓ Fold03: preprocessor 1/1, model 1/1\n## \n## i Fold03: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold03: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold04: preprocessor 1/1\n## \n## ✓ Fold04: preprocessor 1/1\n## \n## i Fold04: preprocessor 1/1, model 1/1\n## \n## ✓ Fold04: preprocessor 1/1, model 1/1\n## \n## i Fold04: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold04: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold05: preprocessor 1/1\n## \n## ✓ Fold05: preprocessor 1/1\n## \n## i Fold05: preprocessor 1/1, model 1/1\n## \n## ✓ Fold05: preprocessor 1/1, model 1/1\n## \n## i Fold05: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold05: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold06: preprocessor 1/1\n## \n## ✓ Fold06: preprocessor 1/1\n## \n## i Fold06: preprocessor 1/1, model 1/1\n## \n## ✓ Fold06: preprocessor 1/1, model 1/1\n## \n## i Fold06: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold06: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold07: preprocessor 1/1\n## \n## ✓ Fold07: preprocessor 1/1\n## \n## i Fold07: preprocessor 1/1, model 1/1\n## \n## ✓ Fold07: preprocessor 1/1, model 1/1\n## \n## i Fold07: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold07: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold08: preprocessor 1/1\n## \n## ✓ Fold08: preprocessor 1/1\n## \n## i Fold08: preprocessor 1/1, model 1/1\n## \n## ✓ Fold08: preprocessor 1/1, model 1/1\n## \n## i Fold08: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold08: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold09: preprocessor 1/1\n## \n## ✓ Fold09: preprocessor 1/1\n## \n## i Fold09: preprocessor 1/1, model 1/1\n## \n## ✓ Fold09: preprocessor 1/1, model 1/1\n## \n## i Fold09: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold09: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold10: preprocessor 1/1\n## \n## ✓ Fold10: preprocessor 1/1\n## \n## i Fold10: preprocessor 1/1, model 1/1\n## \n## ✓ Fold10: preprocessor 1/1, model 1/1\n## \n## i Fold10: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold10: preprocessor 1/1, model 1/1 (predictions)\n## \n## 41 + better suboptimal  roc_auc=0.89419 (+/-0.008628)\n## \n## i Fold01: preprocessor 1/1\n## \n## ✓ Fold01: preprocessor 1/1\n## \n## i Fold01: preprocessor 1/1, model 1/1\n## \n## ✓ Fold01: preprocessor 1/1, model 1/1\n## \n## i Fold01: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold01: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold02: preprocessor 1/1\n## \n## ✓ Fold02: preprocessor 1/1\n## \n## i Fold02: preprocessor 1/1, model 1/1\n## \n## ✓ Fold02: preprocessor 1/1, model 1/1\n## \n## i Fold02: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold02: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold03: preprocessor 1/1\n## \n## ✓ Fold03: preprocessor 1/1\n## \n## i Fold03: preprocessor 1/1, model 1/1\n## \n## ✓ Fold03: preprocessor 1/1, model 1/1\n## \n## i Fold03: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold03: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold04: preprocessor 1/1\n## \n## ✓ Fold04: preprocessor 1/1\n## \n## i Fold04: preprocessor 1/1, model 1/1\n## \n## ✓ Fold04: preprocessor 1/1, model 1/1\n## \n## i Fold04: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold04: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold05: preprocessor 1/1\n## \n## ✓ Fold05: preprocessor 1/1\n## \n## i Fold05: preprocessor 1/1, model 1/1\n## \n## ✓ Fold05: preprocessor 1/1, model 1/1\n## \n## i Fold05: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold05: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold06: preprocessor 1/1\n## \n## ✓ Fold06: preprocessor 1/1\n## \n## i Fold06: preprocessor 1/1, model 1/1\n## \n## ✓ Fold06: preprocessor 1/1, model 1/1\n## \n## i Fold06: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold06: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold07: preprocessor 1/1\n## \n## ✓ Fold07: preprocessor 1/1\n## \n## i Fold07: preprocessor 1/1, model 1/1\n## \n## ✓ Fold07: preprocessor 1/1, model 1/1\n## \n## i Fold07: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold07: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold08: preprocessor 1/1\n## \n## ✓ Fold08: preprocessor 1/1\n## \n## i Fold08: preprocessor 1/1, model 1/1\n## \n## ✓ Fold08: preprocessor 1/1, model 1/1\n## \n## i Fold08: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold08: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold09: preprocessor 1/1\n## \n## ✓ Fold09: preprocessor 1/1\n## \n## i Fold09: preprocessor 1/1, model 1/1\n## \n## ✓ Fold09: preprocessor 1/1, model 1/1\n## \n## i Fold09: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold09: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold10: preprocessor 1/1\n## \n## ✓ Fold10: preprocessor 1/1\n## \n## i Fold10: preprocessor 1/1, model 1/1\n## \n## ✓ Fold10: preprocessor 1/1, model 1/1\n## \n## i Fold10: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold10: preprocessor 1/1, model 1/1 (predictions)\n## \n## 42 + better suboptimal  roc_auc=0.8979 (+/-0.007741)\n## \n## i Fold01: preprocessor 1/1\n## \n## ✓ Fold01: preprocessor 1/1\n## \n## i Fold01: preprocessor 1/1, model 1/1\n## \n## ✓ Fold01: preprocessor 1/1, model 1/1\n## \n## i Fold01: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold01: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold02: preprocessor 1/1\n## \n## ✓ Fold02: preprocessor 1/1\n## \n## i Fold02: preprocessor 1/1, model 1/1\n## \n## ✓ Fold02: preprocessor 1/1, model 1/1\n## \n## i Fold02: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold02: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold03: preprocessor 1/1\n## \n## ✓ Fold03: preprocessor 1/1\n## \n## i Fold03: preprocessor 1/1, model 1/1\n## \n## ✓ Fold03: preprocessor 1/1, model 1/1\n## \n## i Fold03: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold03: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold04: preprocessor 1/1\n## \n## ✓ Fold04: preprocessor 1/1\n## \n## i Fold04: preprocessor 1/1, model 1/1\n## \n## ✓ Fold04: preprocessor 1/1, model 1/1\n## \n## i Fold04: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold04: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold05: preprocessor 1/1\n## \n## ✓ Fold05: preprocessor 1/1\n## \n## i Fold05: preprocessor 1/1, model 1/1\n## \n## ✓ Fold05: preprocessor 1/1, model 1/1\n## \n## i Fold05: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold05: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold06: preprocessor 1/1\n## \n## ✓ Fold06: preprocessor 1/1\n## \n## i Fold06: preprocessor 1/1, model 1/1\n## \n## ✓ Fold06: preprocessor 1/1, model 1/1\n## \n## i Fold06: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold06: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold07: preprocessor 1/1\n## \n## ✓ Fold07: preprocessor 1/1\n## \n## i Fold07: preprocessor 1/1, model 1/1\n## \n## ✓ Fold07: preprocessor 1/1, model 1/1\n## \n## i Fold07: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold07: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold08: preprocessor 1/1\n## \n## ✓ Fold08: preprocessor 1/1\n## \n## i Fold08: preprocessor 1/1, model 1/1\n## \n## ✓ Fold08: preprocessor 1/1, model 1/1\n## \n## i Fold08: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold08: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold09: preprocessor 1/1\n## \n## ✓ Fold09: preprocessor 1/1\n## \n## i Fold09: preprocessor 1/1, model 1/1\n## \n## ✓ Fold09: preprocessor 1/1, model 1/1\n## \n## i Fold09: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold09: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold10: preprocessor 1/1\n## \n## ✓ Fold10: preprocessor 1/1\n## \n## i Fold10: preprocessor 1/1, model 1/1\n## \n## ✓ Fold10: preprocessor 1/1, model 1/1\n## \n## i Fold10: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold10: preprocessor 1/1, model 1/1 (predictions)\n## \n## 43 ─ discard suboptimal roc_auc=0.86412 (+/-0.009579)\n## \n## i Fold01: preprocessor 1/1\n## \n## ✓ Fold01: preprocessor 1/1\n## \n## i Fold01: preprocessor 1/1, model 1/1\n## \n## ✓ Fold01: preprocessor 1/1, model 1/1\n## \n## i Fold01: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold01: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold02: preprocessor 1/1\n## \n## ✓ Fold02: preprocessor 1/1\n## \n## i Fold02: preprocessor 1/1, model 1/1\n## \n## ✓ Fold02: preprocessor 1/1, model 1/1\n## \n## i Fold02: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold02: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold03: preprocessor 1/1\n## \n## ✓ Fold03: preprocessor 1/1\n## \n## i Fold03: preprocessor 1/1, model 1/1\n## \n## ✓ Fold03: preprocessor 1/1, model 1/1\n## \n## i Fold03: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold03: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold04: preprocessor 1/1\n## \n## ✓ Fold04: preprocessor 1/1\n## \n## i Fold04: preprocessor 1/1, model 1/1\n## \n## ✓ Fold04: preprocessor 1/1, model 1/1\n## \n## i Fold04: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold04: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold05: preprocessor 1/1\n## \n## ✓ Fold05: preprocessor 1/1\n## \n## i Fold05: preprocessor 1/1, model 1/1\n## \n## ✓ Fold05: preprocessor 1/1, model 1/1\n## \n## i Fold05: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold05: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold06: preprocessor 1/1\n## \n## ✓ Fold06: preprocessor 1/1\n## \n## i Fold06: preprocessor 1/1, model 1/1\n## \n## ✓ Fold06: preprocessor 1/1, model 1/1\n## \n## i Fold06: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold06: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold07: preprocessor 1/1\n## \n## ✓ Fold07: preprocessor 1/1\n## \n## i Fold07: preprocessor 1/1, model 1/1\n## \n## ✓ Fold07: preprocessor 1/1, model 1/1\n## \n## i Fold07: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold07: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold08: preprocessor 1/1\n## \n## ✓ Fold08: preprocessor 1/1\n## \n## i Fold08: preprocessor 1/1, model 1/1\n## \n## ✓ Fold08: preprocessor 1/1, model 1/1\n## \n## i Fold08: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold08: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold09: preprocessor 1/1\n## \n## ✓ Fold09: preprocessor 1/1\n## \n## i Fold09: preprocessor 1/1, model 1/1\n## \n## ✓ Fold09: preprocessor 1/1, model 1/1\n## \n## i Fold09: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold09: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold10: preprocessor 1/1\n## \n## ✓ Fold10: preprocessor 1/1\n## \n## i Fold10: preprocessor 1/1, model 1/1\n## \n## ✓ Fold10: preprocessor 1/1, model 1/1\n## \n## i Fold10: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold10: preprocessor 1/1, model 1/1 (predictions)\n## \n## 44 ◯ accept suboptimal  roc_auc=0.89119 (+/-0.008697)\n## \n## i Fold01: preprocessor 1/1\n## \n## ✓ Fold01: preprocessor 1/1\n## \n## i Fold01: preprocessor 1/1, model 1/1\n## \n## ✓ Fold01: preprocessor 1/1, model 1/1\n## \n## i Fold01: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold01: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold02: preprocessor 1/1\n## \n## ✓ Fold02: preprocessor 1/1\n## \n## i Fold02: preprocessor 1/1, model 1/1\n## \n## ✓ Fold02: preprocessor 1/1, model 1/1\n## \n## i Fold02: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold02: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold03: preprocessor 1/1\n## \n## ✓ Fold03: preprocessor 1/1\n## \n## i Fold03: preprocessor 1/1, model 1/1\n## \n## ✓ Fold03: preprocessor 1/1, model 1/1\n## \n## i Fold03: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold03: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold04: preprocessor 1/1\n## \n## ✓ Fold04: preprocessor 1/1\n## \n## i Fold04: preprocessor 1/1, model 1/1\n## \n## ✓ Fold04: preprocessor 1/1, model 1/1\n## \n## i Fold04: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold04: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold05: preprocessor 1/1\n## \n## ✓ Fold05: preprocessor 1/1\n## \n## i Fold05: preprocessor 1/1, model 1/1\n## \n## ✓ Fold05: preprocessor 1/1, model 1/1\n## \n## i Fold05: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold05: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold06: preprocessor 1/1\n## \n## ✓ Fold06: preprocessor 1/1\n## \n## i Fold06: preprocessor 1/1, model 1/1\n## \n## ✓ Fold06: preprocessor 1/1, model 1/1\n## \n## i Fold06: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold06: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold07: preprocessor 1/1\n## \n## ✓ Fold07: preprocessor 1/1\n## \n## i Fold07: preprocessor 1/1, model 1/1\n## \n## ✓ Fold07: preprocessor 1/1, model 1/1\n## \n## i Fold07: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold07: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold08: preprocessor 1/1\n## \n## ✓ Fold08: preprocessor 1/1\n## \n## i Fold08: preprocessor 1/1, model 1/1\n## \n## ✓ Fold08: preprocessor 1/1, model 1/1\n## \n## i Fold08: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold08: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold09: preprocessor 1/1\n## \n## ✓ Fold09: preprocessor 1/1\n## \n## i Fold09: preprocessor 1/1, model 1/1\n## \n## ✓ Fold09: preprocessor 1/1, model 1/1\n## \n## i Fold09: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold09: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold10: preprocessor 1/1\n## \n## ✓ Fold10: preprocessor 1/1\n## \n## i Fold10: preprocessor 1/1, model 1/1\n## \n## ✓ Fold10: preprocessor 1/1, model 1/1\n## \n## i Fold10: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold10: preprocessor 1/1, model 1/1 (predictions)\n## \n## 45 ✖ restart from best  roc_auc=0.8921 (+/-0.00885)\n## \n## i Fold01: preprocessor 1/1\n## \n## ✓ Fold01: preprocessor 1/1\n## \n## i Fold01: preprocessor 1/1, model 1/1\n## \n## ✓ Fold01: preprocessor 1/1, model 1/1\n## \n## i Fold01: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold01: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold02: preprocessor 1/1\n## \n## ✓ Fold02: preprocessor 1/1\n## \n## i Fold02: preprocessor 1/1, model 1/1\n## \n## ✓ Fold02: preprocessor 1/1, model 1/1\n## \n## i Fold02: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold02: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold03: preprocessor 1/1\n## \n## ✓ Fold03: preprocessor 1/1\n## \n## i Fold03: preprocessor 1/1, model 1/1\n## \n## ✓ Fold03: preprocessor 1/1, model 1/1\n## \n## i Fold03: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold03: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold04: preprocessor 1/1\n## \n## ✓ Fold04: preprocessor 1/1\n## \n## i Fold04: preprocessor 1/1, model 1/1\n## \n## ✓ Fold04: preprocessor 1/1, model 1/1\n## \n## i Fold04: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold04: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold05: preprocessor 1/1\n## \n## ✓ Fold05: preprocessor 1/1\n## \n## i Fold05: preprocessor 1/1, model 1/1\n## \n## ✓ Fold05: preprocessor 1/1, model 1/1\n## \n## i Fold05: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold05: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold06: preprocessor 1/1\n## \n## ✓ Fold06: preprocessor 1/1\n## \n## i Fold06: preprocessor 1/1, model 1/1\n## \n## ✓ Fold06: preprocessor 1/1, model 1/1\n## \n## i Fold06: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold06: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold07: preprocessor 1/1\n## \n## ✓ Fold07: preprocessor 1/1\n## \n## i Fold07: preprocessor 1/1, model 1/1\n## \n## ✓ Fold07: preprocessor 1/1, model 1/1\n## \n## i Fold07: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold07: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold08: preprocessor 1/1\n## \n## ✓ Fold08: preprocessor 1/1\n## \n## i Fold08: preprocessor 1/1, model 1/1\n## \n## ✓ Fold08: preprocessor 1/1, model 1/1\n## \n## i Fold08: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold08: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold09: preprocessor 1/1\n## \n## ✓ Fold09: preprocessor 1/1\n## \n## i Fold09: preprocessor 1/1, model 1/1\n## \n## ✓ Fold09: preprocessor 1/1, model 1/1\n## \n## i Fold09: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold09: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold10: preprocessor 1/1\n## \n## ✓ Fold10: preprocessor 1/1\n## \n## i Fold10: preprocessor 1/1, model 1/1\n## \n## ✓ Fold10: preprocessor 1/1, model 1/1\n## \n## i Fold10: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold10: preprocessor 1/1, model 1/1 (predictions)\n## \n## 46 ◯ accept suboptimal  roc_auc=0.89657 (+/-0.008539)\n## \n## i Fold01: preprocessor 1/1\n## \n## ✓ Fold01: preprocessor 1/1\n## \n## i Fold01: preprocessor 1/1, model 1/1\n## \n## ✓ Fold01: preprocessor 1/1, model 1/1\n## \n## i Fold01: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold01: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold02: preprocessor 1/1\n## \n## ✓ Fold02: preprocessor 1/1\n## \n## i Fold02: preprocessor 1/1, model 1/1\n## \n## ✓ Fold02: preprocessor 1/1, model 1/1\n## \n## i Fold02: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold02: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold03: preprocessor 1/1\n## \n## ✓ Fold03: preprocessor 1/1\n## \n## i Fold03: preprocessor 1/1, model 1/1\n## \n## ✓ Fold03: preprocessor 1/1, model 1/1\n## \n## i Fold03: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold03: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold04: preprocessor 1/1\n## \n## ✓ Fold04: preprocessor 1/1\n## \n## i Fold04: preprocessor 1/1, model 1/1\n## \n## ✓ Fold04: preprocessor 1/1, model 1/1\n## \n## i Fold04: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold04: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold05: preprocessor 1/1\n## \n## ✓ Fold05: preprocessor 1/1\n## \n## i Fold05: preprocessor 1/1, model 1/1\n## \n## ✓ Fold05: preprocessor 1/1, model 1/1\n## \n## i Fold05: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold05: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold06: preprocessor 1/1\n## \n## ✓ Fold06: preprocessor 1/1\n## \n## i Fold06: preprocessor 1/1, model 1/1\n## \n## ✓ Fold06: preprocessor 1/1, model 1/1\n## \n## i Fold06: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold06: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold07: preprocessor 1/1\n## \n## ✓ Fold07: preprocessor 1/1\n## \n## i Fold07: preprocessor 1/1, model 1/1\n## \n## ✓ Fold07: preprocessor 1/1, model 1/1\n## \n## i Fold07: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold07: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold08: preprocessor 1/1\n## \n## ✓ Fold08: preprocessor 1/1\n## \n## i Fold08: preprocessor 1/1, model 1/1\n## \n## ✓ Fold08: preprocessor 1/1, model 1/1\n## \n## i Fold08: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold08: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold09: preprocessor 1/1\n## \n## ✓ Fold09: preprocessor 1/1\n## \n## i Fold09: preprocessor 1/1, model 1/1\n## \n## ✓ Fold09: preprocessor 1/1, model 1/1\n## \n## i Fold09: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold09: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold10: preprocessor 1/1\n## \n## ✓ Fold10: preprocessor 1/1\n## \n## i Fold10: preprocessor 1/1, model 1/1\n## \n## ✓ Fold10: preprocessor 1/1, model 1/1\n## \n## i Fold10: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold10: preprocessor 1/1, model 1/1 (predictions)\n## \n## 47 ◯ accept suboptimal  roc_auc=0.88647 (+/-0.008694)\n## \n## i Fold01: preprocessor 1/1\n## \n## ✓ Fold01: preprocessor 1/1\n## \n## i Fold01: preprocessor 1/1, model 1/1\n## \n## ✓ Fold01: preprocessor 1/1, model 1/1\n## \n## i Fold01: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold01: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold02: preprocessor 1/1\n## \n## ✓ Fold02: preprocessor 1/1\n## \n## i Fold02: preprocessor 1/1, model 1/1\n## \n## ✓ Fold02: preprocessor 1/1, model 1/1\n## \n## i Fold02: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold02: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold03: preprocessor 1/1\n## \n## ✓ Fold03: preprocessor 1/1\n## \n## i Fold03: preprocessor 1/1, model 1/1\n## \n## ✓ Fold03: preprocessor 1/1, model 1/1\n## \n## i Fold03: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold03: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold04: preprocessor 1/1\n## \n## ✓ Fold04: preprocessor 1/1\n## \n## i Fold04: preprocessor 1/1, model 1/1\n## \n## ✓ Fold04: preprocessor 1/1, model 1/1\n## \n## i Fold04: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold04: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold05: preprocessor 1/1\n## \n## ✓ Fold05: preprocessor 1/1\n## \n## i Fold05: preprocessor 1/1, model 1/1\n## \n## ✓ Fold05: preprocessor 1/1, model 1/1\n## \n## i Fold05: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold05: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold06: preprocessor 1/1\n## \n## ✓ Fold06: preprocessor 1/1\n## \n## i Fold06: preprocessor 1/1, model 1/1\n## \n## ✓ Fold06: preprocessor 1/1, model 1/1\n## \n## i Fold06: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold06: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold07: preprocessor 1/1\n## \n## ✓ Fold07: preprocessor 1/1\n## \n## i Fold07: preprocessor 1/1, model 1/1\n## \n## ✓ Fold07: preprocessor 1/1, model 1/1\n## \n## i Fold07: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold07: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold08: preprocessor 1/1\n## \n## ✓ Fold08: preprocessor 1/1\n## \n## i Fold08: preprocessor 1/1, model 1/1\n## \n## ✓ Fold08: preprocessor 1/1, model 1/1\n## \n## i Fold08: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold08: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold09: preprocessor 1/1\n## \n## ✓ Fold09: preprocessor 1/1\n## \n## i Fold09: preprocessor 1/1, model 1/1\n## \n## ✓ Fold09: preprocessor 1/1, model 1/1\n## \n## i Fold09: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold09: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold10: preprocessor 1/1\n## \n## ✓ Fold10: preprocessor 1/1\n## \n## i Fold10: preprocessor 1/1, model 1/1\n## \n## ✓ Fold10: preprocessor 1/1, model 1/1\n## \n## i Fold10: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold10: preprocessor 1/1, model 1/1 (predictions)\n## \n## 48 + better suboptimal  roc_auc=0.89035 (+/-0.008894)\n## \n## i Fold01: preprocessor 1/1\n## \n## ✓ Fold01: preprocessor 1/1\n## \n## i Fold01: preprocessor 1/1, model 1/1\n## \n## ✓ Fold01: preprocessor 1/1, model 1/1\n## \n## i Fold01: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold01: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold02: preprocessor 1/1\n## \n## ✓ Fold02: preprocessor 1/1\n## \n## i Fold02: preprocessor 1/1, model 1/1\n## \n## ✓ Fold02: preprocessor 1/1, model 1/1\n## \n## i Fold02: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold02: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold03: preprocessor 1/1\n## \n## ✓ Fold03: preprocessor 1/1\n## \n## i Fold03: preprocessor 1/1, model 1/1\n## \n## ✓ Fold03: preprocessor 1/1, model 1/1\n## \n## i Fold03: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold03: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold04: preprocessor 1/1\n## \n## ✓ Fold04: preprocessor 1/1\n## \n## i Fold04: preprocessor 1/1, model 1/1\n## \n## ✓ Fold04: preprocessor 1/1, model 1/1\n## \n## i Fold04: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold04: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold05: preprocessor 1/1\n## \n## ✓ Fold05: preprocessor 1/1\n## \n## i Fold05: preprocessor 1/1, model 1/1\n## \n## ✓ Fold05: preprocessor 1/1, model 1/1\n## \n## i Fold05: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold05: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold06: preprocessor 1/1\n## \n## ✓ Fold06: preprocessor 1/1\n## \n## i Fold06: preprocessor 1/1, model 1/1\n## \n## ✓ Fold06: preprocessor 1/1, model 1/1\n## \n## i Fold06: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold06: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold07: preprocessor 1/1\n## \n## ✓ Fold07: preprocessor 1/1\n## \n## i Fold07: preprocessor 1/1, model 1/1\n## \n## ✓ Fold07: preprocessor 1/1, model 1/1\n## \n## i Fold07: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold07: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold08: preprocessor 1/1\n## \n## ✓ Fold08: preprocessor 1/1\n## \n## i Fold08: preprocessor 1/1, model 1/1\n## \n## ✓ Fold08: preprocessor 1/1, model 1/1\n## \n## i Fold08: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold08: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold09: preprocessor 1/1\n## \n## ✓ Fold09: preprocessor 1/1\n## \n## i Fold09: preprocessor 1/1, model 1/1\n## \n## ✓ Fold09: preprocessor 1/1, model 1/1\n## \n## i Fold09: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold09: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold10: preprocessor 1/1\n## \n## ✓ Fold10: preprocessor 1/1\n## \n## i Fold10: preprocessor 1/1, model 1/1\n## \n## ✓ Fold10: preprocessor 1/1, model 1/1\n## \n## i Fold10: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold10: preprocessor 1/1, model 1/1 (predictions)\n## \n## 49 + better suboptimal  roc_auc=0.8977 (+/-0.007898)\n## \n## i Fold01: preprocessor 1/1\n## \n## ✓ Fold01: preprocessor 1/1\n## \n## i Fold01: preprocessor 1/1, model 1/1\n## \n## ✓ Fold01: preprocessor 1/1, model 1/1\n## \n## i Fold01: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold01: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold02: preprocessor 1/1\n## \n## ✓ Fold02: preprocessor 1/1\n## \n## i Fold02: preprocessor 1/1, model 1/1\n## \n## ✓ Fold02: preprocessor 1/1, model 1/1\n## \n## i Fold02: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold02: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold03: preprocessor 1/1\n## \n## ✓ Fold03: preprocessor 1/1\n## \n## i Fold03: preprocessor 1/1, model 1/1\n## \n## ✓ Fold03: preprocessor 1/1, model 1/1\n## \n## i Fold03: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold03: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold04: preprocessor 1/1\n## \n## ✓ Fold04: preprocessor 1/1\n## \n## i Fold04: preprocessor 1/1, model 1/1\n## \n## ✓ Fold04: preprocessor 1/1, model 1/1\n## \n## i Fold04: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold04: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold05: preprocessor 1/1\n## \n## ✓ Fold05: preprocessor 1/1\n## \n## i Fold05: preprocessor 1/1, model 1/1\n## \n## ✓ Fold05: preprocessor 1/1, model 1/1\n## \n## i Fold05: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold05: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold06: preprocessor 1/1\n## \n## ✓ Fold06: preprocessor 1/1\n## \n## i Fold06: preprocessor 1/1, model 1/1\n## \n## ✓ Fold06: preprocessor 1/1, model 1/1\n## \n## i Fold06: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold06: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold07: preprocessor 1/1\n## \n## ✓ Fold07: preprocessor 1/1\n## \n## i Fold07: preprocessor 1/1, model 1/1\n## \n## ✓ Fold07: preprocessor 1/1, model 1/1\n## \n## i Fold07: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold07: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold08: preprocessor 1/1\n## \n## ✓ Fold08: preprocessor 1/1\n## \n## i Fold08: preprocessor 1/1, model 1/1\n## \n## ✓ Fold08: preprocessor 1/1, model 1/1\n## \n## i Fold08: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold08: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold09: preprocessor 1/1\n## \n## ✓ Fold09: preprocessor 1/1\n## \n## i Fold09: preprocessor 1/1, model 1/1\n## \n## ✓ Fold09: preprocessor 1/1, model 1/1\n## \n## i Fold09: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold09: preprocessor 1/1, model 1/1 (predictions)\n## \n## i Fold10: preprocessor 1/1\n## \n## ✓ Fold10: preprocessor 1/1\n## \n## i Fold10: preprocessor 1/1, model 1/1\n## \n## ✓ Fold10: preprocessor 1/1, model 1/1\n## \n## i Fold10: preprocessor 1/1, model 1/1 (extracts)\n## \n## i Fold10: preprocessor 1/1, model 1/1 (predictions)\n## \n## 50 ─ discard suboptimal roc_auc=0.86146 (+/-0.009902)\n\nAs with the other tune_*() functions, the corresponding autoplot() function produces visual assessments of the results. Using autoplot(svm_sa, type = \"performance\") shows the performance over iterations (Figure @ref(fig:sa-iterations)) while autoplot(svm_sa, type = \"parameters\") plots performance versus specific tuning parameter values (Figure @ref(fig:sa-parameters)).\n\n\n\n\nProgress of the simulated annealing process shown when the autoplot() method is used with type = \"performance\"\n\n\n\n\n\n\n\nPerformance versus tuning parameter values when the autoplot() method is used with type = \"parameters\".\n\n\n\nA visualization of the search path helps to understand where the search process did well and where it went astray:\nLike tune_bayes(), manually stopping execution will return the completed iterations."
  },
  {
    "objectID": "14-iterative-search.html#iterative-summary",
    "href": "14-iterative-search.html#iterative-summary",
    "title": "14  Iterative Search",
    "section": "\n14.4 Chapter Summary",
    "text": "14.4 Chapter Summary\nThis chapter described two iterative search methods for optimizing tuning parameters. Bayes optimization uses a predictive model trained on existing resampling results to suggest tuning parameter values, while simulated annealing walks through the hyperparameter space to find good values. Both can be effective at finding good values alone or as a follow-up method used after an initial grid search to further finetune performance.\n\n\n\n\nBohachevsky, I, M Johnson, and M Stein. 1986. “Generalized Simulated Annealing for Function Optimization.” Technometrics 28 (3): 209–17.\n\n\nFrazier, R. 2018. “A Tutorial on Bayesian Optimization.” https://arxiv.org/abs/1807.02811.\n\n\nKirkpatrick, S, D Gelatt, and M Vecchi. 1983. “Optimization by Simulated Annealing.” Science 220 (4598): 671–80.\n\n\nKuhn, M, and K Johnson. 2013. Applied Predictive Modeling. Springer.\n\n\nRasmussen, C, and C Williams. 2006. Gaussian Processes for Machine Learning. Gaussian Processes for Machine Learning. MIT Press.\n\n\nSchulz, E, M Speekenbrink, and A Krause. 2018. “A Tutorial on Gaussian Process Regression: Modelling, Exploring, and Exploiting Functions.” Journal of Mathematical Psychology 85: 1–16.\n\n\nShahriari, B., K. Swersky, Z. Wang, R. P. Adams, and N. de Freitas. 2016. “Taking the Human Out of the Loop: A Review of Bayesian Optimization.” Proceedings of the IEEE 104 (1): 148–75.\n\n\nVan Laarhoven, P, and E Aarts. 1987. “Simulated Annealing.” In Simulated Annealing: Theory and Applications, 7–15. Springer."
  },
  {
    "objectID": "14-iterative-search.html#footnotes",
    "href": "14-iterative-search.html#footnotes",
    "title": "14  Iterative Search",
    "section": "",
    "text": "This equation is also the same as the radial basis function used in kernel methods, such as the SVM model that is currently being used. This is a coincidence; this covariance function is unrelated to the SVM tuning parameter that we are using.↩︎"
  },
  {
    "objectID": "18-explaining-models-and-predictions.html#software-for-model-explanations",
    "href": "18-explaining-models-and-predictions.html#software-for-model-explanations",
    "title": "18  Explaining Models and Predictions",
    "section": "\n18.1 Software for Model Explanations",
    "text": "18.1 Software for Model Explanations\nThe tidymodels framework does not itself contain software for model explanations. Instead, models trained and evaluated with tidymodels can be explained with other, supplementary software in R packages such as lime, vip, and DALEX. We often choose:\n\n\nvip functions when we want to use model-based methods that take advantage of model structure (and are often faster)\n\nDALEX functions when we want to use model-agnostic methods that can be applied to any model\n\nIn Chapters @ref(resampling) and @ref(compare), we trained and compared several models to predict the price of homes in Ames, IA, including a linear model with interactions and a random forest model, with results shown in Figure @ref(fig:explain-obs-pred).\n\n\n\n\nComparing predicted prices for a linear model with interactions and a random forest model\n\n\n\nLet’s build model-agnostic explainers for both of these models to find out why they make these predictions. We can use the DALEXtra add-on package for DALEX, which provides support for tidymodels. Biecek and Burzykowski (2021) provide a thorough exploration of how to use DALEX for model explanations; this chapter only summarizes some important approaches, specific to tidymodels. To compute any kind of model explanation, global or local, using DALEX, we first prepare the appropriate data and then create an explainer for each model:\n\nlibrary(DALEXtra)\nvip_features &lt;- c(\"Neighborhood\", \"Gr_Liv_Area\", \"Year_Built\", \n                  \"Bldg_Type\", \"Latitude\", \"Longitude\")\n\nvip_train &lt;- \n  ames_train %&gt;% \n  select(all_of(vip_features))\n\nexplainer_lm &lt;- \n  explain_tidymodels(\n    lm_fit, \n    data = vip_train, \n    y = ames_train$Sale_Price,\n    label = \"lm + interactions\",\n    verbose = FALSE\n  )\n\nexplainer_rf &lt;- \n  explain_tidymodels(\n    rf_fit, \n    data = vip_train, \n    y = ames_train$Sale_Price,\n    label = \"random forest\",\n    verbose = FALSE\n  )\n\n\nA linear model is typically straightforward to interpret and explain; you may not often find yourself using separate model explanation algorithms for a linear model. However, it can sometimes be difficult to understand or explain the predictions of even a linear model once it has splines and interaction terms!\n\nDealing with significant feature engineering transformations during model explainability highlights some options we have (or sometimes, ambiguity in such analyses). We can quantify global or local model explanations either in terms of:\n\n\noriginal, basic predictors as they existed without significant feature engineering transformations, or\n\nderived features, such as those created via dimensionality reduction (Chapter @ref(dimensionality)) or interactions and spline terms, as in this example."
  },
  {
    "objectID": "18-explaining-models-and-predictions.html#local-explanations",
    "href": "18-explaining-models-and-predictions.html#local-explanations",
    "title": "18  Explaining Models and Predictions",
    "section": "\n18.2 Local Explanations",
    "text": "18.2 Local Explanations\nLocal model explanations provide information about a prediction for a single observation. For example, let’s consider an older duplex in the North Ames neighborhood (Section @ref(exploring-features-of-homes-in-ames)):\n\nduplex &lt;- vip_train[120,]\nduplex\n## # A tibble: 1 × 6\n##   Neighborhood Gr_Liv_Area Year_Built Bldg_Type Latitude Longitude\n##   &lt;fct&gt;              &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n## 1 North_Ames          1040       1949 Duplex        42.0     -93.6\n\nThere are multiple possible approaches to understanding why a model predicts a given price for this duplex. One is a break-down explanation, implemented with the DALEX function predict_parts(); it computes how contributions attributed to individual features change the mean model’s prediction for a particular observation, like our duplex. For the linear model, the duplex status (Bldg_Type = 3),1 size, longitude, and age all contribute the most to the price being driven down from the intercept:\n\nlm_breakdown &lt;- predict_parts(explainer = explainer_lm, new_observation = duplex)\nlm_breakdown\n##                                           contribution\n## lm + interactions: intercept                     5.221\n## lm + interactions: Gr_Liv_Area = 1040           -0.082\n## lm + interactions: Bldg_Type = 3                -0.049\n## lm + interactions: Longitude = -93.608903       -0.043\n## lm + interactions: Year_Built = 1949            -0.039\n## lm + interactions: Latitude = 42.035841         -0.007\n## lm + interactions: Neighborhood = 1              0.001\n## lm + interactions: prediction                    5.002\n\nSince this linear model was trained using spline terms for latitude and longitude, the contribution to price for Longitude shown here combines the effects of all of its individual spline terms. The contribution is in terms of the original Longitude feature, not the derived spline features.\nThe most important features are slightly different for the random forest model, with the size, age, and duplex status being most important:\n\nrf_breakdown &lt;- predict_parts(explainer = explainer_rf, new_observation = duplex)\nrf_breakdown\n##                                       contribution\n## random forest: intercept                     5.221\n## random forest: Year_Built = 1949            -0.076\n## random forest: Gr_Liv_Area = 1040           -0.075\n## random forest: Bldg_Type = 3                -0.027\n## random forest: Longitude = -93.608903       -0.043\n## random forest: Latitude = 42.035841         -0.028\n## random forest: Neighborhood = 1             -0.003\n## random forest: prediction                    4.969\n\n\nModel break-down explanations like these depend on the order of the features.\n\nIf we choose the order for the random forest model explanation to be the same as the default for the linear model (chosen via a heuristic), we can change the relative importance of the features:\n\npredict_parts(\n  explainer = explainer_rf, \n  new_observation = duplex,\n  order = lm_breakdown$variable_name\n)\n##                                       contribution\n## random forest: intercept                     5.221\n## random forest: Gr_Liv_Area = 1040           -0.075\n## random forest: Bldg_Type = 3                -0.019\n## random forest: Longitude = -93.608903       -0.023\n## random forest: Year_Built = 1949            -0.104\n## random forest: Latitude = 42.035841         -0.028\n## random forest: Neighborhood = 1             -0.003\n## random forest: prediction                    4.969\n\nWe can use the fact that these break-down explanations change based on order to compute the most important features over all (or many) possible orderings. This is the idea behind Shapley Additive Explanations (Lundberg and Lee 2017), where the average contributions of features are computed under different combinations or “coalitions” of feature orderings. Let’s compute SHAP attributions for our duplex, using B = 20 random orderings:\n\nset.seed(1801)\nshap_duplex &lt;- \n  predict_parts(\n    explainer = explainer_rf, \n    new_observation = duplex, \n    type = \"shap\",\n    B = 20\n  )\n\nWe could use the default plot method from DALEX by calling plot(shap_duplex), or we can access the underlying data and create a custom plot. The box plots in Figure @ref(fig:duplex-rf-shap) display the distribution of contributions across all the orderings we tried, and the bars display the average attribution for each feature:\n\nlibrary(forcats)\nshap_duplex %&gt;%\n  group_by(variable) %&gt;%\n  mutate(mean_val = mean(contribution)) %&gt;%\n  ungroup() %&gt;%\n  mutate(variable = fct_reorder(variable, abs(mean_val))) %&gt;%\n  ggplot(aes(contribution, variable, fill = mean_val &gt; 0)) +\n  geom_col(data = ~distinct(., variable, mean_val), \n           aes(mean_val, variable), \n           alpha = 0.5) +\n  geom_boxplot(width = 0.5) +\n  theme(legend.position = \"none\") +\n  scale_fill_viridis_d() +\n  labs(y = NULL)\n\n\n\n\n\nShapley additive explanations from the random forest model for a duplex property\n\n\n\nWhat about a different observation in our data set? Let’s look at a larger, newer one-family home in the Gilbert neighborhood:\n\nbig_house &lt;- vip_train[1269,]\nbig_house\n## # A tibble: 1 × 6\n##   Neighborhood Gr_Liv_Area Year_Built Bldg_Type Latitude Longitude\n##   &lt;fct&gt;              &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n## 1 Gilbert             2267       2002 OneFam        42.1     -93.6\n\nWe can compute SHAP average attributions for this house in the same way:\n\nset.seed(1802)\nshap_house &lt;- \n  predict_parts(\n    explainer = explainer_rf, \n    new_observation = big_house, \n    type = \"shap\",\n    B = 20\n  )\n\nThe results are shown in Figure @ref(fig:gilbert-shap); unlike the duplex, the size and age of this house contribute to its price being higher.\n\n\n\n\nShapley additive explanations from the random forest model for a one-family home in Gilbert"
  },
  {
    "objectID": "18-explaining-models-and-predictions.html#global-explanations",
    "href": "18-explaining-models-and-predictions.html#global-explanations",
    "title": "18  Explaining Models and Predictions",
    "section": "\n18.3 Global Explanations",
    "text": "18.3 Global Explanations\nGlobal model explanations, also called global feature importance or variable importance, help us understand which features are most important in driving the predictions of the linear and random forest models overall, aggregated over the whole training set. While the previous section addressed which variables or features are most important in predicting sale price for an individual home, global feature importance addresses the most important variables for a model in aggregate.\n\nOne way to compute variable importance is to permute the features (Breiman 2001). We can permute or shuffle the values of a feature, predict from the model, and then measure how much worse the model fits the data compared to before shuffling.\n\nIf shuffling a column causes a large degradation in model performance, it is important; if shuffling a column’s values doesn’t make much difference to how the model performs, it must not be an important variable. This approach can be applied to any kind of model (it is model agnostic), and the results are straightforward to understand.\nUsing DALEX, we compute this kind of variable importance via the model_parts() function.\n\nset.seed(1803)\nvip_lm &lt;- model_parts(explainer_lm, loss_function = loss_root_mean_square)\nset.seed(1804)\nvip_rf &lt;- model_parts(explainer_rf, loss_function = loss_root_mean_square)\n\nAgain, we could use the default plot method from DALEX by calling plot(vip_lm, vip_rf) but the underlying data is available for exploration, analysis, and plotting. Let’s create a function for plotting:\n\nggplot_imp &lt;- function(...) {\n  obj &lt;- list(...)\n  metric_name &lt;- attr(obj[[1]], \"loss_name\")\n  metric_lab &lt;- paste(metric_name, \n                      \"after permutations\\n(higher indicates more important)\")\n  \n  full_vip &lt;- bind_rows(obj) %&gt;%\n    filter(variable != \"_baseline_\")\n  \n  perm_vals &lt;- full_vip %&gt;% \n    filter(variable == \"_full_model_\") %&gt;% \n    group_by(label) %&gt;% \n    summarise(dropout_loss = mean(dropout_loss))\n  \n  p &lt;- full_vip %&gt;%\n    filter(variable != \"_full_model_\") %&gt;% \n    mutate(variable = fct_reorder(variable, dropout_loss)) %&gt;%\n    ggplot(aes(dropout_loss, variable)) \n  if(length(obj) &gt; 1) {\n    p &lt;- p + \n      facet_wrap(vars(label)) +\n      geom_vline(data = perm_vals, aes(xintercept = dropout_loss, color = label),\n                 linewidth = 1.4, lty = 2, alpha = 0.7) +\n      geom_boxplot(aes(color = label, fill = label), alpha = 0.2)\n  } else {\n    p &lt;- p + \n      geom_vline(data = perm_vals, aes(xintercept = dropout_loss),\n                 linewidth = 1.4, lty = 2, alpha = 0.7) +\n      geom_boxplot(fill = \"#91CBD765\", alpha = 0.4)\n    \n  }\n  p +\n    theme(legend.position = \"none\") +\n    labs(x = metric_lab, \n         y = NULL,  fill = NULL,  color = NULL)\n}\n\nUsing ggplot_imp(vip_lm, vip_rf) produces Figure @ref(fig:global-rf).\n\n\n\n\nGlobal explainer for the random forest and linear regression models\n\n\n\nThe dashed line in each panel of Figure @ref(fig:global-rf) shows the RMSE for the full model, either the linear model or the random forest model. Features farther to the right are more important, because permuting them results in higher RMSE. There is quite a lot of interesting information to learn from this plot; for example, neighborhood is quite important in the linear model with interactions/splines but the second least important feature for the random forest model."
  },
  {
    "objectID": "18-explaining-models-and-predictions.html#building-global-explanations-from-local-explanations",
    "href": "18-explaining-models-and-predictions.html#building-global-explanations-from-local-explanations",
    "title": "18  Explaining Models and Predictions",
    "section": "\n18.4 Building Global Explanations from Local Explanations",
    "text": "18.4 Building Global Explanations from Local Explanations\nSo far in this chapter, we have focused on local model explanations for a single observation (via Shapley additive explanations) and global model explanations for a data set as a whole (via permuting features). It is also possible to build global model explanations by aggregating local model explanations, as with partial dependence profiles.\n\nPartial dependence profiles show how the expected value of a model prediction, like the predicted price of a home in Ames, changes as a function of a feature, like the age or gross living area.\n\nOne way to build such a profile is by aggregating or averaging profiles for individual observations. A profile showing how an individual observation’s prediction changes as a function of a given feature is called an ICE (individual conditional expectation) profile or a CP (ceteris paribus) profile. We can compute such individual profiles (for 500 of the observations in our training set) and then aggregate them using the DALEX function model_profile():\n\nset.seed(1805)\npdp_age &lt;- model_profile(explainer_rf, N = 500, variables = \"Year_Built\")\n\nLet’s create another function for plotting the underlying data in this object:\n\nggplot_pdp &lt;- function(obj, x) {\n  \n  p &lt;- \n    as_tibble(obj$agr_profiles) %&gt;%\n    mutate(`_label_` = stringr::str_remove(`_label_`, \"^[^_]*_\")) %&gt;%\n    ggplot(aes(`_x_`, `_yhat_`)) +\n    geom_line(data = as_tibble(obj$cp_profiles),\n              aes(x = {{ x }}, group = `_ids_`),\n              linewidth = 0.5, alpha = 0.05, color = \"gray50\")\n  \n  num_colors &lt;- n_distinct(obj$agr_profiles$`_label_`)\n  \n  if (num_colors &gt; 1) {\n    p &lt;- p + geom_line(aes(color = `_label_`), linewidth = 1.2, alpha = 0.8)\n  } else {\n    p &lt;- p + geom_line(color = \"midnightblue\", linewidth = 1.2, alpha = 0.8)\n  }\n  \n  p\n}\n\nUsing this function generates Figure @ref(fig:year-built), where we can see the nonlinear behavior of the random forest model.\n\nggplot_pdp(pdp_age, Year_Built)  +\n  labs(x = \"Year built\", \n       y = \"Sale Price (log)\", \n       color = NULL)\n\n\n\n\n\nPartial dependence profiles for the random forest model focusing on the year built predictor\n\n\n\nSale price for houses built in different years is mostly flat, with a modest rise after about 1960. Partial dependence profiles can be computed for any other feature in the model, and also for groups in the data, such as Bldg_Type. Let’s use 1,000 observations for these profiles.\n\nset.seed(1806)\npdp_liv &lt;- model_profile(explainer_rf, N = 1000, \n                         variables = \"Gr_Liv_Area\", \n                         groups = \"Bldg_Type\")\n\nggplot_pdp(pdp_liv, Gr_Liv_Area) +\n  scale_x_log10() +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(x = \"Gross living area\", \n       y = \"Sale Price (log)\", \n       color = NULL)\n\nThis code produces Figure @ref(fig:building-type-profiles), where we see that sale price increases the most between about 1,000 and 3,000 square feet of living area, and that different home types (like single family homes or different types of townhouses) mostly exhibit similar increasing trends in price with more living space.\n\n\n\n\nPartial dependence profiles for the random forest model focusing on building types and gross living area\n\n\n\nWe have the option of using plot(pdp_liv) for default DALEX plots, but since we are making plots with the underlying data here, we can even facet by one of the features to visualize if the predictions change differently and highlighting the imbalance in these subgroups (as shown in Figure @ref(fig:building-type-facets)).\n\nas_tibble(pdp_liv$agr_profiles) %&gt;%\n  mutate(Bldg_Type = stringr::str_remove(`_label_`, \"random forest_\")) %&gt;%\n  ggplot(aes(`_x_`, `_yhat_`, color = Bldg_Type)) +\n  geom_line(data = as_tibble(pdp_liv$cp_profiles),\n            aes(x = Gr_Liv_Area, group = `_ids_`),\n            linewidth = 0.5, alpha = 0.1, color = \"gray50\") +\n  geom_line(linewidth = 1.2, alpha = 0.8, show.legend = FALSE) +\n  scale_x_log10() +\n  facet_wrap(~Bldg_Type) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(x = \"Gross living area\", \n       y = \"Sale Price (log)\", \n       color = NULL)\n\n\n\n\n\nPartial dependence profiles for the random forest model focusing on building types and gross living area using facets\n\n\n\nThere is no one correct approach for building model explanations, and the options outlined in this chapter are not exhaustive. We have highlighted good options for explanations at both the individual and global level, as well as how to bridge from one to the other, and we point you to Biecek and Burzykowski (2021) and Molnar (2020) for further reading."
  },
  {
    "objectID": "18-explaining-models-and-predictions.html#back-to-beans",
    "href": "18-explaining-models-and-predictions.html#back-to-beans",
    "title": "18  Explaining Models and Predictions",
    "section": "\n18.5 Back to Beans!",
    "text": "18.5 Back to Beans!\nIn Chapter @ref(dimensionality), we discussed how to use dimensionality reduction as a feature engineering or preprocessing step when modeling high-dimensional data. For our example data set of dry bean morphology measures predicting bean type, we saw great results from partial least squares (PLS) dimensionality reduction combined with a regularized discriminant analysis model. Which of those morphological characteristics were most important in the bean type predictions? We can use the same approach outlined throughout this chapter to create a model-agnostic explainer and compute, say, global model explanations via model_parts():\n\nset.seed(1807)\nvip_beans &lt;- \n  explain_tidymodels(\n    rda_wflow_fit, \n    data = bean_train %&gt;% select(-class), \n    y = bean_train$class,\n    label = \"RDA\",\n    verbose = FALSE\n  ) %&gt;% \n  model_parts() \n\nUsing our previously defined importance plotting function, ggplot_imp(vip_beans) produces Figure @ref(fig:bean-explainer).\n\n\n\n\nGlobal explainer for the regularized discriminant analysis model on the beans data\n\n\n\n\nThe measures of global feature importance that we see in Figure @ref(fig:bean-explainer) incorporate the effects of all of the PLS components, but in terms of the original variables.\n\nFigure @ref(fig:bean-explainer) shows us that shape factors are among the most important features for predicting bean type, especially shape factor 4, a measure of solidity that takes into account the area \\(A\\), major axis \\(L\\), and minor axis \\(l\\):\n\\[\\text{SF4} = \\frac{A}{\\pi(L/2)(l/2)}\\]\nWe can see from Figure @ref(fig:bean-explainer) that shape factor 1 (the ratio of the major axis to the area), the minor axis length, and roundness are the next most important bean characteristics for predicting bean variety."
  },
  {
    "objectID": "18-explaining-models-and-predictions.html#explain-summary",
    "href": "18-explaining-models-and-predictions.html#explain-summary",
    "title": "18  Explaining Models and Predictions",
    "section": "\n18.6 Chapter Summary",
    "text": "18.6 Chapter Summary\nFor some types of models, the answer to why a model made a certain prediction is straightforward, but for other types of models, we must use separate explainer algorithms to understand what features are relatively most important for predictions. You can generate two main kinds of model explanations from a trained model. Global explanations provide information aggregated over an entire data set, while local explanations provide understanding about a model’s predictions for a single observation.\nPackages such as DALEX and its supporting package DALEXtra, vip, and lime can be integrated into a tidymodels analysis to provide these model explainers. Model explanations are just one piece of understanding whether your model is appropriate and effective, along with estimates of model performance; Chapter @ref(trust) further explores the quality and trustworthiness of predictions.\n\n\n\n\nBiecek, Przemyslaw, and Tomasz Burzykowski. 2021. Explanatory Model Analysis. Chapman; Hall/CRC, New York. https://ema.drwhy.ai/.\n\n\nBreiman, L. 2001. “Random Forests.” Machine Learning 45 (1): 5–32.\n\n\nLundberg, Scott M., and Su-In Lee. 2017. “A Unified Approach to Interpreting Model Predictions.” In Proceedings of the 31st International Conference on Neural Information Processing Systems, 4768–77. NIPS’17. Red Hook, NY, USA: Curran Associates Inc.\n\n\nMolnar, Christopher. 2020. Interpretable Machine Learning. lulu.com. https://christophm.github.io/interpretable-ml-book/."
  },
  {
    "objectID": "18-explaining-models-and-predictions.html#footnotes",
    "href": "18-explaining-models-and-predictions.html#footnotes",
    "title": "18  Explaining Models and Predictions",
    "section": "",
    "text": "Notice that this package for model explanations focuses on the level of categorical predictors in this type of output, like Bldg_Type = 3 for duplex and Neighborhood = 1 for North Ames.↩︎"
  },
  {
    "objectID": "19-when-should-you-trust-predictions.html#equivocal-zones",
    "href": "19-when-should-you-trust-predictions.html#equivocal-zones",
    "title": "19  When Should You Trust Your Predictions?",
    "section": "\n19.1 Equivocal Results",
    "text": "19.1 Equivocal Results\n\nIn some cases, the amount of uncertainty associated with a prediction is too high to be trusted.\n\nIf a model result indicated that you had a 51% chance of having contracted COVID-19, it would be natural to view the diagnosis with some skepticism. In fact, regulatory bodies often require many medical diagnostics to have an equivocal zone. This zone is a range of results in which the prediction should not be reported to patients, for example, some range of COVID-19 test results that are too uncertain to be reported to a patient. See Danowski et al. (1970) and Kerleguer et al. (2003) for examples. The same notion can be applied to models created outside of medical diagnostics.\nLet’s use a function that can simulate classification data with two classes and two predictors (x and y). The true model is a logistic regression model with the equation:\n\\[\n\\mathrm{logit}(p) = -1 - 2x - \\frac{x^2}{5} + 2y^2\n\\]\nThe two predictors follow a bivariate normal distribution with a correlation of 0.70. We’ll create a training set of 200 samples and a test set of 50:\n\nlibrary(tidymodels)\ntidymodels_prefer()\n\nsimulate_two_classes &lt;- \n  function (n, error = 0.1, eqn = quote(-1 - 2 * x - 0.2 * x^2 + 2 * y^2))  {\n    # Slightly correlated predictors\n    sigma &lt;- matrix(c(1, 0.7, 0.7, 1), nrow = 2, ncol = 2)\n    dat &lt;- MASS::mvrnorm(n = n, mu = c(0, 0), Sigma = sigma)\n    colnames(dat) &lt;- c(\"x\", \"y\")\n    cls &lt;- paste0(\"class_\", 1:2)\n    dat &lt;- \n      as_tibble(dat) %&gt;% \n      mutate(\n        linear_pred = !!eqn,\n        # Add some misclassification noise\n        linear_pred = linear_pred + rnorm(n, sd = error),\n        prob = binomial()$linkinv(linear_pred),\n        class = ifelse(prob &gt; runif(n), cls[1], cls[2]),\n        class = factor(class, levels = cls)\n      )\n    dplyr::select(dat, x, y, class)\n  }\n\nset.seed(1901)\ntraining_set &lt;- simulate_two_classes(200)\ntesting_set  &lt;- simulate_two_classes(50)\n\nWe estimate a logistic regression model using Bayesian methods (using the default Gaussian prior distributions for the parameters):\n\ntwo_class_mod &lt;- \n  logistic_reg() %&gt;% \n  set_engine(\"stan\", seed = 1902) %&gt;% \n  fit(class ~ . + I(x^2)+ I(y^2), data = training_set)\nprint(two_class_mod, digits = 3)\n## parsnip model object\n## \n## stan_glm\n##  family:       binomial [logit]\n##  formula:      class ~ . + I(x^2) + I(y^2)\n##  observations: 200\n##  predictors:   5\n## ------\n##             Median MAD_SD\n## (Intercept)  1.092  0.287\n## x            2.290  0.423\n## y            0.314  0.354\n## I(x^2)       0.077  0.307\n## I(y^2)      -2.465  0.424\n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\n\nThe fitted class boundary is overlaid onto the test set in Figure @ref(fig:glm-boundaries). The data points closest to the class boundary are the most uncertain. If their values changed slightly, their predicted class might change. One simple method for disqualifying some results is to call them “equivocal” if the values are within some range around 50% (or the appropriate probability cutoff for a certain situation). Depending on the problem the model is being applied to, this might indicate we should collect another measurement or we require more information before a trustworthy prediction is possible.\n\n\n\n\nSimulated two-class data set with a logistic regression fit and decision boundary.\n\n\n\nWe could base the width of the band around the cutoff on how performance improves when the uncertain results are removed. However, we should also estimate the reportable rate (the expected proportion of usable results). For example, it would not be useful in real-world situations to have perfect performance but release predictions on only 2% of the samples passed to the model.\nLet’s use the test set to determine the balance between improving performance and having enough reportable results. The predictions are created using:\n\ntest_pred &lt;- augment(two_class_mod, testing_set)\ntest_pred %&gt;% head()\n## # A tibble: 6 × 6\n##   .pred_class .pred_class_1 .pred_class_2      x      y class  \n##   &lt;fct&gt;               &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;  \n## 1 class_2           0.0256          0.974  1.12  -0.176 class_2\n## 2 class_1           0.555           0.445 -0.126 -0.582 class_2\n## 3 class_2           0.00620         0.994  1.92   0.615 class_2\n## 4 class_2           0.472           0.528 -0.400  0.252 class_2\n## 5 class_2           0.163           0.837  1.30   1.09  class_1\n## 6 class_2           0.0317          0.968  2.59   1.36  class_2\n\nWith tidymodels, the probably package contains functions for equivocal zones. For cases with two classes, the make_two_class_pred() function creates a factor-like column that has the predicted classes with an equivocal zone:\n\nlibrary(probably)\n\nlvls &lt;- levels(training_set$class)\n\ntest_pred &lt;- \n  test_pred %&gt;% \n  mutate(.pred_with_eqz = make_two_class_pred(.pred_class_1, lvls, buffer = 0.15))\n\ntest_pred %&gt;% count(.pred_with_eqz)\n## # A tibble: 3 × 2\n##   .pred_with_eqz     n\n##       &lt;clss_prd&gt; &lt;int&gt;\n## 1           [EQ]     9\n## 2        class_1    20\n## 3        class_2    21\n\nRows that are within \\(0.50\\pm0.15\\) are given a value of [EQ].\n\nThe notation [EQ] in this example is not a factor level but an attribute of that column.\n\nSince the factor levels are the same as the original data, confusion matrices and other statistics can be computed without error. When using standard functions from the yardstick package, the equivocal results are converted to NA and are not used in the calculations that use the hard class predictions. Notice the differences in these confusion matrices:\n\n# All data\ntest_pred %&gt;% conf_mat(class, .pred_class)\n##           Truth\n## Prediction class_1 class_2\n##    class_1      20       6\n##    class_2       5      19\n\n# Reportable results only: \ntest_pred %&gt;% conf_mat(class, .pred_with_eqz)\n##           Truth\n## Prediction class_1 class_2\n##    class_1      17       3\n##    class_2       5      16\n\nAn is_equivocal() function is also available for filtering these rows from the data.\nDoes the equivocal zone help improve accuracy? Let’s look at different buffer sizes, as shown in Figure @ref(fig:equivocal-zone-results):\n\n# A function to change the buffer then compute performance.\neq_zone_results &lt;- function(buffer) {\n  test_pred &lt;- \n    test_pred %&gt;% \n    mutate(.pred_with_eqz = make_two_class_pred(.pred_class_1, lvls, buffer = buffer))\n  acc &lt;- test_pred %&gt;% accuracy(class, .pred_with_eqz)\n  rep_rate &lt;- reportable_rate(test_pred$.pred_with_eqz)\n  tibble(accuracy = acc$.estimate, reportable = rep_rate, buffer = buffer)\n}\n\n# Evaluate a sequence of buffers and plot the results. \nmap(seq(0, .1, length.out = 40), eq_zone_results) %&gt;% \n  list_rbind() %&gt;% \n  pivot_longer(c(-buffer), names_to = \"statistic\", values_to = \"value\") %&gt;% \n  ggplot(aes(x = buffer, y = value, lty = statistic)) + \n  geom_step(linewidth = 1.2, alpha = 0.8) + \n  labs(y = NULL, lty = NULL)\n\n\n\n\n\nThe effect of equivocal zones on model performance\n\n\n\nFigure @ref(fig:equivocal-zone-results) shows us that accuracy improves by a few percentage points but at the cost of nearly 10% of predictions being unusable! The value of such a compromise depends on how the model predictions will be used.\nThis analysis focused on using the predicted class probability to disqualify points, since this is a fundamental measure of uncertainty in classification models. A slightly better approach would be to use the standard error of the class probability. Since we used a Bayesian model, the probability estimates we found are actually the mean of the posterior predictive distribution. In other words, the Bayesian model gives us a distribution for the class probability. Measuring the standard deviation of this distribution gives us a standard error of prediction of the probability. In most cases, this value is directly related to the mean class probability. You might recall that, for a Bernoulli random variable with probability \\(p\\), the variance is \\(p(1-p)\\). Because of this relationship, the standard error is largest when the probability is 50%. Instead of assigning an equivocal result using the class probability, we could instead use a cutoff on the standard error of prediction.\nOne important aspect of the standard error of prediction is that it takes into account more than just the class probability. In cases where there is significant extrapolation or aberrant predictor values, the standard error might increase. The benefit of using the standard error of prediction is that it might also flag predictions that are problematic (as opposed to simply uncertain). One reason we used the Bayesian model is that it naturally estimates the standard error of prediction; not many models can calculate this. For our test set, using type = \"pred_int\" will produce upper and lower limits and the std_error adds a column for that quantity. For 80% intervals:\n\ntest_pred &lt;- \n  test_pred %&gt;% \n  bind_cols(\n    predict(two_class_mod, testing_set, type = \"pred_int\", std_error = TRUE)\n  )\n\nFor our example where the model and data are well behaved, Figure @ref(fig:std-errors) shows the standard error of prediction across the space:\n\n\n\n\nThe effect of the standard error of prediction overlaid with the test set data\n\n\n\nUsing the standard error as a measure to preclude samples from being predicted can also be applied to models with numeric outcomes. However, as shown in the next section, this may not always work."
  },
  {
    "objectID": "19-when-should-you-trust-predictions.html#applicability-domains",
    "href": "19-when-should-you-trust-predictions.html#applicability-domains",
    "title": "19  When Should You Trust Your Predictions?",
    "section": "\n19.2 Determining Model Applicability",
    "text": "19.2 Determining Model Applicability\nEquivocal zones try to measure the reliability of a prediction based on the model outputs. It may be that model statistics, such as the standard error of prediction, cannot measure the impact of extrapolation, and so we need another way to assess whether to trust a prediction and answer, “Is our model applicable for predicting a specific data point?” Let’s take the Chicago train data used extensively in Kuhn and Johnson (2019) and first shown in Section @ref(examples-of-tidyverse-syntax). The goal is to predict the number of customers entering the Clark and Lake train station each day.\nThe data set in the modeldata package (a tidymodels package with example data sets) has daily values between enero 22, 2001 and agosto 28, 2016. Let’s create a small test set using the last two weeks of the data:\n\n## loads both `Chicago` data set as well as `stations`\ndata(Chicago)\n\nChicago &lt;- Chicago %&gt;% select(ridership, date, one_of(stations))\n\nn &lt;- nrow(Chicago)\n\nChicago_train &lt;- Chicago %&gt;% slice(1:(n - 14))\nChicago_test  &lt;- Chicago %&gt;% slice((n - 13):n)\n\nThe main predictors are lagged ridership data at different train stations, including Clark and Lake, as well as the date. The ridership predictors are highly correlated with one another. In the following recipe, the date column is expanded into several new features, and the ridership predictors are represented using partial least squares (PLS) components. PLS (Geladi and Kowalski 1986), as we discussed in Section @ref(partial-least-squares), is a supervised version of principal component analysis where the new features have been decorrelated but are predictive of the outcome data.\nUsing the preprocessed data, we fit a standard linear model:\n\nbase_recipe &lt;-\n  recipe(ridership ~ ., data = Chicago_train) %&gt;%\n  # Create date features\n  step_date(date) %&gt;%\n  step_holiday(date, keep_original_cols = FALSE) %&gt;%\n  # Create dummy variables from factor columns\n  step_dummy(all_nominal()) %&gt;%\n  # Remove any columns with a single unique value\n  step_zv(all_predictors()) %&gt;%\n  step_normalize(!!!stations)%&gt;%\n  step_pls(!!!stations, num_comp = 10, outcome = vars(ridership))\n\nlm_spec &lt;-\n  linear_reg() %&gt;%\n  set_engine(\"lm\") \n\nlm_wflow &lt;-\n  workflow() %&gt;%\n  add_recipe(base_recipe) %&gt;%\n  add_model(lm_spec)\n\nset.seed(1902)\nlm_fit &lt;- fit(lm_wflow, data = Chicago_train)\n\nHow well do the data fit on the test set? We can predict() for the test set to find both predictions and prediction intervals:\n\nres_test &lt;-\n  predict(lm_fit, Chicago_test) %&gt;%\n  bind_cols(\n    predict(lm_fit, Chicago_test, type = \"pred_int\"),\n    Chicago_test\n  )\n\nres_test %&gt;% select(date, ridership, starts_with(\".pred\"))\n## # A tibble: 14 × 5\n##   date       ridership .pred .pred_lower .pred_upper\n##   &lt;date&gt;         &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n## 1 2016-08-15     20.6  20.3        16.2         24.5\n## 2 2016-08-16     21.0  21.3        17.1         25.4\n## 3 2016-08-17     21.0  21.4        17.3         25.6\n## 4 2016-08-18     21.3  21.4        17.3         25.5\n## 5 2016-08-19     20.4  20.9        16.7         25.0\n## 6 2016-08-20      6.22  7.52        3.34        11.7\n## # ℹ 8 more rows\nres_test %&gt;% rmse(ridership, .pred)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 rmse    standard       0.865\n\nThese are fairly good results. Figure @ref(fig:chicago-2016) visualizes the predictions along with 95% prediction intervals.\n\n\n\n\nTwo weeks of 2016 predictions for the Chicago data along with 95% prediction intervals\n\n\n\nGiven the scale of the ridership numbers, these results look particularly good for such a simple model. If this model were deployed, how well would it have done a few years later in June 2020? The model successfully makes a prediction, as a predictive model almost always will when given input data:\n\nres_2020 &lt;-\n  predict(lm_fit, Chicago_2020) %&gt;%\n  bind_cols(\n    predict(lm_fit, Chicago_2020, type = \"pred_int\"),\n    Chicago_2020\n  ) \n\nres_2020 %&gt;% select(date, contains(\".pred\"))\n## # A tibble: 14 × 4\n##   date       .pred .pred_lower .pred_upper\n##   &lt;date&gt;     &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n## 1 2020-06-01 20.1        15.9         24.3\n## 2 2020-06-02 21.4        17.2         25.6\n## 3 2020-06-03 21.5        17.3         25.6\n## 4 2020-06-04 21.3        17.1         25.4\n## 5 2020-06-05 20.7        16.6         24.9\n## 6 2020-06-06  9.04        4.88        13.2\n## # ℹ 8 more rows\n\nThe prediction intervals are about the same width, even though these data are well beyond the time period of the original training set. However, given the global pandemic in 2020, the performance on these data are abysmal:\n\nres_2020 %&gt;% select(date, ridership, starts_with(\".pred\"))\n## # A tibble: 14 × 5\n##   date       ridership .pred .pred_lower .pred_upper\n##   &lt;date&gt;         &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n## 1 2020-06-01     0.002 20.1        15.9         24.3\n## 2 2020-06-02     0.005 21.4        17.2         25.6\n## 3 2020-06-03     0.566 21.5        17.3         25.6\n## 4 2020-06-04     1.66  21.3        17.1         25.4\n## 5 2020-06-05     1.95  20.7        16.6         24.9\n## 6 2020-06-06     1.08   9.04        4.88        13.2\n## # ℹ 8 more rows\nres_2020 %&gt;% rmse(ridership, .pred)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 rmse    standard        17.2\n\nYou can see this terrible model performance visually in Figure @ref(fig:chicago-2020).\n\n\n\n\nTwo weeks of 2020 predictions for the Chicago data along with 95% prediction intervals\n\n\n\nConfidence and prediction intervals for linear regression expand as the data become more and more removed from the center of the training set. However, that effect is not dramatic enough to flag these predictions as being poor.\n\nSometimes the statistics produced by models don’t measure the quality of predictions very well.\n\nThis situation can be avoided by having a secondary methodology that can quantify how applicable the model is for any new prediction (i.e., the model’s applicability domain). There are a variety of methods to compute an applicability domain model, such as Jaworska, Nikolova-Jeliazkova, and Aldenberg (2005) or Netzeva et al. (2005). The approach used in this chapter is a fairly simple unsupervised method that attempts to measure how much (if any) a new data point is beyond the training data.1\n\nThe idea is to accompany a prediction with a score that measures how similar the new point is to the training set.\n\nOne method that works well uses principal component analysis (PCA) on the numeric predictor values. We’ll illustrate the process by using only two of the predictors that correspond to ridership at different stations (California and Austin stations). The training set are shown in panel (a) in Figure @ref(fig:pca-reference-dist). The ridership data for these stations are highly correlated, and the two distributions shown in the scatter plot correspond to ridership on the weekends and week days.\nThe first step is to conduct PCA on the training data. The PCA scores for the training set are shown in panel (b) in Figure @ref(fig:pca-reference-dist). Next, using these results, we measure the distance of each training set point to the center of the PCA data (panel (c) of Figure @ref(fig:pca-reference-dist)). We can then use this reference distribution (panel (d) of Figure @ref(fig:pca-reference-dist)) to estimate how far a data point is from the mainstream of the training data.\n\n\n\n\nThe PCA reference distribution based on the training set\n\n\n\nFor a new sample, the PCA scores are computed along with the distance to the center of the training set.\nHowever, what does it mean when a new sample has a distance of X? Since the PCA components can have different ranges from data set to data set, there is no obvious limit to say that a distance is too large.\nOne approach is to treat the distances from the training set data as “normal.” For new samples, we can determine how the new distance compares to the range in the reference distribution (from the training set). A percentile can be computed for new samples that reflect how much of the training set is less extreme than the new samples.\n\nA percentile of 90% means that most of the training set data are closer to the data center than the new sample.\n\nThe plot in Figure @ref(fig:two-new-points) overlays a testing set sample (triangle and dashed line) and a 2020 sample (circle and solid line) with the PCA distances from the training set.\n\n\n\n\nThe reference distribution with two new points: one using the test set and one from the 2020 data\n\n\n\nThe test set point has a distance of 1.28. It is in the 51.8% percentile of the training set distribution, indicating that it is snugly within the mainstream of the training set.\nThe 2020 sample is farther from the center than any of the training set samples (with a percentile of 100%). This indicates the sample is very extreme and that its corresponding prediction would be a severe extrapolation (and probably should not be reported).\nThe applicable package can develop an applicability domain model using PCA. We’ll use the 20 lagged station ridership predictors as inputs into the PCA analysis. There is an additional argument called threshold that determines how many components are used in the distance calculation. For our example, we’ll use a large value that indicates we should use enough components to account for 99% of the variation in the ridership predictors:\n\nlibrary(applicable)\npca_stat &lt;- apd_pca(~ ., data = Chicago_train %&gt;% select(one_of(stations)), \n                    threshold = 0.99)\npca_stat\n## # Predictors:\n##    20\n## # Principal Components:\n##    9 components were needed\n##    to capture at least 99% of the\n##    total variation in the predictors.\n\nThe autoplot() method plots the reference distribution. It has an optional argument for which data to plot. We’ll add a value of distance to plot only the training set distance distribution. This code generates the plot in Figure @ref(fig:ap-autoplot):\n\nautoplot(pca_stat, distance) + labs(x = \"distance\")\n\n\n\n\n\nThe results of using the autoplot() method on an applicable object\n\n\n\nThe x-axis shows the values of the distance and the y-axis displays the distribution’s percentiles. For example, half of the training set samples had distances less than 3.7.\nTo compute the percentiles for new data, the score() function works in the same way as predict():\n\nscore(pca_stat, Chicago_test) %&gt;% select(starts_with(\"distance\"))\n## # A tibble: 14 × 2\n##   distance distance_pctl\n##      &lt;dbl&gt;         &lt;dbl&gt;\n## 1     4.88          66.7\n## 2     5.21          71.4\n## 3     5.19          71.1\n## 4     5.00          68.5\n## 5     4.36          59.3\n## 6     4.10          55.2\n## # ℹ 8 more rows\n\nThese seem fairly reasonable. For the 2020 data:\n\nscore(pca_stat, Chicago_2020) %&gt;% select(starts_with(\"distance\"))\n## # A tibble: 14 × 2\n##   distance distance_pctl\n##      &lt;dbl&gt;         &lt;dbl&gt;\n## 1     9.39          99.8\n## 2     9.40          99.8\n## 3     9.30          99.7\n## 4     9.30          99.7\n## 5     9.29          99.7\n## 6    10.1            1  \n## # ℹ 8 more rows\n\nThe 2020 distance values indicate that these predictor values are outside of the vast majority of data seen by the model at training time. These should be flagged so that the predictions are either not reported at all or viewed with skepticism.\n\nOne important aspect of this analysis concerns which predictors are used to develop the applicability domain model. In our analysis, we used the raw predictor columns. However, in building the model, PLS score features were used in their place. Which of these should apd_pca() use? The apd_pca() function can also take a recipe as the input (instead of a formula) so that the distances reflect the PLS scores instead of the individual predictor columns. You can evaluate both methods to understand which one gives more relevant results."
  },
  {
    "objectID": "19-when-should-you-trust-predictions.html#trust-summary",
    "href": "19-when-should-you-trust-predictions.html#trust-summary",
    "title": "19  When Should You Trust Your Predictions?",
    "section": "\n19.3 Chapter Summary",
    "text": "19.3 Chapter Summary\nThis chapter showed two methods for evaluating whether predictions should be reported to the consumers of models. Equivocal zones deal with outcomes/predictions and can be helpful when the amount of uncertainty in a prediction is too large.\nApplicability domain models deal with features/predictors and quantify the amount of extrapolation (if any) that occurs when making a prediction. This chapter showed a basic method using principal component analysis, although there are many other ways to measure applicability. The applicable package also contains specialized methods for data sets where all of the predictors are binary. This method computes similarity scores between training set data points to define the reference distribution.\n\n\n\n\nBartley, E AND Schliep, M . AND Hanks. 2019. “Identifying and Characterizing Extrapolation in Multivariate Response Data.” PLOS ONE 14 (December): 1–20.\n\n\nDanowski, T, J Aarons, J Hydovitz, and J Wingert. 1970. “Utility of Equivocal Glucose Tolerances.” Diabetes 19 (7): 524–26.\n\n\nGeladi, P., and B Kowalski. 1986. “Partial Least-Squares Regression: A Tutorial.” Analytica Chimica Acta 185: 1–17.\n\n\nJaworska, J, N Nikolova-Jeliazkova, and T Aldenberg. 2005. “QSAR Applicability Domain Estimation by Projection of the Training Set in Descriptor Space: A Review.” Alternatives to Laboratory Animals 33 (5): 445–59.\n\n\nKerleguer, A., J.-L. Koeck, M. Fabre, P. Gérôme, R. Teyssou, and V. Hervé. 2003. “Use of Equivocal Zone in Interpretation of Results of the Amplified Mycobacterium Tuberculosis Direct Test for Diagnosis of Tuberculosis.” Journal of Clinical Microbiology 41 (4): 1783–84.\n\n\nNetzeva, T, A Worth, T Aldenberg, R Benigni, M Cronin, P Gramatica, J Jaworska, et al. 2005. “Current Status of Methods for Defining the Applicability Domain of (Quantitative) Structure-Activity Relationships: The Report and Recommendations of ECVAM Workshop 52.” Alternatives to Laboratory Animals 33 (2): 155–73."
  },
  {
    "objectID": "19-when-should-you-trust-predictions.html#footnotes",
    "href": "19-when-should-you-trust-predictions.html#footnotes",
    "title": "19  When Should You Trust Your Predictions?",
    "section": "",
    "text": "Bartley (2019) shows yet another method and applies it to ecological studies.↩︎"
  },
  {
    "objectID": "20-ensemble-models.html#data-stack",
    "href": "20-ensemble-models.html#data-stack",
    "title": "20  Ensembles of Models",
    "section": "\n20.1 Creating the Training Set for Stacking",
    "text": "20.1 Creating the Training Set for Stacking\nThe first step for building a stacked ensemble relies on the assessment set predictions from a resampling scheme with multiple splits. For each data point in the training set, stacking requires an out-of-sample prediction of some sort. For regression models, this is the predicted outcome. For classification models, the predicted classes or probabilities are available for use, although the latter contains more information than the hard class predictions. For a set of models, a data set is assembled where rows are the training set samples and columns are the out-of-sample predictions from the set of multiple models.\nBack in Chapter @ref(workflow-sets), we used five repeats of 10-fold cross-validation to resample the data. This resampling scheme generates five assessment set predictions for each training set sample. Multiple out-of-sample predictions can occur in several other resampling techniques (e.g., bootstrapping). For the purpose of stacking, any replicate predictions for a data point in the training set are averaged so that there is a single prediction per training set sample per candidate member.\n\nSimple validation sets can also be used with stacking since tidymodels considers this to be a single resample.\n\nFor the concrete example, the training set used for model stacking has columns for all of the candidate tuning parameter results. Table @ref(tab:ensemble-candidate-preds) presents the first six rows and selected columns.\n\n\n\nPredictions from candidate tuning parameter configurations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnsemble Candidate Predictions\n\n\n\nSample #\nBagged Tree\nMARS 1\nMARS 2\nCubist 1\n...\nCubist 25\n...\n\n\n\n\n1\n25.18\n18.20\n17.15\n17.79\n\n17.82\n\n\n\n2\n5.18\n-1.77\n-0.72\n2.83\n\n3.87\n\n\n\n3\n9.71\n7.55\n5.91\n6.31\n\n8.60\n\n\n\n4\n25.21\n20.93\n21.35\n23.72\n\n21.61\n\n\n\n5\n6.33\n1.53\n0.27\n3.60\n\n4.57\n\n\n\n6\n7.88\n5.00\n1.74\n7.69\n\n7.55\n\n\n\n\n\n\nThere is a single column for the bagged tree model since it has no tuning parameters. Also, recall that MARS was tuned over a single parameter (the product degree) with two possible configurations, so this model is represented by two columns. Most of the other models have 25 corresponding columns, as shown for Cubist in this example.\n\nFor classification models, the candidate prediction columns would be predicted class probabilities. Since these columns add to one for each model, the probabilities for one of the classes can be left out.\n\nTo summarize where we are so far, the first step to stacking is to assemble the assessment set predictions for the training set from each candidate model. We can use these assessment set predictions to move forward and build a stacked ensemble.\nTo start ensembling with the stacks package, create an empty data stack using the stacks() function and then add candidate models. Recall that we used workflow sets to fit a wide variety of models to these data. We’ll use the racing results:\n\nrace_results\n## # A workflow set/tibble: 12 × 4\n##   wflow_id    info             option    result   \n##   &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;    &lt;list&gt;   \n## 1 MARS        &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n## 2 CART        &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n## 3 CART_bagged &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;rsmp[+]&gt;\n## 4 RF          &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n## 5 boosting    &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n## 6 Cubist      &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;race[+]&gt;\n## # ℹ 6 more rows\n\nIn this case, our syntax is:\n\nlibrary(tidymodels)\nlibrary(stacks)\ntidymodels_prefer()\n\nconcrete_stack &lt;- \n  stacks() %&gt;% \n  add_candidates(race_results)\n\nconcrete_stack\n## # A data stack with 12 model definitions and 19 candidate members:\n## #   MARS: 1 model configuration\n## #   CART: 1 model configuration\n## #   CART_bagged: 1 model configuration\n## #   RF: 1 model configuration\n## #   boosting: 1 model configuration\n## #   Cubist: 1 model configuration\n## #   SVM_radial: 1 model configuration\n## #   SVM_poly: 1 model configuration\n## #   KNN: 3 model configurations\n## #   neural_network: 2 model configurations\n## #   full_quad_linear_reg: 5 model configurations\n## #   full_quad_KNN: 1 model configuration\n## # Outcome: compressive_strength (numeric)\n\nRecall that racing methods (Section @ref(racing)) are more efficient since they might not evaluate all configurations on all resamples. Stacking requires that all candidate members have the complete set of resamples. add_candidates() includes only the model configurations that have complete results.\n\nWhy use the racing results instead of the full set of candidate models contained in grid_results? Either can be used. We found better performance for these data using the racing results. This might be due to the racing method pre-selecting the best model(s) from the larger grid.\n\nIf we had not used the workflowsets package, objects from the tune and finetune could also be passed to add_candidates(). This can include both grid and iterative search objects."
  },
  {
    "objectID": "20-ensemble-models.html#blend-predictions",
    "href": "20-ensemble-models.html#blend-predictions",
    "title": "20  Ensembles of Models",
    "section": "\n20.2 Blend the Predictions",
    "text": "20.2 Blend the Predictions\nThe training set predictions and the corresponding observed outcome data are used to create a meta-learning model where the assessment set predictions are the predictors of the observed outcome data. Meta-learning can be accomplished using any model. The most commonly used model is a regularized generalized linear model, which encompasses linear, logistic, and multinomial models. Specifically, regularization via the lasso penalty (Tibshirani 1996), which uses shrinkage to pull points toward a central value, has several advantages:\n\nUsing the lasso penalty can remove candidates (and sometimes whole model types) from the ensemble.\nThe correlation between ensemble candidates tends to be very high, and regularization helps alleviate this issue.\n\nBreiman (1996b) also suggested that, when a linear model is used to blend the predictions, it might be helpful to constrain the blending coefficients to be nonnegative. We have generally found this to be good advice and it is the default for the stacks package (but it can be changed via an optional argument).\nSince our outcome is numeric, linear regression is used for the metamodel. Fitting the metamodel is as straightforward as using:\n\nset.seed(2001)\nens &lt;- blend_predictions(concrete_stack)\n\nThis evaluates the meta-learning model over a predefined grid of lasso penalty values and uses an internal resampling method to determine the best value. The autoplot() method, shown in Figure @ref(fig:stacking-autoplot), helps us understand if the default penalization method was sufficient:\n\nautoplot(ens)\n\n\n\n\n\nResults of using the autoplot() method on the blended stacks object\n\n\n\nThe top panel of Figure @ref(fig:stacking-autoplot) shows the average number of candidate ensemble members retained by the meta-learning model. We can see that the number of members is fairly constant and, as it increases, the RMSE also increases.\nThe default range may not have served us well here. To evaluate the meta-learning model with larger penalties, let’s pass an additional option:\n\nset.seed(2002)\nens &lt;- blend_predictions(concrete_stack, penalty = 10^seq(-2, -0.5, length = 20))\n\nNow, in Figure @ref(fig:stacking-autoplot-redo), we see a range where the ensemble model becomes worse than with our first blend (but not by much). The \\(R^2\\) values increase with more members and larger penalties.\n\nautoplot(ens)\n\n\n\n\n\nThe results of using the autoplot() method on the updated blended stacks object\n\n\n\nWhen blending predictions using a regression model, it is common to constrain the blending parameters to be nonnegative. For these data, this constraint has the effect of eliminating many of the potential ensemble members; even at fairly low penalties, the ensemble is limited to a fraction of the original eighteen.\nThe penalty value associated with the smallest RMSE was 0.036. Printing the object shows the details of the meta-learning model:\n\nens\n## ── A stacked ensemble model ─────────────────────────────────────\n## \n## \n## Out of 19 possible candidate members, the ensemble retained 4.\n## \n## Penalty: 0.0356969884682606.\n## \n## Mixture: 1.\n## \n## \n## The 4 highest weighted members are:\n## # A tibble: 4 × 3\n##   member                    type         weight\n##   &lt;chr&gt;                     &lt;chr&gt;         &lt;dbl&gt;\n## 1 boosting_1_04             boost_tree   0.761 \n## 2 neural_network_1_12       mlp          0.127 \n## 3 Cubist_1_25               cubist_rules 0.102 \n## 4 full_quad_linear_reg_1_16 linear_reg   0.0325\n## \n## Members have not yet been fitted with `fit_members()`.\n\nThe regularized linear regression meta-learning model contained four blending coefficients across four types of models. The autoplot() method can be used again to show the contributions of each model type, to produce Figure @ref(fig:blending-weights).\n\nautoplot(ens, \"weights\") +\n  geom_text(aes(x = weight + 0.01, label = model), hjust = 0) + \n  theme(legend.position = \"none\") +\n  lims(x = c(-0.01, 0.8))\n\n\n\n\n\nBlending coefficients for the stacking ensemble\n\n\n\nThe boosted tree and neural network models have the largest contributions to the ensemble. For this ensemble, the outcome is predicted with the equation:\n\n\\[\\begin{align}\n\\text{ensemble prediction} &=-0.76 \\\\\n    +&0.76 \\times \\text{boost tree prediction} \\notag \\\\\n    +&0.13 \\times \\text{mlp prediction} \\notag \\\\\n    +&0.1 \\times \\text{cubist rules prediction} \\notag \\\\\n    +&0.032 \\times \\text{linear reg prediction} \\notag\n\\end{align}\\]\n\nwhere the predictors in the equation are the predicted compressive strength values from those models."
  },
  {
    "objectID": "20-ensemble-models.html#fit-members",
    "href": "20-ensemble-models.html#fit-members",
    "title": "20  Ensembles of Models",
    "section": "\n20.3 Fit the Member Models",
    "text": "20.3 Fit the Member Models\nThe ensemble contains four candidate members, and we now know how their predictions can be blended into a final prediction for the ensemble. However, these individual model fits have not yet been created. To be able to use the stacking model, four additional model fits are required. These use the entire training set with the original predictors.\nThe four models to be fit are:\n\n\nboosting: number of trees = 1957, minimal node size = 8, tree depth = 7, learning rate = 0.0756, minimum loss reduction = 1.45e-07, and proportion of observations sampled = 0.679\nCubist: number of committees = 98 and number of nearest neighbors = 2\nlinear regression (quadratic features): amount of regularization = 6.28e-09 and proportion of lasso penalty = 0.636\nneural network: number of hidden units = 22, amount of regularization = 2.08e-10, and number of epochs = 92\n\n\nThe stacks package has a function, fit_members(), that trains and returns these models:\n\nens &lt;- fit_members(ens)\n\nThis updates the stacking object with the fitted workflow objects for each member. At this point, the stacking model can be used for prediction."
  },
  {
    "objectID": "20-ensemble-models.html#test-set-results",
    "href": "20-ensemble-models.html#test-set-results",
    "title": "20  Ensembles of Models",
    "section": "\n20.4 Test Set Results",
    "text": "20.4 Test Set Results\nSince the blending process used resampling, we can estimate that the ensemble with four members had an estimated RMSE of 4.13. Recall from Chapter @ref(workflow-sets) that the best boosted tree had a test set RMSE of 3.41. How will the ensemble model compare on the test set? We can predict() to find out:\n\nreg_metrics &lt;- metric_set(rmse, rsq)\nens_test_pred &lt;- \n  predict(ens, concrete_test) %&gt;% \n  bind_cols(concrete_test)\n\nens_test_pred %&gt;% \n  reg_metrics(compressive_strength, .pred)\n## # A tibble: 2 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 rmse    standard       3.35 \n## 2 rsq     standard       0.956\n\nThis is moderately better than our best single model. It is fairly common for stacking to produce incremental benefits when compared to the best single model."
  },
  {
    "objectID": "20-ensemble-models.html#ensembles-summary",
    "href": "20-ensemble-models.html#ensembles-summary",
    "title": "20  Ensembles of Models",
    "section": "\n20.5 Chapter Summary",
    "text": "20.5 Chapter Summary\nThis chapter demonstrated how to combine different models into an ensemble for better predictive performance. The process of creating the ensemble can automatically eliminate candidate models to find a small subset that improves performance. The stacks package has a fluent interface for combining resampling and tuning results into a meta-model.\n\n\n\n\nBreiman, L. 1996a. “Bagging Predictors.” Machine Learning 24 (2): 123–40.\n\n\n———. 1996b. “Stacked Regressions.” Machine Learning 24 (1): 49–64.\n\n\n———. 2001. “Random Forests.” Machine Learning 45 (1): 5–32.\n\n\nFreund, Y, and R Schapire. 1997. “A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting.” Journal of Computer and System Sciences 55 (1): 119–39.\n\n\nHo, T. 1995. “Random Decision Forests.” In Proceedings of 3rd International Conference on Document Analysis and Recognition, 1:278–82. IEEE.\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society. Series B (Methodological) 58 (1): 267–88. http://www.jstor.org/stable/2346178.\n\n\nWolpert, D. 1992. “Stacked Generalization.” Neural Networks 5 (2): 241–59."
  },
  {
    "objectID": "21-inferential-analysis.html#inference-for-count-data",
    "href": "21-inferential-analysis.html#inference-for-count-data",
    "title": "21  Inferential Analysis",
    "section": "\n21.1 Inference for Count Data",
    "text": "21.1 Inference for Count Data\nTo understand how tidymodels packages can be used for inferential modeling, let’s focus on an example with count data. We’ll use biochemistry publication data from the pscl package. These data consist of information on 915 Ph.D. biochemistry graduates and tries to explain factors that impact their academic productivity (measured via number or count of articles published within three years). The predictors include the gender of the graduate, their marital status, the number of children of the graduate that are at least five years old, the prestige of their department, and the number of articles produced by their mentor in the same time period. The data reflect biochemistry doctorates who finished their education between 1956 and 1963. The data are a somewhat biased sample of all of the biochemistry doctorates given during this period (based on completeness of information).\n\nRecall that in Chapter @ref(trust) we asked the question “Is our model applicable for predicting a specific data point?” It is very important to define what populations an inferential analysis applies to. For these data, the results would likely apply to biochemistry doctorates given around the time frame that the data were collected. Does it also apply to other chemistry doctorate types (e.g., medicinal chemistry, etc)? These are important questions to address (and document) when conducting inferential analyses.\n\nA plot of the data shown in Figure @ref(fig:counts) indicates that many graduates did not publish any articles in this time and that the outcome follows a right-skewed distribution:\n\nlibrary(tidymodels)\ntidymodels_prefer()\n\ndata(\"bioChemists\", package = \"pscl\")\n\nggplot(bioChemists, aes(x = art)) + \n  geom_histogram(binwidth = 1, color = \"white\") + \n  labs(x = \"Number of articles within 3y of graduation\")\n\n\n\n\n\nDistribution of the number of articles written within 3 years of graduation\n\n\n\nSince the outcome data are counts, the most common distribution assumption to make is that the outcome has a Poisson distribution. This chapter will use these data for several types of analyses."
  },
  {
    "objectID": "21-inferential-analysis.html#comparisons-with-two-sample-tests",
    "href": "21-inferential-analysis.html#comparisons-with-two-sample-tests",
    "title": "21  Inferential Analysis",
    "section": "\n21.2 Comparisons with Two-Sample Tests",
    "text": "21.2 Comparisons with Two-Sample Tests\nWe can start with hypothesis testing. The original author’s goal with this data set on biochemistry publication data was to determine if there is a difference in publications between men and women (Long 1992). The data from the study show:\n\nbioChemists %&gt;% \n  group_by(fem) %&gt;% \n  summarize(counts = sum(art), n = length(art))\n## # A tibble: 2 × 3\n##   fem   counts     n\n##   &lt;fct&gt;  &lt;int&gt; &lt;int&gt;\n## 1 Men      930   494\n## 2 Women    619   421\n\nThere were many more publications by men, although there were also more men in the data. The simplest approach to analyzing these data would be to do a two-sample comparison using the poisson.test() function in the stats package. It requires the counts for one or two groups.\nFor our application, the hypotheses to compare the two sexes are:\n\\[\\begin{align}\nH_0&: \\lambda_m = \\lambda_f \\notag \\\\\nH_a&: \\lambda_m \\ne \\lambda_f \\notag\n\\end{align}\\]\nwhere the \\(\\lambda\\) values are the rates of publications (over the same time period).\nA basic application of the test is:1\n\npoisson.test(c(930, 619), T = 3)\n## \n##  Comparison of Poisson rates\n## \n## data:  c(930, 619) time base: 3\n## count1 = 930, expected count1 = 774, p-value = 3e-15\n## alternative hypothesis: true rate ratio is not equal to 1\n## 95 percent confidence interval:\n##  1.356 1.666\n## sample estimates:\n## rate ratio \n##      1.502\n\nThe function reports a p-value as well as a confidence interval for the ratio of the publication rates. The results indicate that the observed difference is greater than the experiential noise and favors \\(H_a\\).\nOne issue with using this function is that the results come back as an htest object. While this type of object has a well-defined structure, it can be difficult to consume for subsequent operations such as reporting or visualizations. The most impactful tool that tidymodels offers for inferential models is the tidy() functions in the broom package. As previously seen, this function makes a well-formed, predictably named tibble from the object. We can tidy() the results of our two-sample comparison test:\n\npoisson.test(c(930, 619)) %&gt;% \n  tidy()\n## # A tibble: 1 × 8\n##   estimate statistic  p.value parameter conf.low conf.high method        alternative\n##      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;      \n## 1     1.50       930 2.73e-15      774.     1.36      1.67 Comparison o… two.sided\n\n\nBetween the broom and broom.mixed packages, there are tidy() methods for more than 150 models.\n\nWhile the Poisson distribution is reasonable, we might also want to assess using fewer distributional assumptions. Two methods that might be helpful are the bootstrap and permutation tests (Davison and Hinkley 1997).\nThe infer package, part of the tidymodels framework, is a powerful and intuitive tool for hypothesis testing (Ismay and Kim 2021). Its syntax is concise and designed for nonstatisticians.\nFirst, we specify() that we will use the difference in the mean number of articles between the sexes and then calculate() the statistic from the data. Recall that the maximum likelihood estimator for the Poisson mean is the sample mean. The hypotheses tested here are the same as the previous test (but are conducted using a different testing procedure).\nWith infer, we specify the outcome and covariate, then state the statistic of interest:\n\nlibrary(infer)\n\nobserved &lt;- \n  bioChemists %&gt;%\n  specify(art ~ fem) %&gt;%\n  calculate(stat = \"diff in means\", order = c(\"Men\", \"Women\"))\nobserved\n## Response: art (numeric)\n## Explanatory: fem (factor)\n## # A tibble: 1 × 1\n##    stat\n##   &lt;dbl&gt;\n## 1 0.412\n\nFrom here, we compute a confidence interval for this mean by creating the bootstrap distribution via generate(); the same statistic is computed for each resampled version of the data:\n\nset.seed(2101)\nbootstrapped &lt;- \n  bioChemists %&gt;%\n  specify(art ~ fem)  %&gt;%\n  generate(reps = 2000, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"diff in means\", order = c(\"Men\", \"Women\"))\nbootstrapped\n## Response: art (numeric)\n## Explanatory: fem (factor)\n## # A tibble: 2,000 × 2\n##   replicate  stat\n##       &lt;int&gt; &lt;dbl&gt;\n## 1         1 0.467\n## 2         2 0.107\n## 3         3 0.467\n## 4         4 0.308\n## 5         5 0.369\n## 6         6 0.428\n## # ℹ 1,994 more rows\n\nA percentile interval is calculated using:\n\npercentile_ci &lt;- get_ci(bootstrapped)\npercentile_ci\n## # A tibble: 1 × 2\n##   lower_ci upper_ci\n##      &lt;dbl&gt;    &lt;dbl&gt;\n## 1    0.158    0.653\n\nThe infer package has a high-level API for showing the analysis results, as shown in Figure @ref(fig:bootstrapped-mean).\n\nvisualize(bootstrapped) +\n    shade_confidence_interval(endpoints = percentile_ci)\n\n\n\n\n\nThe bootstrap distribution of the difference in means. The highlighted region is the confidence interval.\n\n\n\nSince the interval visualized in in Figure @ref(fig:bootstrapped-mean) does not include zero, these results indicate that men have published more articles than women.\nIf we require a p-value, the infer package can compute the value via a permutation test, shown in the following code. The syntax is very similar to the bootstrapping code we used earlier. We add a hypothesize() verb to state the type of assumption to test and the generate() call contains an option to shuffle the data.\n\nset.seed(2102)\npermuted &lt;- \n  bioChemists %&gt;%\n  specify(art ~ fem)  %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 2000, type = \"permute\") %&gt;%\n  calculate(stat = \"diff in means\", order = c(\"Men\", \"Women\"))\npermuted\n## Response: art (numeric)\n## Explanatory: fem (factor)\n## Null Hypothesis: independence\n## # A tibble: 2,000 × 2\n##   replicate     stat\n##       &lt;int&gt;    &lt;dbl&gt;\n## 1         1  0.201  \n## 2         2 -0.133  \n## 3         3  0.109  \n## 4         4 -0.195  \n## 5         5 -0.00128\n## 6         6 -0.102  \n## # ℹ 1,994 more rows\n\nThe following visualization code is also very similar to the bootstrap approach. This code generates Figure @ref(fig:permutation-dist) where the vertical line signifies the observed value:\n\nvisualize(permuted) +\n    shade_p_value(obs_stat = observed, direction = \"two-sided\")\n\n\n\n\n\nEmpirical distribution of the test statistic under the null hypothesis. The vertical line indicates the observed test statistic.\n\n\n\nThe actual p-value is:\n\npermuted %&gt;%\n  get_p_value(obs_stat = observed, direction = \"two-sided\")\n## # A tibble: 1 × 1\n##   p_value\n##     &lt;dbl&gt;\n## 1   0.002\n\nThe vertical line representing the null hypothesis in Figure @ref(fig:permutation-dist) is far away from the permutation distribution. This means, if in fact the null hypothesis were true, the likelihood is exceedingly small of observing data at least as extreme as what is at hand.\nThe two-sample tests shown in this section are probably suboptimal because they do not account for other factors that might explain the observed relationship between publication rate and sex. Let’s move to a more complex model that can consider additional covariates."
  },
  {
    "objectID": "21-inferential-analysis.html#log-linear-models",
    "href": "21-inferential-analysis.html#log-linear-models",
    "title": "21  Inferential Analysis",
    "section": "\n21.3 Log-Linear Models",
    "text": "21.3 Log-Linear Models\nThe focus of the rest of this chapter will be on a generalized linear model (Dobson 1999) where we assume the counts follow a Poisson distribution. For this model, the covariates/predictors enter the model in a log-linear fashion:\n\\[\n\\log(\\lambda) = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_px_p\n\\]\nwhere \\(\\lambda\\) is the expected value of the counts.\nLet’s fit a simple model that contains all of the predictor columns. The poissonreg package, a parsnip extension package in tidymodels, will fit this model specification:\n\nlibrary(poissonreg)\n\n# default engine is 'glm'\nlog_lin_spec &lt;- poisson_reg()\n\nlog_lin_fit &lt;- \n  log_lin_spec %&gt;% \n  fit(art ~ ., data = bioChemists)\nlog_lin_fit\n## parsnip model object\n## \n## \n## Call:  stats::glm(formula = art ~ ., family = stats::poisson, data = data)\n## \n## Coefficients:\n## (Intercept)     femWomen   marMarried         kid5          phd         ment  \n##      0.3046      -0.2246       0.1552      -0.1849       0.0128       0.0255  \n## \n## Degrees of Freedom: 914 Total (i.e. Null);  909 Residual\n## Null Deviance:       1820 \n## Residual Deviance: 1630  AIC: 3310\n\nThe tidy() method succinctly summarizes the coefficients for the model (along with 90% confidence intervals):\n\ntidy(log_lin_fit, conf.int = TRUE, conf.level = 0.90)\n## # A tibble: 6 × 7\n##   term        estimate std.error statistic  p.value conf.low conf.high\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept)   0.305    0.103       2.96  3.10e- 3   0.134     0.473 \n## 2 femWomen     -0.225    0.0546     -4.11  3.92e- 5  -0.315    -0.135 \n## 3 marMarried    0.155    0.0614      2.53  1.14e- 2   0.0545    0.256 \n## 4 kid5         -0.185    0.0401     -4.61  4.08e- 6  -0.251    -0.119 \n## 5 phd           0.0128   0.0264      0.486 6.27e- 1  -0.0305    0.0563\n## 6 ment          0.0255   0.00201    12.7   3.89e-37   0.0222    0.0288\n\nIn this output, the p-values correspond to separate hypothesis tests for each parameter:\n\\[\\begin{align}\nH_0&: \\beta_j = 0 \\notag \\\\\nH_a&: \\beta_j \\ne 0 \\notag\n\\end{align}\\]\nfor each of the model parameters. Looking at these results, phd (the prestige of their department) may not have any relationship with the outcome.\nWhile the Poisson distribution is the routine assumption for data like these, it may be beneficial to conduct a rough check of the model assumptions by fitting the models without using the Poisson likelihood to calculate the confidence intervals. The rsample package has a convenience function to compute bootstrap confidence intervals for lm() and glm() models. We can use this function, while explicitly declaring family = poisson, to compute a large number of model fits. By default, we compute a 90% confidence bootstrap-t interval (percentile intervals are also available):\n\nset.seed(2103)\nglm_boot &lt;- \n  reg_intervals(art ~ ., data = bioChemists, model_fn = \"glm\", family = poisson)\nglm_boot\n## # A tibble: 5 × 6\n##   term          .lower .estimate  .upper .alpha .method  \n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    \n## 1 femWomen   -0.358      -0.226  -0.0856   0.05 student-t\n## 2 kid5       -0.298      -0.184  -0.0789   0.05 student-t\n## 3 marMarried  0.000264    0.155   0.317    0.05 student-t\n## 4 ment        0.0182      0.0256  0.0322   0.05 student-t\n## 5 phd        -0.0707      0.0130  0.102    0.05 student-t\n\n\nWhen we compare these results (in Figure @ref(fig:glm-intervals)) to the purely parametric results from glm(), the bootstrap intervals are somewhat wider. If the data were truly Poisson, these intervals would have more similar widths.\n\n\n\n\n\nTwo types of confidence intervals for the Poisson regression model\n\n\n\nDetermining which predictors to include in the model is a difficult problem. One approach is to conduct likelihood ratio tests (LRT) (McCullagh and Nelder 1989) between nested models. Based on the confidence intervals, we have evidence that a simpler model without phd may be sufficient. Let’s fit a smaller model, then conduct a statistical test:\n\\[\\begin{align}\nH_0&: \\beta_{phd} = 0 \\notag \\\\\nH_a&: \\beta_{phd} \\ne 0 \\notag\n\\end{align}\\]\nThis hypothesis was previously tested when we showed the tidied results for log_lin_fit. That particular approach used results from a single model fit via a Wald statistic (i.e., the parameter divided by its standard error). For that approach, the p-value was 0.63. We can tidy the results for the LRT to get the p-value:\n\nlog_lin_reduced &lt;- \n  log_lin_spec %&gt;% \n  fit(art ~ ment + kid5 + fem + mar, data = bioChemists)\n\nanova(\n  extract_fit_engine(log_lin_reduced),\n  extract_fit_engine(log_lin_fit),\n  test = \"LRT\"\n) %&gt;%\n  tidy()\n## # A tibble: 2 × 6\n##   term                          df.residual residual.deviance    df deviance p.value\n##   &lt;chr&gt;                               &lt;dbl&gt;             &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n## 1 art ~ ment + kid5 + fem + mar         910             1635.    NA   NA      NA    \n## 2 art ~ fem + mar + kid5 + phd…         909             1634.     1    0.236   0.627\n\nThe results are the same and, based on these and the confidence interval for this parameter, we’ll exclude phd from further analyses since it does not appear to be associated with the outcome."
  },
  {
    "objectID": "21-inferential-analysis.html#a-more-complex-model",
    "href": "21-inferential-analysis.html#a-more-complex-model",
    "title": "21  Inferential Analysis",
    "section": "\n21.4 A More Complex Model",
    "text": "21.4 A More Complex Model\nWe can move into even more complex models within our tidymodels approach. For count data, there are occasions where the number of zero counts is larger than what a simple Poisson distribution would prescribe. A more complex model appropriate for this situation is the zero-inflated Poisson (ZIP) model; see Mullahy (1986), Lambert (1992), and Zeileis, Kleiber, and Jackman (2008). Here, there are two sets of covariates: one for the count data and others that affect the probability (denoted as \\(\\pi\\)) of zeros. The equation for the mean \\(\\lambda\\) is:\n\\[\\lambda = 0 \\pi + (1 - \\pi) \\lambda_{nz}\\]\nwhere\n\\[\\begin{align}\n\\log(\\lambda_{nz}) &= \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_px_p \\notag \\\\\n\\log\\left(\\frac{\\pi}{1-\\pi}\\right) &= \\gamma_0 + \\gamma_1z_1 + \\ldots + \\gamma_qz_q \\notag\n\\end{align}\\]\nand the \\(x\\) covariates affect the count values while the \\(z\\) covariates influence the probability of a zero. The two sets of predictors do not need to be mutually exclusive.\nWe’ll fit a model with a full set of \\(z\\) covariates:\n\nzero_inflated_spec &lt;- poisson_reg() %&gt;% set_engine(\"zeroinfl\")\n\nzero_inflated_fit &lt;- \n  zero_inflated_spec %&gt;% \n  fit(art ~ fem + mar + kid5 + ment | fem + mar + kid5 + phd + ment,\n      data = bioChemists)\n\nzero_inflated_fit\n## parsnip model object\n## \n## \n## Call:\n## pscl::zeroinfl(formula = art ~ fem + mar + kid5 + ment | fem + mar + kid5 + \n##     phd + ment, data = data)\n## \n## Count model coefficients (poisson with log link):\n## (Intercept)     femWomen   marMarried         kid5         ment  \n##       0.621       -0.209        0.105       -0.143        0.018  \n## \n## Zero-inflation model coefficients (binomial with logit link):\n## (Intercept)     femWomen   marMarried         kid5          phd         ment  \n##     -0.6086       0.1093      -0.3529       0.2195       0.0124      -0.1351\n\nSince the coefficients for this model are also estimated using maximum likelihood, let’s try to use another likelihood ratio test to understand if the new model terms are helpful. We will simultaneously test that:\n\\[\\begin{align}\nH_0&: \\gamma_1 = 0, \\gamma_2 = 0, \\cdots, \\gamma_5 = 0 \\notag \\\\\nH_a&: \\text{at least one } \\gamma \\ne 0  \\notag\n\\end{align}\\]\nLet’s try ANOVA again:\n\nanova(\n  extract_fit_engine(zero_inflated_fit),\n  extract_fit_engine(log_lin_reduced),\n  test = \"LRT\"\n) %&gt;%\n  tidy()\n## Error in UseMethod(\"anova\"): no applicable method for 'anova' applied to an object of class \"zeroinfl\"\n\nAn anova() method isn’t implemented for zeroinfl objects!\nAn alternative is to use an information criterion statistic, such as the Akaike information criterion (AIC) (Claeskens 2016). This computes the log-likelihood (from the training set) and penalizes that value based on the training set size and the number of model parameters. In R’s parameterization, smaller AIC values are better. In this case, we are not conducting a formal statistical test but estimating the ability of the data to fit the data.\nThe results indicate that the ZIP model is preferable:\n\nzero_inflated_fit %&gt;% extract_fit_engine() %&gt;% AIC()\n## [1] 3232\nlog_lin_reduced   %&gt;% extract_fit_engine() %&gt;% AIC()\n## [1] 3312\n\nHowever, it’s hard to contextualize this pair of single values and assess how different they actually are. To solve this problem, we’ll resample a large number of each of these two models. From these, we can compute the AIC values for each and determine how often the results favor the ZIP model. Basically, we will be characterizing the uncertainty of the AIC statistics to gauge their difference relative to the noise in the data.\nWe’ll also compute more bootstrap confidence intervals for the parameters in a bit so we specify the apparent = TRUE option when creating the bootstrap samples. This is required for some types of intervals.\nFirst, we create the 4,000 model fits:\n\nzip_form &lt;- art ~ fem + mar + kid5 + ment | fem + mar + kid5 + phd + ment\nglm_form &lt;- art ~ fem + mar + kid5 + ment\n\nset.seed(2104)\nbootstrap_models &lt;-\n  bootstraps(bioChemists, times = 2000, apparent = TRUE) %&gt;%\n  mutate(\n    glm = map(splits, ~ fit(log_lin_spec,       glm_form, data = analysis(.x))),\n    zip = map(splits, ~ fit(zero_inflated_spec, zip_form, data = analysis(.x)))\n  )\nbootstrap_models\n## # Bootstrap sampling with apparent sample \n## # A tibble: 2,001 × 4\n##   splits            id            glm      zip     \n##   &lt;list&gt;            &lt;chr&gt;         &lt;list&gt;   &lt;list&gt;  \n## 1 &lt;split [915/355]&gt; Bootstrap0001 &lt;fit[+]&gt; &lt;fit[+]&gt;\n## 2 &lt;split [915/333]&gt; Bootstrap0002 &lt;fit[+]&gt; &lt;fit[+]&gt;\n## 3 &lt;split [915/337]&gt; Bootstrap0003 &lt;fit[+]&gt; &lt;fit[+]&gt;\n## 4 &lt;split [915/344]&gt; Bootstrap0004 &lt;fit[+]&gt; &lt;fit[+]&gt;\n## 5 &lt;split [915/351]&gt; Bootstrap0005 &lt;fit[+]&gt; &lt;fit[+]&gt;\n## 6 &lt;split [915/354]&gt; Bootstrap0006 &lt;fit[+]&gt; &lt;fit[+]&gt;\n## # ℹ 1,995 more rows\n\nNow we can extract the model fits and their corresponding AIC values:\n\nbootstrap_models &lt;-\n  bootstrap_models %&gt;%\n  mutate(\n    glm_aic = map_dbl(glm, ~ extract_fit_engine(.x) %&gt;% AIC()),\n    zip_aic = map_dbl(zip, ~ extract_fit_engine(.x) %&gt;% AIC())\n  )\nmean(bootstrap_models$zip_aic &lt; bootstrap_models$glm_aic)\n## [1] 1\n\nIt seems definitive from these results that accounting for the excessive number of zero counts is a good idea.\n\nWe could have used fit_resamples() or a workflow set to conduct these computations. In this section, we used mutate() and map() to compute the models to demonstrate how one might use tidymodels tools for models that are not supported by one of the parsnip packages.\n\nSince we have computed the resampled model fits, let’s create bootstrap intervals for the zero probability model coefficients (i.e., the \\(\\gamma_j\\)). We can extract these with the tidy() method and use the type = \"zero\" option to obtain these estimates:\n\nbootstrap_models &lt;-\n  bootstrap_models %&gt;%\n  mutate(zero_coefs  = map(zip, ~ tidy(.x, type = \"zero\")))\n\n# One example:\nbootstrap_models$zero_coefs[[1]]\n## # A tibble: 6 × 6\n##   term        type  estimate std.error statistic   p.value\n##   &lt;chr&gt;       &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1 (Intercept) zero   -0.128     0.497     -0.257 0.797    \n## 2 femWomen    zero   -0.0764    0.319     -0.240 0.811    \n## 3 marMarried  zero   -0.112     0.365     -0.307 0.759    \n## 4 kid5        zero    0.270     0.186      1.45  0.147    \n## 5 phd         zero   -0.178     0.132     -1.35  0.177    \n## 6 ment        zero   -0.123     0.0315    -3.91  0.0000935\n\nIt’s a good idea to visualize the bootstrap distributions of the coefficients, as in Figure @ref(fig:zip-bootstrap).\n\nbootstrap_models %&gt;% \n  unnest(zero_coefs) %&gt;% \n  ggplot(aes(x = estimate)) +\n  geom_histogram(bins = 25, color = \"white\") + \n  facet_wrap(~ term, scales = \"free_x\") + \n  geom_vline(xintercept = 0, lty = 2, color = \"gray70\")\n\n\n\n\n\nBootstrap distributions of the ZIP model coefficients. The vertical lines indicate the observed estimates.\n\n\n\nOne of the covariates (ment) that appears to be important has a very skewed distribution. The extra space in some of the facets indicates there are some outliers in the estimates. This might occur when models did not converge; those results probably should be excluded from the resamples. For the results visualized in Figure @ref(fig:zip-bootstrap), the outliers are due only to extreme parameter estimates; all of the models converged.\nThe rsample package contains a set of functions named int_*() that compute different types of bootstrap intervals. Since the tidy() method contains standard error estimates, the bootstrap-t intervals can be computed. We’ll also compute the standard percentile intervals. By default, 90% confidence intervals are computed.\n\nbootstrap_models %&gt;% int_pctl(zero_coefs)\n## # A tibble: 6 × 6\n##   term        .lower .estimate  .upper .alpha .method   \n##   &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n## 1 (Intercept) -1.75    -0.621   0.423    0.05 percentile\n## 2 femWomen    -0.521    0.115   0.818    0.05 percentile\n## 3 kid5        -0.327    0.218   0.677    0.05 percentile\n## 4 marMarried  -1.20    -0.381   0.362    0.05 percentile\n## 5 ment        -0.401   -0.162  -0.0513   0.05 percentile\n## 6 phd         -0.276    0.0220  0.327    0.05 percentile\nbootstrap_models %&gt;% int_t(zero_coefs)\n## # A tibble: 6 × 6\n##   term        .lower .estimate  .upper .alpha .method  \n##   &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    \n## 1 (Intercept) -1.61    -0.621   0.321    0.05 student-t\n## 2 femWomen    -0.482    0.115   0.671    0.05 student-t\n## 3 kid5        -0.211    0.218   0.599    0.05 student-t\n## 4 marMarried  -0.988   -0.381   0.290    0.05 student-t\n## 5 ment        -0.324   -0.162  -0.0275   0.05 student-t\n## 6 phd         -0.274    0.0220  0.291    0.05 student-t\n\nFrom these results, we can get a good idea of which predictor(s) to include in the zero count probability model. It may be sensible to refit a smaller model to assess if the bootstrap distribution for ment is still skewed."
  },
  {
    "objectID": "21-inferential-analysis.html#inference-options",
    "href": "21-inferential-analysis.html#inference-options",
    "title": "21  Inferential Analysis",
    "section": "\n21.5 More Inferential Analysis",
    "text": "21.5 More Inferential Analysis\nThis chapter demonstrated just a small subset of what is available for inferential analysis in tidymodels and has focused on resampling and frequentist methods. Arguably, Bayesian analysis is a very effective and often superior approach for inference. A variety of Bayesian models are available via parsnip. Additionally, the multilevelmod package enables users to fit hierarchical Bayesian and non-Bayesian models (e.g., mixed models). The broom.mixed and tidybayes packages are excellent tools for extracting data for plots and summaries. Finally, for data sets with a single hierarchy, such as simple longitudinal or repeated measures data, rsample’s group_vfold_cv() function facilitates straightforward out-of-sample characterizations of model performance."
  },
  {
    "objectID": "21-inferential-analysis.html#inference-summary",
    "href": "21-inferential-analysis.html#inference-summary",
    "title": "21  Inferential Analysis",
    "section": "\n21.6 Chapter Summary",
    "text": "21.6 Chapter Summary\nThe tidymodels framework is for more than predictive modeling alone. Packages and functions from tidymodels can be used for hypothesis testing, as well as fitting and assessing inferential models. The tidymodels framework provides support for working with non-tidymodels R models, and can help assess the statistical qualities of your models.\n\n\n\n\nClaeskens, G. 2016. “Statistical Model Choice.” Annual Review of Statistics and Its Application 3: 233–56.\n\n\nDavison, A, and D Hinkley. 1997. Bootstrap Methods and Their Application. Vol. 1. Cambridge university press.\n\n\nDobson, A. 1999. An Introduction to Generalized Linear Models. Chapman; Hall: Boca Raton.\n\n\nIsmay, C, and A Kim. 2021. Statistical Inference via Data Science: A ModernDive into r and the Tidyverse. Chapman; Hall/CRC. https://moderndive.com/.\n\n\nLambert, D. 1992. “Zero-Inflated Poisson Regression, with an Application to Defects in Manufacturing.” Technometrics 34 (1): 1–14.\n\n\nLong, J. 1992. “Measures of Sex Differences in Scientific Productivity*.” Social Forces 71 (1): 159–78.\n\n\nMcCullagh, P, and J Nelder. 1989. Generalized Linear Models. London: Chapman; Hall.\n\n\nMullahy, J. 1986. “Specification and Testing of Some Modified Count Data Models.” Journal of Econometrics 33 (3): 341–65.\n\n\nWasserstein, R, and N Lazar. 2016. “The ASA Statement on p-Values: Context, Process, and Purpose.” The American Statistician 70 (2): 129–33.\n\n\nZeileis, A, C Kleiber, and S Jackman. 2008. “Regression Models for Count Data in R.” Journal of Statistical Software 27 (8): 1–25. https://www.jstatsoft.org/v027/i08."
  },
  {
    "objectID": "21-inferential-analysis.html#footnotes",
    "href": "21-inferential-analysis.html#footnotes",
    "title": "21  Inferential Analysis",
    "section": "",
    "text": "The T argument allows us to account for the time when the events (publications) were counted, which was three years for both men and women. There are more men than women in these data, but poisson.test() has limited functionality so more sophisticated analysis can be used to account for this difference.↩︎"
  },
  {
    "objectID": "pre-proc-table.html",
    "href": "pre-proc-table.html",
    "title": "Appendix A — Recommended Preprocessing",
    "section": "",
    "text": "The type of preprocessing needed depends on the type of model being fit. For example, models that use distance functions or dot products should have all of their predictors on the same scale so that distance is measured appropriately.\nTo learn more about each of these models, and others that might be available, see https://www.tidymodels.org/find/parsnip/.\nThis Appendix provides recommendations for baseline levels of preprocessing that are needed for various model functions. In Table @ref(tab:preprocessing), the preprocessing methods are categorized as:\n\ndummy: Do qualitative predictors require a numeric encoding (e.g., via dummy variables or other methods)?\nzv: Should columns with a single unique value be removed?\nimpute: If some predictors are missing, should they be estimated via imputation?\ndecorrelate: If there are correlated predictors, should this correlation be mitigated? This might mean filtering out predictors, using principal component analysis, or a model-based technique (e.g., regularization).\nnormalize: Should predictors be centered and scaled?\ntransform: Is it helpful to transform predictors to be more symmetric?\n\nThe information in Table @ref(tab:preprocessing) is not exhaustive and somewhat depends on the implementation. For example, as noted below the table, some models may not require a particular preprocessing operation but the implementation may require it. In the table, ✔ indicates that the method is required for the model and × indicates that it is not. The ◌ symbol means that the model may be helped by the technique but it is not required.\n\n\n\nPreprocessing methods for different models.\n\nmodel\ndummy\nzv\nimpute\ndecorrelate\nnormalize\ntransform\n\n\n\nC5_rules()\n×\n×\n×\n×\n×\n×\n\n\nbag_mars()\n✔\n×\n✔\n◌\n×\n◌\n\n\nbag_tree()\n×\n×\n×\n◌¹\n×\n×\n\n\nbart()\n×\n×\n×\n◌¹\n×\n×\n\n\nboost_tree()\n×²\n◌\n✔²\n◌¹\n×\n×\n\n\ncubist_rules()\n×\n×\n×\n×\n×\n×\n\n\ndecision_tree()\n×\n×\n×\n◌¹\n×\n×\n\n\ndiscrim_flexible()\n✔\n×\n✔\n✔\n×\n◌\n\n\ndiscrim_linear()\n✔\n✔\n✔\n✔\n×\n◌\n\n\ndiscrim_regularized()\n✔\n✔\n✔\n✔\n×\n◌\n\n\ngen_additive_mod()\n✔\n✔\n✔\n✔\n×\n◌\n\n\nlinear_reg()\n✔\n✔\n✔\n✔\n×\n◌\n\n\nlogistic_reg()\n✔\n✔\n✔\n✔\n×\n◌\n\n\nmars()\n✔\n×\n✔\n◌\n×\n◌\n\n\nmlp()\n✔\n✔\n✔\n✔\n✔\n✔\n\n\nmultinom_reg()\n✔\n✔\n✔\n✔\n×²\n◌\n\n\nnaive_Bayes()\n×\n✔\n✔\n◌¹\n×\n×\n\n\nnearest_neighbor()\n✔\n✔\n✔\n◌\n✔\n✔\n\n\npls()\n✔\n✔\n✔\n×\n✔\n✔\n\n\npoisson_reg()\n✔\n✔\n✔\n✔\n×\n◌\n\n\nrand_forest()\n×\n◌\n✔²\n◌¹\n×\n×\n\n\nrule_fit()\n✔\n×\n✔\n◌¹\n✔\n×\n\n\nsvm_*()\n✔\n✔\n✔\n✔\n✔\n✔\n\n\n\n\n\nFootnotes:\n\nDecorrelating predictors may not help improve performance. However, fewer correlated predictors can improve the estimation of variance importance scores (see Fig. 11.4 of Kuhn and Johnson (2020)). Essentially, the selection of highly correlated predictors is almost random.\nThe needed preprocessing for these models depends on the implementation. Specifically:\n\n\n\nTheoretically, any tree-based model does not require imputation. However, many tree ensemble implementations require imputation.\nWhile tree-based boosting methods generally do not require the creation of dummy variables, models using the xgboost engine do.\n\n\n\n\n\nKuhn, M, and K Johnson. 2020. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abrams, B. 2003. “The Pit of Success.” https://blogs.msdn.microsoft.com/brada/2003/10/02/the-pit-of-success/.\n\n\nBaggerly, K, and K Coombes. 2009. “Deriving Chemosensitivity from\nCell Lines: Forensic Bioinformatics and Reproducible\nResearch in High-Throughput Biology.” The Annals of Applied\nStatistics 3 (4): 1309–34.\n\n\nBartley, E AND Schliep, M . AND Hanks. 2019. “Identifying and\nCharacterizing Extrapolation in Multivariate Response Data.”\nPLOS ONE 14 (December): 1–20.\n\n\nBiecek, Przemyslaw, and Tomasz Burzykowski. 2021. Explanatory\nModel Analysis. Chapman; Hall/CRC, New York. https://ema.drwhy.ai/.\n\n\nBohachevsky, I, M Johnson, and M Stein. 1986. “Generalized\nSimulated Annealing for Function Optimization.”\nTechnometrics 28 (3): 209–17.\n\n\nBolstad, B. 2004. Low-Level Analysis of High-Density Oligonucleotide\nArray Data: Background, Normalization and Summarization. University\nof California, Berkeley.\n\n\nBox, GEP, W Hunter, and J Hunter. 2005. Statistics for\nExperimenters: An Introduction to Design, Data Analysis, and Model\nBuilding. Wiley.\n\n\nBradley, R, and M Terry. 1952. “Rank Analysis of Incomplete Block\nDesigns: I. The Method of Paired Comparisons.”\nBiometrika 39 (3/4): 324–45.\n\n\nBreiman, L. 1996a. “Bagging Predictors.” Machine\nLearning 24 (2): 123–40.\n\n\n———. 1996b. “Stacked Regressions.” Machine\nLearning 24 (1): 49–64.\n\n\n———. 2001a. “Random Forests.” Machine Learning 45\n(1): 5–32.\n\n\n———. 2001b. “Statistical Modeling: The Two Cultures.”\nStatistical Science 16 (3): 199–231.\n\n\nCarlson, B. 2012. “Putting Oncology Patients at Risk.”\nBiotechnology Healthcare 9 (3): 17–21.\n\n\nChambers, J. 1998. Programming with Data: A Guide to the\nS Language. Berlin, Heidelberg: Springer-Verlag.\n\n\nChambers, J, and T Hastie, eds. 1992. Statistical Models in\nS. Boca Raton, FL: CRC Press, Inc.\n\n\nClaeskens, G. 2016. “Statistical Model Choice.” Annual\nReview of Statistics and Its Application 3: 233–56.\n\n\nCleveland, W. 1979. “Robust Locally Weighted Regression and\nSmoothing Scatterplots.” Journal of the American Statistical\nAssociation 74 (368): 829–36.\n\n\nCraig–Schapiro, R, M Kuhn, C Xiong, E Pickering, J Liu, T Misko, R\nPerrin, et al. 2011. “Multiplexed Immunoassay Panel Identifies\nNovel CSF Biomarkers for Alzheimer’s Disease Diagnosis and\nPrognosis.” PLoS ONE 6 (4): e18850.\n\n\nCybenko, G. 1989. “Approximation by Superpositions of a Sigmoidal\nFunction.” Mathematics of Control, Signals and Systems 2\n(4): 303–14.\n\n\nDanowski, T, J Aarons, J Hydovitz, and J Wingert. 1970. “Utility\nof Equivocal Glucose Tolerances.” Diabetes 19 (7):\n524–26.\n\n\nDavison, A, and D Hinkley. 1997. Bootstrap Methods and Their\nApplication. Vol. 1. Cambridge university press.\n\n\nDe Cock, D. 2011. “Ames, Iowa: Alternative to the\nBoston Housing Data as an End of Semester Regression\nProject.” Journal of Statistics Education 19 (3).\n\n\nDobson, A. 1999. An Introduction to Generalized Linear Models.\nChapman; Hall: Boca Raton.\n\n\nDurrleman, S, and R Simon. 1989. “Flexible Regression Models with\nCubic Splines.” Statistics in Medicine 8 (5): 551–61.\n\n\nFaraway, J. 2016. Extending the Linear Model with R:\nGeneralized Linear, Mixed Effects and Nonparametric Regression\nModels. CRC press.\n\n\nFox, J. 2008. Applied Regression Analysis and Generalized Linear\nModels. Second. Thousand Oaks, CA: Sage.\n\n\nFrazier, R. 2018. “A Tutorial on Bayesian Optimization.” https://arxiv.org/abs/1807.02811.\n\n\nFreund, Y, and R Schapire. 1997. “A Decision-Theoretic\nGeneralization of on-Line Learning and an Application to\nBoosting.” Journal of Computer and System Sciences 55\n(1): 119–39.\n\n\nFriedman, J. 1991. “Multivariate Adaptive Regression\nSplines.” The Annals of Statistics 19 (1): 1–141.\n\n\n———. 2001. “Greedy Function Approximation: A Gradient Boosting\nMachine.” Annals of Statistics 29 (5): 1189–1232.\n\n\nFriedman, J, T Hastie, and R Tibshirani. 2010. “Regularization\nPaths for Generalized Linear Models via Coordinate Descent.”\nJournal of Statistical Software 33 (1): 1.\n\n\nGeladi, P., and B Kowalski. 1986. “Partial Least-Squares\nRegression: A Tutorial.” Analytica Chimica Acta 185:\n1–17.\n\n\nGentleman, R, V Carey, W Huber, R Irizarry, and S Dudoit. 2005.\nBioinformatics and Computational Biology Solutions Using\nR and Bioconductor. Berlin, Heidelberg:\nSpringer-Verlag.\n\n\nGood, I. J. 1985. “Weight of Evidence: A Brief Survey.”\nBayesian Statistics 2: 249–70.\n\n\nGoodfellow, I, Y Bengio, and A Courville. 2016. Deep Learning.\nMIT Press.\n\n\nGuo, Cheng, and Felix Berkhahn. 2016. “Entity Embeddings of\nCategorical Variables.” http://arxiv.org/abs/1604.06737.\n\n\nHand, D, and R Till. 2001. “A Simple Generalisation of the Area\nUnder the ROC Curve for Multiple Class Classification\nProblems.” Machine Learning 45 (August): 171–86.\n\n\nHill, A, P LaPan, Y Li, and S Haney. 2007. “Impact of Image\nSegmentation on High-Content Screening Data Quality for\nSK-BR-3 Cells.” BMC\nBioinformatics 8 (1): 340.\n\n\nHo, T. 1995. “Random Decision Forests.” In Proceedings\nof 3rd International Conference on Document Analysis and\nRecognition, 1:278–82. IEEE.\n\n\nHosmer, D, and Sy Lemeshow. 2000. Applied Logistic Regression.\nNew York: John Wiley; Sons.\n\n\nHvitfeldt, E., and J. Silge. 2021. Supervised Machine Learning for\nText Analysis in r. A Chapman & Hall Book. CRC Press. https://smltar.com/.\n\n\nHyndman, R, and G Athanasopoulos. 2018. Forecasting: Principles and\nPractice. OTexts.\n\n\nIsmay, C, and A Kim. 2021. Statistical Inference via Data Science: A\nModernDive into r and the Tidyverse. Chapman; Hall/CRC. https://moderndive.com/.\n\n\nJaworska, J, N Nikolova-Jeliazkova, and T Aldenberg. 2005. “QSAR\nApplicability Domain Estimation by Projection of the Training Set in\nDescriptor Space: A Review.” Alternatives to Laboratory\nAnimals 33 (5): 445–59.\n\n\nJohnson, D, P Eckart, N Alsamadisi, H Noble, C Martin, and R Spicer.\n2018. “Polar Auxin Transport Is Implicated in Vessel\nDifferentiation and Spatial Patterning During Secondary Growth in\nPopulus.” American Journal of Botany 105 (2): 186–96.\n\n\nJoseph, V, E Gul, and S Ba. 2015. “Maximum Projection Designs for\nComputer Experiments.” Biometrika 102 (2): 371–80.\n\n\nJungsu, K, D Basak, and D Holtzman. 2009. “The Role of\nApolipoprotein E in Alzheimer’s\nDisease.” Neuron 63 (3): 287–303.\n\n\nKerleguer, A., J.-L. Koeck, M. Fabre, P. Gérôme, R. Teyssou, and V.\nHervé. 2003. “Use of Equivocal Zone in Interpretation of Results\nof the Amplified Mycobacterium Tuberculosis Direct Test for\nDiagnosis of Tuberculosis.” Journal of Clinical\nMicrobiology 41 (4): 1783–84.\n\n\nKirkpatrick, S, D Gelatt, and M Vecchi. 1983. “Optimization by\nSimulated Annealing.” Science 220 (4598): 671–80.\n\n\nKoklu, M, and IA Ozkan. 2020. “Multiclass Classification of Dry\nBeans Using Computer Vision and Machine Learning Techniques.”\nComputers and Electronics in Agriculture 174: 105507.\n\n\nKrueger, T, D Panknin, and M Braun. 2015. “Fast Cross-Validation\nvia Sequential Testing.” Journal of Machine Learning\nResearch 16 (33): 1103–55.\n\n\nKruschke, J, and T Liddell. 2018. “The Bayesian New\nStatistics: Hypothesis Testing, Estimation, Meta-Analysis, and Power\nAnalysis from a Bayesian Perspective.”\nPsychonomic Bulletin and Review 25 (1): 178–206.\n\n\nKuhn, Max. 2014. “Futility Analysis in the Cross-Validation of\nMachine Learning Models.” https://arxiv.org/abs/1405.6974.\n\n\nKuhn, M, and K Johnson. 2013. Applied Predictive Modeling.\nSpringer.\n\n\n———. 2020. Feature Engineering and Selection: A Practical Approach\nfor Predictive Models. CRC Press.\n\n\nLambert, D. 1992. “Zero-Inflated Poisson Regression, with an\nApplication to Defects in Manufacturing.” Technometrics\n34 (1): 1–14.\n\n\nLittell, R, J Pendergast, and R Natarajan. 2000. “Modelling\nCovariance Structure in the Analysis of Repeated Measures Data.”\nStatistics in Medicine 19 (13): 1793–1819.\n\n\nLong, J. 1992. “Measures of Sex Differences\nin Scientific Productivity*.” Social Forces 71\n(1): 159–78.\n\n\nLundberg, Scott M., and Su-In Lee. 2017. “A Unified Approach to\nInterpreting Model Predictions.” In Proceedings of the 31st\nInternational Conference on Neural Information Processing Systems,\n4768–77. NIPS’17. Red Hook, NY, USA: Curran Associates Inc.\n\n\nMangiafico, S. 2015. “An R Companion for the Handbook\nof Biological Statistics.” https://rcompanion.org/handbook/.\n\n\nMaron, O, and A Moore. 1994. “Hoeffding Races: Accelerating Model\nSelection Search for Classification and Function Approximation.”\nIn Advances in Neural Information Processing Systems, 59–66.\n\n\nMcCullagh, P, and J Nelder. 1989. Generalized Linear Models.\nLondon: Chapman; Hall.\n\n\nMcDonald, J. 2009. Handbook of Biological Statistics. Sparky\nHouse Publishing.\n\n\nMcElreath, R. 2020. Statistical Rethinking: A Bayesian\nCourse with Examples in R and Stan. CRC\npress.\n\n\nMcInnes, L, J Healy, and J Melville. 2020. “UMAP: Uniform Manifold\nApproximation and Projection for Dimension Reduction.”\n\n\nMcKay, M, R Beckman, and W Conover. 1979. “A Comparison of Three\nMethods for Selecting Values of Input Variables in the Analysis of\nOutput from a Computer Code.” Technometrics 21 (2):\n239–45.\n\n\nMicci-Barreca, Daniele. 2001. “A Preprocessing Scheme for\nHigh-Cardinality Categorical Attributes in Classification and Prediction\nProblems.” SIGKDD Explor. Newsl. 3 (1): 27–32. https://doi.org/10.1145/507533.507538.\n\n\nMingqiang, Y, K Kidiyo, and R Joseph. 2008. “A Survey of Shape\nFeature Extraction Techniques.” In Pattern Recognition,\nedited by PY Yin. Rijeka: IntechOpen. https://doi.org/10.5772/6237.\n\n\nMolnar, Christopher. 2020. Interpretable Machine\nLearning. lulu.com. https://christophm.github.io/interpretable-ml-book/.\n\n\nMullahy, J. 1986. “Specification and Testing of Some Modified\nCount Data Models.” Journal of Econometrics 33 (3):\n341–65.\n\n\nNetzeva, T, A Worth, T Aldenberg, R Benigni, M Cronin, P Gramatica, J\nJaworska, et al. 2005. “Current Status of Methods for Defining the\nApplicability Domain of (Quantitative) Structure-Activity Relationships:\nThe Report and Recommendations of ECVAM Workshop 52.”\nAlternatives to Laboratory Animals 33 (2): 155–73.\n\n\nOlsson, D, and L Nelson. 1975. “The\nNelder-Mead Simplex Procedure for Function\nMinimization.” Technometrics 17 (1): 45–51.\n\n\nOpitz, J, and S Burst. 2019. “Macro F1 and Macro F1.” https://arxiv.org/abs/1911.03347.\n\n\nR Core Team. 2014. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttp://www.R-project.org/.\n\n\nRasmussen, C, and C Williams. 2006. Gaussian Processes for Machine\nLearning. Gaussian Processes for Machine Learning. MIT\nPress.\n\n\nSantner, T, B Williams, W Notz, and B Williams. 2003. The Design and\nAnalysis of Computer Experiments. Springer.\n\n\nSchmidberger, M, M Morgan, D Eddelbuettel, H Yu, L Tierney, and U\nMansmann. 2009. “State of the Art in Parallel Computing with\nR.” Journal of Statistical Software 31 (1):\n1–27. https://www.jstatsoft.org/v031/i01.\n\n\nSchulz, E, M Speekenbrink, and A Krause. 2018. “A Tutorial on\nGaussian Process Regression: Modelling, Exploring, and Exploiting\nFunctions.” Journal of Mathematical Psychology 85: 1–16.\n\n\nShahriari, B., K. Swersky, Z. Wang, R. P. Adams, and N. de Freitas.\n2016. “Taking the Human Out of the Loop: A Review of Bayesian\nOptimization.” Proceedings of the IEEE 104 (1): 148–75.\n\n\nShewry, M, and H Wynn. 1987. “Maximum Entropy Sampling.”\nJournal of Applied Statistics 14 (2): 165–70.\n\n\nShmueli, G. 2010. “To Explain or to Predict?”\nStatistical Science 25 (3): 289–310.\n\n\nSymons, S, and RG Fulcher. 1988. “Determination of Wheat Kernel\nMorphological Variation by Digital Image Analysis: I.\nVariation in Eastern Canadian Milling Quality\nWheats.” Journal of Cereal Science 8 (3): 211–18.\n\n\nThomas, R, and D Uminsky. 2020. “The Problem with Metrics Is a\nFundamental Problem for AI.” https://arxiv.org/abs/2002.08512.\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via\nthe Lasso.” Journal of the Royal Statistical Society. Series\nB (Methodological) 58 (1): 267–88. http://www.jstor.org/stable/2346178.\n\n\nVan Laarhoven, P, and E Aarts. 1987. “Simulated Annealing.”\nIn Simulated Annealing: Theory and Applications, 7–15.\nSpringer.\n\n\nWasserstein, R, and N Lazar. 2016. “The ASA Statement\non p-Values: Context, Process, and Purpose.” The American\nStatistician 70 (2): 129–33.\n\n\nWeinberger, K, A Dasgupta, J Langford, A Smola, and J Attenberg. 2009.\n“Feature Hashing for Large Scale Multitask Learning.” In\nProceedings of the 26th Annual International Conference on Machine\nLearning, 1113–20. ACM.\n\n\nWickham, H. 2019. Advanced r. 2nd ed. Chapman & Hall/CRC\nthe r Series. Taylor & Francis. https://doi.org/10.1201/9781351201315.\n\n\nWickham, H, M Averick, J Bryan, W Chang, L McGowan, R François, G\nGrolemund, et al. 2019. “Welcome to the\nTidyverse.” Journal of Open Source Software\n4 (43).\n\n\nWickham, H, and G Grolemund. 2016. R\nfor Data Science: Import, Tidy, Transform, Visualize, and\nModel Data. O’Reilly Media, Inc.\n\n\nWolpert, D. 1992. “Stacked Generalization.” Neural\nNetworks 5 (2): 241–59.\n\n\nWu, X, and Z Zhou. 2017. “A Unified View of Multi-Label\nPerformance Measures.” In International Conference on Machine\nLearning, 3780–88.\n\n\nWundervald, B, A Parnell, and K Domijan. 2020. “Generalizing Gain\nPenalization for Feature Selection in Tree-Based Models.” https://arxiv.org/abs/2006.07515.\n\n\nXu, Q, and Y Liang. 2001. “Monte Carlo Cross\nValidation.” Chemometrics and Intelligent Laboratory\nSystems 56 (1): 1–11.\n\n\nYeo, I-K, and R Johnson. 2000. “A New Family of Power\nTransformations to Improve Normality or Symmetry.”\nBiometrika 87 (4): 954–59.\n\n\nZeileis, A, C Kleiber, and S Jackman. 2008. “Regression Models for\nCount Data in R.” Journal of Statistical\nSoftware 27 (8): 1–25. https://www.jstatsoft.org/v027/i08.\n\n\nZumel, Nina, and John Mount. 2019. “Vtreat: A Data.frame Processor\nfor Predictive Modeling.” http://arxiv.org/abs/1611.09477."
  },
  {
    "objectID": "01-software-modeling.html#fundamentals-for-modeling-software",
    "href": "01-software-modeling.html#fundamentals-for-modeling-software",
    "title": "1  Software for modeling",
    "section": "\n1.1 Fundamentals for Modeling Software",
    "text": "1.1 Fundamentals for Modeling Software\nIt is important that the modeling software you use is easy to operate properly. The user interface should not be so poorly designed that the user would not know that they used it inappropriately. For example, Baggerly and Coombes (2009) report myriad problems in the data analyses from a high profile computational biology publication. One of the issues was related to how the users were required to add the names of the model inputs. The software user interface made it easy to offset the column names of the data from the actual data columns. This resulted in the wrong genes being identified as important for treating cancer patients and eventually contributed to the termination of several clinical trials (Carlson 2012).\nIf we need high quality models, software must facilitate proper usage. Abrams (2003) describes an interesting principle to guide us:\n\nThe Pit of Success: in stark contrast to a summit, a peak, or a journey across a desert to find victory through many trials and surprises, we want our customers to simply fall into winning practices by using our platform and frameworks.\n\nData analysis and modeling software should espouse this idea.\nSecond, modeling software should promote good scientific methodology. When working with complex predictive models, it can be easy to unknowingly commit errors related to logical fallacies or inappropriate assumptions. Many machine learning models are so adept at discovering patterns that they can effortlessly find empirical patterns in the data that fail to reproduce later. Some of methodological errors are insidious in that the issue can go undetected until a later time when new data that contain the true result are obtained.\n\nAs our models have become more powerful and complex, it has also become easier to commit latent errors.\n\nThis same principle also applies to programming. Whenever possible, the software should be able to protect users from committing mistakes. Software should make it easy for users to do the right thing.\nThese two aspects of model development – ease of proper use and good methodological practice – are crucial. Since tools for creating models are easily accessible and models can have such a profound impact, many more people are creating them. In terms of technical expertise and training, creators’ backgrounds will vary. It is important that their tools be robust to the user’s experience. Tools should be powerful enough to create high-performance models, but, on the other hand, should be easy to use appropriately. This book describes a suite of software for modeling that has been designed with these characteristics in mind.\nThe software is based on the R programming language (R Core Team 2014). R has been designed especially for data analysis and modeling. It is an implementation of the S language (with lexical scoping rules adapted from Scheme and Lisp) which was created in the 1970s to\n\n“turn ideas into software, quickly and faithfully” (Chambers 1998)\n\nR is open source and free. It is a powerful programming language that can be used for many different purposes but specializes in data analysis, modeling, visualization, and machine learning. R is easily extensible; it has a vast ecosystem of packages, mostly user-contributed modules that focus on a specific theme, such as modeling, visualization, and so on.\nOne collection of packages is called the tidyverse (Wickham et al. 2019). The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. Several of these design philosophies are directly informed by the aspects of software for modeling described in this chapter. If you’ve never used the tidyverse packages, Chapter -Chapter 2 contains a review of basic concepts. Within the tidyverse, the subset of packages specifically focused on modeling are referred to as the tidymodels packages. This book is a practical guide for conducting modeling using the tidyverse and tidymodels packages. It shows how to use a set of packages, each with its own specific purpose, together to create high-quality models."
  },
  {
    "objectID": "01-software-modeling.html#model-types",
    "href": "01-software-modeling.html#model-types",
    "title": "1  Software for modeling",
    "section": "\n1.2 Types of Models",
    "text": "1.2 Types of Models\nBefore proceeding, let’s describe a taxonomy for types of models, grouped by purpose. This taxonomy informs both how a model is used and many aspects of how the model may be created or evaluated. While this list is not exhaustive, most models fall into at least one of these categories:\nDescriptive models\nThe purpose of a descriptive model is to describe or illustrate characteristics of some data. The analysis might have no other purpose than to visually emphasize some trend or artifact in the data.\nFor example, large scale measurements of RNA have been possible for some time using microarrays. Early laboratory methods placed a biological sample on a small microchip. Very small locations on the chip can measure a signal based on the abundance of a specific RNA sequence. The chip would contain thousands (or more) outcomes, each a quantification of the RNA related to a biological process. However, there could be quality issues on the chip that might lead to poor results. For example, a fingerprint accidentally left on a portion of the chip could cause inaccurate measurements when scanned.\nAn early method for evaluating such issues were probe-level models, or PLMs (Bolstad 2004). A statistical model would be created that accounted for the known differences in the data, such as the chip, the RNA sequence, the type of sequence, and so on. If there were other, unknown factors in the data, these effects would be captured in the model residuals. When the residuals were plotted by their location on the chip, a good quality chip would show no patterns. When a problem did occur, some sort of spatial pattern would be discernible. Often the type of pattern would suggest the underlying issue (e.g., a fingerprint) and a possible solution (wipe off the chip and rescan, repeat the sample, etc.). Figure 1.1 (a) shows an application of this method for two microarrays taken from Gentleman et al. (2005). The images show two different color values; areas that are darker are where the signal intensity was larger than the model expects while the lighter color shows lower than expected values. The left-hand panel demonstrates a fairly random pattern while the right-hand panel exhibits an undesirable artifact in the middle of the chip.\n\n\n\n\nFigure 1.1: Two examples of how descriptive models can be used to illustrate specific patterns\n\n\n\nAnother example of a descriptive model is the locally estimated scatterplot smoothing model, more commonly known as LOESS (Cleveland 1979). Here, a smooth and flexible regression model is fit to a data set, usually with a single independent variable, and the fitted regression line is used to elucidate some trend in the data. These types of smoothers are used to discover potential ways to represent a variable in a model. This is demonstrated in Figure 1.1 (b) where a nonlinear trend is illuminated by the flexible smoother. From this plot, it is clear that there is a highly nonlinear relationship between the sale price of a house and its latitude.\nInferential models\nThe goal of an inferential model is to produce a decision for a research question or to explore a specific hypothesis, similar to how statistical tests are used.1 An inferential model starts with a predefined conjecture or idea about a population and produces a statistical conclusion such as an interval estimate or the rejection of a hypothesis.\nFor example, the goal of a clinical trial might be to provide confirmation that a new therapy does a better job in prolonging life than an alternative, such as an existing therapy or no treatment at all. If the clinical endpoint related to survival of a patient, the null hypothesis might be that the new treatment has an equal or lower median survival time, with the alternative hypothesis being that the new therapy has higher median survival. If this trial were evaluated using traditional null hypothesis significance testing via modeling, the significance testing would produce a p-value using some pre-defined methodology based on a set of assumptions for the data. Small values for the p-value in the model results would indicate there is evidence that the new therapy helps patients live longer. Large values for the p-value in the model results would conclude there is a failure to show such a difference; this lack of evidence could be due to a number of reasons, including the therapy not working.\nWhat are the important aspects of this type of analysis? Inferential modeling techniques typically produce some type of probabilistic output, such as a p-value, confidence interval, or posterior probability. Generally, to compute such a quantity, formal probabilistic assumptions must be made about the data and the underlying processes that generated the data. The quality of the statistical modeling results are highly dependent on these pre-defined assumptions as well as how much the observed data appear to agree with them. The most critical factors here are theoretical: “If my data were independent and the residuals follow distribution X, then test statistic Y can be used to produce a p-value. Otherwise, the resulting p-value might be inaccurate.”\n\nOne aspect of inferential analyses is that there tends to be a delayed feedback loop in understanding how well the data match the model assumptions. In our clinical trial example, if statistical (and clinical) significance indicate that the new therapy should be available for patients to use, it still may be years before it is used in the field and enough data are generated for an independent assessment of whether the original statistical analysis led to the appropriate decision.\n\nPredictive models\nSometimes data are modeled to produce the most accurate prediction possible for new data. Here, the primary goal is that the predicted values have the highest possible fidelity to the true value of the new data.\nA simple example would be for a book buyer to predict how many copies of a particular book should be shipped to their store for the next month. An over-prediction wastes space and money due to excess books. If the prediction is smaller than it should be, there is opportunity loss and less profit.\nFor this type of model, the problem type is one of estimation rather than inference. For example, the buyer is usually not concerned with a question such as “Will I sell more than 100 copies of book X next month?” but rather “How many copies of book X will customers purchase next month?” Also, depending on the context, there may not be any interest in why the predicted value is X. In other words, there is more interest in the value itself than in evaluating a formal hypothesis related to the data. The prediction can also include measures of uncertainty. In the case of the book buyer, providing a forecasting error may be helpful in deciding how many books to purchase. It can also serve as a metric to gauge how well the prediction method worked.\nWhat are the most important factors affecting predictive models? There are many different ways that a predictive model can be created, so the important factors depend on how the model was developed.2\nA mechanistic model could be derived using first principles to produce a model equation that depends on assumptions. For example, when predicting the amount of a drug that is in a person’s body at a certain time, some formal assumptions are made on how the drug is administered, absorbed, metabolized, and eliminated. Based on this, a set of differential equations can be used to derive a specific model equation. Data are used to estimate the unknown parameters of this equation so that predictions can be generated. Like inferential models, mechanistic predictive models greatly depend on the assumptions that define their model equations. However, unlike inferential models, it is easy to make data-driven statements about how well the model performs based on how well it predicts the existing data. Here the feedback loop for the modeling practitioner is much faster than it would be for a hypothesis test.\nEmpirically driven models are created with more vague assumptions. These models tend to fall into the machine learning category. A good example is the K-nearest neighbor (KNN) model. Given a set of reference data, a new sample is predicted by using the values of the K most similar data in the reference set. For example, if a book buyer needs a prediction for a new book, historical data from existing books may be available. A 5-nearest neighbor model would estimate the number of the new books to purchase based on the sales numbers of the five books that are most similar to the new one (for some definition of “similar”). This model is defined only by the structure of the prediction (the average of five similar books). No theoretical or probabilistic assumptions are made about the sales numbers or the variables that are used to define similarity. In fact, the primary method of evaluating the appropriateness of the model is to assess its accuracy using existing data. If the structure of this type of model was a good choice, the predictions would be close to the actual values."
  },
  {
    "objectID": "01-software-modeling.html#connections-between-types-of-models",
    "href": "01-software-modeling.html#connections-between-types-of-models",
    "title": "1  Software for modeling",
    "section": "\n1.3 Connections Between Types of Models",
    "text": "1.3 Connections Between Types of Models\n\nNote that we have defined the type of a model by how it is used, rather than its mathematical qualities.\n\nAn ordinary linear regression model might fall into any of these three classes of model, depending on how it is used:\n\nA descriptive smoother, similar to LOESS, called restricted smoothing splines (Durrleman and Simon 1989) can be used to describe trends in data using ordinary linear regression with specialized terms.\nAn analysis of variance (ANOVA) model is a popular method for producing the p-values used for inference. ANOVA models are a special case of linear regression.\nIf a simple linear regression model produces accurate predictions, it can be used as a predictive model.\n\nThere are many examples of predictive models that cannot (or at least should not) be used for inference. Even if probabilistic assumptions were made for the data, the nature of the K-nearest neighbors model, for example, makes the math required for inference intractable.\nThere is an additional connection between the types of models. While the primary purpose of descriptive and inferential models might not be related to prediction, the predictive capacity of the model should not be ignored. For example, logistic regression is a popular model for data in which the outcome is qualitative with two possible values. It can model how variables are related to the probability of the outcomes. When used inferentially, an abundance of attention is paid to the statistical qualities of the model. For example, analysts tend to strongly focus on the selection of independent variables contained in the model. Many iterations of model building may be used to determine a minimal subset of independent variables that have a “statistically significant” relationship to the outcome variable. This is usually achieved when all of the p-values for the independent variables are below a certain value (e.g., 0.05). From here, the analyst may focus on making qualitative statements about the relative influence that the variables have on the outcome (e.g., “There is a statistically significant relationship between age and the odds of heart disease.”).\nHowever, this approach can be dangerous when statistical significance is used as the only measure of model quality. It is possible that this statistically optimized model has poor model accuracy, or it performs poorly on some other measure of predictive capacity. While the model might not be used for prediction, how much should inferences be trusted from a model that has significant p-values but dismal accuracy? Predictive performance tends to be related to how close the model’s fitted values are to the observed data.\n\nIf a model has limited fidelity to the data, the inferences generated by the model should be highly suspect. In other words, statistical significance may not be sufficient proof that a model is appropriate.\n\nThis may seem intuitively obvious, but it is often ignored in real-world data analysis."
  },
  {
    "objectID": "01-software-modeling.html#model-terminology",
    "href": "01-software-modeling.html#model-terminology",
    "title": "1  Software for modeling",
    "section": "\n1.4 Some Terminology",
    "text": "1.4 Some Terminology\nBefore proceeding, we will outline additional terminology related to modeling and data. These descriptions are intended to be helpful as you read this book, but they are not exhaustive.\nFirst, many models can be categorized as being supervised or unsupervised. Unsupervised models are those that learn patterns, clusters, or other characteristics of the data but lack an outcome, i.e., a dependent variable. Principal component analysis (PCA), clustering, and autoencoders are examples of unsupervised models; they are used to understand relationships between variables or sets of variables without an explicit relationship between predictors and an outcome. Supervised models are those that have an outcome variable. Linear regression, neural networks, and numerous other methodologies fall into this category.\nWithin supervised models, there are two main sub-categories:\n\nRegression predicts a numeric outcome.\nClassification predicts an outcome that is an ordered or unordered set of qualitative values.\n\nThese are imperfect definitions and do not account for all possible model types. In ?sec-models, we refer to this characteristic of supervised techniques as the model mode.\nDifferent variables can have different roles, especially in a supervised modeling analysis. Outcomes (otherwise known as the labels, endpoints, or dependent variables) are the value being predicted in supervised models. The independent variables, which are the substrate for making predictions of the outcome, are also referred to as predictors, features, or covariates (depending on the context). The terms outcomes and predictors are used most frequently in this book.\nIn terms of the data or variables themselves, whether used for supervised or unsupervised models, as predictors or outcomes, the two main categories are quantitative and qualitative. Examples of the former are real numbers like 3.14159 and integers like 42. Qualitative values, also known as nominal data, are those that represent some sort of discrete state that cannot be naturally placed on a numeric scale, like “red”, “green”, and “blue”."
  },
  {
    "objectID": "01-software-modeling.html#model-phases",
    "href": "01-software-modeling.html#model-phases",
    "title": "1  Software for modeling",
    "section": "\n1.5 How Does Modeling Fit into the Data Analysis Process?",
    "text": "1.5 How Does Modeling Fit into the Data Analysis Process?\nIn what circumstances are models created? Are there steps that precede such an undertaking? Is model creation the first step in data analysis?\n\nThere are a few critical phases of data analysis that always come before modeling.\n\nFirst, there is the chronically underestimated process of cleaning the data. No matter the circumstances, you should investigate the data to make sure that they are applicable to your project goals, accurate, and appropriate. These steps can easily take more time than the rest of the data analysis process (depending on the circumstances).\nData cleaning can also overlap with the second phase of understanding the data, often referred to as exploratory data analysis (EDA). EDA brings to light how the different variables are related to one another, their distributions, typical ranges, and other attributes. A good question to ask at this phase is, “How did I come by these data?” This question can help you understand how the data at hand have been sampled or filtered and if these operations were appropriate. For example, when merging database tables, a join may go awry that could accidentally eliminate one or more subpopulations. Another good idea is to ask if the data are relevant. For example, to predict whether patients have Alzheimer’s disease, it would be unwise to have a data set containing subjects with the disease and a random sample of healthy adults from the general population. Given the progressive nature of the disease, the model may simply predict who are the oldest patients.\nFinally, before starting a data analysis process, there should be clear expectations of the model’s goal and how performance (and success) will be judged. At least one performance metric should be identified with realistic goals of what can be achieved. Common statistical metrics, discussed in more detail in ?sec-performance, are classification accuracy, true and false positive rates, root mean squared error, and so on. The relative benefits and drawbacks of these metrics should be weighed. It is also important that the metric be germane; alignment with the broader data analysis goals is critical.\nThe process of investigating the data may not be simple. Wickham and Grolemund (2016) contains an excellent illustration of the general data analysis process, reproduced in Figure Figure 1.2. Data ingestion and cleaning/tidying are shown as the initial steps. When the analytical steps for understanding commence, they are a heuristic process; we cannot pre-determine how long they may take. The cycle of transformation, modeling, and visualization often requires multiple iterations.\n\n\n\n\nFigure 1.2: The data science process (from R for Data Science, used with permission)\n\n\n\nThis iterative process is especially true for modeling. Figure Figure 1.3 emulates the typical path to determining an appropriate model. The general phases are:\n\nExploratory data analysis (EDA): Initially there is a back and forth between numerical analysis and data visualization (represented in Figure 1.2) where different discoveries lead to more questions and data analysis side-quests to gain more understanding.\nFeature engineering: The understanding gained from EDA results in the creation of specific model terms that make it easier to accurately model the observed data. This can include complex methodologies (e.g., PCA) or simpler features (using the ratio of two predictors). ?sec-recipes focuses entirely on this important step.\nModel tuning and selection (large circles with alternating segments): A variety of models are generated and their performance is compared. Some models require parameter tuning in which some structural parameters must be specified or optimized. The alternating segments within the circles signify the repeated data splitting used during resampling (see ?sec-resampling).\nModel evaluation: During this phase of model development, we assess the model’s performance metrics, examine residual plots, and conduct other EDA-like analyses to understand how well the models work. In some cases, formal between-model comparisons (?sec-compare) help you understand whether any differences in models are within the experimental noise.\n\n\n\n\n\nFigure 1.3: A schematic for the typical modeling process\n\n\n\nAfter an initial sequence of these tasks, more understanding is gained regarding which models are superior as well as which data subpopulations are not being effectively estimated. This leads to additional EDA and feature engineering, another round of modeling, and so on. Once the data analysis goals are achieved, typically the last steps are to finalize, document, and communicate the model. For predictive models, it is common at the end to validate the model on an additional set of data reserved for this specific purpose.\nAs an example, Kuhn and Johnson (2020) use data to model the daily ridership of Chicago’s public train system using predictors such as the date, the previous ridership results, the weather, and other factors. Table 1.1 shows an approximation of these authors’ hypothetical inner monologue when analyzing these data and eventually selecting a model with sufficient performance.\n\n\nTable 1.1: ?(caption)\n\n\n\n\n(a) Hypothetical inner monologue of a model developer.\n\nThoughts\nActivity\n\n\n\nThe daily ridership values between stations are extremely correlated.\nEDA\n\n\nWeekday and weekend ridership look very different.\nEDA\n\n\nOne day in the summer of 2010 has an abnormally large number of riders.\nEDA\n\n\nWhich stations had the lowest daily ridership values?\nEDA\n\n\nDates should at least be encoded as day-of-the-week, and year.\nFeature Engineering\n\n\nMaybe PCA could be used on the correlated predictors to make it easier for the models to use them.\nFeature Engineering\n\n\nHourly weather records should probably be summarized into daily measurements.\nFeature Engineering\n\n\nLet’s start with simple linear regression, K-nearest neighbors, and a boosted decision tree.\nModel Fitting\n\n\nHow many neighbors should be used?\nModel Tuning\n\n\nShould we run a lot of boosting iterations or just a few?\nModel Tuning\n\n\nHow many neighbors seemed to be optimal for these data?\nModel Tuning\n\n\nWhich models have the lowest root mean squared errors?\nModel Evaluation\n\n\nWhich days were poorly predicted?\nEDA\n\n\nVariable importance scores indicate that the weather information is not predictive. We’ll drop them from the next set of models.\nModel Evaluation\n\n\nIt seems like we should focus on a lot of boosting iterations for that model.\nModel Evaluation\n\n\nWe need to encode holiday features to improve predictions on (and around) those dates.\nFeature Engineering\n\n\nLet’s drop KNN from the model list.\nModel Evaluation"
  },
  {
    "objectID": "01-software-modeling.html#software-summary",
    "href": "01-software-modeling.html#software-summary",
    "title": "1  Software for modeling",
    "section": "\n1.6 Chapter Summary",
    "text": "1.6 Chapter Summary\nThis chapter focused on how models describe relationships in data, and different types of models such as descriptive models, inferential models, and predictive models. The predictive capacity of a model can be used to evaluate it, even when its main goal is not prediction. Modeling itself sits within the broader data analysis process, and exploratory data analysis is a key part of building high-quality models.\n\n\n\n\nAbrams, B. 2003. “The Pit of Success.” https://blogs.msdn.microsoft.com/brada/2003/10/02/the-pit-of-success/.\n\n\nBaggerly, K, and K Coombes. 2009. “Deriving Chemosensitivity from Cell Lines: Forensic Bioinformatics and Reproducible Research in High-Throughput Biology.” The Annals of Applied Statistics 3 (4): 1309–34.\n\n\nBolstad, B. 2004. Low-Level Analysis of High-Density Oligonucleotide Array Data: Background, Normalization and Summarization. University of California, Berkeley.\n\n\nBreiman, L. 2001. “Statistical Modeling: The Two Cultures.” Statistical Science 16 (3): 199–231.\n\n\nCarlson, B. 2012. “Putting Oncology Patients at Risk.” Biotechnology Healthcare 9 (3): 17–21.\n\n\nChambers, J. 1998. Programming with Data: A Guide to the S Language. Berlin, Heidelberg: Springer-Verlag.\n\n\nCleveland, W. 1979. “Robust Locally Weighted Regression and Smoothing Scatterplots.” Journal of the American Statistical Association 74 (368): 829–36.\n\n\nDurrleman, S, and R Simon. 1989. “Flexible Regression Models with Cubic Splines.” Statistics in Medicine 8 (5): 551–61.\n\n\nGentleman, R, V Carey, W Huber, R Irizarry, and S Dudoit. 2005. Bioinformatics and Computational Biology Solutions Using R and Bioconductor. Berlin, Heidelberg: Springer-Verlag.\n\n\nKuhn, M, and K Johnson. 2020. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.\n\n\nR Core Team. 2014. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. http://www.R-project.org/.\n\n\nShmueli, G. 2010. “To Explain or to Predict?” Statistical Science 25 (3): 289–310.\n\n\nWickham, H, M Averick, J Bryan, W Chang, L McGowan, R François, G Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43).\n\n\nWickham, H, and G Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media, Inc."
  },
  {
    "objectID": "01-software-modeling.html#footnotes",
    "href": "01-software-modeling.html#footnotes",
    "title": "1  Software for modeling",
    "section": "",
    "text": "Many specific statistical tests are in fact equivalent to models. For example, t-tests and analysis of variance (ANOVA) methods are particular cases of the generalized linear model.↩︎\nBroader discussions of these distinctions can be found in Breiman (2001) and Shmueli (2010).↩︎"
  },
  {
    "objectID": "01-software-modeling.html#sec-software-summary",
    "href": "01-software-modeling.html#sec-software-summary",
    "title": "1  Software for modeling",
    "section": "\n1.6 Chapter Summary",
    "text": "1.6 Chapter Summary\nThis chapter focused on how models describe relationships in data, and different types of models such as descriptive models, inferential models, and predictive models. The predictive capacity of a model can be used to evaluate it, even when its main goal is not prediction. Modeling itself sits within the broader data analysis process, and exploratory data analysis is a key part of building high-quality models.\n\n\n\n\nAbrams, B. 2003. “The Pit of Success.” https://blogs.msdn.microsoft.com/brada/2003/10/02/the-pit-of-success/.\n\n\nBaggerly, K, and K Coombes. 2009. “Deriving Chemosensitivity from Cell Lines: Forensic Bioinformatics and Reproducible Research in High-Throughput Biology.” The Annals of Applied Statistics 3 (4): 1309–34.\n\n\nBolstad, B. 2004. Low-Level Analysis of High-Density Oligonucleotide Array Data: Background, Normalization and Summarization. University of California, Berkeley.\n\n\nBreiman, L. 2001. “Statistical Modeling: The Two Cultures.” Statistical Science 16 (3): 199–231.\n\n\nCarlson, B. 2012. “Putting Oncology Patients at Risk.” Biotechnology Healthcare 9 (3): 17–21.\n\n\nChambers, J. 1998. Programming with Data: A Guide to the S Language. Berlin, Heidelberg: Springer-Verlag.\n\n\nCleveland, W. 1979. “Robust Locally Weighted Regression and Smoothing Scatterplots.” Journal of the American Statistical Association 74 (368): 829–36.\n\n\nDurrleman, S, and R Simon. 1989. “Flexible Regression Models with Cubic Splines.” Statistics in Medicine 8 (5): 551–61.\n\n\nGentleman, R, V Carey, W Huber, R Irizarry, and S Dudoit. 2005. Bioinformatics and Computational Biology Solutions Using R and Bioconductor. Berlin, Heidelberg: Springer-Verlag.\n\n\nKuhn, M, and K Johnson. 2020. Feature Engineering and Selection: A Practical Approach for Predictive Models. CRC Press.\n\n\nR Core Team. 2014. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. http://www.R-project.org/.\n\n\nShmueli, G. 2010. “To Explain or to Predict?” Statistical Science 25 (3): 289–310.\n\n\nWickham, H, M Averick, J Bryan, W Chang, L McGowan, R François, G Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43).\n\n\nWickham, H, and G Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media, Inc."
  },
  {
    "objectID": "01-software-modeling.html#sec-model-types",
    "href": "01-software-modeling.html#sec-model-types",
    "title": "1  Software for modeling",
    "section": "\n1.2 Types of Models",
    "text": "1.2 Types of Models\nBefore proceeding, let’s describe a taxonomy for types of models, grouped by purpose. This taxonomy informs both how a model is used and many aspects of how the model may be created or evaluated. While this list is not exhaustive, most models fall into at least one of these categories:\nDescriptive models\nThe purpose of a descriptive model is to describe or illustrate characteristics of some data. The analysis might have no other purpose than to visually emphasize some trend or artifact in the data.\nFor example, large scale measurements of RNA have been possible for some time using microarrays. Early laboratory methods placed a biological sample on a small microchip. Very small locations on the chip can measure a signal based on the abundance of a specific RNA sequence. The chip would contain thousands (or more) outcomes, each a quantification of the RNA related to a biological process. However, there could be quality issues on the chip that might lead to poor results. For example, a fingerprint accidentally left on a portion of the chip could cause inaccurate measurements when scanned.\nAn early method for evaluating such issues were probe-level models, or PLMs (Bolstad 2004). A statistical model would be created that accounted for the known differences in the data, such as the chip, the RNA sequence, the type of sequence, and so on. If there were other, unknown factors in the data, these effects would be captured in the model residuals. When the residuals were plotted by their location on the chip, a good quality chip would show no patterns. When a problem did occur, some sort of spatial pattern would be discernible. Often the type of pattern would suggest the underlying issue (e.g., a fingerprint) and a possible solution (wipe off the chip and rescan, repeat the sample, etc.). Figure 1.1 (a) shows an application of this method for two microarrays taken from Gentleman et al. (2005). The images show two different color values; areas that are darker are where the signal intensity was larger than the model expects while the lighter color shows lower than expected values. The left-hand panel demonstrates a fairly random pattern while the right-hand panel exhibits an undesirable artifact in the middle of the chip.\n\n\n\n\nFigure 1.1: Two examples of how descriptive models can be used to illustrate specific patterns\n\n\n\nAnother example of a descriptive model is the locally estimated scatterplot smoothing model, more commonly known as LOESS (Cleveland 1979). Here, a smooth and flexible regression model is fit to a data set, usually with a single independent variable, and the fitted regression line is used to elucidate some trend in the data. These types of smoothers are used to discover potential ways to represent a variable in a model. This is demonstrated in Figure 1.1 (b) where a nonlinear trend is illuminated by the flexible smoother. From this plot, it is clear that there is a highly nonlinear relationship between the sale price of a house and its latitude.\nInferential models\nThe goal of an inferential model is to produce a decision for a research question or to explore a specific hypothesis, similar to how statistical tests are used.1 An inferential model starts with a predefined conjecture or idea about a population and produces a statistical conclusion such as an interval estimate or the rejection of a hypothesis.\nFor example, the goal of a clinical trial might be to provide confirmation that a new therapy does a better job in prolonging life than an alternative, such as an existing therapy or no treatment at all. If the clinical endpoint related to survival of a patient, the null hypothesis might be that the new treatment has an equal or lower median survival time, with the alternative hypothesis being that the new therapy has higher median survival. If this trial were evaluated using traditional null hypothesis significance testing via modeling, the significance testing would produce a p-value using some pre-defined methodology based on a set of assumptions for the data. Small values for the p-value in the model results would indicate there is evidence that the new therapy helps patients live longer. Large values for the p-value in the model results would conclude there is a failure to show such a difference; this lack of evidence could be due to a number of reasons, including the therapy not working.\nWhat are the important aspects of this type of analysis? Inferential modeling techniques typically produce some type of probabilistic output, such as a p-value, confidence interval, or posterior probability. Generally, to compute such a quantity, formal probabilistic assumptions must be made about the data and the underlying processes that generated the data. The quality of the statistical modeling results are highly dependent on these pre-defined assumptions as well as how much the observed data appear to agree with them. The most critical factors here are theoretical: “If my data were independent and the residuals follow distribution X, then test statistic Y can be used to produce a p-value. Otherwise, the resulting p-value might be inaccurate.”\n\nOne aspect of inferential analyses is that there tends to be a delayed feedback loop in understanding how well the data match the model assumptions. In our clinical trial example, if statistical (and clinical) significance indicate that the new therapy should be available for patients to use, it still may be years before it is used in the field and enough data are generated for an independent assessment of whether the original statistical analysis led to the appropriate decision.\n\nPredictive models\nSometimes data are modeled to produce the most accurate prediction possible for new data. Here, the primary goal is that the predicted values have the highest possible fidelity to the true value of the new data.\nA simple example would be for a book buyer to predict how many copies of a particular book should be shipped to their store for the next month. An over-prediction wastes space and money due to excess books. If the prediction is smaller than it should be, there is opportunity loss and less profit.\nFor this type of model, the problem type is one of estimation rather than inference. For example, the buyer is usually not concerned with a question such as “Will I sell more than 100 copies of book X next month?” but rather “How many copies of book X will customers purchase next month?” Also, depending on the context, there may not be any interest in why the predicted value is X. In other words, there is more interest in the value itself than in evaluating a formal hypothesis related to the data. The prediction can also include measures of uncertainty. In the case of the book buyer, providing a forecasting error may be helpful in deciding how many books to purchase. It can also serve as a metric to gauge how well the prediction method worked.\nWhat are the most important factors affecting predictive models? There are many different ways that a predictive model can be created, so the important factors depend on how the model was developed.2\nA mechanistic model could be derived using first principles to produce a model equation that depends on assumptions. For example, when predicting the amount of a drug that is in a person’s body at a certain time, some formal assumptions are made on how the drug is administered, absorbed, metabolized, and eliminated. Based on this, a set of differential equations can be used to derive a specific model equation. Data are used to estimate the unknown parameters of this equation so that predictions can be generated. Like inferential models, mechanistic predictive models greatly depend on the assumptions that define their model equations. However, unlike inferential models, it is easy to make data-driven statements about how well the model performs based on how well it predicts the existing data. Here the feedback loop for the modeling practitioner is much faster than it would be for a hypothesis test.\nEmpirically driven models are created with more vague assumptions. These models tend to fall into the machine learning category. A good example is the K-nearest neighbor (KNN) model. Given a set of reference data, a new sample is predicted by using the values of the K most similar data in the reference set. For example, if a book buyer needs a prediction for a new book, historical data from existing books may be available. A 5-nearest neighbor model would estimate the number of the new books to purchase based on the sales numbers of the five books that are most similar to the new one (for some definition of “similar”). This model is defined only by the structure of the prediction (the average of five similar books). No theoretical or probabilistic assumptions are made about the sales numbers or the variables that are used to define similarity. In fact, the primary method of evaluating the appropriateness of the model is to assess its accuracy using existing data. If the structure of this type of model was a good choice, the predictions would be close to the actual values."
  },
  {
    "objectID": "01-software-modeling.html#sec-model-terminology",
    "href": "01-software-modeling.html#sec-model-terminology",
    "title": "1  Software for modeling",
    "section": "\n1.4 Some Terminology",
    "text": "1.4 Some Terminology\nBefore proceeding, we will outline additional terminology related to modeling and data. These descriptions are intended to be helpful as you read this book, but they are not exhaustive.\nFirst, many models can be categorized as being supervised or unsupervised. Unsupervised models are those that learn patterns, clusters, or other characteristics of the data but lack an outcome, i.e., a dependent variable. Principal component analysis (PCA), clustering, and autoencoders are examples of unsupervised models; they are used to understand relationships between variables or sets of variables without an explicit relationship between predictors and an outcome. Supervised models are those that have an outcome variable. Linear regression, neural networks, and numerous other methodologies fall into this category.\nWithin supervised models, there are two main sub-categories:\n\nRegression predicts a numeric outcome.\nClassification predicts an outcome that is an ordered or unordered set of qualitative values.\n\nThese are imperfect definitions and do not account for all possible model types. In ?sec-models, we refer to this characteristic of supervised techniques as the model mode.\nDifferent variables can have different roles, especially in a supervised modeling analysis. Outcomes (otherwise known as the labels, endpoints, or dependent variables) are the value being predicted in supervised models. The independent variables, which are the substrate for making predictions of the outcome, are also referred to as predictors, features, or covariates (depending on the context). The terms outcomes and predictors are used most frequently in this book.\nIn terms of the data or variables themselves, whether used for supervised or unsupervised models, as predictors or outcomes, the two main categories are quantitative and qualitative. Examples of the former are real numbers like 3.14159 and integers like 42. Qualitative values, also known as nominal data, are those that represent some sort of discrete state that cannot be naturally placed on a numeric scale, like “red”, “green”, and “blue”."
  },
  {
    "objectID": "01-software-modeling.html#sec-model-phases",
    "href": "01-software-modeling.html#sec-model-phases",
    "title": "1  Software for modeling",
    "section": "\n1.5 How Does Modeling Fit into the Data Analysis Process?",
    "text": "1.5 How Does Modeling Fit into the Data Analysis Process?\nIn what circumstances are models created? Are there steps that precede such an undertaking? Is model creation the first step in data analysis?\n\nThere are a few critical phases of data analysis that always come before modeling.\n\nFirst, there is the chronically underestimated process of cleaning the data. No matter the circumstances, you should investigate the data to make sure that they are applicable to your project goals, accurate, and appropriate. These steps can easily take more time than the rest of the data analysis process (depending on the circumstances).\nData cleaning can also overlap with the second phase of understanding the data, often referred to as exploratory data analysis (EDA). EDA brings to light how the different variables are related to one another, their distributions, typical ranges, and other attributes. A good question to ask at this phase is, “How did I come by these data?” This question can help you understand how the data at hand have been sampled or filtered and if these operations were appropriate. For example, when merging database tables, a join may go awry that could accidentally eliminate one or more subpopulations. Another good idea is to ask if the data are relevant. For example, to predict whether patients have Alzheimer’s disease, it would be unwise to have a data set containing subjects with the disease and a random sample of healthy adults from the general population. Given the progressive nature of the disease, the model may simply predict who are the oldest patients.\nFinally, before starting a data analysis process, there should be clear expectations of the model’s goal and how performance (and success) will be judged. At least one performance metric should be identified with realistic goals of what can be achieved. Common statistical metrics, discussed in more detail in ?sec-performance, are classification accuracy, true and false positive rates, root mean squared error, and so on. The relative benefits and drawbacks of these metrics should be weighed. It is also important that the metric be germane; alignment with the broader data analysis goals is critical.\nThe process of investigating the data may not be simple. Wickham and Grolemund (2016) contains an excellent illustration of the general data analysis process, reproduced in Figure Figure 1.2. Data ingestion and cleaning/tidying are shown as the initial steps. When the analytical steps for understanding commence, they are a heuristic process; we cannot pre-determine how long they may take. The cycle of transformation, modeling, and visualization often requires multiple iterations.\n\n\n\n\nFigure 1.2: The data science process (from R for Data Science, used with permission)\n\n\n\nThis iterative process is especially true for modeling. Figure Figure 1.3 emulates the typical path to determining an appropriate model. The general phases are:\n\nExploratory data analysis (EDA): Initially there is a back and forth between numerical analysis and data visualization (represented in Figure 1.2) where different discoveries lead to more questions and data analysis side-quests to gain more understanding.\nFeature engineering: The understanding gained from EDA results in the creation of specific model terms that make it easier to accurately model the observed data. This can include complex methodologies (e.g., PCA) or simpler features (using the ratio of two predictors). ?sec-recipes focuses entirely on this important step.\nModel tuning and selection (large circles with alternating segments): A variety of models are generated and their performance is compared. Some models require parameter tuning in which some structural parameters must be specified or optimized. The alternating segments within the circles signify the repeated data splitting used during resampling (see ?sec-resampling).\nModel evaluation: During this phase of model development, we assess the model’s performance metrics, examine residual plots, and conduct other EDA-like analyses to understand how well the models work. In some cases, formal between-model comparisons (?sec-compare) help you understand whether any differences in models are within the experimental noise.\n\n\n\n\n\nFigure 1.3: A schematic for the typical modeling process\n\n\n\nAfter an initial sequence of these tasks, more understanding is gained regarding which models are superior as well as which data subpopulations are not being effectively estimated. This leads to additional EDA and feature engineering, another round of modeling, and so on. Once the data analysis goals are achieved, typically the last steps are to finalize, document, and communicate the model. For predictive models, it is common at the end to validate the model on an additional set of data reserved for this specific purpose.\nAs an example, Kuhn and Johnson (2020) use data to model the daily ridership of Chicago’s public train system using predictors such as the date, the previous ridership results, the weather, and other factors. Table 1.1 shows an approximation of these authors’ hypothetical inner monologue when analyzing these data and eventually selecting a model with sufficient performance.\n\n\n\n\nTable 1.1: Hypothetical inner monologue of a model developer.\n\nThoughts\nActivity\n\n\n\nThe daily ridership values between stations are extremely correlated.\nEDA\n\n\nWeekday and weekend ridership look very different.\nEDA\n\n\nOne day in the summer of 2010 has an abnormally large number of riders.\nEDA\n\n\nWhich stations had the lowest daily ridership values?\nEDA\n\n\nDates should at least be encoded as day-of-the-week, and year.\nFeature Engineering\n\n\nMaybe PCA could be used on the correlated predictors to make it easier for the models to use them.\nFeature Engineering\n\n\nHourly weather records should probably be summarized into daily measurements.\nFeature Engineering\n\n\nLet’s start with simple linear regression, K-nearest neighbors, and a boosted decision tree.\nModel Fitting\n\n\nHow many neighbors should be used?\nModel Tuning\n\n\nShould we run a lot of boosting iterations or just a few?\nModel Tuning\n\n\nHow many neighbors seemed to be optimal for these data?\nModel Tuning\n\n\nWhich models have the lowest root mean squared errors?\nModel Evaluation\n\n\nWhich days were poorly predicted?\nEDA\n\n\nVariable importance scores indicate that the weather information is not predictive. We’ll drop them from the next set of models.\nModel Evaluation\n\n\nIt seems like we should focus on a lot of boosting iterations for that model.\nModel Evaluation\n\n\nWe need to encode holiday features to improve predictions on (and around) those dates.\nFeature Engineering\n\n\nLet’s drop KNN from the model list.\nModel Evaluation"
  },
  {
    "objectID": "02-tidyverse.html#tidyverse-principles",
    "href": "02-tidyverse.html#tidyverse-principles",
    "title": "2  A Tidyverse Primer",
    "section": "\n2.1 Tidyverse Principles",
    "text": "2.1 Tidyverse Principles\nThe full set of strategies and tactics for writing R code in the tidyverse style can be found at the website https://design.tidyverse.org. Here we can briefly describe several of the general tidyverse design principles, their motivation, and how we think about modeling as an application of these principles.\n\n2.1.1 Design for humans\nThe tidyverse focuses on designing R packages and functions that can be easily understood and used by a broad range of people. Both historically and today, a substantial percentage of R users are not people who create software or tools but instead people who create analyses or models. As such, R users do not typically have (or need) computer science backgrounds, and many are not interested in writing their own R packages.\nFor this reason, it is critical that R code be easy to work with to accomplish your goals. Documentation, training, accessibility, and other factors play an important part in achieving this. However, if the syntax itself is difficult for people to easily comprehend, documentation is a poor solution. The software itself must be intuitive.\nTo contrast the tidyverse approach with more traditional R semantics, consider sorting a data frame. Data frames can represent different types of data in each column, and multiple values in each row. Using only the core language, we can sort a data frame using one or more columns by reordering the rows via R’s subscripting rules in conjunction with order(); you cannot successfully use a function you might be tempted to try in such a situation because of its name, sort(). To sort the mtcars data by two of its columns, the call might look like:\n\nmtcars[order(mtcars$gear, mtcars$mpg), ]\n\nWhile very computationally efficient, it would be difficult to argue that this is an intuitive user interface. In dplyr by contrast, the tidyverse function arrange() takes a set of variable names as input arguments directly:\n\nlibrary(dplyr)\narrange(.data = mtcars, gear, mpg)\n\n\nThe variable names used here are “unquoted”; many traditional R functions require a character string to specify variables, but tidyverse functions take unquoted names or selector functions. The selectors allow for one or more readable rules that are applied to the column names. For example, ends_with(\"t\") would select the drat and wt columns of the mtcars data frame.\n\nAdditionally, naming is crucial. If you were new to R and were writing data analysis or modeling code involving linear algebra, you might be stymied when searching for a function that computes the matrix inverse. Using apropos(\"inv\") yields no candidates. It turns out that the base R function for this task is solve(), for solving systems of linear equations. For a matrix X, you would use solve(X) to invert X (with no vector for the right-hand side of the equation). This is only documented in the description of one of the arguments in the help file. In essence, you need to know the name of the solution to be able to find the solution.\nThe tidyverse approach is to use function names that are descriptive and explicit over those that are short and implicit. There is a focus on verbs (e.g., fit, arrange, etc.) for general methods. Verb-noun pairs are particularly effective; consider invert_matrix() as a hypothetical function name. In the context of modeling, it is also important to avoid highly technical jargon, such as Greek letters or obscure terms in terms. Names should be as self-documenting as possible.\nWhen there are similar functions in a package, function names are designed to be optimized for tab-completion. For example, the glue package has a collection of functions starting with a common prefix (glue_) that enables users to quickly find the function they are looking for.\n\n2.1.2 Reuse existing data structures\nWhenever possible, functions should avoid returning a novel data structure. If the results are conducive to an existing data structure, it should be used. This reduces the cognitive load when using software; no additional syntax or methods are required.\nThe data frame is the preferred data structure in tidyverse and tidymodels packages, because its structure is a good fit for such a broad swath of data science tasks. Specifically, the tidyverse and tidymodels favor the tibble, a modern reimagining of R’s data frame that we describe in the next section on example tidyverse syntax.\nAs an example, the rsample package can be used to create resamples of a data set, such as cross-validation or the bootstrap (described in ?sec-resampling). The resampling functions return a tibble with a column called splits of objects that define the resampled data sets. Three bootstrap samples of a data set might look like:\n\nboot_samp &lt;- rsample::bootstraps(mtcars, times = 3)\nboot_samp\n## # Bootstrap sampling \n## # A tibble: 3 × 2\n##   splits          id        \n##   &lt;list&gt;          &lt;chr&gt;     \n## 1 &lt;split [32/10]&gt; Bootstrap1\n## 2 &lt;split [32/12]&gt; Bootstrap2\n## 3 &lt;split [32/13]&gt; Bootstrap3\nclass(boot_samp)\n## [1] \"bootstraps\" \"rset\"       \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nWith this approach, vector-based functions can be used with these columns, such as vapply() or purrr::map().1 This boot_samp object has multiple classes but inherits methods for data frames (\"data.frame\") and tibbles (\"tbl_df\"). Additionally, new columns can be added to the results without affecting the class of the data. This is much easier and more versatile for users to work with than a completely new object type that does not make its data structure obvious.\nOne downside to relying on common data structures is the potential loss of computational performance. In some situations, data can be encoded in specialized formats that are more efficient representations of the data. For example:\n\nIn computational chemistry, the structure-data file format (SDF) is a tool to take chemical structures and encode them in a format that is computationally efficient to work with.\nData that have a large number of values that are the same (such as zeros for binary data) can be stored in a sparse matrix format. This format can reduce the size of the data as well as enable more efficient computational techniques.\n\nThese formats are advantageous when the problem is well scoped and the potential data processing methods are both well defined and suited to such a format.2 However, once such constraints are violated, specialized data formats are less useful. For example, if we perform a transformation of the data that converts the data into fractional numbers, the output is no longer sparse; the sparse matrix representation is helpful for one specific algorithmic step in modeling, but this is often not true before or after that specific step.\n\nA specialized data structure is not flexible enough for an entire modeling workflow in the way that a common data structure is.\n\nOne important feature in the tibble produced by rsample is that the splits column is a list. In this instance, each element of the list has the same type of object: an rsplit object that contains the information about which rows of mtcars belong in the bootstrap sample. List columns can be very useful in data analysis and, as will be seen throughout this book, are important to tidymodels.\n\n2.1.3 Design for the pipe and functional programming\nThe magrittr pipe operator (%&gt;%) is a tool for chaining together a sequence of R functions.3 To demonstrate, consider the following commands that sort a data frame and then retain the first 10 rows:\n\nsmall_mtcars &lt;- arrange(mtcars, gear)\nsmall_mtcars &lt;- slice(small_mtcars, 1:10)\n\n# or more compactly: \nsmall_mtcars &lt;- slice(arrange(mtcars, gear), 1:10)\n\nThe pipe operator substitutes the value of the left-hand side of the operator as the first argument to the right-hand side, so we can implement the same result as before with:\n\nsmall_mtcars &lt;- \n  mtcars %&gt;% \n  arrange(gear) %&gt;% \n  slice(1:10)\n\nThe piped version of this sequence is more readable; this readability increases as more operations are added to a sequence. This approach to programming works in this example because all of the functions we used return the same data structure (a data frame) that is then the first argument to the next function. This is by design. When possible, create functions that can be incorporated into a pipeline of operations.\nIf you have used ggplot2, this is not unlike the layering of plot components into a ggplot object with the + operator. To make a scatter plot with a regression line, the initial ggplot() call is augmented with two additional operations:\n\nlibrary(ggplot2)\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() + \n  geom_smooth(method = lm)\n\nWhile similar to the dplyr pipeline, note that the first argument to this pipeline is a data set (mtcars) and that each function call returns a ggplot object. Not all pipelines need to keep the returned values (plot objects) the same as the initial value (a data frame). Using the pipe operator with dplyr operations has acclimated many R users to expect to return a data frame when pipelines are used; as shown with ggplot2, this does not need to be the case. Pipelines are incredibly useful in modeling workflows but modeling pipelines can return, instead of a data frame, objects such as model components.\nR has excellent tools for creating, changing, and operating on functions, making it a great language for functional programming. This approach can replace iterative loops in many situations, such as when a function returns a value without other side effects.4\nLet’s look at an example. Suppose you are interested in the logarithm of the ratio of the fuel efficiency to the car weight. To those new to R and/or coming from other programming languages, a loop might seem like a good option:\n\nn &lt;- nrow(mtcars)\nratios &lt;- rep(NA_real_, n)\nfor (car in 1:n) {\n  ratios[car] &lt;- log(mtcars$mpg[car]/mtcars$wt[car])\n}\nhead(ratios)\n## [1] 2.081 1.988 2.285 1.896 1.693 1.655\n\nThose with more experience in R may know that there is a much simpler and faster vectorized version that can be computed by:\n\nratios &lt;- log(mtcars$mpg/mtcars$wt)\n\nHowever, in many real-world cases, the element-wise operation of interest is too complex for a vectorized solution. In such a case, a good approach is to write a function to do the computations. When we design for functional programming, it is important that the output depends only on the inputs and that the function has no side effects. Violations of these ideas in the following function are shown with comments:\n\ncompute_log_ratio &lt;- function(mpg, wt) {\n  log_base &lt;- getOption(\"log_base\", default = exp(1)) # gets external data\n  results &lt;- log(mpg/wt, base = log_base)\n  print(mean(results))                                # prints to the console\n  done &lt;&lt;- TRUE                                       # sets external data\n  results\n}\n\nA better version would be:\n\ncompute_log_ratio &lt;- function(mpg, wt, log_base = exp(1)) {\n  log(mpg/wt, base = log_base)\n}\n\nThe purrr package contains tools for functional programming. Let’s focus on the map() family of functions, which operates on vectors and always returns the same type of output. The most basic function, map(), always returns a list and uses the basic syntax of map(vector, function). For example, to take the square root of our data, we could:\n\nmap(head(mtcars$mpg, 3), sqrt)\n## [[1]]\n## [1] 4.583\n## \n## [[2]]\n## [1] 4.583\n## \n## [[3]]\n## [1] 4.775\n\nThere are specialized variants of map() that return values when we know or expect that the function will generate one of the basic vector types. For example, since the square root returns a double-precision number:\n\nmap_dbl(head(mtcars$mpg, 3), sqrt)\n## [1] 4.583 4.583 4.775\n\nThere are also mapping functions that operate across multiple vectors:\n\nlog_ratios &lt;- map2_dbl(mtcars$mpg, mtcars$wt, compute_log_ratio)\nhead(log_ratios)\n## [1] 2.081 1.988 2.285 1.896 1.693 1.655\n\nThe map() functions also allow for temporary, anonymous functions defined using the tilde character. The argument values are .x and .y for map2():\n\nmap2_dbl(mtcars$mpg, mtcars$wt, ~ log(.x/.y)) %&gt;% \n  head()\n## [1] 2.081 1.988 2.285 1.896 1.693 1.655\n\nThese examples have been trivial but, in later sections, will be applied to more complex problems.\n\nFor functional programming in tidy modeling, functions should be defined so that functions like map() can be used for iterative computations."
  },
  {
    "objectID": "02-tidyverse.html#examples-of-tidyverse-syntax",
    "href": "02-tidyverse.html#examples-of-tidyverse-syntax",
    "title": "2  A Tidyverse Primer",
    "section": "\n2.2 Examples of Tidyverse Syntax",
    "text": "2.2 Examples of Tidyverse Syntax\nLet’s begin our discussion of tidyverse syntax by exploring more deeply what a tibble is, and how tibbles work. Tibbles have slightly different rules than basic data frames in R. For example, tibbles naturally work with column names that are not syntactically valid variable names:\n\n# Wants valid names:\ndata.frame(`variable 1` = 1:2, two = 3:4)\n##   variable.1 two\n## 1          1   3\n## 2          2   4\n# But can be coerced to use them with an extra option:\ndf &lt;- data.frame(`variable 1` = 1:2, two = 3:4, check.names = FALSE)\ndf\n##   variable 1 two\n## 1          1   3\n## 2          2   4\n\n# But tibbles just work:\ntbbl &lt;- tibble(`variable 1` = 1:2, two = 3:4)\ntbbl\n## # A tibble: 2 × 2\n##   `variable 1`   two\n##          &lt;int&gt; &lt;int&gt;\n## 1            1     3\n## 2            2     4\n\nStandard data frames enable partial matching of arguments so that code using only a portion of the column names still works. Tibbles prevent this from happening since it can lead to accidental errors.\n\ndf$tw\n## [1] 3 4\n\ntbbl$tw\n## Warning: Unknown or uninitialised column: `tw`.\n## NULL\n\nTibbles also prevent one of the most common R errors: dropping dimensions. If a standard data frame subsets the columns down to a single column, the object is converted to a vector. Tibbles never do this:\n\ndf[, \"two\"]\n## [1] 3 4\n\ntbbl[, \"two\"]\n## # A tibble: 2 × 1\n##     two\n##   &lt;int&gt;\n## 1     3\n## 2     4\n\nThere are other advantages to using tibbles instead of data frames, such as better printing and more.5\nTo demonstrate some syntax, let’s use tidyverse functions to read in data that could be used in modeling. The data set comes from the city of Chicago’s data portal and contains daily ridership data for the city’s elevated train stations. The data set has columns for:\n\nthe station identifier (numeric)\nthe station name (character)\nthe date (character in mm/dd/yyyy format)\nthe day of the week (character)\nthe number of riders (numeric)\n\nOur tidyverse pipeline will conduct the following tasks, in order:\n\nUse the tidyverse package readr to read the data from the source website and convert them into a tibble. To do this, the read_csv() function can determine the type of data by reading an initial number of rows. Alternatively, if the column names and types are already known, a column specification can be created in R and passed to read_csv().\nFilter the data to eliminate a few columns that are not needed (such as the station ID) and change the column stationname to station. The function select() is used for this. When filtering, use either the column names or a dplyr selector function. When selecting names, a new variable name can be declared using the argument format new_name = old_name.\nConvert the date field to the R date format using the mdy() function from the lubridate package. We also convert the ridership numbers to thousands. Both of these computations are executed using the dplyr::mutate() function.\nUse the maximum number of rides for each station and day combination. This mitigates the issue of a small number of days that have more than one record of ridership numbers at certain stations. We group the ridership data by station and day, and then summarize within each of the 1999 unique combinations with the maximum statistic.\n\nThe tidyverse code for these steps is:\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\nurl &lt;- \"https://data.cityofchicago.org/api/views/5neh-572f/rows.csv?accessType=DOWNLOAD&bom=true&format=true\"\n\nall_stations &lt;- \n  # Step 1: Read in the data.\n  read_csv(url) %&gt;% \n  # Step 2: filter columns and rename stationname\n  dplyr::select(station = stationname, date, rides) %&gt;% \n  # Step 3: Convert the character date field to a date encoding.\n  # Also, put the data in units of 1K rides\n  mutate(date = mdy(date), rides = rides / 1000) %&gt;% \n  # Step 4: Summarize the multiple records using the maximum.\n  group_by(date, station) %&gt;% \n  summarize(rides = max(rides), .groups = \"drop\")\n\nThis pipeline of operations illustrates why the tidyverse is popular. A series of data manipulations is used that have simple and easy to understand functions for each transformation; the series is bundled in a streamlined, readable way. The focus is on how the user interacts with the software. This approach enables more people to learn R and achieve their analysis goals, and adopting these same principles for modeling in R has the same benefits."
  },
  {
    "objectID": "02-tidyverse.html#chapter-summary",
    "href": "02-tidyverse.html#chapter-summary",
    "title": "2  A Tidyverse Primer",
    "section": "\n2.3 Chapter Summary",
    "text": "2.3 Chapter Summary\nThis chapter introduced the tidyverse, with a focus on applications for modeling and how tidyverse design principles inform the tidymodels framework. Think of the tidymodels framework as applying tidyverse principles to the domain of building models. We described differences in conventions between the tidyverse and base R, and introduced two important components of the tidyverse system, tibbles and the pipe operator %&gt;%. Data cleaning and processing can feel mundane at times, but these tasks are important for modeling in the real world; we illustrated how to use tibbles, the pipe, and tidyverse functions in an example data import and processing exercise.\n\n\n\n\nWickham, H, M Averick, J Bryan, W Chang, L McGowan, R François, G Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43).\n\n\nWickham, H, and G Grolemund. 2016. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media, Inc."
  },
  {
    "objectID": "02-tidyverse.html#footnotes",
    "href": "02-tidyverse.html#footnotes",
    "title": "2  A Tidyverse Primer",
    "section": "",
    "text": "If you’ve never seen :: in R code before, it is an explicit method for calling a function. The value of the left-hand side is the namespace where the function lives (usually a package name). The right-hand side is the function name. In cases where two packages use the same function name, this syntax ensures that the correct function is called.↩︎\nNot all algorithms can take advantage of sparse representations of data. In such cases, a sparse matrix must be converted to a more conventional format before proceeding.↩︎\nIn R 4.1.0, a native pipe operator |&gt; was introduced as well. In this book, we use the magrittr pipe since users on older versions of R will not have the new native pipe.↩︎\nExamples of function side effects could include changing global data or printing a value.↩︎\nChapter 10 of Wickham and Grolemund (2016) has more details on tibbles.↩︎"
  },
  {
    "objectID": "03-base-r.html#an-example",
    "href": "03-base-r.html#an-example",
    "title": "3  A Review of R Modeling Fundamentals",
    "section": "\n3.1 An Example",
    "text": "3.1 An Example\nTo demonstrate some fundamentals for modeling in base R, let’s use experimental data from McDonald (2009), by way of Mangiafico (2015), on the relationship between the ambient temperature and the rate of cricket chirps per minute. Data were collected for two species: O. exclamationis and O. niveus. The data are contained in a data frame called crickets with a total of 31 data points. These data are shown in Figure 3.1 using the following ggplot2 code.\n\nlibrary(tidyverse)\n\ndata(crickets, package = \"modeldata\")\nnames(crickets)\n\n# Plot the temperature on the x-axis, the chirp rate on the y-axis. The plot\n# elements will be colored differently for each species:\nggplot(crickets, \n       aes(x = temp, y = rate, color = species, pch = species, lty = species)) + \n  # Plot points for each data point and color by species\n  geom_point(size = 2) + \n  # Show a simple linear model fit created separately for each species:\n  geom_smooth(method = lm, se = FALSE, alpha = 0.5) + \n  scale_color_brewer(palette = \"Paired\") +\n  labs(x = \"Temperature (C)\", y = \"Chirp Rate (per minute)\")\n\n\n## [1] \"species\" \"temp\"    \"rate\"\n\n\n\nFigure 3.1: Relationship between chirp rate and temperature for two different species of crickets\n\n\n\nThe data exhibit fairly linear trends for each species. For a given temperature, O. exclamationis appears to chirp more per minute than the other species. For an inferential model, the researchers might have specified the following null hypotheses prior to seeing the data:\n\nTemperature has no effect on the chirp rate.\nThere are no differences between the species’ chirp rate.\n\nThere may be some scientific or practical value in predicting the chirp rate but in this example we will focus on inference.\nTo fit an ordinary linear model in R, the lm() function is commonly used. The important arguments to this function are a model formula and a data frame that contains the data. The formula is symbolic. For example, the simple formula:\nrate ~ temp\nspecifies that the chirp rate is the outcome (since it is on the left-hand side of the tilde ~) and that the temperature value is the predictor.1 Suppose the data contained the time of day in which the measurements were obtained in a column called time. The formula:\nrate ~ temp + time\nwould not add the time and temperature values together. This formula would symbolically represent that temperature and time should be added as separate main effects to the model. A main effect is a model term that contains a single predictor variable.\nThere are no time measurements in these data but the species can be added to the model in the same way:\nrate ~ temp + species\nSpecies is not a quantitative variable; in the data frame, it is represented as a factor column with levels \"O. exclamationis\" and \"O. niveus\". The vast majority of model functions cannot operate on nonnumeric data. For species, the model needs to encode the species data in a numeric format. The most common approach is to use indicator variables (also known as dummy variables) in place of the original qualitative values. In this instance, since species has two possible values, the model formula will automatically encode this column as numeric by adding a new column that has a value of zero when the species is \"O. exclamationis\" and a value of one when the data correspond to \"O. niveus\". The underlying formula machinery automatically converts these values for the data set used to create the model, as well as for any new data points (for example, when the model is used for prediction).\n\nSuppose there were five species instead of two. The model formula, in this case, would create four binary columns that are binary indicators for four of the species. The reference level of the factor (i.e., the first level) is always left out of the predictor set. The idea is that, if you know the values of the four indicator variables, the value of the species can be determined. We discuss binary indicator variables in more detail in ?sec-dummies.\n\nThe model formula rate ~ temp + species creates a model with different y-intercepts for each species; the slopes of the regression lines could be different for each species as well. To accommodate this structure, an interaction term can be added to the model. This can be specified in a few different ways, and the most basic uses the colon:\nrate ~ temp + species + temp:species\n\n# A shortcut can be used to expand all interactions containing\n# interactions with two variables:\nrate ~ (temp + species)^2\n\n# Another shortcut to expand factors to include all possible\n# interactions (equivalent for this example):\nrate ~ temp * species\nIn addition to the convenience of automatically creating indicator variables, the formula offers a few other niceties:\n\nIn-line functions can be used in the formula. For example, to use the natural log of the temperature, we can create the formula rate ~ log(temp). Since the formula is symbolic by default, literal math can also be applied to the predictors using the identity function I(). To use Fahrenheit units, the formula could be rate ~ I( (temp * 9/5) + 32 ) to convert from Celsius.\nR has many functions that are useful inside of formulas. For example, poly(x, 3) adds linear, quadratic, and cubic terms for x to the model as main effects. The splines package also has several functions to create nonlinear spline terms in the formula.\nFor data sets where there are many predictors, the period shortcut is available. The period represents main effects for all of the columns that are not on the left-hand side of the tilde. Using ~ (.)^3 would add main effects as well as all two- and three-variable interactions to the model.\n\nReturning to our chirping crickets, let’s use a two-way interaction model. In this book, we use the suffix _fit for R objects that are fitted models.\n\ninteraction_fit &lt;-  lm(rate ~ (temp + species)^2, data = crickets) \n\n# To print a short summary of the model:\ninteraction_fit\n## \n## Call:\n## lm(formula = rate ~ (temp + species)^2, data = crickets)\n## \n## Coefficients:\n##           (Intercept)                   temp       speciesO. niveus  \n##               -11.041                  3.751                 -4.348  \n## temp:speciesO. niveus  \n##                -0.234\n\nThis output is a little hard to read. For the species indicator variables, R mashes the variable name (species) together with the factor level (O. niveus) with no delimiter.\nBefore going into any inferential results for this model, the fit should be assessed using diagnostic plots. We can use the plot() method for lm objects. This method produces a set of four plots for the object, each showing different aspects of the fit, as shown in Figure Figure 3.2.\n\n# Place two plots next to one another:\npar(mfrow = c(1, 2))\n\n# Show residuals vs predicted values:\nplot(interaction_fit, which = 1)\n\n# A normal quantile plot on the residuals:\nplot(interaction_fit, which = 2)\n\n\n\n\n\nFigure 3.2: Residual diagnostic plots for the linear model with interactions, which appear reasonable enough to conduct inferential analysis\n\n\n\n\nWhen it comes to the technical details of evaluating expressions, R is lazy (as opposed to eager). This means that model fitting functions typically compute the minimum possible quantities at the last possible moment. For example, if you are interested in the coefficient table for each model term, this is not automatically computed with the model but is instead computed via the summary() method.\n\nOur next order of business with the crickets is to assess if the inclusion of the interaction term is necessary. The most appropriate approach for this model is to recompute the model without the interaction term and use the anova() method.\n\n# Fit a reduced model:\nmain_effect_fit &lt;-  lm(rate ~ temp + species, data = crickets) \n\n# Compare the two:\nanova(main_effect_fit, interaction_fit)\n## Analysis of Variance Table\n## \n## Model 1: rate ~ temp + species\n## Model 2: rate ~ (temp + species)^2\n##   Res.Df  RSS Df Sum of Sq    F Pr(&gt;F)\n## 1     28 89.3                         \n## 2     27 85.1  1      4.28 1.36   0.25\n\nThis statistical test generates a p-value of 0.25. This implies that there is a lack of evidence against the null hypothesis that the interaction term is not needed by the model. For this reason, we will conduct further analysis on the model without the interaction.\nResidual plots should be reassessed to make sure that our theoretical assumptions are valid enough to trust the p-values produced by the model (plots not shown here but spoiler alert: they are).\nWe can use the summary() method to inspect the coefficients, standard errors, and p-values of each model term:\n\nsummary(main_effect_fit)\n## \n## Call:\n## lm(formula = rate ~ temp + species, data = crickets)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -3.013 -1.130 -0.391  0.965  3.780 \n## \n## Coefficients:\n##                  Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)       -7.2109     2.5509   -2.83   0.0086 ** \n## temp               3.6028     0.0973   37.03  &lt; 2e-16 ***\n## speciesO. niveus -10.0653     0.7353  -13.69  6.3e-14 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.79 on 28 degrees of freedom\n## Multiple R-squared:  0.99,   Adjusted R-squared:  0.989 \n## F-statistic: 1.33e+03 on 2 and 28 DF,  p-value: &lt;2e-16\n\nThe chirp rate for each species increases by 3.6 chirps as the temperature increases by a single degree. This term shows strong statistical significance as evidenced by the p-value. The species term has a value of -10.07. This indicates that, across all temperature values, O. niveus has a chirp rate that is about 10 fewer chirps per minute than O. exclamationis. Similar to the temperature term, the species effect is associated with a very small p-value.\nThe only issue in this analysis is the intercept value. It indicates that at 0° C, there are negative chirps per minute for both species. While this doesn’t make sense, the data only go as low as 17.2° C and interpreting the model at 0° C would be an extrapolation. This would be a bad idea. That being said, the model fit is good within the applicable range of the temperature values; the conclusions should be limited to the observed temperature range.\nIf we needed to estimate the chirp rate at a temperature that was not observed in the experiment, we could use the predict() method. It takes the model object and a data frame of new values for prediction. For example, the model estimates the chirp rate for O. exclamationis for temperatures between 15° C and 20° C can be computed via:\n\nnew_values &lt;- data.frame(species = \"O. exclamationis\", temp = 15:20)\npredict(main_effect_fit, new_values)\n##     1     2     3     4     5     6 \n## 46.83 50.43 54.04 57.64 61.24 64.84\n\n\nNote that the non-numeric value of species is passed to the predict method, as opposed to the numeric, binary indicator variable.\n\nWhile this analysis has obviously not been an exhaustive demonstration of R’s modeling capabilities, it does highlight some major features important for the rest of this book:\n\nThe language has an expressive syntax for specifying model terms for both simple and quite complex models.\nThe R formula method has many conveniences for modeling that are also applied to new data when predictions are generated.\nThere are numerous helper functions (e.g., anova(), summary() and predict()) that you can use to conduct specific calculations after the fitted model is created.\n\nFinally, as previously mentioned, this framework was first published in 1992. Most of these ideas and methods were developed in that period but have remained remarkably relevant to this day. It highlights that the S language and, by extension R, has been designed for data analysis since its inception."
  },
  {
    "objectID": "03-base-r.html#formula",
    "href": "03-base-r.html#formula",
    "title": "3  A Review of R Modeling Fundamentals",
    "section": "\n3.2 What Does the R Formula Do?",
    "text": "3.2 What Does the R Formula Do?\nThe R model formula is used by many modeling packages. It usually serves multiple purposes:\n\nThe formula defines the columns that the model uses.\nThe standard R machinery uses the formula to encode the columns into an appropriate format.\nThe roles of the columns are defined by the formula.\n\nFor the most part, practitioners’ understanding of what the formula does is dominated by the last purpose. Our focus when typing out a formula is often to declare how the columns should be used. For example, the previous specification we discussed sets up predictors to be used in a specific way:\n(temp + species)^2\nOur focus, when seeing this, is that there are two predictors and the model should contain their main effects and the two-way interactions. However, this formula also implies that, since species is a factor, it should also create indicator variable columns for this predictor (see ?sec-dummies) and multiply those columns by the temp column to create the interactions. This transformation represents our second bullet point on encoding; the formula also defines how each column is encoded and can create additional columns that are not in the original data.\n\nThis is an important point that will come up multiple times in this text, especially when we discuss more complex feature engineering in ?sec-recipes and beyond. The formula in R has some limitations, and our approaches to overcoming them contend with all three aspects."
  },
  {
    "objectID": "03-base-r.html#tidiness-modeling",
    "href": "03-base-r.html#tidiness-modeling",
    "title": "3  A Review of R Modeling Fundamentals",
    "section": "\n3.3 Why Tidiness Is Important for Modeling",
    "text": "3.3 Why Tidiness Is Important for Modeling\nOne of the strengths of R is that it encourages developers to create a user interface that fits their needs. As an example, here are three common methods for creating a scatter plot of two numeric variables in a data frame called plot_data:\n\nplot(plot_data$x, plot_data$y)\n\nlibrary(lattice)\nxyplot(y ~ x, data = plot_data)\n\nlibrary(ggplot2)\nggplot(plot_data, aes(x = x, y = y)) + geom_point()\n\nIn these three cases, separate groups of developers devised three distinct interfaces for the same task. Each has advantages and disadvantages.\nIn comparison, the Python Developer’s Guide espouses the notion that, when approaching a problem:\n\n“There should be one – and preferably only one – obvious way to do it.”\n\nR is quite different from Python in this respect. An advantage of R’s diversity of interfaces is that it can evolve over time and fit different needs for different users.\nUnfortunately, some of the syntactical diversity is due to a focus on the needs of the person developing the code instead of the needs of the person using the code. Inconsistencies among packages can be a stumbling block for R users.\nSuppose your modeling project has an outcome with two classes. There are a variety of statistical and machine learning models you could choose from. In order to produce a class probability estimate for each sample, it is common for a model function to have a corresponding predict() method. However, there is significant heterogeneity in the argument values used by those methods to make class probability predictions; this heterogeneity can be difficult for even experienced users to navigate. A sampling of these argument values for different models is shown in Table 3.1.\n\n\n\n\nTable 3.1: Heterogeneous argument names for different modeling functions.\n\nFunction\nPackage\nCode\n\n\n\nlda()\nMASS\npredict(object)\n\n\nglm()\nstats\npredict(object, type = \"response\")\n\n\ngbm()\ngbm\npredict(object, type = \"response\", n.trees)\n\n\nmda()\nmda\npredict(object, type = \"posterior\")\n\n\nrpart()\nrpart\npredict(object, type = \"prob\")\n\n\nvarious\nRWeka\npredict(object, type = \"probability\")\n\n\nlogitboost()\nLogitBoost\npredict(object, type = \"raw\", nIter)\n\n\npamr.train()\npamr\npamr.predict(object, type = \"posterior\")\n\n\n\n\n\n\n\n\nNote that the last example has a custom function to make predictions instead of using the more common predict() interface (the generic predict() method). This lack of consistency is a barrier to day-to-day usage of R for modeling.\nAs another example of unpredictability, the R language has conventions for missing data that are handled inconsistently. The general rule is that missing data propagate more missing data; the average of a set of values with a missing data point is itself missing and so on. When models make predictions, the vast majority require all of the predictors to have complete values. There are several options baked in to R at this point with the generic function na.action(). This sets the policy for how a function should behave if there are missing values. The two most common policies are na.fail() and na.omit(). The former produces an error if missing data are present while the latter removes the missing data prior to calculations by case-wise deletion. From our previous example:\n\n# Add a missing value to the prediction set\nnew_values$temp[1] &lt;- NA\n\n# The predict method for `lm` defaults to `na.pass`:\npredict(main_effect_fit, new_values)\n##     1     2     3     4     5     6 \n##    NA 50.43 54.04 57.64 61.24 64.84\n\n# Alternatively \npredict(main_effect_fit, new_values, na.action = na.fail)\n## Error in na.fail.default(structure(list(temp = c(NA, 16L, 17L, 18L, 19L, : missing values in object\n\npredict(main_effect_fit, new_values, na.action = na.omit)\n##     2     3     4     5     6 \n## 50.43 54.04 57.64 61.24 64.84\n\nFrom a user’s point of view, na.omit() can be problematic. In our example, new_values has 6 rows but only 5 would be returned with na.omit(). To adjust for this, the user would have to determine which row had the missing value and interleave a missing value in the appropriate place if the predictions were merged into new_values.2 While it is rare that a prediction function uses na.omit() as its missing data policy, this does occur. Users who have determined this as the cause of an error in their code find it quite memorable.\nTo resolve the usage issues described here, the tidymodels packages have a set of design goals. Most of the tidymodels design goals fall under the existing rubric of “Design for Humans” from the tidyverse (Wickham et al. 2019), but with specific applications for modeling code. There are a few additional tidymodels design goals that complement those of the tidyverse. Some examples:\n\nR has excellent capabilities for object-oriented programming, and we use this in lieu of creating new function names (such as a hypothetical new predict_samples() function).\nSensible defaults are very important. Also, functions should have no default for arguments when it is more appropriate to force the user to make a choice (e.g., the file name argument for read_csv()).\nSimilarly, argument values whose default can be derived from the data should be. For example, for glm() the family argument could check the type of data in the outcome and, if no family was given, a default could be determined internally.\nFunctions should take the data structures that users have as opposed to the data structure that developers want. For example, a model function’s only interface should not be constrained to matrices. Frequently, users will have non-numeric predictors such as factors.\n\nMany of these ideas are described in the tidymodels guidelines for model implementation.3 In subsequent chapters, we will illustrate examples of existing issues, along with their solutions.\n\nA few existing R packages provide a unified interface to harmonize these heterogeneous modeling APIs, such as caret and mlr. The tidymodels framework is similar to these in adopting a unification of the function interface, as well as enforcing consistency in the function names and return values. It is different in its opinionated design goals and modeling implementation, discussed in detail throughout this book.\n\nThe broom::tidy() function, which we use throughout this book, is another tool for standardizing the structure of R objects. It can return many types of R objects in a more usable format. For example, suppose that predictors are being screened based on their correlation to the outcome column. Using purrr::map(), the results from cor.test() can be returned in a list for each predictor:\n\ncorr_res &lt;- map(mtcars %&gt;% select(-mpg), cor.test, y = mtcars$mpg)\n\n# The first of ten results in the vector: \ncorr_res[[1]]\n## \n##  Pearson's product-moment correlation\n## \n## data:  .x[[i]] and mtcars$mpg\n## t = -8.9, df = 30, p-value = 6e-10\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  -0.9258 -0.7163\n## sample estimates:\n##     cor \n## -0.8522\n\nIf we want to use these results in a plot, the standard format of hypothesis test results are not very useful. The tidy() method can return this as a tibble with standardized names:\n\nlibrary(broom)\n\ntidy(corr_res[[1]])\n## # A tibble: 1 × 8\n##   estimate statistic  p.value parameter conf.low conf.high method        alternative\n##      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;      \n## 1   -0.852     -8.92 6.11e-10        30   -0.926    -0.716 Pearson's pr… two.sided\n\nThese results can be “stacked” and added to a ggplot(), as shown in Figure 3.3.\n\ncorr_res %&gt;% \n  # Convert each to a tidy format; `map_dfr()` stacks the data frames \n  map_dfr(tidy, .id = \"predictor\") %&gt;% \n  ggplot(aes(x = fct_reorder(predictor, estimate))) + \n  geom_point(aes(y = estimate)) + \n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .1) +\n  labs(x = NULL, y = \"Correlation with mpg\")\n\n\n\n\n\nFigure 3.3: Correlations (and 95% confidence intervals) between predictors and the outcome in the mtcars data set\n\n\n\nCreating such a plot is possible using core R language functions, but automatically reformatting the results makes for more concise code with less potential for errors."
  },
  {
    "objectID": "03-base-r.html#combining-base-r-models-and-the-tidyverse",
    "href": "03-base-r.html#combining-base-r-models-and-the-tidyverse",
    "title": "3  A Review of R Modeling Fundamentals",
    "section": "\n3.4 Combining Base R Models and the Tidyverse",
    "text": "3.4 Combining Base R Models and the Tidyverse\nR modeling functions from the core language or other R packages can be used in conjunction with the tidyverse, especially with the dplyr, purrr, and tidyr packages. For example, if we wanted to fit separate models for each cricket species, we can first break out the cricket data by this column using dplyr::group_nest():\n\nsplit_by_species &lt;- \n  crickets %&gt;% \n  group_nest(species) \nsplit_by_species\n## # A tibble: 2 × 2\n##   species                        data\n##   &lt;fct&gt;            &lt;list&lt;tibble[,2]&gt;&gt;\n## 1 O. exclamationis           [14 × 2]\n## 2 O. niveus                  [17 × 2]\n\nThe data column contains the rate and temp columns from crickets in a list column. From this, the purrr::map() function can create individual models for each species:\n\nmodel_by_species &lt;- \n  split_by_species %&gt;% \n  mutate(model = map(data, ~ lm(rate ~ temp, data = .x)))\nmodel_by_species\n## # A tibble: 2 × 3\n##   species                        data model \n##   &lt;fct&gt;            &lt;list&lt;tibble[,2]&gt;&gt; &lt;list&gt;\n## 1 O. exclamationis           [14 × 2] &lt;lm&gt;  \n## 2 O. niveus                  [17 × 2] &lt;lm&gt;\n\nTo collect the coefficients for each of these models, use broom::tidy() to convert them to a consistent data frame format so that they can be unnested:\n\nmodel_by_species %&gt;% \n  mutate(coef = map(model, tidy)) %&gt;% \n  select(species, coef) %&gt;% \n  unnest(cols = c(coef))\n## # A tibble: 4 × 6\n##   species          term        estimate std.error statistic  p.value\n##   &lt;fct&gt;            &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 O. exclamationis (Intercept)   -11.0      4.77      -2.32 3.90e- 2\n## 2 O. exclamationis temp            3.75     0.184     20.4  1.10e-10\n## 3 O. niveus        (Intercept)   -15.4      2.35      -6.56 9.07e- 6\n## 4 O. niveus        temp            3.52     0.105     33.6  1.57e-15\n\n\nList columns can be very powerful in modeling projects. List columns provide containers for any type of R objects, from a fitted model itself to the important data frame structure."
  },
  {
    "objectID": "03-base-r.html#the-tidymodels-metapackage",
    "href": "03-base-r.html#the-tidymodels-metapackage",
    "title": "3  A Review of R Modeling Fundamentals",
    "section": "\n3.5 The tidymodels Metapackage",
    "text": "3.5 The tidymodels Metapackage\nThe tidyverse (Chapter 2) is designed as a set of modular R packages, each with a fairly narrow scope. The tidymodels framework follows a similar design. For example, the rsample package focuses on data splitting and resampling. Although resampling methods are critical to other activities of modeling (e.g., measuring performance), they reside in a single package, and performance metrics are contained in a different, separate package, yardstick. There are many benefits to adopting this philosophy of modular packages, from less bloated model deployment to smoother package maintenance.\nThe downside to this philosophy is that there are a lot of packages in the tidymodels framework. To compensate for this, the tidymodels package (which you can think of as a metapackage like the tidyverse package) loads a core set of tidymodels and tidyverse packages. Loading the package shows which packages are attached:\n\nlibrary(tidymodels)\n## ── Attaching packages ─────────────────────────────────────────── tidymodels 1.1.1 ──\n## ✔ broom        1.0.5     ✔ recipes      1.0.8\n## ✔ dials        1.2.0     ✔ rsample      1.2.0\n## ✔ dplyr        1.1.3     ✔ tibble       3.2.1\n## ✔ ggplot2      3.4.3     ✔ tidyr        1.3.0\n## ✔ infer        1.0.5     ✔ tune         1.1.2\n## ✔ modeldata    1.2.0     ✔ workflows    1.1.3\n## ✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n## ✔ purrr        1.0.2     ✔ yardstick    1.2.0\n## ── Conflicts ────────────────────────────────────────────── tidymodels_conflicts() ──\n## ✖ purrr::discard() masks scales::discard()\n## ✖ dplyr::filter()  masks stats::filter()\n## ✖ dplyr::lag()     masks stats::lag()\n## ✖ recipes::step()  masks stats::step()\n## • Learn how to get started at https://www.tidymodels.org/start/\n\nIf you have used the tidyverse, you’ll notice some familiar names as a few tidyverse packages, such as dplyr and ggplot2, are loaded together with the tidymodels packages. We’ve already said that the tidymodels framework applies tidyverse principles to modeling, but the tidymodels framework also literally builds on some of the most fundamental tidyverse packages such as these.\nLoading the metapackage also shows if there are function naming conflicts with previously loaded packages. As an example of a naming conflict, before loading tidymodels, invoking the filter() function will execute the function in the stats package. After loading tidymodels, it will execute the dplyr function of the same name.\nThere are a few ways to handle naming conflicts. The function can be called with its namespace (e.g., stats::filter()). This is not bad practice, but it does make the code less readable.\nAnother option is to use the conflicted package. We can set a rule that remains in effect until the end of the R session to ensure that one specific function will always run if no namespace is given in the code. As an example, if we prefer the dplyr version of the previous function:\n\nlibrary(conflicted)\nconflict_prefer(\"filter\", winner = \"dplyr\")\n\nFor convenience, tidymodels contains a function that captures most of the common naming conflicts that we might encounter:\n\ntidymodels_prefer(quiet = FALSE)\n## [conflicted] Will prefer agua::refit over any other package.\n## [conflicted] Will prefer dials::Laplace over any other package.\n## [conflicted] Will prefer dials::max_rules over any other package.\n## [conflicted] Will prefer dials::neighbors over any other package.\n## [conflicted] Will prefer dials::prune over any other package.\n## [conflicted] Will prefer dials::smoothness over any other package.\n## [conflicted] Will prefer dplyr::collapse over any other package.\n## [conflicted] Will prefer dplyr::combine over any other package.\n## [conflicted] Will prefer dplyr::filter over any other package.\n## [conflicted] Will prefer dplyr::rename over any other package.\n## [conflicted] Will prefer dplyr::select over any other package.\n## [conflicted] Will prefer dplyr::slice over any other package.\n## [conflicted] Will prefer ggplot2::`%+%` over any other package.\n## [conflicted] Will prefer ggplot2::margin over any other package.\n## [conflicted] Will prefer parsnip::bart over any other package.\n## [conflicted] Will prefer parsnip::fit over any other package.\n## [conflicted] Will prefer parsnip::mars over any other package.\n## [conflicted] Will prefer parsnip::pls over any other package.\n## [conflicted] Will prefer purrr::cross over any other package.\n## [conflicted] Will prefer purrr::invoke over any other package.\n## [conflicted] Will prefer purrr::map over any other package.\n## [conflicted] Will prefer recipes::discretize over any other package.\n## [conflicted] Will prefer recipes::step over any other package.\n## [conflicted] Will prefer rsample::populate over any other package.\n## [conflicted] Will prefer scales::rescale over any other package.\n## [conflicted] Will prefer themis::step_downsample over any other package.\n## [conflicted] Will prefer themis::step_upsample over any other package.\n## [conflicted] Will prefer tidyr::expand over any other package.\n## [conflicted] Will prefer tidyr::extract over any other package.\n## [conflicted] Will prefer tidyr::pack over any other package.\n## [conflicted] Will prefer tidyr::unpack over any other package.\n## [conflicted] Will prefer tune::parameters over any other package.\n## [conflicted] Will prefer tune::tune over any other package.\n## [conflicted] Will prefer yardstick::get_weights over any other package.\n## [conflicted] Will prefer yardstick::precision over any other package.\n## [conflicted] Will prefer yardstick::recall over any other package.\n## [conflicted] Will prefer yardstick::spec over any other package.\n## [conflicted] Will prefer recipes::update over Matrix::update.\n## ── Conflicts ───────────────────────────────────────────────── tidymodels_prefer() ──\n\n\nBe aware that using this function opts you in to using conflicted::conflict_prefer() for all namespace conflicts, making every conflict an error and forcing you to choose which function to use. The function tidymodels::tidymodels_prefer() handles the most common conflicts from tidymodels functions, but you will need to handle other conflicts in your R session yourself."
  },
  {
    "objectID": "03-base-r.html#chapter-summary",
    "href": "03-base-r.html#chapter-summary",
    "title": "3  A Review of R Modeling Fundamentals",
    "section": "\n3.6 Chapter Summary",
    "text": "3.6 Chapter Summary\nThis chapter reviewed core R language conventions for creating and using models that are an important foundation for the rest of this book. The formula operator is an expressive and important aspect of fitting models in R and often serves multiple purposes in non-tidymodels functions. Traditional R approaches to modeling have some limitations, especially when it comes to fluently handling and visualizing model output. The tidymodels metapackage applies tidyverse design philosophy to modeling packages.\n\n\n\n\nChambers, J, and T Hastie, eds. 1992. Statistical Models in S. Boca Raton, FL: CRC Press, Inc.\n\n\nMangiafico, S. 2015. “An R Companion for the Handbook of Biological Statistics.” https://rcompanion.org/handbook/.\n\n\nMcDonald, J. 2009. Handbook of Biological Statistics. Sparky House Publishing.\n\n\nWickham, H, M Averick, J Bryan, W Chang, L McGowan, R François, G Grolemund, et al. 2019. “Welcome to the Tidyverse.” Journal of Open Source Software 4 (43)."
  },
  {
    "objectID": "03-base-r.html#footnotes",
    "href": "03-base-r.html#footnotes",
    "title": "3  A Review of R Modeling Fundamentals",
    "section": "",
    "text": "Most model functions implicitly add an intercept column.↩︎\nA base R policy called na.exclude() does exactly this.↩︎\nhttps://tidymodels.github.io/model-implementation-principles↩︎"
  },
  {
    "objectID": "04-ames.html#exploring-features-of-homes-in-ames",
    "href": "04-ames.html#exploring-features-of-homes-in-ames",
    "title": "4  The Ames Housing Data",
    "section": "\n4.1 Exploring Features of Homes in Ames",
    "text": "4.1 Exploring Features of Homes in Ames\nLet’s start our exploratory data analysis by focusing on the outcome we want to predict: the last sale price of the house (in USD). We can create a histogram to see the distribution of sale prices in ?fig-ames-sale-price-hist.\n\nlibrary(tidymodels)\ntidymodels_prefer()\n\nggplot(ames, aes(x = Sale_Price)) + \n  geom_histogram(bins = 50, col= \"white\")\n\n{r  #| label: fig-ames-sale-price-hist #| ref.label: \"ames-sale-price-code\" #| out.width: '100%' #| fig.width: 8 #| fig.height: 3 #| echo: FALSE #| fig.cap: \"Sale prices of houses in Ames, Iowa\" #| fig.alt: \"A histogram of the sale prices of houses in Ames, Iowa. The distribution has a long right tail.\"\nThis plot shows us that the data are right-skewed; there are more inexpensive houses than expensive ones. The median sale price was $160,000, and the most expensive house was $755,000. When modeling this outcome, a strong argument can be made that the price should be log-transformed. The advantages of this type of transformation are that no houses would be predicted with negative sale prices and that errors in predicting expensive houses will not have an undue influence on the model. Also, from a statistical perspective, a logarithmic transform may also stabilize the variance in a way that makes inference more legitimate. We can use similar steps to now visualize the transformed data, shown in ?fig-ames-log-sale-price-hist.\n\nggplot(ames, aes(x = Sale_Price)) + \n  geom_histogram(bins = 50, col= \"white\") +\n  scale_x_log10()\n\n{r  #| label: fig-ames-log-sale-price-hist #| ref.label: \"ames-log-sale-price-code\" #| out.width: '100%' #| fig.width: 8 #| fig.height: 3 #| echo: FALSE #| fig.cap: \"Sale prices of houses in Ames, Iowa after a log (base 10) transformation\", #| fig.alt: \"A histogram of the sale prices of houses in Ames, Iowa after a log (base 10) transformation. The distribution, while not perfectly symmetric, exhibits far less skewness.\"\nWhile not perfect, this will likely result in better models than using the untransformed data, for the reasons just outlined.\n\nThe disadvantages of transforming the outcome mostly relate to interpretation of model results.\n\nThe units of the model coefficients might be more difficult to interpret, as will measures of performance. For example, the root mean squared error (RMSE) is a common performance metric used in regression models. It uses the difference between the observed and predicted values in its calculations. If the sale price is on the log scale, these differences (i.e., the residuals) are also on the log scale. It can be difficult to understand the quality of a model whose RMSE is 0.15 on such a log scale.\nDespite these drawbacks, the models used in this book use the log transformation for this outcome. From this point on, the outcome column is prelogged in the ames data frame:\n\names &lt;- ames %&gt;% mutate(Sale_Price = log10(Sale_Price))\n\nAnother important aspect of these data for our modeling is their geographic locations. This spatial information is contained in the data in two ways: a qualitative Neighborhood label as well as quantitative longitude and latitude data. To visualize the spatial information, let’s use both together to plot the data on a map in Figure 4.1.\n\n\n\n\nFigure 4.1: Neighborhoods in Ames, IA\n\n\n\nWe can see a few noticeable patterns. First, there is a void of data points in the center of Ames. This corresponds to the campus of Iowa State University where there are no residential houses. Second, while there are a number of adjacent neighborhoods, others are geographically isolated. For example, as Figure 4.2 shows, Timberland is located apart from almost all other neighborhoods.\n\n\n\n\nFigure 4.2: Locations of homes in Timberland\n\n\n\nFigure 4.3 visualizes how the Meadow Village neighborhood in southwest Ames is like an island of properties inside the sea of properties that make up the Mitchell neighborhood.\n\n\n\n\nFigure 4.3: Locations of homes in Meadow Village and Mitchell\n\n\n\nA detailed inspection of the map also shows that the neighborhood labels are not completely reliable. For example, Figure 4.4 shows some properties labeled as being in Northridge are surrounded by homes in the adjacent Somerset neighborhood.\n\n\n\n\nFigure 4.4: Locations of homes in Somerset and Northridge\n\n\n\nAlso, there are ten isolated homes labeled as being in Crawford that, as you can see in Figure 4.5, are not close to the majority of the other homes in that neighborhood.\n\n\n\n\nFigure 4.5: Locations of homes in Crawford\n\n\n\nAlso notable is the “Iowa Department of Transportation (DOT) and Rail Road” neighborhood adjacent to the main road on the east side of Ames, shown in Figure 4.6. There are several clusters of homes within this neighborhood as well as some longitudinal outliers; the two homes farthest east are isolated from the other locations.\n\n\n\n\nFigure 4.6: Homes labeled as Iowa Department of Transportation (DOT) and Rail Road\n\n\n\nAs described in Chapter 1, it is critical to conduct exploratory data analysis prior to beginning any modeling. These housing data have characteristics that present interesting challenges about how the data should be processed and modeled. We describe many of these in later chapters. Some basic questions that could be examined during this exploratory stage include:\n\nIs there anything odd or noticeable about the distributions of the individual predictors? Is there much skewness or any pathological distributions?\nAre there high correlations between predictors? For example, there are multiple predictors related to house size. Are some redundant?\nAre there associations between predictors and the outcomes?\n\nMany of these questions will be revisited as these data are used throughout this book."
  },
  {
    "objectID": "04-ames.html#sec-ames-summary",
    "href": "04-ames.html#sec-ames-summary",
    "title": "4  The Ames Housing Data",
    "section": "\n4.2 Chapter Summary",
    "text": "4.2 Chapter Summary\nThis chapter introduced the Ames housing data set and investigated some of its characteristics. This data set will be used in later chapters to demonstrate tidymodels syntax. Exploratory data analysis like this is an essential component of any modeling project; EDA uncovers information that contributes to better modeling practice.\nThe important code for preparing the Ames data set that we will carry forward into subsequent chapters is:\n\nlibrary(tidymodels)\ndata(ames)\names &lt;- ames %&gt;% mutate(Sale_Price = log10(Sale_Price))\n\n\n\n\n\nDe Cock, D. 2011. “Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project.” Journal of Statistics Education 19 (3)."
  },
  {
    "objectID": "04-ames.html#footnotes",
    "href": "04-ames.html#footnotes",
    "title": "4  The Ames Housing Data",
    "section": "",
    "text": "For a complete account of the differences, see https://github.com/topepo/AmesHousing/blob/master/R/make_ames.R.↩︎"
  },
  {
    "objectID": "03-base-r.html#sec-formula",
    "href": "03-base-r.html#sec-formula",
    "title": "3  A Review of R Modeling Fundamentals",
    "section": "\n3.2 What Does the R Formula Do?",
    "text": "3.2 What Does the R Formula Do?\nThe R model formula is used by many modeling packages. It usually serves multiple purposes:\n\nThe formula defines the columns that the model uses.\nThe standard R machinery uses the formula to encode the columns into an appropriate format.\nThe roles of the columns are defined by the formula.\n\nFor the most part, practitioners’ understanding of what the formula does is dominated by the last purpose. Our focus when typing out a formula is often to declare how the columns should be used. For example, the previous specification we discussed sets up predictors to be used in a specific way:\n(temp + species)^2\nOur focus, when seeing this, is that there are two predictors and the model should contain their main effects and the two-way interactions. However, this formula also implies that, since species is a factor, it should also create indicator variable columns for this predictor (see ?sec-dummies) and multiply those columns by the temp column to create the interactions. This transformation represents our second bullet point on encoding; the formula also defines how each column is encoded and can create additional columns that are not in the original data.\n\nThis is an important point that will come up multiple times in this text, especially when we discuss more complex feature engineering in ?sec-recipes and beyond. The formula in R has some limitations, and our approaches to overcoming them contend with all three aspects."
  },
  {
    "objectID": "03-base-r.html#sec-tidiness-modeling",
    "href": "03-base-r.html#sec-tidiness-modeling",
    "title": "3  A Review of R Modeling Fundamentals",
    "section": "\n3.3 Why Tidiness Is Important for Modeling",
    "text": "3.3 Why Tidiness Is Important for Modeling\nOne of the strengths of R is that it encourages developers to create a user interface that fits their needs. As an example, here are three common methods for creating a scatter plot of two numeric variables in a data frame called plot_data:\n\nplot(plot_data$x, plot_data$y)\n\nlibrary(lattice)\nxyplot(y ~ x, data = plot_data)\n\nlibrary(ggplot2)\nggplot(plot_data, aes(x = x, y = y)) + geom_point()\n\nIn these three cases, separate groups of developers devised three distinct interfaces for the same task. Each has advantages and disadvantages.\nIn comparison, the Python Developer’s Guide espouses the notion that, when approaching a problem:\n\n“There should be one – and preferably only one – obvious way to do it.”\n\nR is quite different from Python in this respect. An advantage of R’s diversity of interfaces is that it can evolve over time and fit different needs for different users.\nUnfortunately, some of the syntactical diversity is due to a focus on the needs of the person developing the code instead of the needs of the person using the code. Inconsistencies among packages can be a stumbling block for R users.\nSuppose your modeling project has an outcome with two classes. There are a variety of statistical and machine learning models you could choose from. In order to produce a class probability estimate for each sample, it is common for a model function to have a corresponding predict() method. However, there is significant heterogeneity in the argument values used by those methods to make class probability predictions; this heterogeneity can be difficult for even experienced users to navigate. A sampling of these argument values for different models is shown in Table 3.1.\n\n\n\n\nTable 3.1: Heterogeneous argument names for different modeling functions.\n\nFunction\nPackage\nCode\n\n\n\nlda()\nMASS\npredict(object)\n\n\nglm()\nstats\npredict(object, type = \"response\")\n\n\ngbm()\ngbm\npredict(object, type = \"response\", n.trees)\n\n\nmda()\nmda\npredict(object, type = \"posterior\")\n\n\nrpart()\nrpart\npredict(object, type = \"prob\")\n\n\nvarious\nRWeka\npredict(object, type = \"probability\")\n\n\nlogitboost()\nLogitBoost\npredict(object, type = \"raw\", nIter)\n\n\npamr.train()\npamr\npamr.predict(object, type = \"posterior\")\n\n\n\n\n\n\n\n\nNote that the last example has a custom function to make predictions instead of using the more common predict() interface (the generic predict() method). This lack of consistency is a barrier to day-to-day usage of R for modeling.\nAs another example of unpredictability, the R language has conventions for missing data that are handled inconsistently. The general rule is that missing data propagate more missing data; the average of a set of values with a missing data point is itself missing and so on. When models make predictions, the vast majority require all of the predictors to have complete values. There are several options baked in to R at this point with the generic function na.action(). This sets the policy for how a function should behave if there are missing values. The two most common policies are na.fail() and na.omit(). The former produces an error if missing data are present while the latter removes the missing data prior to calculations by case-wise deletion. From our previous example:\n\n# Add a missing value to the prediction set\nnew_values$temp[1] &lt;- NA\n\n# The predict method for `lm` defaults to `na.pass`:\npredict(main_effect_fit, new_values)\n##     1     2     3     4     5     6 \n##    NA 50.43 54.04 57.64 61.24 64.84\n\n# Alternatively \npredict(main_effect_fit, new_values, na.action = na.fail)\n## Error in na.fail.default(structure(list(temp = c(NA, 16L, 17L, 18L, 19L, : missing values in object\n\npredict(main_effect_fit, new_values, na.action = na.omit)\n##     2     3     4     5     6 \n## 50.43 54.04 57.64 61.24 64.84\n\nFrom a user’s point of view, na.omit() can be problematic. In our example, new_values has 6 rows but only 5 would be returned with na.omit(). To adjust for this, the user would have to determine which row had the missing value and interleave a missing value in the appropriate place if the predictions were merged into new_values.2 While it is rare that a prediction function uses na.omit() as its missing data policy, this does occur. Users who have determined this as the cause of an error in their code find it quite memorable.\nTo resolve the usage issues described here, the tidymodels packages have a set of design goals. Most of the tidymodels design goals fall under the existing rubric of “Design for Humans” from the tidyverse (Wickham et al. 2019), but with specific applications for modeling code. There are a few additional tidymodels design goals that complement those of the tidyverse. Some examples:\n\nR has excellent capabilities for object-oriented programming, and we use this in lieu of creating new function names (such as a hypothetical new predict_samples() function).\nSensible defaults are very important. Also, functions should have no default for arguments when it is more appropriate to force the user to make a choice (e.g., the file name argument for read_csv()).\nSimilarly, argument values whose default can be derived from the data should be. For example, for glm() the family argument could check the type of data in the outcome and, if no family was given, a default could be determined internally.\nFunctions should take the data structures that users have as opposed to the data structure that developers want. For example, a model function’s only interface should not be constrained to matrices. Frequently, users will have non-numeric predictors such as factors.\n\nMany of these ideas are described in the tidymodels guidelines for model implementation.3 In subsequent chapters, we will illustrate examples of existing issues, along with their solutions.\n\nA few existing R packages provide a unified interface to harmonize these heterogeneous modeling APIs, such as caret and mlr. The tidymodels framework is similar to these in adopting a unification of the function interface, as well as enforcing consistency in the function names and return values. It is different in its opinionated design goals and modeling implementation, discussed in detail throughout this book.\n\nThe broom::tidy() function, which we use throughout this book, is another tool for standardizing the structure of R objects. It can return many types of R objects in a more usable format. For example, suppose that predictors are being screened based on their correlation to the outcome column. Using purrr::map(), the results from cor.test() can be returned in a list for each predictor:\n\ncorr_res &lt;- map(mtcars %&gt;% select(-mpg), cor.test, y = mtcars$mpg)\n\n# The first of ten results in the vector: \ncorr_res[[1]]\n## \n##  Pearson's product-moment correlation\n## \n## data:  .x[[i]] and mtcars$mpg\n## t = -8.9, df = 30, p-value = 6e-10\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  -0.9258 -0.7163\n## sample estimates:\n##     cor \n## -0.8522\n\nIf we want to use these results in a plot, the standard format of hypothesis test results are not very useful. The tidy() method can return this as a tibble with standardized names:\n\nlibrary(broom)\n\ntidy(corr_res[[1]])\n## # A tibble: 1 × 8\n##   estimate statistic  p.value parameter conf.low conf.high method        alternative\n##      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;      \n## 1   -0.852     -8.92 6.11e-10        30   -0.926    -0.716 Pearson's pr… two.sided\n\nThese results can be “stacked” and added to a ggplot(), as shown in Figure 3.3.\n\ncorr_res %&gt;% \n  # Convert each to a tidy format; `map_dfr()` stacks the data frames \n  map_dfr(tidy, .id = \"predictor\") %&gt;% \n  ggplot(aes(x = fct_reorder(predictor, estimate))) + \n  geom_point(aes(y = estimate)) + \n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .1) +\n  labs(x = NULL, y = \"Correlation with mpg\")\n\n\n\n\n\nFigure 3.3: Correlations (and 95% confidence intervals) between predictors and the outcome in the mtcars data set\n\n\n\nCreating such a plot is possible using core R language functions, but automatically reformatting the results makes for more concise code with less potential for errors."
  }
]