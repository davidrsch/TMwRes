```{r performance-setup, include = FALSE}
knitr::opts_chunk$set(fig.path = "figures/")
library(tidymodels)
library(kableExtra)
tidymodels_prefer()
source("ames_snippets.R")
load("RData/lm_fit.RData")

data(ad_data)
set.seed(245)
ad_folds <- vfold_cv(ad_data, repeats = 5)
```

# Juzgar La Eficacia Del Modelo {#sec-performance}

Una vez que tenemos un modelo, necesitamos saber qué tan bien funciona. Un enfoque cuantitativo para estimar la efectividad nos permite comprender el modelo, comparar diferentes modelos o modificar el modelo para mejorar el rendimiento. Nuestro enfoque en tidymodels está en la validación empírica; Esto generalmente significa usar datos que no se usaron para crear el modelo como sustrato para medir la efectividad.

::: rmdwarning
El mejor enfoque para la validación empírica implica el uso de métodos de *resampling* que se presentarán en el [Capítulo @sec-resampling]. En este capítulo, motivaremos la necesidad de una validación empírica mediante el uso del conjunto de pruebas. Tenga en cuenta que el conjunto de prueba solo se puede utilizar una vez, como se explica en @sec-splitting-methods.
:::

Al juzgar la eficacia del modelo, su decisión sobre qué métricas examinar puede ser fundamental. En capítulos posteriores, se optimizarán empíricamente ciertos parámetros del modelo y se utilizará una métrica de rendimiento primaria para elegir el mejor submodelo. Elegir la métrica incorrecta puede fácilmente tener consecuencias no deseadas. Por ejemplo, dos métricas comunes para los modelos de regresión son la raíz del error cuadrático medio (RMSE) y el coeficiente de determinación (también conocido como $R^2$). El primero mide la *precisión* mientras que el segundo mide la *correlación*. Estos no son necesariamente lo mismo. @fig-performance-reg-metrics demuestra la diferencia entre los dos.

```{r}
#| label: fig-performance-reg-metrics
#| echo: FALSE
#| fig.cap: "Valores observados versus valores predichos para modelos optimizados utilizando el RMSE en comparación con el coeficiente de determinación"
#| fig.alt: "Gráficos de dispersión de valores numéricos observados versus valores predichos para modelos optimizados utilizando el RMSE y el coeficiente de determinación. El primero produce resultados cercanos a la línea de identidad de 45 grados, mientras que el segundo muestra resultados con una estrecha correlación lineal pero se aleja mucho de la línea de identidad."

set.seed(234)
n <- 200
obs <- runif(n, min = 2, max = 20)

reg_ex <- 
  tibble(
    observado = c(obs, obs),
    predecido = c(obs + rnorm(n, sd = 1.5), 5 + .5 * obs + rnorm(n, sd = .5)),
    enfoque = rep(c("RMSE optimizado", "R^2 optimizado"), each = n)
  ) %>% 
  mutate(enfoque = factor(
    enfoque, 
    levels = c("RMSE optimizado", "R^2 optimizado"),
    labels = c(expression(RMSE ~ optimizado), expression(italic(R^2) ~ optimizado)))
  )

ggplot(reg_ex, aes(x = observado, y = predecido)) + 
  geom_abline(lty = 2) + 
  geom_point(alpha = 0.5) + 
  coord_obs_pred() + 
  facet_wrap(~ enfoque, labeller = "label_parsed")
```

Un modelo optimizado para RMSE tiene más variabilidad pero tiene una precisión relativamente uniforme en todo el rango del resultado. El panel derecho muestra que existe una correlación más estrecha entre los valores observados y predichos, pero este modelo funciona mal en las colas.

Este capítulo demostrará el paquete `r pkg(yardstick)`, un paquete central de tidymodels cuyo objetivo es medir el rendimiento del modelo. Antes de ilustrar la sintaxis, exploremos si vale la pena la validación empírica mediante métricas de rendimiento cuando un modelo se centra en la inferencia en lugar de la predicción.

## Métricas De Rendimiento E Inferencia

```{r performance ad-model, include = FALSE}
ad_mod <- logistic_reg() %>% set_engine("glm") 
full_model_fit <-
  ad_mod %>% 
  fit(Class ~ (Genotype + male + age)^3, data = ad_data)

full_model_fit %>% extract_fit_engine() 

two_way_fit <-
  ad_mod %>% 
  fit(Class ~ (Genotype + male + age)^2, data = ad_data)

three_factor_test <- 
  anova(
    full_model_fit %>% extract_fit_engine(), 
    two_way_fit %>% extract_fit_engine(),
    test = "LRT"
  )

main_effects_fit <-
  ad_mod %>% 
  fit(Class ~ Genotype + male + age, data = ad_data)

two_factor_test <- 
  anova(
    two_way_fit %>% extract_fit_engine(), 
    main_effects_fit %>% extract_fit_engine(),
    test = "LRT"
  )

two_factor_rs <- 
  ad_mod %>% 
  fit_resamples(Class ~ (Genotype + male + age)^2, ad_folds)

two_factor_res <- 
  collect_metrics(two_factor_rs) %>% 
  filter(.metric == "accuracy") %>% 
  pull(mean)
```

La eficacia de cualquier modelo depende de cómo se utilizará. Un modelo inferencial se utiliza principalmente para comprender las relaciones y normalmente enfatiza la elección (y validez) de distribuciones probabilísticas y otras cualidades generativas que definen el modelo. Por el contrario, para un modelo utilizado principalmente para la predicción, la fuerza predictiva es de primordial importancia y otras preocupaciones sobre las cualidades estadísticas subyacentes pueden ser menos importantes. La fuerza predictiva generalmente está determinada por qué tan cerca están nuestras predicciones de los datos observados, es decir, la fidelidad de las predicciones del modelo a los resultados reales. Este capítulo se centra en funciones que se pueden utilizar para medir la fuerza predictiva. Sin embargo, nuestro consejo para quienes desarrollan modelos inferenciales es utilizar estas técnicas incluso cuando el modelo no se utilizará con el objetivo principal de predicción.

Un problema de larga data con la práctica de la estadística inferencial es que, centrándose exclusivamente en la inferencia, es difícil evaluar la credibilidad de un modelo. Por ejemplo, considere los datos sobre la enfermedad de Alzheimer de @CraigSchapiro cuando se estudiaron pacientes con `r nrow(ad_data)` para determinar los factores que influyen en el deterioro cognitivo. Un análisis podría tomar los factores de riesgo conocidos y construir un modelo de regresión logística donde el resultado sea binario (deteriorado/no deteriorado). Consideremos los predictores de edad, sexo y genotipo de apolipoproteína E. Esta última es una variable categórica con las seis combinaciones posibles de las tres variantes principales de este gen. Se sabe que la apolipoproteína E tiene una asociación con la demencia [@Kim:2009p4370].

Un enfoque superficial, pero no infrecuente, para este análisis sería ajustar un modelo grande con efectos e interacciones principales y luego utilizar pruebas estadísticas para encontrar el conjunto mínimo de términos del modelo que sean estadísticamente significativos en algún nivel predefinido. Si se utilizara un modelo completo con los tres factores y sus interacciones de dos y tres vías, una fase inicial sería probar las interacciones utilizando pruebas de índice de probabilidad secuencial [@HosmerLemeshow]. Analicemos este tipo de enfoque para el ejemplo de datos sobre la enfermedad de Alzheimer:

-   Al comparar el modelo con todas las interacciones de dos vías con uno con la interacción de tres vías adicional, las pruebas de razón de verosimilitud producen un valor p de `r three_factor_test[2, "Pr(>Chi)"]`. Esto implica que no hay evidencia de que los términos del modelo adicionales `r xfun::numbers_to_words(abs(three_factor_test[2, "Df"]))` asociados con la interacción de tres vías expliquen suficiente variación en los datos para mantenerlos en el modelo.

-   A continuación, las interacciones bidireccionales se evalúan de manera similar con respecto al modelo sin interacciones. El valor p aquí es `r two_factor_test[2, "Pr(>Chi)"]`. Esto es algo dudoso, pero, dado el pequeño tamaño de la muestra, sería prudente concluir que hay evidencia de que algunas de las posibles interacciones bidireccionales `r abs(two_factor_test[2, "Df"])` son importantes para la modelo.

-   A partir de aquí, construiríamos alguna explicación de los resultados. Sería particularmente importante discutir las interacciones, ya que pueden generar hipótesis fisiológicas o neurológicas interesantes que se explorarán más a fondo.

Si bien superficial, esta estrategia de análisis es común tanto en la práctica como en la literatura. Esto es especialmente cierto si el profesional tiene una formación formal limitada en análisis de datos.

Un dato que falta en este enfoque es qué tan cerca se ajusta este modelo a los datos reales. Usando métodos de remuestreo, discutidos en el [Capítulo @sec-resampling], podemos estimar que la precisión de este modelo es aproximadamente `r round(two_factor_res * 100, 1)`%. La precisión es a menudo una mala medida del rendimiento del modelo; Lo usamos aquí porque se entiende comúnmente. Si el modelo tiene `r round(two_factor_res * 100, 1)`% de fidelidad a los datos, ¿deberíamos confiar en las conclusiones que produce? Podríamos pensar así hasta que nos demos cuenta de que la tasa inicial de pacientes no deteriorados en los datos es `r round(mean(ad_data$Class == "Control") * 100, 1)`%. Esto significa que, a pesar de nuestro análisis estadístico, el modelo de dos factores parece ser sólo `r round((two_factor_res - mean(ad_data$Class == "Control")) * 100, 1)`% mejor que una simple heurística que siempre predice que los pacientes no sufrirán daños, independientemente de los datos observados.

::: rmdnote
El objetivo de este análisis es demostrar la idea de que la optimización de las características estadísticas del modelo no implica que el modelo se ajuste bien a los datos. Incluso para modelos puramente inferenciales, alguna medida de fidelidad a los datos debería acompañar a los resultados inferenciales. Con esto, los consumidores de los análisis pueden calibrar sus expectativas sobre los resultados.
:::

En el resto de este capítulo, discutiremos enfoques generales para evaluar modelos mediante validación empírica. Estos enfoques se agrupan según la naturaleza de los datos de resultados: puramente numéricos, clases binarias y tres o más niveles de clase.

## Métricas De Regresión

Recuerde de @sec-parsnip-predictions que las funciones de predicción de tidymodels producen tibbles con columnas para los valores predichos. Estas columnas tienen nombres consistentes y las funciones en el paquete `r pkg(yardstick)` que producen métricas de rendimiento tienen interfaces consistentes. Las funciones están basadas en marcos de datos, a diferencia de vectores, con la sintaxis general de:

``` r
function(data, truth, ...)
```

donde `data` es un marco de datos o tibble y `truth` es la columna con los valores de resultados observados. Las elipses u otros argumentos se utilizan para especificar las columnas que contienen las predicciones.

Para ilustrar, tomemos el modelo de @sec-recipes-summary. Este modelo `lm_wflow_fit` combina un modelo de regresión lineal con un conjunto de predictores complementado con una interacción y funciones spline para longitud y latitud. Fue creado a partir de un conjunto de entrenamiento (llamado `ames_train`). Aunque no recomendamos utilizar el conjunto de pruebas en este punto del proceso de modelado, se utilizará aquí para ilustrar la funcionalidad y la sintaxis. El marco de datos `ames_test` consta de las propiedades `r nrow(ames_test)`. Para empezar, hagamos predicciones:

```{r performance-predict-ames}
ames_test_res <- predict(lm_fit, new_data = ames_test %>% select(-Sale_Price))
ames_test_res
```

El resultado numérico previsto por el modelo de regresión se denomina `.pred`. Hagamos coincidir los valores predichos con sus correspondientes valores de resultado observados:

```{r performance-ames-outcome}
ames_test_res <- bind_cols(ames_test_res, ames_test %>% select(Sale_Price))
ames_test_res
```

Vemos que estos valores en su mayoría parecen cercanos, pero aún no tenemos una comprensión cuantitativa de cómo funciona el modelo porque no hemos calculado ninguna métrica de rendimiento. Tenga en cuenta que tanto los resultados previstos como los observados están en unidades log-10. Es una buena práctica analizar las predicciones en la escala transformada (si se usara una), incluso si las predicciones se informan utilizando las unidades originales.

Trazamos los datos en @fig-ames-performance-plot antes de calcular las métricas:

```{r performance-ames-plot, eval=FALSE}
ggplot(ames_test_res, aes(x = Sale_Price, y = .pred)) + 
  # Crear una línea diagonal:
  geom_abline(lty = 2) + 
  geom_point(alpha = 0.5) + 
  labs(y = "Precio de Venta Predecido (log10)", x = "Precio de Venta (log10)") +
  # Escale y dimensione los ejes x e y de manera uniforme:
  coord_obs_pred()
```

```{r}
#| label: fig-ames-performance-plot
#| ref.label: "performance-ames-plot"
#| echo: FALSE
#| fig.cap: "Valores observados versus valores predichos para un modelo de regresión de Ames, con unidades log-10 en ambos ejes"
#| fig.alt: "Gráficos de dispersión de valores numéricos observados versus valores predichos para un modelo de regresión de Ames. Ambos ejes utilizan unidades log-10. El modelo muestra una buena concordancia con algunos puntos que no se ajustan bien a precios altos y bajos."
```

Hay una propiedad de bajo precio que está sustancialmente sobreestimada, es decir, bastante por encima de la línea discontinua.

Calculemos la raíz del error cuadrático medio para este modelo usando la función `rmse()`:

```{r performance-ames-rmse}
rmse(ames_test_res, truth = Sale_Price, estimate = .pred)
```

Esto nos muestra el formato estándar de salida de las funciones `r pkg(yardstick)`. Las métricas para resultados numéricos suelen tener un valor "estándar" para la columna `.estimator`. En las siguientes secciones se muestran ejemplos con diferentes valores para esta columna.

Para calcular varias métricas a la vez, podemos crear un *conjunto de métricas*. Sumemos $R^2$ y el error absoluto medio:

```{r performance-metric-set}
ames_metrics <- metric_set(rmse, rsq, mae)
ames_metrics(ames_test_res, truth = Sale_Price, estimate = .pred)
```

Este formato de datos ordenado apila las métricas verticalmente. Las métricas del error cuadrático medio y del error absoluto medio están en la escala del resultado (por lo tanto, `log10(Sale_Price)` para nuestro ejemplo) y miden la diferencia entre los valores previstos y observados. El valor de $R^2$ mide la correlación al cuadrado entre los valores previstos y observados, por lo que los valores más cercanos a uno son mejores.

::: rmdwarning
El paquete `r pkg(yardstick)` *no* contiene una función para $R^2$ ajustado. Esta modificación del coeficiente de determinación se utiliza comúnmente cuando los mismos datos utilizados para ajustar el modelo se utilizan para evaluar el modelo. Esta métrica no es totalmente compatible con tidymodels porque siempre es un mejor enfoque para calcular el rendimiento en un conjunto de datos separado que el utilizado para ajustar el modelo.
:::

## Métricas De Clasificación Binaria

Para ilustrar otras formas de medir el rendimiento del modelo, cambiaremos a un ejemplo diferente. El paquete `r pkg(modeldata)` (otro de los paquetes tidymodels) contiene predicciones de ejemplo de un conjunto de datos de prueba con dos clases ("Class1" y "Class2"):

```{r performance-two-class-example}
data(two_class_example)
tibble(two_class_example)
```

La segunda y tercera columnas son las probabilidades de clase predichas para el conjunto de prueba, mientras que `predicted` son las predicciones discretas.

Para las predicciones de clases difíciles, una variedad de funciones `r pkg(yardstick)` son útiles:

```{r performance-class-metrics}
# Una matriz de confusión:
conf_mat(two_class_example, truth = truth, estimate = predicted)

# Exactitud:
accuracy(two_class_example, truth, predicted)

# Coeficiente de correlación de Matthews:
mcc(two_class_example, truth, predicted)

# Métrica F1:
f_meas(two_class_example, truth, predicted)

# Combinando estas tres métricas de clasificación juntas
classification_metrics <- metric_set(accuracy, mcc, f_meas)
classification_metrics(two_class_example, truth = truth, estimate = predicted)
```

El coeficiente de correlación de Matthews y la puntuación F1 resumen la matriz de confusión, pero en comparación con `mcc()`, que mide la calidad de ejemplos tanto positivos como negativos, la métrica `f_meas()` enfatiza la clase positiva, es decir, el evento de interés. Para conjuntos de datos de clasificación binaria como este ejemplo, las funciones `r pkg(yardstick)` tienen un argumento estándar llamado `event_level` para distinguir los niveles positivos y negativos. El valor predeterminado (que utilizamos en este código) es que el *primer* nivel del factor de resultado es el evento de interés.

::: rmdnote
Existe cierta heterogeneidad en las funciones R a este respecto; algunos utilizan el primer nivel y otros el segundo para denotar el evento de interés. Consideramos más intuitivo que el primer nivel es el más importante. La lógica de segundo nivel surge de codificar el resultado como 0/1 (en cuyo caso el segundo valor es el evento) y desafortunadamente permanece en algunos paquetes. Sin embargo, tidymodels (junto con muchos otros paquetes de R) requieren que se codifique un resultado categórico como factor y, por esta razón, la justificación heredada para el segundo nivel como evento se vuelve irrelevante.
:::

Como ejemplo donde el segundo nivel es el evento:

```{r performance-2nd-level}
f_meas(two_class_example, truth, predicted, event_level = "second")
```

En este resultado, el valor `.estimator` de "binario" indica que se utilizará la fórmula estándar para clases binarias.

Existen numerosas métricas de clasificación que utilizan las probabilidades predichas como entradas en lugar de las predicciones de clase estrictas. Por ejemplo, la curva de características operativas del receptor (ROC) calcula la sensibilidad y la especificidad sobre un continuo de diferentes umbrales de eventos. La columna de clase prevista no se utiliza. Hay dos funciones `r pkg(yardstick)` para este método: `roc_curve()` calcula los puntos de datos que forman la curva ROC y `roc_auc()` calcula el área bajo la curva.

Las interfaces para estos tipos de funciones métricas utilizan el marcador de posición del argumento `...` para pasar la columna de probabilidad de clase apropiada. Para problemas de dos clases, la columna de probabilidad del evento de interés se pasa a la función:

```{r performance-2class-roc}
two_class_curve <- roc_curve(two_class_example, truth, Class1)
two_class_curve

roc_auc(two_class_example, truth, Class1)
```

El objeto `two_class_curve` se puede usar en una llamada `ggplot` para visualizar la curva, como se muestra en @fig-example-roc-curve. Existe un método `autoplot()` que se encargará de los detalles:

```{r performance-2class-roc-curve, eval=FALSE}
autoplot(two_class_curve)
```

```{r}
#| label: fig-example-roc-curve
#| ref.label: "performance-2class-roc-curve"
#| echo: FALSE
#| fig.cap: "Ejemplo de curva ROC"
#| fig.alt: "Un ejemplo de curva ROC. El eje x es uno menos la especificidad y el eje y es la sensibilidad. La curva se inclina hacia el lado superior izquierdo del área de la gráfica."
```

Si la curva estuviera cerca de la línea diagonal, entonces las predicciones del modelo no serían mejores que las conjeturas aleatorias. Dado que la curva está arriba en la esquina superior izquierda, vemos que nuestro modelo funciona bien en diferentes umbrales.

Hay otras funciones que utilizan estimaciones de probabilidad, incluidas `gain_curve()`, `lift_curve()` y `pr_curve()`.

## Métricas De Clasificación Multiclase

¿Qué pasa con los datos con tres o más clases? Para demostrarlo, exploremos un conjunto de datos de ejemplo diferente que tiene cuatro clases:

```{r performance-hpc-example}
data(hpc_cv)
tibble(hpc_cv)
```

Como antes, hay factores para los resultados observados y previstos junto con otras cuatro columnas de probabilidades previstas para cada clase. (Estos datos también incluyen una columna `Resample`. Estos resultados `hpc_cv` son para predicciones fuera de muestra asociadas con una validación cruzada de 10 veces. Por el momento, esta columna se ignorará y discutiremos el remuestreo en profundidad en [Capítulo @sec-resampling].)

Las funciones para las métricas que utilizan predicciones de clases discretas son idénticas a sus contrapartes binarias:

```{r performance-mutliclass-pred}
accuracy(hpc_cv, obs, pred)

mcc(hpc_cv, obs, pred)
```

Tenga en cuenta que, en estos resultados, aparece un `.estimator` "multiclase". Al igual que "binario", esto indica que se utilizó la fórmula para resultados con tres o más niveles de clase. El coeficiente de correlación de Matthews se diseñó originalmente para dos clases, pero se ha extendido a casos con más niveles de clase.

Existen métodos para tomar métricas diseñadas para manejar resultados con solo dos clases y extenderlas para resultados con más de dos clases. Por ejemplo, una métrica como la sensibilidad mide la tasa de verdaderos positivos que, por definición, es específica de dos clases (es decir, "evento" y "no evento"). ¿Cómo se puede utilizar esta métrica en nuestros datos de ejemplo?

Existen métodos contenedores que se pueden utilizar para aplicar sensibilidad a nuestro resultado de cuatro clases. Estas opciones son promedio macro, promedio macroponderado y micropromedio:

-   El promedio macro calcula un conjunto de métricas de uno contra todos utilizando las estadísticas estándar de dos clases. Estos están promediados.

-   El promedio macroponderado hace lo mismo, pero el promedio se pondera según el número de muestras de cada clase.

-   El micropromedio calcula la contribución de cada clase, las agrega y luego calcula una única métrica a partir de los agregados.

Consulte @wu2017unified y @OpitzBurst para obtener más información sobre cómo ampliar las métricas de clasificación a resultados con más de dos clases.

Usando la sensibilidad como ejemplo, el cálculo habitual de dos clases es la relación entre el número de eventos predichos correctamente dividido por el número de eventos verdaderos. Los cálculos manuales para estos métodos de promediación son:

```{r performance-sens-manual}
class_totals <- 
  count(hpc_cv, obs, name = "totals") %>% 
  mutate(class_wts = totals / sum(totals))
class_totals

cell_counts <- 
  hpc_cv %>% 
  group_by(obs, pred) %>% 
  count() %>% 
  ungroup()

# Calcule las cuatro sensibilidades usando 1 contra todos
one_versus_all <- 
  cell_counts %>% 
  filter(obs == pred) %>% 
  full_join(class_totals, by = "obs") %>% 
  mutate(sens = n / totals)
one_versus_all

# Tres estimaciones diferentes:
one_versus_all %>% 
  summarize(
    macro = mean(sens), 
    macro_wts = weighted.mean(sens, class_wts),
    micro = sum(n) / sum(totals)
  )
```

Afortunadamente, no es necesario implementar manualmente estos métodos de promedio. En cambio, las funciones `r pkg(yardstick)` pueden aplicar automáticamente estos métodos a través del argumento `estimator`:

```{r performance-sens}
sensitivity(hpc_cv, obs, pred, estimator = "macro")
sensitivity(hpc_cv, obs, pred, estimator = "macro_weighted")
sensitivity(hpc_cv, obs, pred, estimator = "micro")
```

Cuando se trata de estimaciones de probabilidad, existen algunas métricas con análogos multiclase. Por ejemplo, @HandTill determinó una técnica multiclase para curvas ROC. En este caso, *todas* las columnas de probabilidad de clase deben asignarse a la función:

```{r performance-multi-class-roc}
roc_auc(hpc_cv, obs, VF, F, M, L)
```

El promedio macroponderado también está disponible como opción para aplicar esta métrica a un resultado multiclase:

```{r performance-multi-class-roc-macro}
roc_auc(hpc_cv, obs, VF, F, M, L, estimator = "macro_weighted")
```

Finalmente, todas estas métricas de rendimiento se pueden calcular utilizando agrupaciones `r pkg(dplyr)`. Recuerde que estos datos tienen una columna para los grupos de remuestreo. Aún no hemos analizado el remuestreo en detalle, pero observe cómo podemos pasar un marco de datos agrupados a la función métrica para calcular las métricas para cada grupo:

```{r performance-multi-class-acc-grouped}
hpc_cv %>% 
  group_by(Resample) %>% 
  accuracy(obs, pred)
```

Las agrupaciones también se traducen a los métodos `autoplot()`, y los resultados se muestran en @fig-grouped-roc-curves.

```{r performance-multi-class-roc-grouped, eval=FALSE}
# Cuatro curvas ROC 1 contra todos para cada pliegue
hpc_cv %>% 
  group_by(Resample) %>% 
  roc_curve(obs, VF, F, M, L) %>% 
  autoplot()
```

```{r}
#| label: fig-grouped-roc-curves
#| ref.label: "performance-multi-class-roc-grouped"
#| echo: FALSE
#| fig.cap: "Curvas ROC remuestreadas para cada una de las cuatro clases de resultados."
#| fig.alt: "Curvas ROC remuestreadas para cada una de las cuatro clases de resultados. Hay cuatro paneles para las clases VF, F, M y L. Cada panel contiene diez curvas ROC para cada uno de los conjuntos de datos remuestreados."
```

Esta visualización nos muestra que todos los diferentes grupos se desempeñan más o menos igual, pero que la clase `VF` se predice mejor que las clases `F` o `M`, ya que las curvas ROC `VF` están más en la esquina superior izquierda. . Este ejemplo utiliza remuestreos como grupos, pero se puede utilizar cualquier agrupación de sus datos. Este método `autoplot()` puede ser un método de visualización rápida para la efectividad del modelo en todas las clases y/o grupos de resultados.

## Resumen Del Capítulo {#sec-performance-summary}

Diferentes métricas miden diferentes aspectos del ajuste de un modelo, por ejemplo, RMSE mide la precisión mientras que $R^2$ mide la correlación. Medir el rendimiento del modelo es importante incluso cuando un modelo determinado no se utilizará principalmente para predicción; el poder predictivo también es importante para los modelos inferenciales o descriptivos. Las funciones del paquete `r pkg(yardstick)` miden la efectividad de un modelo utilizando datos. La interfaz principal de tidymodels utiliza principios de tidyverse y marcos de datos (en lugar de tener argumentos vectoriales). Diferentes métricas son apropiadas para las métricas de regresión y clasificación y, dentro de ellas, a veces hay diferentes formas de estimar las estadísticas, como para resultados multiclase.
